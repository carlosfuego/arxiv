[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v1",
                "updated": "2025-06-24T14:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From memories to maps: Mechanisms of in context reinforcement learning\n  in transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From memories to maps: Mechanisms of in context reinforcement learning\n  in transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rúben Adão"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "João Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.19852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19852v1",
                "updated": "2025-06-24T17:59:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:59:59Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    59,
                    1,
                    175,
                    0
                ],
                "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for\n  Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for\n  Long Video Generation"
                },
                "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with $O(n \\log\nn)$ complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard $O(n^2)$ dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4$\\times$ longer while reducing training costs by up to\n4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to\n3.7$\\times$ compared to dense attention inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with $O(n \\log\nn)$ complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard $O(n^2)$ dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4$\\times$ longer while reducing training costs by up to\n4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to\n3.7$\\times$ compared to dense attention inference."
                },
                "authors": [
                    {
                        "name": "Xingyang Li"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Jinbo Hu"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code: https://github.com/mit-han-lab/radial-attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19848v1",
                "updated": "2025-06-24T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    55,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    55,
                    1,
                    175,
                    0
                ],
                "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing"
                },
                "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap."
                },
                "authors": [
                    {
                        "name": "Long Xing"
                    },
                    {
                        "name": "Qidong Huang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Pan Zhang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jinsong Li"
                    },
                    {
                        "name": "Shuangrui Ding"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "arxiv_comment": "Code is available at https://github.com/Cooperx521/ScaleCap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19846v1",
                "updated": "2025-06-24T17:59:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    31,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:59:31Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    31,
                    1,
                    175,
                    0
                ],
                "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents\n  with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents\n  with Reinforcement Learning"
                },
                "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models."
                },
                "authors": [
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "Junxing Hu"
                    },
                    {
                        "name": "Pu Wei"
                    },
                    {
                        "name": "Zhiqian Zhang"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Jiawei Lu"
                    },
                    {
                        "name": "Zicheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zicheng Zhang"
                },
                "author": "Zicheng Zhang",
                "arxiv_comment": "33 pages, 7 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19838v1",
                "updated": "2025-06-24T17:57:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    57,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:57:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    57,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution"
                },
                "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems."
                },
                "authors": [
                    {
                        "name": "Liangbin Xie"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Shian Du"
                    },
                    {
                        "name": "Menghan Xia"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Fanghua Yu"
                    },
                    {
                        "name": "Ziyan Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Jiantao Zhou"
                    },
                    {
                        "name": "Chao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Chao Dong"
                },
                "author": "Chao Dong",
                "arxiv_comment": "Project webpage available at https://simplegvr.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19836v1",
                "updated": "2025-06-24T17:53:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    53,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:53:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    53,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "Machine Learning with Privacy for Protected Attributes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning with Privacy for Protected Attributes"
                },
                "summary": "Differential privacy (DP) has become the standard for private data analysis.\nCertain machine learning applications only require privacy protection for\nspecific protected attributes. Using naive variants of differential privacy in\nsuch use cases can result in unnecessary degradation of utility. In this work,\nwe refine the definition of DP to create a more general and flexible framework\nthat we call feature differential privacy (FDP). Our definition is\nsimulation-based and allows for both addition/removal and replacement variants\nof privacy, and can handle arbitrary and adaptive separation of protected and\nnon-protected features. We prove the properties of FDP, such as adaptive\ncomposition, and demonstrate its implications for limiting attribute inference\nattacks. We also propose a modification of the standard DP-SGD algorithm that\nsatisfies FDP while leveraging desirable properties such as amplification via\nsub-sampling. We apply our framework to various machine learning tasks and show\nthat it can significantly improve the utility of DP-trained models when public\nfeatures are available. For example, we train diffusion models on the AFHQ\ndataset of animal faces and observe a drastic improvement in FID compared to\nDP, from 286.7 to 101.9 at $\\epsilon=8$, assuming that the blurred version of a\ntraining image is available as a public feature. Overall, our work provides a\nnew approach to private data analysis that can help reduce the utility cost of\nDP while still providing strong privacy guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) has become the standard for private data analysis.\nCertain machine learning applications only require privacy protection for\nspecific protected attributes. Using naive variants of differential privacy in\nsuch use cases can result in unnecessary degradation of utility. In this work,\nwe refine the definition of DP to create a more general and flexible framework\nthat we call feature differential privacy (FDP). Our definition is\nsimulation-based and allows for both addition/removal and replacement variants\nof privacy, and can handle arbitrary and adaptive separation of protected and\nnon-protected features. We prove the properties of FDP, such as adaptive\ncomposition, and demonstrate its implications for limiting attribute inference\nattacks. We also propose a modification of the standard DP-SGD algorithm that\nsatisfies FDP while leveraging desirable properties such as amplification via\nsub-sampling. We apply our framework to various machine learning tasks and show\nthat it can significantly improve the utility of DP-trained models when public\nfeatures are available. For example, we train diffusion models on the AFHQ\ndataset of animal faces and observe a drastic improvement in FID compared to\nDP, from 286.7 to 101.9 at $\\epsilon=8$, assuming that the blurred version of a\ntraining image is available as a public feature. Overall, our work provides a\nnew approach to private data analysis that can help reduce the utility cost of\nDP while still providing strong privacy guarantees."
                },
                "authors": [
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "G. Edward Suh"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Kamalika Chaudhuri"
                },
                "author": "Kamalika Chaudhuri",
                "arxiv_journal_ref": "2025 IEEE Symposium on Security and Privacy (SP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19835v1",
                "updated": "2025-06-24T17:52:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    52,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:52:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    52,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via\n  Role-Specialized Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via\n  Role-Specialized Collaboration"
                },
                "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM."
                },
                "authors": [
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Lingran Song"
                    },
                    {
                        "name": "Jianbing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Shen"
                },
                "author": "Jianbing Shen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06108v3",
                "updated": "2025-06-24T17:51:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    51,
                    24,
                    1,
                    175,
                    0
                ],
                "published": "2025-01-10T17:01:09Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    17,
                    1,
                    9,
                    4,
                    10,
                    0
                ],
                "title": "Inferring Higher-Order Couplings with Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Higher-Order Couplings with Neural Networks"
                },
                "summary": "Maximum entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical physics, are widely used to model pairwise interactions in complex\nsystems across disciplines such as bioinformatics and neuroscience. While\nsuccessful, these approaches often fail to capture higher-order interactions\nthat are critical for understanding collective behavior. In contrast, modern\nmachine learning methods can model such interactions, but their\ninterpretability often comes at a prohibitive computational cost. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient alternative by\nencoding statistical correlations through hidden units in a bipartite\narchitecture. In this work, we introduce a method that maps RBMs onto\ngeneralized Potts models, enabling the systematic extraction of interactions up\nto arbitrary order. Leveraging large-$N$ approximations -- made tractable by\nthe RBM's structure -- we extract effective many-body couplings with minimal\ncomputational effort. We further propose a robust framework for recovering\nhigher-order interactions in more complex generative models, and introduce a\nsimple gauge-fixing scheme for the effective Potts representation. Validation\non synthetic data demonstrates accurate recovery of two- and three-body\ninteractions. Applied to protein sequence data, our method reconstructs contact\nmaps with high fidelity and outperforms state-of-the-art inverse Potts models.\nThese results establish RBMs as a powerful and efficient tool for modeling\nhigher-order structure in high-dimensional categorical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum entropy methods, rooted in the inverse Ising/Potts problem from\nstatistical physics, are widely used to model pairwise interactions in complex\nsystems across disciplines such as bioinformatics and neuroscience. While\nsuccessful, these approaches often fail to capture higher-order interactions\nthat are critical for understanding collective behavior. In contrast, modern\nmachine learning methods can model such interactions, but their\ninterpretability often comes at a prohibitive computational cost. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient alternative by\nencoding statistical correlations through hidden units in a bipartite\narchitecture. In this work, we introduce a method that maps RBMs onto\ngeneralized Potts models, enabling the systematic extraction of interactions up\nto arbitrary order. Leveraging large-$N$ approximations -- made tractable by\nthe RBM's structure -- we extract effective many-body couplings with minimal\ncomputational effort. We further propose a robust framework for recovering\nhigher-order interactions in more complex generative models, and introduce a\nsimple gauge-fixing scheme for the effective Potts representation. Validation\non synthetic data demonstrates accurate recovery of two- and three-body\ninteractions. Applied to protein sequence data, our method reconstructs contact\nmaps with high fidelity and outperforms state-of-the-art inverse Potts models.\nThese results establish RBMs as a powerful and efficient tool for modeling\nhigher-order structure in high-dimensional categorical data."
                },
                "authors": [
                    {
                        "name": "Aurélien Decelle"
                    },
                    {
                        "name": "Alfonso de Jesús Navas Gómez"
                    },
                    {
                        "name": "Beatriz Seoane"
                    }
                ],
                "author_detail": {
                    "name": "Beatriz Seoane"
                },
                "author": "Beatriz Seoane",
                "arxiv_comment": "24 Pages and 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08911v3",
                "updated": "2025-06-24T17:42:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    42,
                    37,
                    1,
                    175,
                    0
                ],
                "published": "2024-07-12T01:06:34Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    1,
                    6,
                    34,
                    4,
                    194,
                    0
                ],
                "title": "The conditional saddlepoint approximation for fast and accurate\n  large-scale hypothesis testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The conditional saddlepoint approximation for fast and accurate\n  large-scale hypothesis testing"
                },
                "summary": "Saddlepoint approximations (SPAs) for resampling-based procedures offer\nstatistically accurate and computationally efficient inference, which is\nparticularly critical in the analysis of large-scale, high-multiplicity data.\nDespite being introduced 70 years ago, SPAs for resampling-based procedures\nlack rigorous justification and have been underutilized in modern applications.\nWe establish a theoretical foundation for the SPA in this context by developing\na general result on its approximation accuracy for conditional tail\nprobabilities of averages of conditionally independent summands. This result\nboth justifies existing SPAs for classical procedures like the sign-flipping\ntest and enables new SPAs for modern resampling methods, including those using\nblack-box machine learning. Capitalizing on this result, we introduce the\nsaddlepoint approximation-based conditional randomization test (spaCRT), a\nresampling-free conditional independence test that is both statistically\naccurate and computationally efficient. The method is especially well-suited\nfor sparse, large-scale datasets such as single-cell CRISPR screens and\ngenome-wide association studies involving rare diseases. We prove the validity\nof the spaCRT when paired with modern regression tools such as lasso and kernel\nridge regression. Extensive analyses of simulated and real data show that the\nspaCRT controls Type-I error, achieves high power, and outperforms existing\nasymptotic and resampling-based alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saddlepoint approximations (SPAs) for resampling-based procedures offer\nstatistically accurate and computationally efficient inference, which is\nparticularly critical in the analysis of large-scale, high-multiplicity data.\nDespite being introduced 70 years ago, SPAs for resampling-based procedures\nlack rigorous justification and have been underutilized in modern applications.\nWe establish a theoretical foundation for the SPA in this context by developing\na general result on its approximation accuracy for conditional tail\nprobabilities of averages of conditionally independent summands. This result\nboth justifies existing SPAs for classical procedures like the sign-flipping\ntest and enables new SPAs for modern resampling methods, including those using\nblack-box machine learning. Capitalizing on this result, we introduce the\nsaddlepoint approximation-based conditional randomization test (spaCRT), a\nresampling-free conditional independence test that is both statistically\naccurate and computationally efficient. The method is especially well-suited\nfor sparse, large-scale datasets such as single-cell CRISPR screens and\ngenome-wide association studies involving rare diseases. We prove the validity\nof the spaCRT when paired with modern regression tools such as lasso and kernel\nridge regression. Extensive analyses of simulated and real data show that the\nspaCRT controls Type-I error, achieves high power, and outperforms existing\nasymptotic and resampling-based alternatives."
                },
                "authors": [
                    {
                        "name": "Ziang Niu"
                    },
                    {
                        "name": "Jyotishka Ray Choudhury"
                    },
                    {
                        "name": "Eugene Katsevich"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Katsevich"
                },
                "author": "Eugene Katsevich",
                "arxiv_comment": "Starting with v3, this manuscript represents a merger of\n  arXiv:2407.08911v2 (a previous version of this manuscript, mostly\n  methodologically focused) and arXiv:2407.08915 (a related manuscript forming\n  the theoretical basis of arXiv:2407.08911)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08780v2",
                "updated": "2025-06-24T17:30:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-05-13T17:56:02Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    56,
                    2,
                    1,
                    133,
                    0
                ],
                "title": "Polar motion of Venus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polar motion of Venus"
                },
                "summary": "Five Venus missions are under development to study the planet in the next\ndecade, with both NASA's VERITAS and ESA's EnVision featuring a geophysical\ninvestigation among their objectives. Their radar and gravity experiments will\ndetermine Venus's orientation, enabling analyses of its spin dynamics to infer\nrelevant geophysical and atmospheric properties. This work aims to characterize\nVenus's polar motion, defined as the motion of its spin axis in a body-fixed\nframe. We focus on signatures from its interior and atmosphere to support\npotential detections of polar motion by future orbiters. We developed a polar\nmotion model for a triaxial planet accounting for solar torque, centrifugal and\ntidal deformations of a viscoelastic mantle, and atmospheric dynamics.\nCore-mantle coupling effects were analyzed separately, considering a simplified\nspherical core. We computed the period and damping time of the free motion\n(i.e., the Chandler wobble) and determined the frequencies and amplitudes of\nthe forced motion. We revisited the Chandler frequency expression. Solar torque\nis the dominant phenomenon affecting Venus's Chandler frequency, increasing it\nby a factor of 2.75. Our model predicts a Chandler period in the range [12900 ;\n18900] years. The Chandler wobble appears as a linear polar drift of about 90\nmeters on Venus's surface during EnVision's 4-year primary mission, at the\nlimit of its resolution. We also predict forced polar motion oscillations with\nan amplitude of about 20 meters, driven by the atmosphere and the solar torque.\nCompared to the 240 meter spin axis precession occurring in inertial space over\nthis duration, these results suggest that Venus's polar motion could also be\ndetectable by future orbiters. Polar motion should be incorporated into\nrotation models when anticipating these missions, providing additional\nconstraints on the interior structure of Venus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Five Venus missions are under development to study the planet in the next\ndecade, with both NASA's VERITAS and ESA's EnVision featuring a geophysical\ninvestigation among their objectives. Their radar and gravity experiments will\ndetermine Venus's orientation, enabling analyses of its spin dynamics to infer\nrelevant geophysical and atmospheric properties. This work aims to characterize\nVenus's polar motion, defined as the motion of its spin axis in a body-fixed\nframe. We focus on signatures from its interior and atmosphere to support\npotential detections of polar motion by future orbiters. We developed a polar\nmotion model for a triaxial planet accounting for solar torque, centrifugal and\ntidal deformations of a viscoelastic mantle, and atmospheric dynamics.\nCore-mantle coupling effects were analyzed separately, considering a simplified\nspherical core. We computed the period and damping time of the free motion\n(i.e., the Chandler wobble) and determined the frequencies and amplitudes of\nthe forced motion. We revisited the Chandler frequency expression. Solar torque\nis the dominant phenomenon affecting Venus's Chandler frequency, increasing it\nby a factor of 2.75. Our model predicts a Chandler period in the range [12900 ;\n18900] years. The Chandler wobble appears as a linear polar drift of about 90\nmeters on Venus's surface during EnVision's 4-year primary mission, at the\nlimit of its resolution. We also predict forced polar motion oscillations with\nan amplitude of about 20 meters, driven by the atmosphere and the solar torque.\nCompared to the 240 meter spin axis precession occurring in inertial space over\nthis duration, these results suggest that Venus's polar motion could also be\ndetectable by future orbiters. Polar motion should be incorporated into\nrotation models when anticipating these missions, providing additional\nconstraints on the interior structure of Venus."
                },
                "authors": [
                    {
                        "name": "Pierre-Louis Phan"
                    },
                    {
                        "name": "Nicolas Rambaux"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Rambaux"
                },
                "author": "Nicolas Rambaux",
                "arxiv_doi": "10.1051/0004-6361/202553658",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202553658",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.08780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures, 6 tables",
                "arxiv_journal_ref": "Astronomy & Astrophysics 699, A65 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19807v1",
                "updated": "2025-06-24T17:17:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:17:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"
                },
                "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."
                },
                "authors": [
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04121v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04121v3",
                "updated": "2025-06-24T17:16:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    16,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-06T14:53:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    14,
                    53,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "First-Passage Approach to Optimizing Perturbations for Improved Training\n  of Machine Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-Passage Approach to Optimizing Perturbations for Improved Training\n  of Machine Learning Models"
                },
                "summary": "Machine learning models have become indispensable tools in applications\nacross the physical sciences. Their training is often time-consuming, vastly\nexceeding the inference timescales. Several protocols have been developed to\nperturb the learning process and improve the training, such as shrink and\nperturb, warm restarts, and stochastic resetting. For classifiers, these\nperturbations have been shown to result in enhanced speedups or improved\ngeneralization. However, the design of such perturbations is usually done ad\nhoc by intuition and trial and error. To rationally optimize training\nprotocols, we frame them as first-passage processes and consider their response\nto perturbations. We show that if the unperturbed learning process reaches a\nquasi-steady state, the response at a single perturbation frequency can predict\nthe behavior at a wide range of frequencies. We employ this approach to a\nCIFAR-10 classifier using the ResNet-18 model and identify a useful\nperturbation and frequency among several possibilities. We demonstrate the\ntransferability of the approach to other datasets, architectures, optimizers\nand even tasks (regression instead of classification). Our work allows\noptimization of perturbations for improving the training of machine learning\nmodels using a first-passage approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models have become indispensable tools in applications\nacross the physical sciences. Their training is often time-consuming, vastly\nexceeding the inference timescales. Several protocols have been developed to\nperturb the learning process and improve the training, such as shrink and\nperturb, warm restarts, and stochastic resetting. For classifiers, these\nperturbations have been shown to result in enhanced speedups or improved\ngeneralization. However, the design of such perturbations is usually done ad\nhoc by intuition and trial and error. To rationally optimize training\nprotocols, we frame them as first-passage processes and consider their response\nto perturbations. We show that if the unperturbed learning process reaches a\nquasi-steady state, the response at a single perturbation frequency can predict\nthe behavior at a wide range of frequencies. We employ this approach to a\nCIFAR-10 classifier using the ResNet-18 model and identify a useful\nperturbation and frequency among several possibilities. We demonstrate the\ntransferability of the approach to other datasets, architectures, optimizers\nand even tasks (regression instead of classification). Our work allows\noptimization of perturbations for improving the training of machine learning\nmodels using a first-passage approach."
                },
                "authors": [
                    {
                        "name": "Sagi Meir"
                    },
                    {
                        "name": "Tommer D. Keidar"
                    },
                    {
                        "name": "Shlomi Reuveni"
                    },
                    {
                        "name": "Barak Hirshberg"
                    }
                ],
                "author_detail": {
                    "name": "Barak Hirshberg"
                },
                "author": "Barak Hirshberg",
                "arxiv_doi": "10.1088/2632-2153/add8df",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/2632-2153/add8df",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04121v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04121v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Machine Learning Science and Technology 6 (2025) 2",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19806v1",
                "updated": "2025-06-24T17:14:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    14,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:14:47Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    14,
                    47,
                    1,
                    175,
                    0
                ],
                "title": "LLM-Based Social Simulations Require a Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Social Simulations Require a Boundary"
                },
                "summary": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations."
                },
                "authors": [
                    {
                        "name": "Zengqing Wu"
                    },
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Takayuki Ito"
                    },
                    {
                        "name": "Chuan Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Xiao"
                },
                "author": "Chuan Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19802v1",
                "updated": "2025-06-24T17:08:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    8,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:08:58Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    8,
                    58,
                    1,
                    175,
                    0
                ],
                "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs"
                },
                "summary": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %."
                },
                "authors": [
                    {
                        "name": "Xin Fan Guo"
                    },
                    {
                        "name": "Albert Merono Penuela"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Fabio Pierazzi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Pierazzi"
                },
                "author": "Fabio Pierazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19794v1",
                "updated": "2025-06-24T17:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study"
                },
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14454v2",
                "updated": "2025-06-24T17:02:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    2,
                    1,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-18T17:30:26Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    30,
                    26,
                    1,
                    77,
                    0
                ],
                "title": "The Atacama Cosmology Telescope: DR6 Constraints on Extended\n  Cosmological Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Atacama Cosmology Telescope: DR6 Constraints on Extended\n  Cosmological Models"
                },
                "summary": "We use new cosmic microwave background (CMB) primary temperature and\npolarization anisotropy measurements from the Atacama Cosmology Telescope (ACT)\nData Release 6 (DR6) to test foundational assumptions of the standard\ncosmological model and set constraints on extensions to it. We derive\nconstraints from the ACT DR6 power spectra alone, as well as in combination\nwith legacy data from Planck. To break geometric degeneracies, we include ACT\nand Planck CMB lensing data and baryon acoustic oscillation data from DESI\nYear-1, and further add supernovae measurements from Pantheon+ for models that\naffect the late-time expansion history. We verify the near-scale-invariance\n(running of the spectral index $d n_s/d\\ln k = 0.0062 \\pm 0.0052$) and\nadiabaticity of the primordial perturbations. Neutrino properties are\nconsistent with Standard Model predictions: we find no evidence for new light,\nrelativistic species that are free-streaming ($N_{\\rm eff} = 2.86 \\pm 0.13$,\nwhich combined with external BBN data becomes $N_{\\rm eff} = 2.89 \\pm 0.11$),\nfor non-zero neutrino masses ($\\sum m_\\nu < 0.082$ eV at 95% CL), or for\nneutrino self-interactions. We also find no evidence for self-interacting dark\nradiation ($N_{\\rm idr} < 0.134$), early-universe variation of fundamental\nconstants, early dark energy, primordial magnetic fields, or modified\nrecombination. Our data are consistent with standard BBN, the FIRAS-inferred\nCMB temperature, a dark matter component that is collisionless and with only a\nsmall fraction allowed as axion-like particles, a cosmological constant, and\nthe late-time growth rate predicted by general relativity. We find no\nstatistically significant preference for a departure from the baseline\n$\\Lambda$CDM model. In general, models introduced to increase the Hubble\nconstant or to decrease the amplitude of density fluctuations inferred from the\nprimary CMB are not favored by our data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use new cosmic microwave background (CMB) primary temperature and\npolarization anisotropy measurements from the Atacama Cosmology Telescope (ACT)\nData Release 6 (DR6) to test foundational assumptions of the standard\ncosmological model and set constraints on extensions to it. We derive\nconstraints from the ACT DR6 power spectra alone, as well as in combination\nwith legacy data from Planck. To break geometric degeneracies, we include ACT\nand Planck CMB lensing data and baryon acoustic oscillation data from DESI\nYear-1, and further add supernovae measurements from Pantheon+ for models that\naffect the late-time expansion history. We verify the near-scale-invariance\n(running of the spectral index $d n_s/d\\ln k = 0.0062 \\pm 0.0052$) and\nadiabaticity of the primordial perturbations. Neutrino properties are\nconsistent with Standard Model predictions: we find no evidence for new light,\nrelativistic species that are free-streaming ($N_{\\rm eff} = 2.86 \\pm 0.13$,\nwhich combined with external BBN data becomes $N_{\\rm eff} = 2.89 \\pm 0.11$),\nfor non-zero neutrino masses ($\\sum m_\\nu < 0.082$ eV at 95% CL), or for\nneutrino self-interactions. We also find no evidence for self-interacting dark\nradiation ($N_{\\rm idr} < 0.134$), early-universe variation of fundamental\nconstants, early dark energy, primordial magnetic fields, or modified\nrecombination. Our data are consistent with standard BBN, the FIRAS-inferred\nCMB temperature, a dark matter component that is collisionless and with only a\nsmall fraction allowed as axion-like particles, a cosmological constant, and\nthe late-time growth rate predicted by general relativity. We find no\nstatistically significant preference for a departure from the baseline\n$\\Lambda$CDM model. In general, models introduced to increase the Hubble\nconstant or to decrease the amplitude of density fluctuations inferred from the\nprimary CMB are not favored by our data."
                },
                "authors": [
                    {
                        "name": "Erminia Calabrese"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Hidde T. Jense"
                    },
                    {
                        "name": "Adrien La Posta"
                    },
                    {
                        "name": "Irene Abril-Cabezas"
                    },
                    {
                        "name": "Graeme E. Addison"
                    },
                    {
                        "name": "Peter A. R. Ade"
                    },
                    {
                        "name": "Simone Aiola"
                    },
                    {
                        "name": "Tommy Alford"
                    },
                    {
                        "name": "David Alonso"
                    },
                    {
                        "name": "Mandana Amiri"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Zachary Atkins"
                    },
                    {
                        "name": "Jason E. Austermann"
                    },
                    {
                        "name": "Eleonora Barbavara"
                    },
                    {
                        "name": "Nicola Barbieri"
                    },
                    {
                        "name": "Nicholas Battaglia"
                    },
                    {
                        "name": "Elia Stefano Battistelli"
                    },
                    {
                        "name": "James A. Beall"
                    },
                    {
                        "name": "Rachel Bean"
                    },
                    {
                        "name": "Ali Beheshti"
                    },
                    {
                        "name": "Benjamin Beringue"
                    },
                    {
                        "name": "Tanay Bhandarkar"
                    },
                    {
                        "name": "Emily Biermann"
                    },
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "J Richard Bond"
                    },
                    {
                        "name": "Valentina Capalbo"
                    },
                    {
                        "name": "Felipe Carrero"
                    },
                    {
                        "name": "Shi-Fan Chen"
                    },
                    {
                        "name": "Grace Chesmore"
                    },
                    {
                        "name": "Hsiao-mei Cho"
                    },
                    {
                        "name": "Steve K. Choi"
                    },
                    {
                        "name": "Susan E. Clark"
                    },
                    {
                        "name": "Nicholas F. Cothard"
                    },
                    {
                        "name": "Kevin Coughlin"
                    },
                    {
                        "name": "William Coulton"
                    },
                    {
                        "name": "Devin Crichton"
                    },
                    {
                        "name": "Kevin T. Crowley"
                    },
                    {
                        "name": "Omar Darwish"
                    },
                    {
                        "name": "Mark J. Devlin"
                    },
                    {
                        "name": "Simon Dicker"
                    },
                    {
                        "name": "Cody J. Duell"
                    },
                    {
                        "name": "Shannon M. Duff"
                    },
                    {
                        "name": "Adriaan J. Duivenvoorden"
                    },
                    {
                        "name": "Jo Dunkley"
                    },
                    {
                        "name": "Rolando Dunner"
                    },
                    {
                        "name": "Carmen Embil Villagra"
                    },
                    {
                        "name": "Max Fankhanel"
                    },
                    {
                        "name": "Gerrit S. Farren"
                    },
                    {
                        "name": "Simone Ferraro"
                    },
                    {
                        "name": "Allen Foster"
                    },
                    {
                        "name": "Rodrigo Freundt"
                    },
                    {
                        "name": "Brittany Fuzia"
                    },
                    {
                        "name": "Patricio A. Gallardo"
                    },
                    {
                        "name": "Xavier Garrido"
                    },
                    {
                        "name": "Martina Gerbino"
                    },
                    {
                        "name": "Serena Giardiello"
                    },
                    {
                        "name": "Ajay Gill"
                    },
                    {
                        "name": "Jahmour Givans"
                    },
                    {
                        "name": "Vera Gluscevic"
                    },
                    {
                        "name": "Samuel Goldstein"
                    },
                    {
                        "name": "Joseph E. Golec"
                    },
                    {
                        "name": "Yulin Gong"
                    },
                    {
                        "name": "Yilun Guan"
                    },
                    {
                        "name": "Mark Halpern"
                    },
                    {
                        "name": "Ian Harrison"
                    },
                    {
                        "name": "Matthew Hasselfield"
                    },
                    {
                        "name": "Adam He"
                    },
                    {
                        "name": "Erin Healy"
                    },
                    {
                        "name": "Shawn Henderson"
                    },
                    {
                        "name": "Brandon Hensley"
                    },
                    {
                        "name": "Carlos Hervías-Caimapo"
                    },
                    {
                        "name": "Gene C. Hilton"
                    },
                    {
                        "name": "Matt Hilton"
                    },
                    {
                        "name": "Adam D. Hincks"
                    },
                    {
                        "name": "Renée Hložek"
                    },
                    {
                        "name": "Shuay-Pwu Patty Ho"
                    },
                    {
                        "name": "John Hood"
                    },
                    {
                        "name": "Erika Hornecker"
                    },
                    {
                        "name": "Zachary B. Huber"
                    },
                    {
                        "name": "Johannes Hubmayr"
                    },
                    {
                        "name": "Kevin M. Huffenberger"
                    },
                    {
                        "name": "John P. Hughes"
                    },
                    {
                        "name": "Margaret Ikape"
                    },
                    {
                        "name": "Kent Irwin"
                    },
                    {
                        "name": "Giovanni Isopi"
                    },
                    {
                        "name": "Neha Joshi"
                    },
                    {
                        "name": "Ben Keller"
                    },
                    {
                        "name": "Joshua Kim"
                    },
                    {
                        "name": "Kenda Knowles"
                    },
                    {
                        "name": "Brian J. Koopman"
                    },
                    {
                        "name": "Arthur Kosowsky"
                    },
                    {
                        "name": "Darby Kramer"
                    },
                    {
                        "name": "Aleksandra Kusiak"
                    },
                    {
                        "name": "Alex Lague"
                    },
                    {
                        "name": "Victoria Lakey"
                    },
                    {
                        "name": "Massimiliano Lattanzi"
                    },
                    {
                        "name": "Eunseong Lee"
                    },
                    {
                        "name": "Yaqiong Li"
                    },
                    {
                        "name": "Zack Li"
                    },
                    {
                        "name": "Michele Limon"
                    },
                    {
                        "name": "Martine Lokken"
                    },
                    {
                        "name": "Thibaut Louis"
                    },
                    {
                        "name": "Marius Lungu"
                    },
                    {
                        "name": "Niall MacCrann"
                    },
                    {
                        "name": "Amanda MacInnis"
                    },
                    {
                        "name": "Mathew S. Madhavacheril"
                    },
                    {
                        "name": "Diego Maldonado"
                    },
                    {
                        "name": "Felipe Maldonado"
                    },
                    {
                        "name": "Maya Mallaby-Kay"
                    },
                    {
                        "name": "Gabriela A. Marques"
                    },
                    {
                        "name": "Joshiwa van Marrewijk"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Jeff McMahon"
                    },
                    {
                        "name": "Yogesh Mehta"
                    },
                    {
                        "name": "Felipe Menanteau"
                    },
                    {
                        "name": "Kavilan Moodley"
                    },
                    {
                        "name": "Thomas W. Morris"
                    },
                    {
                        "name": "Tony Mroczkowski"
                    },
                    {
                        "name": "Sigurd Naess"
                    },
                    {
                        "name": "Toshiya Namikawa"
                    },
                    {
                        "name": "Federico Nati"
                    },
                    {
                        "name": "Simran K. Nerval"
                    },
                    {
                        "name": "Laura Newburgh"
                    },
                    {
                        "name": "Andrina Nicola"
                    },
                    {
                        "name": "Michael D. Niemack"
                    },
                    {
                        "name": "Michael R. Nolta"
                    },
                    {
                        "name": "John Orlowski-Scherer"
                    },
                    {
                        "name": "Luca Pagano"
                    },
                    {
                        "name": "Lyman A. Page"
                    },
                    {
                        "name": "Shivam Pandey"
                    },
                    {
                        "name": "Bruce Partridge"
                    },
                    {
                        "name": "Karen Perez Sarmiento"
                    },
                    {
                        "name": "Heather Prince"
                    },
                    {
                        "name": "Roberto Puddu"
                    },
                    {
                        "name": "Frank J. Qu"
                    },
                    {
                        "name": "Damien C. Ragavan"
                    },
                    {
                        "name": "Bernardita Ried Guachalla"
                    },
                    {
                        "name": "Keir K. Rogers"
                    },
                    {
                        "name": "Felipe Rojas"
                    },
                    {
                        "name": "Tai Sakuma"
                    },
                    {
                        "name": "Emmanuel Schaan"
                    },
                    {
                        "name": "Benjamin L. Schmitt"
                    },
                    {
                        "name": "Neelima Sehgal"
                    },
                    {
                        "name": "Shabbir Shaikh"
                    },
                    {
                        "name": "Blake D. Sherwin"
                    },
                    {
                        "name": "Carlos Sierra"
                    },
                    {
                        "name": "Jon Sievers"
                    },
                    {
                        "name": "Cristóbal Sifón"
                    },
                    {
                        "name": "Sara Simon"
                    },
                    {
                        "name": "Rita Sonka"
                    },
                    {
                        "name": "David N. Spergel"
                    },
                    {
                        "name": "Suzanne T. Staggs"
                    },
                    {
                        "name": "Emilie Storer"
                    },
                    {
                        "name": "Kristen Surrao"
                    },
                    {
                        "name": "Eric R. Switzer"
                    },
                    {
                        "name": "Niklas Tampier"
                    },
                    {
                        "name": "Leander Thiele"
                    },
                    {
                        "name": "Robert Thornton"
                    },
                    {
                        "name": "Hy Trac"
                    },
                    {
                        "name": "Carole Tucker"
                    },
                    {
                        "name": "Joel Ullom"
                    },
                    {
                        "name": "Leila R. Vale"
                    },
                    {
                        "name": "Alexander Van Engelen"
                    },
                    {
                        "name": "Jeff Van Lanen"
                    },
                    {
                        "name": "Cristian Vargas"
                    },
                    {
                        "name": "Eve M. Vavagiakis"
                    },
                    {
                        "name": "Kasey Wagoner"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Lukas Wenzl"
                    },
                    {
                        "name": "Edward J. Wollack"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Kaiwen Zheng"
                },
                "author": "Kaiwen Zheng",
                "arxiv_comment": "Matches version accepted by JCAP. 56+36 pages, 46+13 figures,\n  abstract abridged here. Part of ACT DR6 suite of papers. Data located at\n  https://lambda.gsfc.nasa.gov/product/act/act_dr6.02/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16553v2",
                "updated": "2025-06-24T16:54:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    54,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-19T15:08:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    8,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "A Foundational individual Mobility Prediction Model based on Open-Source\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundational individual Mobility Prediction Model based on Open-Source\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs."
                },
                "authors": [
                    {
                        "name": "Zhenlin Qin"
                    },
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Francisco Camara Pereira"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19782v1",
                "updated": "2025-06-24T16:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:50:51Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "title": "'Mic drop': on estimating the size of sub-mm droplets using a simple\n  condenser microphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "'Mic drop': on estimating the size of sub-mm droplets using a simple\n  condenser microphone"
                },
                "summary": "The size distribution of aerosol droplets is a key parameter in a myriad of\nprocesses, and it is typically measured with optical aids (e.g., lasers or\ncameras) that require sophisticated calibration, thus making the measurement\ncost intensive. We developed a new method to indirectly measure the size of\nsmall droplets using off-the-shelf <\\$1 electret microphones. In this method we\nexploit the natural oscillations that small droplets undergo after impacting a\nflat surface: by allowing droplets to land directly on a microphone diaphragm,\nwe record the impact force they exert onto it and calculate the complex\nresonant frequencies of oscillations, from which their size can be inferred. To\ntest this method, we recorded the impact signals of droplets of varying sizes\nand extracted the resonant frequencies that characterize each signal. Various\nsources of uncertainty in the experiments led to a range of frequencies that\ncan characterize each droplet size, and hence a data-driven approach was taken\nto estimate the size from each set of measured frequencies. We employed a\nsimple setting of neural network and trained it on the frequencies we measured\nfrom impact of droplets of prescribed radius. The network was then able to\npredict the droplet radius in the test group with an average relative error of\n2.7\\% and a maximum of 8.6\\%. These results, achieved with a data set of only\n320 measurements, demonstrate the potential for reliable size-distribution\nmeasurements via a simple and inexpensive method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size distribution of aerosol droplets is a key parameter in a myriad of\nprocesses, and it is typically measured with optical aids (e.g., lasers or\ncameras) that require sophisticated calibration, thus making the measurement\ncost intensive. We developed a new method to indirectly measure the size of\nsmall droplets using off-the-shelf <\\$1 electret microphones. In this method we\nexploit the natural oscillations that small droplets undergo after impacting a\nflat surface: by allowing droplets to land directly on a microphone diaphragm,\nwe record the impact force they exert onto it and calculate the complex\nresonant frequencies of oscillations, from which their size can be inferred. To\ntest this method, we recorded the impact signals of droplets of varying sizes\nand extracted the resonant frequencies that characterize each signal. Various\nsources of uncertainty in the experiments led to a range of frequencies that\ncan characterize each droplet size, and hence a data-driven approach was taken\nto estimate the size from each set of measured frequencies. We employed a\nsimple setting of neural network and trained it on the frequencies we measured\nfrom impact of droplets of prescribed radius. The network was then able to\npredict the droplet radius in the test group with an average relative error of\n2.7\\% and a maximum of 8.6\\%. These results, achieved with a data set of only\n320 measurements, demonstrate the potential for reliable size-distribution\nmeasurements via a simple and inexpensive method."
                },
                "authors": [
                    {
                        "name": "Avshalom Offner"
                    }
                ],
                "author_detail": {
                    "name": "Avshalom Offner"
                },
                "author": "Avshalom Offner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19783v1",
                "updated": "2025-06-24T16:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:50:51Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting"
                },
                "summary": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10326v2",
                "updated": "2025-06-24T16:45:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    45,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-01-17T17:56:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Large language models for automated scholarly paper review: A survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for automated scholarly paper review: A survey"
                },
                "summary": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publication, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nLLMs hold transformative potential for the full-scale implementation of\nautomated scholarly paper review (ASPR), but they also pose new issues and\nchallenges that need to be addressed. In this survey paper, we aim to provide a\nholistic view of ASPR in the era of LLMs. We begin with a survey to find out\nwhich LLMs are used to conduct ASPR. Then, we review what ASPR-related\ntechnological bottlenecks have been solved with the incorporation of LLM\ntechnology. After that, we move on to explore new methods, new datasets, new\nsource code, and new online systems that come with LLMs for ASPR. Furthermore,\nwe summarize the performance and issues of LLMs in ASPR, and investigate the\nattitudes and reactions of publishers and academia to ASPR. Lastly, we discuss\nthe challenges and future directions associated with the development of LLMs\nfor ASPR. This survey serves as an inspirational reference for the researchers\nand can promote the progress of ASPR for its actual implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publication, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nLLMs hold transformative potential for the full-scale implementation of\nautomated scholarly paper review (ASPR), but they also pose new issues and\nchallenges that need to be addressed. In this survey paper, we aim to provide a\nholistic view of ASPR in the era of LLMs. We begin with a survey to find out\nwhich LLMs are used to conduct ASPR. Then, we review what ASPR-related\ntechnological bottlenecks have been solved with the incorporation of LLM\ntechnology. After that, we move on to explore new methods, new datasets, new\nsource code, and new online systems that come with LLMs for ASPR. Furthermore,\nwe summarize the performance and issues of LLMs in ASPR, and investigate the\nattitudes and reactions of publishers and academia to ASPR. Lastly, we discuss\nthe challenges and future directions associated with the development of LLMs\nfor ASPR. This survey serves as an inspirational reference for the researchers\nand can promote the progress of ASPR for its actual implementation."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Zhuang"
                    },
                    {
                        "name": "Jiandong Chen"
                    },
                    {
                        "name": "Hongfeng Xu"
                    },
                    {
                        "name": "Yuwen Jiang"
                    },
                    {
                        "name": "Jialiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lin"
                },
                "author": "Jialiang Lin",
                "arxiv_doi": "10.1016/j.inffus.2025.103332",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.103332",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please cite the version of Information Fusion",
                "arxiv_journal_ref": "Information Fusion, Vol. 124, 103332 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19777v1",
                "updated": "2025-06-24T16:42:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    42,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:42:46Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    42,
                    46,
                    1,
                    175,
                    0
                ],
                "title": "Alleviating User-Sensitive bias with Fair Generative Sequential\n  Recommendation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating User-Sensitive bias with Fair Generative Sequential\n  Recommendation Model"
                },
                "summary": "Recommendation fairness has recently attracted much attention. In the real\nworld, recommendation systems are driven by user behavior, and since users with\nthe same sensitive feature (e.g., gender and age) tend to have the same\npatterns, recommendation models can easily capture the strong correlation\npreference of sensitive features and thus cause recommendation unfairness.\nDiffusion model (DM) as a new generative model paradigm has achieved great\nsuccess in recommendation systems. DM's ability to model uncertainty and\nrepresent diversity, and its modeling mechanism has a high degree of\nadaptability with the real-world recommendation process with bias. Therefore,\nwe use DM to effectively model the fairness of recommendation and enhance the\ndiversity. This paper proposes a FairGENerative sequential Recommendation model\nbased on DM, FairGENRec. In the training phase, we inject random noise into the\noriginal distribution under the guidance of the sensitive feature recognition\nmodel, and a sequential denoise model is designed for the reverse\nreconstruction of items. Simultaneously, recommendation fairness modeling is\ncompleted by injecting multi-interests representational information that\neliminates the bias of sensitive user features into the generated results. In\nthe inference phase, the model obtains the noise in the form of noise addition\nby using the history interactions which is followed by reverse iteration to\nreconstruct the target item representation. Finally, our extensive experiments\non three datasets demonstrate the dual enhancement effect of FairGENRec on\naccuracy and fairness, while the statistical analysis of the cases visualizes\nthe degree of improvement on the fairness of the recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation fairness has recently attracted much attention. In the real\nworld, recommendation systems are driven by user behavior, and since users with\nthe same sensitive feature (e.g., gender and age) tend to have the same\npatterns, recommendation models can easily capture the strong correlation\npreference of sensitive features and thus cause recommendation unfairness.\nDiffusion model (DM) as a new generative model paradigm has achieved great\nsuccess in recommendation systems. DM's ability to model uncertainty and\nrepresent diversity, and its modeling mechanism has a high degree of\nadaptability with the real-world recommendation process with bias. Therefore,\nwe use DM to effectively model the fairness of recommendation and enhance the\ndiversity. This paper proposes a FairGENerative sequential Recommendation model\nbased on DM, FairGENRec. In the training phase, we inject random noise into the\noriginal distribution under the guidance of the sensitive feature recognition\nmodel, and a sequential denoise model is designed for the reverse\nreconstruction of items. Simultaneously, recommendation fairness modeling is\ncompleted by injecting multi-interests representational information that\neliminates the bias of sensitive user features into the generated results. In\nthe inference phase, the model obtains the noise in the form of noise addition\nby using the history interactions which is followed by reverse iteration to\nreconstruct the target item representation. Finally, our extensive experiments\non three datasets demonstrate the dual enhancement effect of FairGENRec on\naccuracy and fairness, while the statistical analysis of the cases visualizes\nthe degree of improvement on the fairness of the recommendation."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Feng Wu"
                    },
                    {
                        "name": "Xuefang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xuefang Zhu"
                },
                "author": "Xuefang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07531v2",
                "updated": "2025-06-24T16:40:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    40,
                    53,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-10T16:58:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    58,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Predicted Neutrino Signal Features of Core-Collapse Supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicted Neutrino Signal Features of Core-Collapse Supernovae"
                },
                "summary": "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode."
                },
                "authors": [
                    {
                        "name": "Lyla Choi"
                    },
                    {
                        "name": "Adam Burrows"
                    },
                    {
                        "name": "David Vartanyan"
                    }
                ],
                "author_detail": {
                    "name": "David Vartanyan"
                },
                "author": "David Vartanyan",
                "arxiv_comment": "18 pages, 11 figures. Published in Physical Review D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19775v1",
                "updated": "2025-06-24T16:40:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    40,
                    40,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:40:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    40,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Canary in the Mine: An LLM Augmented Survey of Disciplinary Complaints\n  to the Ordre des ingénieurs du Québec (OIQ)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary in the Mine: An LLM Augmented Survey of Disciplinary Complaints\n  to the Ordre des ingénieurs du Québec (OIQ)"
                },
                "summary": "This study uses pre-trained LLMs to conduct thematic analysis to investigate\ndisciplinary incidents involving engineers in Quebec, shedding light on\ncritical gaps in engineering education. Through a comprehensive review of the\ndisciplinary register of the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)'s\ndisciplinary register for 2010 to 2024, researchers from engineering education\nand human resources management in technological development laboratories\nconducted a thematic analysis of reported incidents to identify patterns,\ntrends, and areas for improvement. The analysis aims to uncover the most common\ntypes of disciplinary incidents, underlying causes, and implications for the\nfield in how engineering education addresses (or fails to address) these\nissues. Our findings identify recurring themes, analyze root causes, and offer\nrecommendations for engineering educators and students to mitigate similar\nincidents. This research has implications for informing curriculum development,\nprofessional development, and performance evaluation, ultimately fostering a\nculture of professionalism and ethical responsibility in engineering. By\nproviding empirical evidence of disciplinary incidents and their causes, this\nstudy contributes to evidence-based practices for engineering education and\nprofessional development, enhancing the engineering education community's\nunderstanding of professionalism and ethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study uses pre-trained LLMs to conduct thematic analysis to investigate\ndisciplinary incidents involving engineers in Quebec, shedding light on\ncritical gaps in engineering education. Through a comprehensive review of the\ndisciplinary register of the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)'s\ndisciplinary register for 2010 to 2024, researchers from engineering education\nand human resources management in technological development laboratories\nconducted a thematic analysis of reported incidents to identify patterns,\ntrends, and areas for improvement. The analysis aims to uncover the most common\ntypes of disciplinary incidents, underlying causes, and implications for the\nfield in how engineering education addresses (or fails to address) these\nissues. Our findings identify recurring themes, analyze root causes, and offer\nrecommendations for engineering educators and students to mitigate similar\nincidents. This research has implications for informing curriculum development,\nprofessional development, and performance evaluation, ultimately fostering a\nculture of professionalism and ethical responsibility in engineering. By\nproviding empirical evidence of disciplinary incidents and their causes, this\nstudy contributes to evidence-based practices for engineering education and\nprofessional development, enhancing the engineering education community's\nunderstanding of professionalism and ethics."
                },
                "authors": [
                    {
                        "name": "Tammy Mackenzie"
                    },
                    {
                        "name": "Varsha Kesavan"
                    },
                    {
                        "name": "Thomas Mekhael"
                    },
                    {
                        "name": "Animesh Paul"
                    },
                    {
                        "name": "Branislav Radeljic"
                    },
                    {
                        "name": "Sara Kodeiri"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    }
                ],
                "author_detail": {
                    "name": "Sreyoshi Bhaduri"
                },
                "author": "Sreyoshi Bhaduri",
                "arxiv_comment": "22 pages, accepted at the American Society of Engineering Education\n  annual conference 2025, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19773v1",
                "updated": "2025-06-24T16:38:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    38,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:38:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    38,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights\n  from an Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights\n  from an Empirical Study"
                },
                "summary": "A KG represents a network of entities and illustrates relationships between\nthem. KGs are used for various applications, including semantic search and\ndiscovery, reasoning, decision-making, natural language processing, machine\nlearning, and recommendation systems. Triple (subject-relation-object)\nextraction from text is the fundamental building block of KG construction and\nhas been widely studied, for example, in early benchmarks such as ACE 2002 to\nmore recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs\nis explored for KG construction, handcrafting reasonable task-specific prompts\nfor LLMs is a labour-intensive exercise and can be brittle due to subtle\nchanges in the LLM models employed. Recent work in NLP tasks (e.g. autonomy\ngeneration) uses automatic prompt optimization/engineering to address this\nchallenge by generating optimal or near-optimal task-specific prompts given\ninput-output examples.\n  This empirical study explores the application of automatic prompt\noptimization for the triple extraction task using experimental benchmarking. We\nevaluate different settings by changing (a) the prompting strategy, (b) the LLM\nbeing used for prompt optimization and task execution, (c) the number of\ncanonical relations in the schema (schema complexity), (d) the length and\ndiversity of input text, (e) the metric used to drive the prompt optimization,\nand (f) the dataset being used for training and testing. We evaluate three\ndifferent automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use\ntwo different triple extraction datasets, SynthIE and REBEL. Through rigorous\nempirical evaluation, our main contribution highlights that automatic prompt\noptimization techniques can generate reasonable prompts similar to humans for\ntriple extraction. In turn, these optimized prompts achieve improved results,\nparticularly with increasing schema complexity and text size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A KG represents a network of entities and illustrates relationships between\nthem. KGs are used for various applications, including semantic search and\ndiscovery, reasoning, decision-making, natural language processing, machine\nlearning, and recommendation systems. Triple (subject-relation-object)\nextraction from text is the fundamental building block of KG construction and\nhas been widely studied, for example, in early benchmarks such as ACE 2002 to\nmore recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs\nis explored for KG construction, handcrafting reasonable task-specific prompts\nfor LLMs is a labour-intensive exercise and can be brittle due to subtle\nchanges in the LLM models employed. Recent work in NLP tasks (e.g. autonomy\ngeneration) uses automatic prompt optimization/engineering to address this\nchallenge by generating optimal or near-optimal task-specific prompts given\ninput-output examples.\n  This empirical study explores the application of automatic prompt\noptimization for the triple extraction task using experimental benchmarking. We\nevaluate different settings by changing (a) the prompting strategy, (b) the LLM\nbeing used for prompt optimization and task execution, (c) the number of\ncanonical relations in the schema (schema complexity), (d) the length and\ndiversity of input text, (e) the metric used to drive the prompt optimization,\nand (f) the dataset being used for training and testing. We evaluate three\ndifferent automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use\ntwo different triple extraction datasets, SynthIE and REBEL. Through rigorous\nempirical evaluation, our main contribution highlights that automatic prompt\noptimization techniques can generate reasonable prompts similar to humans for\ntriple extraction. In turn, these optimized prompts achieve improved results,\nparticularly with increasing schema complexity and text size."
                },
                "authors": [
                    {
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "name": "Niharika S. D'Souza"
                    },
                    {
                        "name": "Faisal Chowdhury"
                    },
                    {
                        "name": "Horst Samulowitz"
                    }
                ],
                "author_detail": {
                    "name": "Horst Samulowitz"
                },
                "author": "Horst Samulowitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19769v1",
                "updated": "2025-06-24T16:34:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    34,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:34:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    34,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background,\n  Methods, Challenges and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background,\n  Methods, Challenges and Prospects"
                },
                "summary": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,\nwhich can serve a variety of downstream tasks (e.g., 3D object detection and\nsemantic segmentation) and application scenarios (e.g., autonomous driving and\nswarm robotics). Recently, impressive achievements on AI-based MSFP methods\nhave been reviewed in relevant surveys. However, we observe that the existing\nsurveys have some limitations after a rigorous and detailed investigation. For\none thing, most surveys are oriented to a single task or research field, such\nas 3D object detection or autonomous driving. Therefore, researchers in other\nrelated tasks often find it difficult to benefit directly. For another, most\nsurveys only introduce MSFP from a single perspective of multi-modal fusion,\nwhile lacking consideration of the diversity of MSFP methods, such as\nmulti-view fusion and time-series fusion. To this end, in this paper, we hope\nto organize MSFP research from a task-agnostic perspective, where methods are\nreported from various technical views. Specifically, we first introduce the\nbackground of MSFP. Next, we review multi-modal and multi-agent fusion methods.\nA step further, time-series fusion methods are analyzed. In the era of LLM, we\nalso investigate multimodal LLM fusion methods. Finally, we discuss open\nchallenges and future directions for MSFP. We hope this survey can help\nresearchers understand the important progress in MSFP and provide possible\ninsights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,\nwhich can serve a variety of downstream tasks (e.g., 3D object detection and\nsemantic segmentation) and application scenarios (e.g., autonomous driving and\nswarm robotics). Recently, impressive achievements on AI-based MSFP methods\nhave been reviewed in relevant surveys. However, we observe that the existing\nsurveys have some limitations after a rigorous and detailed investigation. For\none thing, most surveys are oriented to a single task or research field, such\nas 3D object detection or autonomous driving. Therefore, researchers in other\nrelated tasks often find it difficult to benefit directly. For another, most\nsurveys only introduce MSFP from a single perspective of multi-modal fusion,\nwhile lacking consideration of the diversity of MSFP methods, such as\nmulti-view fusion and time-series fusion. To this end, in this paper, we hope\nto organize MSFP research from a task-agnostic perspective, where methods are\nreported from various technical views. Specifically, we first introduce the\nbackground of MSFP. Next, we review multi-modal and multi-agent fusion methods.\nA step further, time-series fusion methods are analyzed. In the era of LLM, we\nalso investigate multimodal LLM fusion methods. Finally, we discuss open\nchallenges and future directions for MSFP. We hope this survey can help\nresearchers understand the important progress in MSFP and provide possible\ninsights for future research."
                },
                "authors": [
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Rongwei Wang"
                    },
                    {
                        "name": "Xuchen Shen"
                    },
                    {
                        "name": "Huijie Liu"
                    },
                    {
                        "name": "Baihui Xiao"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "You He"
                    }
                ],
                "author_detail": {
                    "name": "You He"
                },
                "author": "You He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19767v1",
                "updated": "2025-06-24T16:31:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    37,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:31:37Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    37,
                    1,
                    175,
                    0
                ],
                "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks."
                },
                "authors": [
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Tinghong Chen"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Yuanheng Zhu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v3",
                "updated": "2025-06-24T16:31:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "arxiv_comment": "Accepted at the 9th Workshop on Online Abuse and Harms (WOAH)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19753v1",
                "updated": "2025-06-24T16:06:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:06:58Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    58,
                    1,
                    175,
                    0
                ],
                "title": "Arabic Dialect Classification using RNNs, Transformers, and Large\n  Language Models: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Dialect Classification using RNNs, Transformers, and Large\n  Language Models: A Comparative Analysis"
                },
                "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities."
                },
                "authors": [
                    {
                        "name": "Omar A. Essameldin"
                    },
                    {
                        "name": "Ali O. Elbeih"
                    },
                    {
                        "name": "Wael H. Gomaa"
                    },
                    {
                        "name": "Wael F. Elsersy"
                    }
                ],
                "author_detail": {
                    "name": "Wael F. Elsersy"
                },
                "author": "Wael F. Elsersy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20185v2",
                "updated": "2025-06-24T16:03:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    3,
                    30,
                    1,
                    175,
                    0
                ],
                "published": "2024-12-28T15:51:02Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    15,
                    51,
                    2,
                    5,
                    363,
                    0
                ],
                "title": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization"
                },
                "summary": "Quantization of Large Language Models (LLMs) has recently gained popularity,\nparticularly for on-device settings with limited hardware resources. While\nefficient, quantization inevitably degrades model quality, especially in\naggressive low-bit settings such as 3-bit and 4-bit precision. In this paper,\nwe propose DecDEC, an inference scheme that improves the quality of low-bit\nLLMs while preserving the key benefits of quantization: GPU memory savings and\nlatency reduction. DecDEC stores the residual matrix -- the difference between\nfull-precision and quantized weights -- in CPU, and dynamically fetches the\nresiduals for only a small portion of the weights. This portion corresponds to\nthe salient channels, marked by activation outliers, with the fetched residuals\nhelping to correct quantization errors in these channels. Salient channels are\nidentified dynamically at each decoding step by analyzing the input activations\n-- this enables adaptation to the dynamic nature of activation distribution,\nthus maximizing the effectiveness of error compensation. We demonstrate the\neffectiveness of DecDEC by augmenting state-of-the-art quantization methods.\nFor example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model\nfrom 10.15 to 9.12 -- outperforming its 3.5-bit counterpart -- while adding\nless than 0.0003\\% to GPU memory usage and incurring only a 1.7\\% inference\nslowdown on NVIDIA RTX 4050 Mobile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of Large Language Models (LLMs) has recently gained popularity,\nparticularly for on-device settings with limited hardware resources. While\nefficient, quantization inevitably degrades model quality, especially in\naggressive low-bit settings such as 3-bit and 4-bit precision. In this paper,\nwe propose DecDEC, an inference scheme that improves the quality of low-bit\nLLMs while preserving the key benefits of quantization: GPU memory savings and\nlatency reduction. DecDEC stores the residual matrix -- the difference between\nfull-precision and quantized weights -- in CPU, and dynamically fetches the\nresiduals for only a small portion of the weights. This portion corresponds to\nthe salient channels, marked by activation outliers, with the fetched residuals\nhelping to correct quantization errors in these channels. Salient channels are\nidentified dynamically at each decoding step by analyzing the input activations\n-- this enables adaptation to the dynamic nature of activation distribution,\nthus maximizing the effectiveness of error compensation. We demonstrate the\neffectiveness of DecDEC by augmenting state-of-the-art quantization methods.\nFor example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model\nfrom 10.15 to 9.12 -- outperforming its 3.5-bit counterpart -- while adding\nless than 0.0003\\% to GPU memory usage and incurring only a 1.7\\% inference\nslowdown on NVIDIA RTX 4050 Mobile."
                },
                "authors": [
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jake Hyun"
                    },
                    {
                        "name": "Hojoon Kim"
                    },
                    {
                        "name": "Jae W. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae W. Lee"
                },
                "author": "Jae W. Lee",
                "arxiv_comment": "OSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12743v2",
                "updated": "2025-06-24T16:03:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    3,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-18T11:00:28Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    0,
                    28,
                    1,
                    49,
                    0
                ],
                "title": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect\n  and Explain LLM-Generated Texts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect\n  and Explain LLM-Generated Texts?"
                },
                "summary": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Jiazhou Ji"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Weidong Qiu"
                    },
                    {
                        "name": "Zheng Huang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Xinru Lu"
                    },
                    {
                        "name": "Xiaoyu Jiang"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19743v1",
                "updated": "2025-06-24T16:02:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    2,
                    2,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:02:02Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    2,
                    2,
                    1,
                    175,
                    0
                ],
                "title": "NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and\n  Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and\n  Ranking"
                },
                "summary": "E-commerce information retrieval (IR) systems struggle to simultaneously\nachieve high accuracy in interpreting complex user queries and maintain\nefficient processing of vast product catalogs. The dual challenge lies in\nprecisely matching user intent with relevant products while managing the\ncomputational demands of real-time search across massive inventories. In this\npaper, we propose a Nested Embedding Approach to product Retrieval and Ranking,\ncalled NEAR$^2$, which can achieve up to $12$ times efficiency in embedding\nsize at inference time while introducing no extra cost in training and\nimproving performance in accuracy for various encoder-based Transformer models.\nWe validate our approach using different loss functions for the retrieval and\nranking task, including multiple negative ranking loss and online contrastive\nloss, on four different test sets with various IR challenges such as short and\nimplicit queries. Our approach achieves an improved performance over a smaller\nembedding dimension, compared to any existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce information retrieval (IR) systems struggle to simultaneously\nachieve high accuracy in interpreting complex user queries and maintain\nefficient processing of vast product catalogs. The dual challenge lies in\nprecisely matching user intent with relevant products while managing the\ncomputational demands of real-time search across massive inventories. In this\npaper, we propose a Nested Embedding Approach to product Retrieval and Ranking,\ncalled NEAR$^2$, which can achieve up to $12$ times efficiency in embedding\nsize at inference time while introducing no extra cost in training and\nimproving performance in accuracy for various encoder-based Transformer models.\nWe validate our approach using different loss functions for the retrieval and\nranking task, including multiple negative ranking loss and online contrastive\nloss, on four different test sets with various IR challenges such as short and\nimplicit queries. Our approach achieves an improved performance over a smaller\nembedding dimension, compared to any existing models."
                },
                "authors": [
                    {
                        "name": "Shenbin Qian"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Samarth Agrawal"
                    },
                    {
                        "name": "Hadeel Saadany"
                    },
                    {
                        "name": "Swapnil Bhosale"
                    },
                    {
                        "name": "Constantin Orasan"
                    },
                    {
                        "name": "Zhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wu"
                },
                "author": "Zhe Wu",
                "arxiv_comment": "This paper is accepted to the 2025 SIGIR Workshop on eCommerce",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00202v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00202v5",
                "updated": "2025-06-24T15:57:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    57,
                    2,
                    1,
                    175,
                    0
                ],
                "published": "2023-07-31T23:48:36Z",
                "published_parsed": [
                    2023,
                    7,
                    31,
                    23,
                    48,
                    36,
                    0,
                    212,
                    0
                ],
                "title": "Randomization Inference of Heterogeneous Treatment Effects under Network\n  Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization Inference of Heterogeneous Treatment Effects under Network\n  Interference"
                },
                "summary": "We develop randomization-based tests for heterogeneous treatment effects in\nthe presence of network interference. Leveraging the exposure mapping\nframework, we study a broad class of null hypotheses that represent various\nforms of constant treatment effects in networked populations. These null\nhypotheses, unlike the classical Fisher sharp null, are not sharp due to\nunknown parameters and multiple potential outcomes. Existing conditional\nrandomization procedures either fail to control size or suffer from low\nstatistical power in this setting. We propose a testing procedure that\nconstructs a data-dependent focal assignment set and permits variation in focal\nunits across focal assignments. These features complicate both estimation and\ninference, necessitating new technical developments. We establish the\nasymptotic validity of the proposed procedure under general conditions on the\ntest statistic and characterize the asymptotic size distortion in terms of\nobservable quantities. The procedure is applied to experimental network data\nand evaluated via Monte Carlo simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop randomization-based tests for heterogeneous treatment effects in\nthe presence of network interference. Leveraging the exposure mapping\nframework, we study a broad class of null hypotheses that represent various\nforms of constant treatment effects in networked populations. These null\nhypotheses, unlike the classical Fisher sharp null, are not sharp due to\nunknown parameters and multiple potential outcomes. Existing conditional\nrandomization procedures either fail to control size or suffer from low\nstatistical power in this setting. We propose a testing procedure that\nconstructs a data-dependent focal assignment set and permits variation in focal\nunits across focal assignments. These features complicate both estimation and\ninference, necessitating new technical developments. We establish the\nasymptotic validity of the proposed procedure under general conditions on the\ntest statistic and characterize the asymptotic size distortion in terms of\nobservable quantities. The procedure is applied to experimental network data\nand evaluated via Monte Carlo simulations."
                },
                "authors": [
                    {
                        "name": "Julius Owusu"
                    }
                ],
                "author_detail": {
                    "name": "Julius Owusu"
                },
                "author": "Julius Owusu",
                "arxiv_comment": "84 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.00202v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00202v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02978v2",
                "updated": "2025-06-24T15:54:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    54,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2024-08-06T06:24:10Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    6,
                    24,
                    10,
                    1,
                    219,
                    0
                ],
                "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval"
                },
                "summary": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval."
                },
                "authors": [
                    {
                        "name": "Ruixiang Zhao"
                    },
                    {
                        "name": "Jian Jia"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Xuehan Bai"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Xirong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xirong Li"
                },
                "author": "Xirong Li",
                "arxiv_comment": "accepted for publication as a REGULAR paper in the IEEE Transactions\n  on Multimedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14388v3",
                "updated": "2025-06-24T15:53:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    53,
                    30,
                    1,
                    175,
                    0
                ],
                "published": "2024-10-18T11:44:29Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    44,
                    29,
                    4,
                    292,
                    0
                ],
                "title": "Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport"
                },
                "summary": "Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility."
                },
                "authors": [
                    {
                        "name": "Peter A. Wijeratne"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. Alexander"
                },
                "author": "Daniel C. Alexander",
                "arxiv_comment": "Camera-ready version of paper accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19733v1",
                "updated": "2025-06-24T15:53:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    53,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:53:10Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    53,
                    10,
                    1,
                    175,
                    0
                ],
                "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To\n  Unseen Domains?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To\n  Unseen Domains?"
                },
                "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns."
                },
                "authors": [
                    {
                        "name": "Chuxuan Hu"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Antony Kellermann"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Suppakit Waiwitlikhit"
                    },
                    {
                        "name": "Jason Benn"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19732v1",
                "updated": "2025-06-24T15:50:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    50,
                    35,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:50:35Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    50,
                    35,
                    1,
                    175,
                    0
                ],
                "title": "Who Does What in Deep Learning? Multidimensional Game-Theoretic\n  Attribution of Function of Neural Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Does What in Deep Learning? Multidimensional Game-Theoretic\n  Attribution of Function of Neural Units"
                },
                "summary": "Neural networks now generate text, images, and speech with billions of\nparameters, producing a need to know how each neural unit contributes to these\nhigh-dimensional outputs. Existing explainable-AI methods, such as SHAP,\nattribute importance to inputs, but cannot quantify the contributions of neural\nunits across thousands of output pixels, tokens, or logits. Here we close that\ngap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic\ngame-theoretic framework. By systematically lesioning combinations of units,\nMSA yields Shapley Modes, unit-wise contribution maps that share the exact\ndimensionality of the model's output. We apply MSA across scales, from\nmulti-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative\nAdversarial Networks (GAN). The approach demonstrates how regularisation\nconcentrates computation in a few hubs, exposes language-specific experts\ninside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.\nTogether, these results showcase MSA as a powerful approach for interpreting,\nediting, and compressing deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks now generate text, images, and speech with billions of\nparameters, producing a need to know how each neural unit contributes to these\nhigh-dimensional outputs. Existing explainable-AI methods, such as SHAP,\nattribute importance to inputs, but cannot quantify the contributions of neural\nunits across thousands of output pixels, tokens, or logits. Here we close that\ngap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic\ngame-theoretic framework. By systematically lesioning combinations of units,\nMSA yields Shapley Modes, unit-wise contribution maps that share the exact\ndimensionality of the model's output. We apply MSA across scales, from\nmulti-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative\nAdversarial Networks (GAN). The approach demonstrates how regularisation\nconcentrates computation in a few hubs, exposes language-specific experts\ninside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.\nTogether, these results showcase MSA as a powerful approach for interpreting,\nediting, and compressing deep neural networks."
                },
                "authors": [
                    {
                        "name": "Shrey Dixit"
                    },
                    {
                        "name": "Kayson Fakhar"
                    },
                    {
                        "name": "Fatemeh Hadaeghi"
                    },
                    {
                        "name": "Patrick Mineault"
                    },
                    {
                        "name": "Konrad P. Kording"
                    },
                    {
                        "name": "Claus C. Hilgetag"
                    }
                ],
                "author_detail": {
                    "name": "Claus C. Hilgetag"
                },
                "author": "Claus C. Hilgetag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14341v2",
                "updated": "2025-06-24T15:49:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    49,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2024-12-18T21:23:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    23,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Inferring protein folding mechanisms from natural sequence diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring protein folding mechanisms from natural sequence diversity"
                },
                "summary": "Protein sequences serve as a natural record of the evolutionary constraints\nthat shape their functional structures. We show that it is possible to use only\nsequence information to go beyond predicting native structures and global\nstability to infer the folding mechanisms of globular proteins. The one- and\ntwo-body evolutionary energy fields at the amino-acid level are mapped to a\ncoarse-grained description of folding, where proteins are divided into\ncontiguous folding elements, commonly referred to as foldons. For 15 diverse\nprotein families, we calculated the folding mechanisms of hundreds of proteins\nby simulating an Ising chain of foldons, with their energetics determined by\nthe amino acid sequences. We show that protein topology imposes limits on the\nvariability of folding cooperativity within a family. While most beta and\nalpha/beta structures exhibit only a few possible mechanisms despite high\nsequence diversity, alpha topologies allow for diverse folding scenarios among\nfamily members. We show that both the stability and cooperativity changes\ninduced by mutations can be computed directly using sequence-based evolutionary\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein sequences serve as a natural record of the evolutionary constraints\nthat shape their functional structures. We show that it is possible to use only\nsequence information to go beyond predicting native structures and global\nstability to infer the folding mechanisms of globular proteins. The one- and\ntwo-body evolutionary energy fields at the amino-acid level are mapped to a\ncoarse-grained description of folding, where proteins are divided into\ncontiguous folding elements, commonly referred to as foldons. For 15 diverse\nprotein families, we calculated the folding mechanisms of hundreds of proteins\nby simulating an Ising chain of foldons, with their energetics determined by\nthe amino acid sequences. We show that protein topology imposes limits on the\nvariability of folding cooperativity within a family. While most beta and\nalpha/beta structures exhibit only a few possible mechanisms despite high\nsequence diversity, alpha topologies allow for diverse folding scenarios among\nfamily members. We show that both the stability and cooperativity changes\ninduced by mutations can be computed directly using sequence-based evolutionary\nmodels."
                },
                "authors": [
                    {
                        "name": "Ezequiel A. Galpern"
                    },
                    {
                        "name": "Ernesto A. Roman"
                    },
                    {
                        "name": "Diego U. Ferreiro"
                    }
                ],
                "author_detail": {
                    "name": "Diego U. Ferreiro"
                },
                "author": "Diego U. Ferreiro",
                "arxiv_comment": "21 pages, 5 figures and Supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18259v2",
                "updated": "2025-06-24T15:45:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    45,
                    5,
                    1,
                    175,
                    0
                ],
                "published": "2024-06-26T11:11:47Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    11,
                    47,
                    2,
                    178,
                    0
                ],
                "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated"
                },
                "summary": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power."
                },
                "authors": [
                    {
                        "name": "Jiazhou Ji"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Shujun Li"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Weidong Qiu"
                    },
                    {
                        "name": "Zheng Huang"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Xiaoyu Jiang"
                    },
                    {
                        "name": "Xinru Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xinru Lu"
                },
                "author": "Xinru Lu",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09730v2",
                "updated": "2025-06-24T15:42:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    42,
                    55,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-12T18:20:47Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    20,
                    47,
                    2,
                    71,
                    0
                ],
                "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem\n  Proving"
                },
                "summary": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the LLMs,\neven for the step-wise rewards, or large quantities of human-annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which, unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the LLMs,\neven for the step-wise rewards, or large quantities of human-annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which, unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Kumar Pratik"
                    },
                    {
                        "name": "Gabriele Cesa"
                    },
                    {
                        "name": "Arash Behboodi"
                    }
                ],
                "author_detail": {
                    "name": "Arash Behboodi"
                },
                "author": "Arash Behboodi",
                "arxiv_comment": "Accepted at the Findings of ACL 2025, Accepted at ICLR 2025 Workshop\n  on Reasoning and Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19726v1",
                "updated": "2025-06-24T15:42:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    42,
                    0,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:42:00Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    42,
                    0,
                    1,
                    175,
                    0
                ],
                "title": "Geometric-Aware Variational Inference: Robust and Adaptive\n  Regularization with Directional Weight Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Aware Variational Inference: Robust and Adaptive\n  Regularization with Directional Weight Uncertainty"
                },
                "summary": "Deep neural networks require principled uncertainty quantification, yet\nexisting variational inference methods often employ isotropic Gaussian\napproximations in weight space that poorly match the network's inherent\ngeometry. We address this mismatch by introducing Concentration-Adapted\nPerturbations (CAP), a variational framework that models weight uncertainties\ndirectly on the unit hypersphere using von Mises-Fisher distributions. Building\non recent work in radial-directional posterior decompositions and spherical\nweight constraints, CAP provides the first complete theoretical framework\nconnecting directional statistics to practical noise regularization in neural\nnetworks. Our key contribution is an analytical derivation linking vMF\nconcentration parameters to activation noise variance, enabling each layer to\nlearn its optimal uncertainty level through a novel closed-form KL divergence\nregularizer. In experiments on CIFAR-10, CAP significantly improves model\ncalibration - reducing Expected Calibration Error by 5.6x - while providing\ninterpretable layer-wise uncertainty profiles. CAP requires minimal\ncomputational overhead and integrates seamlessly into standard architectures,\noffering a theoretically grounded yet practical approach to uncertainty\nquantification in deep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks require principled uncertainty quantification, yet\nexisting variational inference methods often employ isotropic Gaussian\napproximations in weight space that poorly match the network's inherent\ngeometry. We address this mismatch by introducing Concentration-Adapted\nPerturbations (CAP), a variational framework that models weight uncertainties\ndirectly on the unit hypersphere using von Mises-Fisher distributions. Building\non recent work in radial-directional posterior decompositions and spherical\nweight constraints, CAP provides the first complete theoretical framework\nconnecting directional statistics to practical noise regularization in neural\nnetworks. Our key contribution is an analytical derivation linking vMF\nconcentration parameters to activation noise variance, enabling each layer to\nlearn its optimal uncertainty level through a novel closed-form KL divergence\nregularizer. In experiments on CIFAR-10, CAP significantly improves model\ncalibration - reducing Expected Calibration Error by 5.6x - while providing\ninterpretable layer-wise uncertainty profiles. CAP requires minimal\ncomputational overhead and integrates seamlessly into standard architectures,\noffering a theoretically grounded yet practical approach to uncertainty\nquantification in deep learning."
                },
                "authors": [
                    {
                        "name": "Carlos Stein Brito"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Stein Brito"
                },
                "author": "Carlos Stein Brito",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19721v1",
                "updated": "2025-06-24T15:31:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    31,
                    35,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:31:35Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    31,
                    35,
                    1,
                    175,
                    0
                ],
                "title": "MUSE-DARK-I: Dark matter halo properties of intermediate-z star-forming\n  galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MUSE-DARK-I: Dark matter halo properties of intermediate-z star-forming\n  galaxies"
                },
                "summary": "[Abridged] We analyse the dark matter (DM) halo properties of 127 0.3<z<1.5\nstar-forming galaxies (SFGs) down to low stellar masses (7<log(Mstar/Msun)<11),\nusing data from the MUSE Hubble Ultra Deep Field Survey and photometry from HST\nand JWST. We employ a 3D forward modelling approach to analyse the\nmorpho-kinematics of our sample, enabling measurement of individual rotation\ncurves out to 2-3 times the effective radius. We perform a disk-halo\ndecomposition with a 3D parametric model that includes stellar, gas, and DM\ncomponents, with pressure support corrections. We validate our methodology on\nmock data cubes generated from idealised disk simulations. We select the\nbest-fitting DM model among six density profiles, including the\nNavarro-Frenk-White and the generalised alpha-beta-gamma profile of Di Cintio\net al. (2014, DC14). Our Bayesian analysis shows that DC14 performs as well as\nor better than the other profiles in >65% of the sample. We find that the\nkinematically inferred stellar masses agree with values from SED fitting. We\nfind that 89% of galaxies have DM fractions >50%. For 70% of SFGs, we infer a\nDM inner slope, gamma < 0.5, indicating cored DM profiles, but no correlation\nis found between gamma and star formation rate of the sample. The stellar- and\nconcentration-mass relations agree with theoretical expectations, but with\nlarger scatter. We confirm the anticorrelation between halo scale radius and DM\ndensity. The halo scale radii and DM surface densities increase with Mstar,\nwhile DM densities stay constant. We find tentative evidence of an evolution of\nthe DM density with z, which suggests that the DM halos of intermediate-z\nsystems are denser than those of local galaxies. In contrast, the halo scale\nradii are z-invariant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Abridged] We analyse the dark matter (DM) halo properties of 127 0.3<z<1.5\nstar-forming galaxies (SFGs) down to low stellar masses (7<log(Mstar/Msun)<11),\nusing data from the MUSE Hubble Ultra Deep Field Survey and photometry from HST\nand JWST. We employ a 3D forward modelling approach to analyse the\nmorpho-kinematics of our sample, enabling measurement of individual rotation\ncurves out to 2-3 times the effective radius. We perform a disk-halo\ndecomposition with a 3D parametric model that includes stellar, gas, and DM\ncomponents, with pressure support corrections. We validate our methodology on\nmock data cubes generated from idealised disk simulations. We select the\nbest-fitting DM model among six density profiles, including the\nNavarro-Frenk-White and the generalised alpha-beta-gamma profile of Di Cintio\net al. (2014, DC14). Our Bayesian analysis shows that DC14 performs as well as\nor better than the other profiles in >65% of the sample. We find that the\nkinematically inferred stellar masses agree with values from SED fitting. We\nfind that 89% of galaxies have DM fractions >50%. For 70% of SFGs, we infer a\nDM inner slope, gamma < 0.5, indicating cored DM profiles, but no correlation\nis found between gamma and star formation rate of the sample. The stellar- and\nconcentration-mass relations agree with theoretical expectations, but with\nlarger scatter. We confirm the anticorrelation between halo scale radius and DM\ndensity. The halo scale radii and DM surface densities increase with Mstar,\nwhile DM densities stay constant. We find tentative evidence of an evolution of\nthe DM density with z, which suggests that the DM halos of intermediate-z\nsystems are denser than those of local galaxies. In contrast, the halo scale\nradii are z-invariant."
                },
                "authors": [
                    {
                        "name": "B. I. Ciocan"
                    },
                    {
                        "name": "N. F. Bouché"
                    },
                    {
                        "name": "J. Fensch"
                    },
                    {
                        "name": "W. Mercier"
                    },
                    {
                        "name": "D. Krajnović"
                    },
                    {
                        "name": "J. Richard"
                    },
                    {
                        "name": "T. Contini"
                    },
                    {
                        "name": "A. Jeanneau"
                    }
                ],
                "author_detail": {
                    "name": "A. Jeanneau"
                },
                "author": "A. Jeanneau",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13339v2",
                "updated": "2025-06-24T15:14:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    14,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2023-10-20T08:07:44Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    8,
                    7,
                    44,
                    4,
                    293,
                    0
                ],
                "title": "Bootstrap-based tests for the total time on test and the excess wealth\n  orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap-based tests for the total time on test and the excess wealth\n  orders"
                },
                "summary": "Given a pair of non-negative random variables $X$ and $Y$, we introduce a\nclass of nonparametric tests for the null hypothesis that $X$ dominates $Y$ in\nthe total time on test order. Critical values are determined using\nbootstrap-based inference, and the tests are shown to be consistent. The same\napproach is used to construct tests for the excess wealth order. As a\nbyproduct, we also obtain a class of goodness-of-fit tests for the NBUE (New\nBetter than Used in Expectation) family of distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a pair of non-negative random variables $X$ and $Y$, we introduce a\nclass of nonparametric tests for the null hypothesis that $X$ dominates $Y$ in\nthe total time on test order. Critical values are determined using\nbootstrap-based inference, and the tests are shown to be consistent. The same\napproach is used to construct tests for the excess wealth order. As a\nbyproduct, we also obtain a class of goodness-of-fit tests for the NBUE (New\nBetter than Used in Expectation) family of distributions."
                },
                "authors": [
                    {
                        "name": "Tommaso Lando"
                    },
                    {
                        "name": "Sirio Legramanti"
                    }
                ],
                "author_detail": {
                    "name": "Sirio Legramanti"
                },
                "author": "Sirio Legramanti",
                "arxiv_doi": "10.1016/j.jspi.2025.106315",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jspi.2025.106315",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G10, 60E15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19702v1",
                "updated": "2025-06-24T15:12:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    12,
                    42,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:12:42Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    12,
                    42,
                    1,
                    175,
                    0
                ],
                "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology\n  and Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology\n  and Differential Diagnosis"
                },
                "summary": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}."
                },
                "authors": [
                    {
                        "name": "Lei Kang"
                    },
                    {
                        "name": "Xuanshuo Fu"
                    },
                    {
                        "name": "Oriol Ramos Terrades"
                    },
                    {
                        "name": "Javier Vazquez-Corral"
                    },
                    {
                        "name": "Ernest Valveny"
                    },
                    {
                        "name": "Dimosthenis Karatzas"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Karatzas"
                },
                "author": "Dimosthenis Karatzas",
                "arxiv_comment": "Accepted at ICDAR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22408v2",
                "updated": "2025-06-24T15:07:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    7,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-28T13:17:58Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    13,
                    17,
                    58,
                    4,
                    87,
                    0
                ],
                "title": "Smart Sensing Breaks the Accuracy Barrier in Battery State Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Sensing Breaks the Accuracy Barrier in Battery State Monitoring"
                },
                "summary": "Accurate state-of-charge (SOC) estimation is essential for optimizing battery\nperformance, ensuring safety, and maximizing economic value. Conventional\ncurrent and voltage measurements, however, have inherent limitations in fully\ninferring the multiphysics-resolved dynamics inside battery cells. This creates\nan accuracy barrier that constrains battery usage and reduces\ncost-competitiveness and sustainability across industries dependent on battery\ntechnology. In this work, we introduce an integrated sensor framework that\ncombines novel mechanical, thermal, gas, optical, and electrical sensors with\ntraditional measurements to break through this barrier. We generate three\nunique datasets with eleven measurement types and propose an explainable\nmachine-learning approach for SOC estimation. This approach renders the\nmeasured signals and the predictive result of machine learning physically\ninterpretable with respect to battery SOC, offering fundamental insights into\nthe time-varying importance of different signals. Our experimental results\nreveal a marked increase in SOC estimation accuracy--enhanced from 46.1% to\n74.5%--compared to conventional methods. This approach not only advances SOC\nmonitoring precision but also establishes a foundation for monitoring\nadditional battery states to further improve safety, extend lifespan, and\nfacilitate fast charging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate state-of-charge (SOC) estimation is essential for optimizing battery\nperformance, ensuring safety, and maximizing economic value. Conventional\ncurrent and voltage measurements, however, have inherent limitations in fully\ninferring the multiphysics-resolved dynamics inside battery cells. This creates\nan accuracy barrier that constrains battery usage and reduces\ncost-competitiveness and sustainability across industries dependent on battery\ntechnology. In this work, we introduce an integrated sensor framework that\ncombines novel mechanical, thermal, gas, optical, and electrical sensors with\ntraditional measurements to break through this barrier. We generate three\nunique datasets with eleven measurement types and propose an explainable\nmachine-learning approach for SOC estimation. This approach renders the\nmeasured signals and the predictive result of machine learning physically\ninterpretable with respect to battery SOC, offering fundamental insights into\nthe time-varying importance of different signals. Our experimental results\nreveal a marked increase in SOC estimation accuracy--enhanced from 46.1% to\n74.5%--compared to conventional methods. This approach not only advances SOC\nmonitoring precision but also establishes a foundation for monitoring\nadditional battery states to further improve safety, extend lifespan, and\nfacilitate fast charging."
                },
                "authors": [
                    {
                        "name": "Xiaolei Bian"
                    },
                    {
                        "name": "Changfu Zou"
                    },
                    {
                        "name": "Björn Fridholm"
                    },
                    {
                        "name": "Christian Sundvall"
                    },
                    {
                        "name": "Torsten Wik"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Wik"
                },
                "author": "Torsten Wik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19697v1",
                "updated": "2025-06-24T15:03:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    3,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:03:57Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    3,
                    57,
                    1,
                    175,
                    0
                ],
                "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models"
                },
                "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19693v1",
                "updated": "2025-06-24T15:00:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    0,
                    14,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:00:14Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    0,
                    14,
                    1,
                    175,
                    0
                ],
                "title": "ReBoot: Encrypted Training of Deep Neural Networks with CKKS\n  Bootstrapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReBoot: Encrypted Training of Deep Neural Networks with CKKS\n  Bootstrapping"
                },
                "summary": "Growing concerns over data privacy underscore the need for deep learning\nmethods capable of processing sensitive information without compromising\nconfidentiality. Among privacy-enhancing technologies, Homomorphic Encryption\n(HE) stands out by providing post-quantum cryptographic security and end-to-end\ndata protection, safeguarding data even during computation. While Deep Neural\nNetworks (DNNs) have gained attention in HE settings, their use has largely\nbeen restricted to encrypted inference. Prior research on encrypted training\nhas primarily focused on logistic regression or has relied on multi-party\ncomputation to enable model fine-tuning. This stems from the substantial\ncomputational overhead and algorithmic complexity involved in DNNs training\nunder HE. In this paper, we present ReBoot, the first framework to enable fully\nencrypted and non-interactive training of DNNs. Built upon the CKKS scheme,\nReBoot introduces a novel HE-compliant neural network architecture based on\nlocal error signals, specifically designed to minimize multiplicative depth and\nreduce noise accumulation. ReBoot employs a tailored packing strategy that\nleverages real-number arithmetic via SIMD operations, significantly lowering\nboth computational and memory overhead. Furthermore, by integrating approximate\nbootstrapping, ReBoot learning algorithm supports effective training of\narbitrarily deep multi-layer perceptrons, making it well-suited for machine\nlearning as-a-service. ReBoot is evaluated on both image recognition and\ntabular benchmarks, achieving accuracy comparable to 32-bit floating-point\nplaintext training while enabling fully encrypted training. It improves test\naccuracy by up to +3.27% over encrypted logistic regression, and up to +6.83%\nover existing encrypted DNN frameworks, while reducing training latency by up\nto 8.83x. ReBoot is made available to the scientific community as a public\nrepository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing concerns over data privacy underscore the need for deep learning\nmethods capable of processing sensitive information without compromising\nconfidentiality. Among privacy-enhancing technologies, Homomorphic Encryption\n(HE) stands out by providing post-quantum cryptographic security and end-to-end\ndata protection, safeguarding data even during computation. While Deep Neural\nNetworks (DNNs) have gained attention in HE settings, their use has largely\nbeen restricted to encrypted inference. Prior research on encrypted training\nhas primarily focused on logistic regression or has relied on multi-party\ncomputation to enable model fine-tuning. This stems from the substantial\ncomputational overhead and algorithmic complexity involved in DNNs training\nunder HE. In this paper, we present ReBoot, the first framework to enable fully\nencrypted and non-interactive training of DNNs. Built upon the CKKS scheme,\nReBoot introduces a novel HE-compliant neural network architecture based on\nlocal error signals, specifically designed to minimize multiplicative depth and\nreduce noise accumulation. ReBoot employs a tailored packing strategy that\nleverages real-number arithmetic via SIMD operations, significantly lowering\nboth computational and memory overhead. Furthermore, by integrating approximate\nbootstrapping, ReBoot learning algorithm supports effective training of\narbitrarily deep multi-layer perceptrons, making it well-suited for machine\nlearning as-a-service. ReBoot is evaluated on both image recognition and\ntabular benchmarks, achieving accuracy comparable to 32-bit floating-point\nplaintext training while enabling fully encrypted training. It improves test\naccuracy by up to +3.27% over encrypted logistic regression, and up to +6.83%\nover existing encrypted DNN frameworks, while reducing training latency by up\nto 8.83x. ReBoot is made available to the scientific community as a public\nrepository."
                },
                "authors": [
                    {
                        "name": "Alberto Pirillo"
                    },
                    {
                        "name": "Luca Colombo"
                    }
                ],
                "author_detail": {
                    "name": "Luca Colombo"
                },
                "author": "Luca Colombo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19683v1",
                "updated": "2025-06-24T14:49:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:49:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance"
                },
                "summary": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries."
                },
                "authors": [
                    {
                        "name": "Xuesong Li"
                    },
                    {
                        "name": "Dianye Huang"
                    },
                    {
                        "name": "Yameng Zhang"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Jiang"
                },
                "author": "Zhongliang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15196v2",
                "updated": "2025-06-24T14:48:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    48,
                    12,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-18T07:20:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    20,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges"
                },
                "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix."
                },
                "authors": [
                    {
                        "name": "Xianliang Yang"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Haolong Qian"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "27 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19681v1",
                "updated": "2025-06-24T14:48:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    48,
                    12,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:48:12Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    48,
                    12,
                    1,
                    175,
                    0
                ],
                "title": "Genome-Anchored Foundation Model Embeddings Improve Molecular Prediction\n  from Histology Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genome-Anchored Foundation Model Embeddings Improve Molecular Prediction\n  from Histology Images"
                },
                "summary": "Precision oncology requires accurate molecular insights, yet obtaining these\ndirectly from genomics is costly and time-consuming for broad clinical use.\nPredicting complex molecular features and patient prognosis directly from\nroutine whole-slide images (WSI) remains a major challenge for current deep\nlearning methods. Here we introduce PathLUPI, which uses transcriptomic\nprivileged information during training to extract genome-anchored histological\nembeddings, enabling effective molecular prediction using only WSIs at\ninference. Through extensive evaluation across 49 molecular oncology tasks\nusing 11,257 cases among 20 cohorts, PathLUPI demonstrated superior performance\ncompared to conventional methods trained solely on WSIs. Crucially, it achieves\nAUC $\\geq$ 0.80 in 14 of the biomarker prediction and molecular subtyping tasks\nand C-index $\\geq$ 0.70 in survival cohorts of 5 major cancer types. Moreover,\nPathLUPI embeddings reveal distinct cellular morphological signatures\nassociated with specific genotypes and related biological pathways within WSIs.\nBy effectively encoding molecular context to refine WSI representations,\nPathLUPI overcomes a key limitation of existing models and offers a novel\nstrategy to bridge molecular insights with routine pathology workflows for\nwider clinical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision oncology requires accurate molecular insights, yet obtaining these\ndirectly from genomics is costly and time-consuming for broad clinical use.\nPredicting complex molecular features and patient prognosis directly from\nroutine whole-slide images (WSI) remains a major challenge for current deep\nlearning methods. Here we introduce PathLUPI, which uses transcriptomic\nprivileged information during training to extract genome-anchored histological\nembeddings, enabling effective molecular prediction using only WSIs at\ninference. Through extensive evaluation across 49 molecular oncology tasks\nusing 11,257 cases among 20 cohorts, PathLUPI demonstrated superior performance\ncompared to conventional methods trained solely on WSIs. Crucially, it achieves\nAUC $\\geq$ 0.80 in 14 of the biomarker prediction and molecular subtyping tasks\nand C-index $\\geq$ 0.70 in survival cohorts of 5 major cancer types. Moreover,\nPathLUPI embeddings reveal distinct cellular morphological signatures\nassociated with specific genotypes and related biological pathways within WSIs.\nBy effectively encoding molecular context to refine WSI representations,\nPathLUPI overcomes a key limitation of existing models and offers a novel\nstrategy to bridge molecular insights with routine pathology workflows for\nwider clinical application."
                },
                "authors": [
                    {
                        "name": "Cheng Jin"
                    },
                    {
                        "name": "Fengtao Zhou"
                    },
                    {
                        "name": "Yunfang Yu"
                    },
                    {
                        "name": "Jiabo Ma"
                    },
                    {
                        "name": "Yihui Wang"
                    },
                    {
                        "name": "Yingxue Xu"
                    },
                    {
                        "name": "Huajun Zhou"
                    },
                    {
                        "name": "Hao Jiang"
                    },
                    {
                        "name": "Luyang Luo"
                    },
                    {
                        "name": "Luhui Mao"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Xiuming Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Ronald Chan"
                    },
                    {
                        "name": "Herui Yao"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18808v2",
                "updated": "2025-06-24T14:44:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T16:19:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    19,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (I): All confounders measured",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (I): All confounders measured"
                },
                "summary": "Whether a variable is the cause of another, or simply associated with it, is\noften an important scientific question. Causal Inference is the name associated\nwith the body of techniques for addressing that question in a statistical\nsetting. Although assessing causality is relatively straightforward in the\npresence of temporal information, outside of that setting - the situation\nconsidered here - it is more difficult to assess causal effects. The\ndevelopment of the field of causal inference has involved concepts from a wide\nrange of topics, thereby limiting its adoption across some fields, including\nmeteorology. However, at its core, the requisite knowledge for causal inference\ninvolves little more than basic probability theory and regression, topics\nfamiliar to most meteorologists. By focusing on these core areas, this and a\ncompanion article provide a steppingstone for the meteorology community into\nthe field of (non-temporal) causal inference. Although some theoretical\nfoundations are presented, the main goal is the application of a specific\nmethod, called matching, to a problem in meteorology. The data for the\napplication are in public domain, and R code is provided as well, forming an\neasy path for meteorology students and researchers to enter the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether a variable is the cause of another, or simply associated with it, is\noften an important scientific question. Causal Inference is the name associated\nwith the body of techniques for addressing that question in a statistical\nsetting. Although assessing causality is relatively straightforward in the\npresence of temporal information, outside of that setting - the situation\nconsidered here - it is more difficult to assess causal effects. The\ndevelopment of the field of causal inference has involved concepts from a wide\nrange of topics, thereby limiting its adoption across some fields, including\nmeteorology. However, at its core, the requisite knowledge for causal inference\ninvolves little more than basic probability theory and regression, topics\nfamiliar to most meteorologists. By focusing on these core areas, this and a\ncompanion article provide a steppingstone for the meteorology community into\nthe field of (non-temporal) causal inference. Although some theoretical\nfoundations are presented, the main goal is the application of a specific\nmethod, called matching, to a problem in meteorology. The data for the\napplication are in public domain, and R code is provided as well, forming an\neasy path for meteorology students and researchers to enter the field."
                },
                "authors": [
                    {
                        "name": "Caren Marzban"
                    },
                    {
                        "name": "Yikun Zhang"
                    },
                    {
                        "name": "Nicholas Bond"
                    },
                    {
                        "name": "Michael Richman"
                    }
                ],
                "author_detail": {
                    "name": "Michael Richman"
                },
                "author": "Michael Richman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19677v1",
                "updated": "2025-06-24T14:44:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:44:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"
                },
                "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."
                },
                "authors": [
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v1",
                "updated": "2025-06-24T14:44:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19665v1",
                "updated": "2025-06-24T14:29:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    29,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:29:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    29,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report\n  Generation"
                },
                "summary": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuanhe Tian"
                    },
                    {
                        "name": "Lei Mao"
                    },
                    {
                        "name": "Yan Song"
                    }
                ],
                "author_detail": {
                    "name": "Yan Song"
                },
                "author": "Yan Song",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v3",
                "updated": "2025-06-24T14:21:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    21,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19655v1",
                "updated": "2025-06-24T14:18:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    18,
                    19,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    18,
                    19,
                    1,
                    175,
                    0
                ],
                "title": "How trust networks shape students' opinions about the proficiency of\n  artificially intelligent assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How trust networks shape students' opinions about the proficiency of\n  artificially intelligent assistants"
                },
                "summary": "The rising use of educational tools controlled by artificial intelligence\n(AI) has provoked a debate about their proficiency. While intrinsic\nproficiency, especially in tasks such as grading, has been measured and studied\nextensively, perceived proficiency remains underexplored. Here it is shown\nthrough Monte Carlo multi-agent simulations that trust networks among students\ninfluence their perceptions of the proficiency of an AI tool. A probabilistic\nopinion dynamics model is constructed, in which every student's perceptions are\ndescribed by a probability density function (PDF), which is updated at every\ntime step through independent, personal observations and peer pressure shaped\nby trust relationships. It is found that students infer correctly the AI tool's\nproficiency $\\theta_{\\rm AI}$ in allies-only networks (i.e.\\ high trust\nnetworks). AI-avoiders reach asymptotic learning faster than AI-users, and the\nasymptotic learning time for AI-users decreases as their number increases.\nHowever, asymptotic learning is disrupted even by a single partisan, who is\nstubbornly incorrect in their belief $\\theta_{\\rm p} \\neq \\theta_{\\rm AI}$,\nmaking other students' beliefs vacillate indefinitely between $\\theta_{\\rm p}$\nand $\\theta_{\\rm AI}$. In opponents-only (low trust) networks, all students\nreach asymptotic learning, but only a minority infer $\\theta_{\\rm AI}$\ncorrectly. AI-users have a small advantage over AI-avoiders in reaching the\nright conclusion. In mixed networks, students may exhibit turbulent\nnonconvergence and intermittency, or achieve asymptotic learning, depending on\nthe relationships between partisans and AI-users. The educational implications\nof the results are discussed briefly in the context of designing robust usage\npolicies for AI tools, with an emphasis on the unintended and inequitable\nconsequences which arise sometimes from counterintuitive network effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising use of educational tools controlled by artificial intelligence\n(AI) has provoked a debate about their proficiency. While intrinsic\nproficiency, especially in tasks such as grading, has been measured and studied\nextensively, perceived proficiency remains underexplored. Here it is shown\nthrough Monte Carlo multi-agent simulations that trust networks among students\ninfluence their perceptions of the proficiency of an AI tool. A probabilistic\nopinion dynamics model is constructed, in which every student's perceptions are\ndescribed by a probability density function (PDF), which is updated at every\ntime step through independent, personal observations and peer pressure shaped\nby trust relationships. It is found that students infer correctly the AI tool's\nproficiency $\\theta_{\\rm AI}$ in allies-only networks (i.e.\\ high trust\nnetworks). AI-avoiders reach asymptotic learning faster than AI-users, and the\nasymptotic learning time for AI-users decreases as their number increases.\nHowever, asymptotic learning is disrupted even by a single partisan, who is\nstubbornly incorrect in their belief $\\theta_{\\rm p} \\neq \\theta_{\\rm AI}$,\nmaking other students' beliefs vacillate indefinitely between $\\theta_{\\rm p}$\nand $\\theta_{\\rm AI}$. In opponents-only (low trust) networks, all students\nreach asymptotic learning, but only a minority infer $\\theta_{\\rm AI}$\ncorrectly. AI-users have a small advantage over AI-avoiders in reaching the\nright conclusion. In mixed networks, students may exhibit turbulent\nnonconvergence and intermittency, or achieve asymptotic learning, depending on\nthe relationships between partisans and AI-users. The educational implications\nof the results are discussed briefly in the context of designing robust usage\npolicies for AI tools, with an emphasis on the unintended and inequitable\nconsequences which arise sometimes from counterintuitive network effects."
                },
                "authors": [
                    {
                        "name": "Yutong Bu"
                    },
                    {
                        "name": "Andrew Melatos"
                    },
                    {
                        "name": "Robin Evans"
                    }
                ],
                "author_detail": {
                    "name": "Robin Evans"
                },
                "author": "Robin Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19652v1",
                "updated": "2025-06-24T14:15:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:15:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"
                },
                "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."
                },
                "authors": [
                    {
                        "name": "Lucie Galland"
                    },
                    {
                        "name": "Catherine Pelachaud"
                    },
                    {
                        "name": "Florian Pecune"
                    }
                ],
                "author_detail": {
                    "name": "Florian Pecune"
                },
                "author": "Florian Pecune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19651v1",
                "updated": "2025-06-24T14:14:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    14,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:14:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    14,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "PEVLM: Parallel Encoding for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEVLM: Parallel Encoding for Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated strong performance in\nvideo-language tasks, yet their application to long video understanding remains\nconstrained by the quadratic complexity of standard attention mechanisms. In\nthis paper, we propose \\textbf{PEVLM}, a parallel encoding strategy\nspecifically designed to improve the prefill efficiency of VLMs without\nrequiring model finetuning. PEVLM partitions the input into block-wise segments\nwith a shared sink, preserves full-attention positional embeddings, and aligns\nattention weights to mimic full-attention distributions. This design reduces\nattention computation from $O((T \\times N)^2)$ to $O(T \\times N)$ while\nmaintaining high accuracy. Extensive experiments on the LongVideoBench\nbenchmark show that PEVLM achieves up to 8.37\\% accuracy improvement over\nexisting inference-efficient methods and delivers up to 7.47x speedup in\nattention computation and 40\\% reduction in end-to-end latency. Under strict\nlatency constraints, PEVLM significantly outperforms baselines, raising\naccuracy from 23.26\\% to 61.03\\%. These results highlight PEVLM's effectiveness\nfor low-latency, long-context video understanding, making it well-suited for\nreal-world applications such as autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated strong performance in\nvideo-language tasks, yet their application to long video understanding remains\nconstrained by the quadratic complexity of standard attention mechanisms. In\nthis paper, we propose \\textbf{PEVLM}, a parallel encoding strategy\nspecifically designed to improve the prefill efficiency of VLMs without\nrequiring model finetuning. PEVLM partitions the input into block-wise segments\nwith a shared sink, preserves full-attention positional embeddings, and aligns\nattention weights to mimic full-attention distributions. This design reduces\nattention computation from $O((T \\times N)^2)$ to $O(T \\times N)$ while\nmaintaining high accuracy. Extensive experiments on the LongVideoBench\nbenchmark show that PEVLM achieves up to 8.37\\% accuracy improvement over\nexisting inference-efficient methods and delivers up to 7.47x speedup in\nattention computation and 40\\% reduction in end-to-end latency. Under strict\nlatency constraints, PEVLM significantly outperforms baselines, raising\naccuracy from 23.26\\% to 61.03\\%. These results highlight PEVLM's effectiveness\nfor low-latency, long-context video understanding, making it well-suited for\nreal-world applications such as autonomous driving."
                },
                "authors": [
                    {
                        "name": "Letian Kang"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Yiqiang Li"
                    },
                    {
                        "name": "Xiaoyang Yu"
                    },
                    {
                        "name": "Shenxuan Zhou"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19646v1",
                "updated": "2025-06-24T14:11:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    11,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:11:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    11,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "Learning to Solve Parametric Mixed-Integer Optimal Control Problems via\n  Differentiable Predictive Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Solve Parametric Mixed-Integer Optimal Control Problems via\n  Differentiable Predictive Control"
                },
                "summary": "We propose a novel approach to solving input- and state-constrained\nparametric mixed-integer optimal control problems using Differentiable\nPredictive Control (DPC). Our approach follows the differentiable programming\nparadigm by learning an explicit neural policy that maps control parameters to\ninteger- and continuous-valued decision variables. This policy is optimized via\nstochastic gradient descent by differentiating the quadratic model predictive\ncontrol objective through the closed-loop finite-horizon response of the system\ndynamics. To handle integrality constraints, we incorporate three\ndifferentiable rounding strategies. The approach is evaluated on a conceptual\nthermal energy system, comparing its performance with the optimal solution for\ndifferent lengths of the prediction horizon. The simulation results indicate\nthat our self-supervised learning approach can achieve near-optimal control\nperformance while significantly reducing inference time by avoiding online\noptimization, thus implying its potential for embedded deployment even on edge\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach to solving input- and state-constrained\nparametric mixed-integer optimal control problems using Differentiable\nPredictive Control (DPC). Our approach follows the differentiable programming\nparadigm by learning an explicit neural policy that maps control parameters to\ninteger- and continuous-valued decision variables. This policy is optimized via\nstochastic gradient descent by differentiating the quadratic model predictive\ncontrol objective through the closed-loop finite-horizon response of the system\ndynamics. To handle integrality constraints, we incorporate three\ndifferentiable rounding strategies. The approach is evaluated on a conceptual\nthermal energy system, comparing its performance with the optimal solution for\ndifferent lengths of the prediction horizon. The simulation results indicate\nthat our self-supervised learning approach can achieve near-optimal control\nperformance while significantly reducing inference time by avoiding online\noptimization, thus implying its potential for embedded deployment even on edge\ndevices."
                },
                "authors": [
                    {
                        "name": "Ján Boldocký"
                    },
                    {
                        "name": "Shahriar Dadras Javan"
                    },
                    {
                        "name": "Martin Gulan"
                    },
                    {
                        "name": "Martin Mönnigmann"
                    },
                    {
                        "name": "Ján Drgoňa"
                    }
                ],
                "author_detail": {
                    "name": "Ján Drgoňa"
                },
                "author": "Ján Drgoňa",
                "arxiv_comment": "7 pages, 2 figures, 1 algorithm, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19645v1",
                "updated": "2025-06-24T14:09:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    9,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:09:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    9,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "Tensor-Parallelism with Partially Synchronized Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor-Parallelism with Partially Synchronized Activations"
                },
                "summary": "Training and inference of Large Language Models (LLMs) with\ntensor-parallelism requires substantial communication to synchronize\nactivations. Our findings suggest that with a few minor adjustments to current\npractices, LLMs can be trained without fully synchronizing activations,\nreducing bandwidth demands. We name this \"Communication-Aware Architecture for\nTensor-parallelism\" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,\nwith a 50% reduction in tensor-parallel communication and no significant drop\nin pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates\nboth training and inference workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and inference of Large Language Models (LLMs) with\ntensor-parallelism requires substantial communication to synchronize\nactivations. Our findings suggest that with a few minor adjustments to current\npractices, LLMs can be trained without fully synchronizing activations,\nreducing bandwidth demands. We name this \"Communication-Aware Architecture for\nTensor-parallelism\" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,\nwith a 50% reduction in tensor-parallel communication and no significant drop\nin pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates\nboth training and inference workloads."
                },
                "authors": [
                    {
                        "name": "Itay Lamprecht"
                    },
                    {
                        "name": "Asaf Karnieli"
                    },
                    {
                        "name": "Yair Hanani"
                    },
                    {
                        "name": "Niv Giladi"
                    },
                    {
                        "name": "Daniel Soudry"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Soudry"
                },
                "author": "Daniel Soudry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06877v2",
                "updated": "2025-06-24T13:55:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    55,
                    38,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-07T17:54:56Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    17,
                    54,
                    56,
                    5,
                    158,
                    0
                ],
                "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training\n  LLMs for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training\n  LLMs for Math Reasoning"
                },
                "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Jiaxing Guo"
                    },
                    {
                        "name": "Wenjie Yang"
                    },
                    {
                        "name": "Shengzhong Zhang"
                    },
                    {
                        "name": "Tongshan Xu"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Zengfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zengfeng Huang"
                },
                "author": "Zengfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19624v1",
                "updated": "2025-06-24T13:42:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    42,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:42:59Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    42,
                    59,
                    1,
                    175,
                    0
                ],
                "title": "Decompiling Smart Contracts with a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiling Smart Contracts with a Large Language Model"
                },
                "summary": "The widespread lack of broad source code verification on blockchain explorers\nsuch as Etherscan, where despite 78,047,845 smart contracts deployed on\nEthereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents\na severe impediment to blockchain security. This opacity necessitates the\nautomated semantic analysis of on-chain smart contract bytecode, a fundamental\nresearch challenge with direct implications for identifying vulnerabilities and\nunderstanding malicious behavior. Prevailing decompilers struggle to reverse\nbytecode in a readable manner, often yielding convoluted code that critically\nhampers vulnerability analysis and thwarts efforts to dissect contract\nfunctionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation\npipeline that, for the first time, successfully leverages Large Language Models\n(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable\nand semantically faithful Solidity code. Our novel methodology first employs\nrigorous static program analysis to convert bytecode into a structured\nthree-address code (TAC) representation. This intermediate representation then\nguides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset\nof 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.\nThis approach uniquely recovers meaningful variable names, intricate control\nflow, and precise function signatures. Our extensive empirical evaluation\ndemonstrates a significant leap beyond traditional decompilers, achieving an\naverage semantic similarity of 0.82 with original source and markedly superior\nreadability. The practical viability and effectiveness of our research are\ndemonstrated through its implementation in a publicly accessible system,\navailable at https://evmdecompiler.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread lack of broad source code verification on blockchain explorers\nsuch as Etherscan, where despite 78,047,845 smart contracts deployed on\nEthereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents\na severe impediment to blockchain security. This opacity necessitates the\nautomated semantic analysis of on-chain smart contract bytecode, a fundamental\nresearch challenge with direct implications for identifying vulnerabilities and\nunderstanding malicious behavior. Prevailing decompilers struggle to reverse\nbytecode in a readable manner, often yielding convoluted code that critically\nhampers vulnerability analysis and thwarts efforts to dissect contract\nfunctionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation\npipeline that, for the first time, successfully leverages Large Language Models\n(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable\nand semantically faithful Solidity code. Our novel methodology first employs\nrigorous static program analysis to convert bytecode into a structured\nthree-address code (TAC) representation. This intermediate representation then\nguides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset\nof 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.\nThis approach uniquely recovers meaningful variable names, intricate control\nflow, and precise function signatures. Our extensive empirical evaluation\ndemonstrates a significant leap beyond traditional decompilers, achieving an\naverage semantic similarity of 0.82 with original source and markedly superior\nreadability. The practical viability and effectiveness of our research are\ndemonstrated through its implementation in a publicly accessible system,\navailable at https://evmdecompiler.com."
                },
                "authors": [
                    {
                        "name": "Isaac David"
                    },
                    {
                        "name": "Liyi Zhou"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Arthur Gervais"
                    },
                    {
                        "name": "Kaihua Qin"
                    }
                ],
                "author_detail": {
                    "name": "Kaihua Qin"
                },
                "author": "Kaihua Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05153v2",
                "updated": "2025-06-24T13:24:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    24,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2024-12-06T16:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "title": "A text-to-tabular approach to generate synthetic patient data using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A text-to-tabular approach to generate synthetic patient data using LLMs"
                },
                "summary": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources."
                },
                "authors": [
                    {
                        "name": "Margaux Tornqvist"
                    },
                    {
                        "name": "Jean-Daniel Zucker"
                    },
                    {
                        "name": "Tristan Fauvel"
                    },
                    {
                        "name": "Nicolas Lambert"
                    },
                    {
                        "name": "Mathilde Berthelot"
                    },
                    {
                        "name": "Antoine Movschin"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Movschin"
                },
                "author": "Antoine Movschin",
                "arxiv_comment": "12 pages, 3 figures. Accepted to the 2025 IEEE International\n  Conference on Healthcare Informatics (IEEE ICHI 2025), 2025, Rende (CS),\n  Calabria, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19609v1",
                "updated": "2025-06-24T13:22:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    22,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:22:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    22,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "Beyond Static Models: Hypernetworks for Adaptive and Generalizable\n  Forecasting in Complex Parametric Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Static Models: Hypernetworks for Adaptive and Generalizable\n  Forecasting in Complex Parametric Dynamical Systems"
                },
                "summary": "Dynamical systems play a key role in modeling, forecasting, and\ndecision-making across a wide range of scientific domains. However, variations\nin system parameters, also referred to as parametric variability, can lead to\ndrastically different model behavior and output, posing challenges for\nconstructing models that generalize across parameter regimes. In this work, we\nintroduce the Parametric Hypernetwork for Learning Interpolated Networks\n(PHLieNet), a framework that simultaneously learns: (a) a global mapping from\nthe parameter space to a nonlinear embedding and (b) a mapping from the\ninferred embedding to the weights of a dynamics propagation network. The\nlearned embedding serves as a latent representation that modulates a base\nnetwork, termed the hypernetwork, enabling it to generate the weights of a\ntarget network responsible for forecasting the system's state evolution\nconditioned on the previous time history. By interpolating in the space of\nmodels rather than observations, PHLieNet facilitates smooth transitions across\nparameterized system behaviors, enabling a unified model that captures the\ndynamic behavior across a broad range of system parameterizations. The\nperformance of the proposed technique is validated in a series of dynamical\nsystems with respect to its ability to extrapolate in time and interpolate and\nextrapolate in the parameter space, i.e., generalize to dynamics that were\nunseen during training. In all cases, our approach outperforms or matches\nstate-of-the-art baselines in both short-term forecast accuracy and in\ncapturing long-term dynamical features, such as attractor statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical systems play a key role in modeling, forecasting, and\ndecision-making across a wide range of scientific domains. However, variations\nin system parameters, also referred to as parametric variability, can lead to\ndrastically different model behavior and output, posing challenges for\nconstructing models that generalize across parameter regimes. In this work, we\nintroduce the Parametric Hypernetwork for Learning Interpolated Networks\n(PHLieNet), a framework that simultaneously learns: (a) a global mapping from\nthe parameter space to a nonlinear embedding and (b) a mapping from the\ninferred embedding to the weights of a dynamics propagation network. The\nlearned embedding serves as a latent representation that modulates a base\nnetwork, termed the hypernetwork, enabling it to generate the weights of a\ntarget network responsible for forecasting the system's state evolution\nconditioned on the previous time history. By interpolating in the space of\nmodels rather than observations, PHLieNet facilitates smooth transitions across\nparameterized system behaviors, enabling a unified model that captures the\ndynamic behavior across a broad range of system parameterizations. The\nperformance of the proposed technique is validated in a series of dynamical\nsystems with respect to its ability to extrapolate in time and interpolate and\nextrapolate in the parameter space, i.e., generalize to dynamics that were\nunseen during training. In all cases, our approach outperforms or matches\nstate-of-the-art baselines in both short-term forecast accuracy and in\ncapturing long-term dynamical features, such as attractor statistics."
                },
                "authors": [
                    {
                        "name": "Pantelis R. Vlachas"
                    },
                    {
                        "name": "Konstantinos Vlachas"
                    },
                    {
                        "name": "Eleni Chatzi"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Chatzi"
                },
                "author": "Eleni Chatzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19607v1",
                "updated": "2025-06-24T13:20:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    20,
                    31,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:20:31Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    20,
                    31,
                    1,
                    175,
                    0
                ],
                "title": "Correcting Hallucinations in News Summaries: Exploration of\n  Self-Correcting LLM Methods with External Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting Hallucinations in News Summaries: Exploration of\n  Self-Correcting LLM Methods with External Knowledge"
                },
                "summary": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ihsan Soydemir"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to FEVER @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01799v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01799v3",
                "updated": "2025-06-24T13:11:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    11,
                    54,
                    1,
                    175,
                    0
                ],
                "published": "2024-04-02T09:58:57Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    9,
                    58,
                    57,
                    1,
                    93,
                    0
                ],
                "title": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language\n  Models against Human Populations: A Case Study of Proficiency in 8th Grade\n  Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language\n  Models against Human Populations: A Case Study of Proficiency in 8th Grade\n  Mathematics"
                },
                "summary": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations."
                },
                "authors": [
                    {
                        "name": "Qixiang Fang"
                    },
                    {
                        "name": "Daniel L. Oberski"
                    },
                    {
                        "name": "Dong Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Nguyen"
                },
                "author": "Dong Nguyen",
                "arxiv_comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01799v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01799v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14677v2",
                "updated": "2025-06-24T13:11:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    11,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-17T16:08:48Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    16,
                    8,
                    48,
                    1,
                    168,
                    0
                ],
                "title": "Human-Centered Editable Speech-to-Sign-Language Generation via Streaming\n  Conformer-Transformer and Resampling Hook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Centered Editable Speech-to-Sign-Language Generation via Streaming\n  Conformer-Transformer and Resampling Hook"
                },
                "summary": "Existing end-to-end sign-language animation systems suffer from low\nnaturalness, limited facial/body expressivity, and no user control. We propose\na human-centered, real-time speech-to-sign animation framework that integrates\n(1) a streaming Conformer encoder with an autoregressive Transformer-MDN\ndecoder for synchronized upper-body and facial motion generation, (2) a\ntransparent, editable JSON intermediate representation empowering deaf users\nand experts to inspect and modify each sign segment, and (3) a\nhuman-in-the-loop optimization loop that refines the model based on user edits\nand ratings. Deployed on Unity3D, our system achieves a 13 ms average\nframe-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key\ncontributions include the design of a JSON-centric editing mechanism for\nfine-grained sign-level personalization and the first application of an\nMDN-based feedback loop for continuous model adaptation. This combination\nestablishes a generalizable, explainable AI paradigm for user-adaptive,\nlow-latency multimodal systems. In studies with 20 deaf signers and 5\nprofessional interpreters, we observe a +13 point SUS improvement, 6.7 point\nreduction in cognitive load, and significant gains in naturalness and trust (p\n$<$ .001) over baselines. This work establishes a scalable, explainable AI\nparadigm for accessible sign-language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing end-to-end sign-language animation systems suffer from low\nnaturalness, limited facial/body expressivity, and no user control. We propose\na human-centered, real-time speech-to-sign animation framework that integrates\n(1) a streaming Conformer encoder with an autoregressive Transformer-MDN\ndecoder for synchronized upper-body and facial motion generation, (2) a\ntransparent, editable JSON intermediate representation empowering deaf users\nand experts to inspect and modify each sign segment, and (3) a\nhuman-in-the-loop optimization loop that refines the model based on user edits\nand ratings. Deployed on Unity3D, our system achieves a 13 ms average\nframe-inference time and a 103 ms end-to-end latency on an RTX 4070. Our key\ncontributions include the design of a JSON-centric editing mechanism for\nfine-grained sign-level personalization and the first application of an\nMDN-based feedback loop for continuous model adaptation. This combination\nestablishes a generalizable, explainable AI paradigm for user-adaptive,\nlow-latency multimodal systems. In studies with 20 deaf signers and 5\nprofessional interpreters, we observe a +13 point SUS improvement, 6.7 point\nreduction in cognitive load, and significant gains in naturalness and trust (p\n$<$ .001) over baselines. This work establishes a scalable, explainable AI\nparadigm for accessible sign-language technologies."
                },
                "authors": [
                    {
                        "name": "Yingchao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingchao Li"
                },
                "author": "Yingchao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08697v2",
                "updated": "2025-06-24T13:11:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    11,
                    18,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-11T17:04:51Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "title": "Large Language Models as Span Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Span Annotators"
                },
                "summary": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research."
                },
                "authors": [
                    {
                        "name": "Zdeněk Kasner"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Patrícia Schmidtová"
                    },
                    {
                        "name": "Ivan Kartáč"
                    },
                    {
                        "name": "Kristýna Onderková"
                    },
                    {
                        "name": "Ondřej Plátek"
                    },
                    {
                        "name": "Dimitra Gkatzia"
                    },
                    {
                        "name": "Saad Mahamood"
                    },
                    {
                        "name": "Ondřej Dušek"
                    },
                    {
                        "name": "Simone Balloccu"
                    }
                ],
                "author_detail": {
                    "name": "Simone Balloccu"
                },
                "author": "Simone Balloccu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19599v1",
                "updated": "2025-06-24T13:09:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    9,
                    53,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:09:53Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    9,
                    53,
                    1,
                    175,
                    0
                ],
                "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of\n  Thought in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of\n  Thought in Large Language Model"
                },
                "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git."
                },
                "authors": [
                    {
                        "name": "Zhenke Duan"
                    },
                    {
                        "name": "Jiqun Pan"
                    },
                    {
                        "name": "Jiani Tu"
                    },
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Yanqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Wang"
                },
                "author": "Yanqing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19592v1",
                "updated": "2025-06-24T13:02:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning"
                },
                "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."
                },
                "authors": [
                    {
                        "name": "Harisankar Babu"
                    },
                    {
                        "name": "Philipp Schillinger"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17361v3",
                "updated": "2025-06-24T12:54:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    54,
                    9,
                    1,
                    175,
                    0
                ],
                "published": "2024-06-25T08:24:35Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    24,
                    35,
                    1,
                    177,
                    0
                ],
                "title": "Tree-based variational inference for Poisson log-normal models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based variational inference for Poisson log-normal models"
                },
                "summary": "When studying ecosystems, hierarchical trees are often used to organize\nentities based on proximity criteria, such as the taxonomy in microbiology,\nsocial classes in geography, or product types in retail businesses, offering\nvaluable insights into entity relationships. Despite their significance,\ncurrent count-data models do not leverage this structured information. In\nparticular, the widely used Poisson log-normal (PLN) model, known for its\nability to model interactions between entities from count data, lacks the\npossibility to incorporate such hierarchical tree structures, limiting its\napplicability in domains characterized by such complexities. To address this\nmatter, we introduce the PLN-Tree model as an extension of the PLN model,\nspecifically designed for modeling hierarchical count data. By integrating\nstructured variational inference techniques, we propose an adapted training\nprocedure and establish identifiability results, enhancing both theoretical\nfoundations and practical interpretability. Experiments on synthetic datasets\nand human gut microbiome data highlight generative improvements when using\nPLN-Tree, demonstrating the practical interest of knowledge graphs like the\ntaxonomy in microbiome modeling. Additionally, we present a proof-of-concept\nimplication of the identifiability results by illustrating the practical\nbenefits of using identifiable features for classification tasks, showcasing\nthe versatility of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When studying ecosystems, hierarchical trees are often used to organize\nentities based on proximity criteria, such as the taxonomy in microbiology,\nsocial classes in geography, or product types in retail businesses, offering\nvaluable insights into entity relationships. Despite their significance,\ncurrent count-data models do not leverage this structured information. In\nparticular, the widely used Poisson log-normal (PLN) model, known for its\nability to model interactions between entities from count data, lacks the\npossibility to incorporate such hierarchical tree structures, limiting its\napplicability in domains characterized by such complexities. To address this\nmatter, we introduce the PLN-Tree model as an extension of the PLN model,\nspecifically designed for modeling hierarchical count data. By integrating\nstructured variational inference techniques, we propose an adapted training\nprocedure and establish identifiability results, enhancing both theoretical\nfoundations and practical interpretability. Experiments on synthetic datasets\nand human gut microbiome data highlight generative improvements when using\nPLN-Tree, demonstrating the practical interest of knowledge graphs like the\ntaxonomy in microbiome modeling. Additionally, we present a proof-of-concept\nimplication of the identifiability results by illustrating the practical\nbenefits of using identifiable features for classification tasks, showcasing\nthe versatility of the framework."
                },
                "authors": [
                    {
                        "name": "Alexandre Chaussard"
                    },
                    {
                        "name": "Anna Bonnet"
                    },
                    {
                        "name": "Elisabeth Gassiat"
                    },
                    {
                        "name": "Sylvain Le Corff"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Le Corff"
                },
                "arxiv_affiliation": "LPSM",
                "author": "Sylvain Le Corff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19585v1",
                "updated": "2025-06-24T12:51:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    51,
                    39,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T12:51:39Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    51,
                    39,
                    1,
                    175,
                    0
                ],
                "title": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing\n  Images"
                },
                "summary": "From optical sensors to microwave radars, leveraging the complementary\nstrengths of remote sensing (RS) sensors is crucial for achieving dense\nspatio-temporal monitoring of our planet. In contrast, recent deep learning\nmodels, whether task-specific or foundational, are often specific to single\nsensors or to fixed combinations: adapting such models to different sensory\ninputs requires both architectural changes and re-training, limiting\nscalability and generalization across multiple RS sensors. On the contrary, a\nsingle model able to modulate its feature representations to accept diverse\nsensors as input would pave the way to agile and flexible multi-sensor RS data\nprocessing. To address this, we introduce SMARTIES, a generic and versatile\nfoundation model lifting sensor-specific/dependent efforts and enabling\nscalability and generalization to diverse RS sensors: SMARTIES projects data\nfrom heterogeneous sensors into a shared spectrum-aware space, enabling the use\nof arbitrary combinations of bands both for training and inference. To obtain\nsensor-agnostic representations, we train a single, unified transformer model\nreconstructing masked multi-sensor data with cross-sensor token mixup. On both\nsingle- and multi-modal tasks across diverse sensors, SMARTIES outperforms\nprevious models that rely on sensor-specific pretraining. Our code and\npretrained models are available at https://gsumbul.github.io/SMARTIES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From optical sensors to microwave radars, leveraging the complementary\nstrengths of remote sensing (RS) sensors is crucial for achieving dense\nspatio-temporal monitoring of our planet. In contrast, recent deep learning\nmodels, whether task-specific or foundational, are often specific to single\nsensors or to fixed combinations: adapting such models to different sensory\ninputs requires both architectural changes and re-training, limiting\nscalability and generalization across multiple RS sensors. On the contrary, a\nsingle model able to modulate its feature representations to accept diverse\nsensors as input would pave the way to agile and flexible multi-sensor RS data\nprocessing. To address this, we introduce SMARTIES, a generic and versatile\nfoundation model lifting sensor-specific/dependent efforts and enabling\nscalability and generalization to diverse RS sensors: SMARTIES projects data\nfrom heterogeneous sensors into a shared spectrum-aware space, enabling the use\nof arbitrary combinations of bands both for training and inference. To obtain\nsensor-agnostic representations, we train a single, unified transformer model\nreconstructing masked multi-sensor data with cross-sensor token mixup. On both\nsingle- and multi-modal tasks across diverse sensors, SMARTIES outperforms\nprevious models that rely on sensor-specific pretraining. Our code and\npretrained models are available at https://gsumbul.github.io/SMARTIES."
                },
                "authors": [
                    {
                        "name": "Gencer Sumbul"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Emanuele Dalsasso"
                    },
                    {
                        "name": "Devis Tuia"
                    }
                ],
                "author_detail": {
                    "name": "Devis Tuia"
                },
                "author": "Devis Tuia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17728v2",
                "updated": "2025-06-24T12:50:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    50,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-21T14:58:53Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    14,
                    58,
                    53,
                    5,
                    172,
                    0
                ],
                "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation"
                },
                "summary": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition..."
                },
                "authors": [
                    {
                        "name": "Dalong Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "QiWei Wang"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Xinkai Du"
                    },
                    {
                        "name": "YangYang Hou"
                    },
                    {
                        "name": "Yu Ao"
                    },
                    {
                        "name": "ZhaoYang Wang"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "ZhiYing Yi"
                    },
                    {
                        "name": "Zhongpu Bo"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpu Bo"
                },
                "author": "Zhongpu Bo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20788v2",
                "updated": "2025-06-24T12:47:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    47,
                    25,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-11T19:32:44Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    19,
                    32,
                    44,
                    1,
                    70,
                    0
                ],
                "title": "When Cubic Law and Darcy Fail: Bayesian Correction of Model\n  Misspecification in Fracture Conductivities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Cubic Law and Darcy Fail: Bayesian Correction of Model\n  Misspecification in Fracture Conductivities"
                },
                "summary": "Structural uncertainties and unresolved features in fault zones hinder the\nassessment of leakage risks in subsurface CO2 storage. Understanding\nmulti-scale uncertainties in fracture network conductivity is crucial for\nmitigating risks and reliably modelling upscaled fault leakage rates.\nConventional models, such as the Cubic Law, which is based on mechanical\naperture measurements, often neglect fracture roughness, leading to model\nmisspecifications and inaccurate conductivity estimates. Here, we develop a\nphysics-informed, AI-driven correction of these model misspecifications by\nautomatically integrating roughness effects and small-scale structural\nuncertainties. Using Bayesian inference combined with data-driven and geometric\ncorrections, we reconstruct local hydraulic aperture fields that reliably\nestimate fracture conductivities. By leveraging interactions across scales, we\nimprove upon traditional empirical corrections and provide a framework for\npropagating uncertainties from individual fractures to network scales. Our\napproach thereby supports robust calibration of conductivity ranges for fault\nleakage sensitivity analyses, offering a scalable solution for subsurface risk\nassessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural uncertainties and unresolved features in fault zones hinder the\nassessment of leakage risks in subsurface CO2 storage. Understanding\nmulti-scale uncertainties in fracture network conductivity is crucial for\nmitigating risks and reliably modelling upscaled fault leakage rates.\nConventional models, such as the Cubic Law, which is based on mechanical\naperture measurements, often neglect fracture roughness, leading to model\nmisspecifications and inaccurate conductivity estimates. Here, we develop a\nphysics-informed, AI-driven correction of these model misspecifications by\nautomatically integrating roughness effects and small-scale structural\nuncertainties. Using Bayesian inference combined with data-driven and geometric\ncorrections, we reconstruct local hydraulic aperture fields that reliably\nestimate fracture conductivities. By leveraging interactions across scales, we\nimprove upon traditional empirical corrections and provide a framework for\npropagating uncertainties from individual fractures to network scales. Our\napproach thereby supports robust calibration of conductivity ranges for fault\nleakage sensitivity analyses, offering a scalable solution for subsurface risk\nassessment."
                },
                "authors": [
                    {
                        "name": "Sarah Perez"
                    },
                    {
                        "name": "Florian Doster"
                    },
                    {
                        "name": "Julien Maes"
                    },
                    {
                        "name": "Hannah Menke"
                    },
                    {
                        "name": "Ahmed ElSheikh"
                    },
                    {
                        "name": "Andreas Busch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Busch"
                },
                "author": "Andreas Busch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v2",
                "updated": "2025-06-24T12:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    36,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelièvre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valdés Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "María José Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19563v1",
                "updated": "2025-06-24T12:22:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    22,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T12:22:59Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    22,
                    59,
                    1,
                    175,
                    0
                ],
                "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic\n  Consistency and Probability Certainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic\n  Consistency and Probability Certainty"
                },
                "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications."
                },
                "authors": [
                    {
                        "name": "Jinwen He"
                    },
                    {
                        "name": "Yiyang Lu"
                    },
                    {
                        "name": "Zijin Lin"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23254v2",
                "updated": "2025-06-24T12:14:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    14,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-05-29T09:00:35Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    0,
                    35,
                    3,
                    149,
                    0
                ],
                "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning"
                },
                "summary": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "15 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11558v2",
                "updated": "2025-06-24T11:59:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    59,
                    30,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-13T08:13:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs"
                },
                "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."
                },
                "authors": [
                    {
                        "name": "Bo-Cheng Chiu"
                    },
                    {
                        "name": "Jen-Jee Chen"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Feng-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Chi Chen"
                },
                "author": "Feng-Chi Chen",
                "arxiv_comment": "I would like to request the withdrawal of this submission because the\n  current version contains significant errors and incomplete results. I intend\n  to revise the manuscript thoroughly before resubmitting. I apologize for the\n  oversight and appreciate your understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19536v1",
                "updated": "2025-06-24T11:45:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    45,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:45:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    45,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Programming Geotechnical Reliability Algorithms using Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Geotechnical Reliability Algorithms using Generative AI"
                },
                "summary": "Programming reliability algorithms is crucial for risk assessment in\ngeotechnical engineering. This study explores the possibility of automating and\naccelerating this task using Generative AI based on Large Language Models\n(LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the\nability to generate MATLAB codes for four classical reliability algorithms. The\nfour specific examples considered in this study are: (1) First Order\nReliability Method (FORM); (2) Subset simulation; (3) Random field simulation;\nand (4) Bayesian update using Gibbs sampling. The results obtained using the\ngenerated codes are compared with benchmark methods. It is found that the use\nof LLMs can be promising for generating reliability codes. Failure,\nlimitations, and challenges of adopting LLMs are also discussed. Overall, this\nstudy demonstrates that existing LLMs can be leveraged powerfully and can\ncontribute toward accelerating the adoption of reliability techniques in\nroutine geotechnical engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming reliability algorithms is crucial for risk assessment in\ngeotechnical engineering. This study explores the possibility of automating and\naccelerating this task using Generative AI based on Large Language Models\n(LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the\nability to generate MATLAB codes for four classical reliability algorithms. The\nfour specific examples considered in this study are: (1) First Order\nReliability Method (FORM); (2) Subset simulation; (3) Random field simulation;\nand (4) Bayesian update using Gibbs sampling. The results obtained using the\ngenerated codes are compared with benchmark methods. It is found that the use\nof LLMs can be promising for generating reliability codes. Failure,\nlimitations, and challenges of adopting LLMs are also discussed. Overall, this\nstudy demonstrates that existing LLMs can be leveraged powerfully and can\ncontribute toward accelerating the adoption of reliability techniques in\nroutine geotechnical engineering."
                },
                "authors": [
                    {
                        "name": "Atma Sharma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Meng Lu"
                    },
                    {
                        "name": "Shuangyi Wu"
                    },
                    {
                        "name": "Baoxiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Baoxiang Li"
                },
                "author": "Baoxiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19527v1",
                "updated": "2025-06-24T11:30:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    30,
                    38,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:30:38Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    30,
                    38,
                    1,
                    175,
                    0
                ],
                "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs"
                },
                "summary": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Kelin Fu"
                    },
                    {
                        "name": "Kaigui Bian"
                    }
                ],
                "author_detail": {
                    "name": "Kaigui Bian"
                },
                "author": "Kaigui Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19525v1",
                "updated": "2025-06-24T11:25:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    25,
                    21,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:25:21Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    25,
                    21,
                    1,
                    175,
                    0
                ],
                "title": "Automatic Posology Structuration : What role for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Posology Structuration : What role for LLMs?"
                },
                "summary": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use."
                },
                "authors": [
                    {
                        "name": "Natalia Bobkova"
                    },
                    {
                        "name": "Laura Zanella-Calzada"
                    },
                    {
                        "name": "Anyes Tafoughalt"
                    },
                    {
                        "name": "Raphaël Teboul"
                    },
                    {
                        "name": "François Plesse"
                    },
                    {
                        "name": "Félix Gaschi"
                    }
                ],
                "author_detail": {
                    "name": "Félix Gaschi"
                },
                "author": "Félix Gaschi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06102v2",
                "updated": "2025-06-24T11:23:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    23,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-09T07:32:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    7,
                    32,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence"
                },
                "summary": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Haining Xie"
                    },
                    {
                        "name": "Siqishen"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Meng Lei"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chunyou Li"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yinjun Wu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Peng Chen"
                },
                "author": "Peng Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09679v2",
                "updated": "2025-06-24T11:16:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    16,
                    20,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-11T12:53:09Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    53,
                    9,
                    2,
                    162,
                    0
                ],
                "title": "Geometric flow regularization in latent spaces for smooth dynamics with\n  the efficient variations of curvature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric flow regularization in latent spaces for smooth dynamics with\n  the efficient variations of curvature"
                },
                "summary": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design strategies in nonlinear geometric analysis to temper the effects of\nadversarial learning for sufficiently smooth data of numerical method-type\ndynamics in encoder-decoder methods, variational and deterministic, through the\nuse of geometric flow regularization. We augment latent spaces with geometric\nflows to control structure. Our techniques rely on adaptations of curvature and\nRicci flow. We invent new geometric flows or discover them neurally and\nnon-parametrically. All of our flows are solved using physics-informed\nlearning. Traditional geometric meaning is traded for computing ability, but we\nmaintain key geometric invariants, the primary of which are maintained,\nintrinsically-low structure, canonicity or a lack of irregularity,\nnontriviality due to sufficient lower bounds on curvature, and distortion of\nvolume element, that develop quality in the inference stage. Our primary\ncontributions are fourfold. We develop a loss based on Gaussian curvature using\nclosed path circulation integration for surfaces, bypassing automatic\ndifferentiation of the Christoffel symbols through use of Stokes' theorem. We\ninvent a new parametric flow derived from a linear version of the Gauss\nequation and a Riemannian decomposition for a custom tensor defined with a\nnormal Hessian and Weyl tensor proxies. We develop two strategies based on time\ndifferentiation of functionals, one with a special case of scalar curvature for\nconformally-changed metrics, and another with harmonic maps, their energy, and\ninduced metrics. Our methods, while diminished analytically, maintain overall\nintegral latent structure. We showcase that curvature flows and the formulation\nof geometric structure in intermediary encoded settings enhance learning and\noverall zero-shot and adversarial fidelity."
                },
                "authors": [
                    {
                        "name": "Andrew Gracyk"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gracyk"
                },
                "author": "Andrew Gracyk",
                "arxiv_comment": "Small improvements to some mathematical arguments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19513v1",
                "updated": "2025-06-24T11:03:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    3,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:03:10Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    3,
                    10,
                    1,
                    175,
                    0
                ],
                "title": "Visual hallucination detection in large vision-language models via\n  evidential conflict",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual hallucination detection in large vision-language models via\n  evidential conflict"
                },
                "summary": "Despite the remarkable multimodal capabilities of Large Vision-Language\nModels (LVLMs), discrepancies often occur between visual inputs and textual\noutputs--a phenomenon we term visual hallucination. This critical reliability\ngap poses substantial risks in safety-critical Artificial Intelligence (AI)\napplications, necessitating a comprehensive evaluation benchmark and effective\ndetection methods. Firstly, we observe that existing visual-centric\nhallucination benchmarks mainly assess LVLMs from a perception perspective,\noverlooking hallucinations arising from advanced reasoning capabilities. We\ndevelop the Perception-Reasoning Evaluation Hallucination (PRE-HAL) dataset,\nwhich enables the systematic evaluation of both perception and reasoning\ncapabilities of LVLMs across multiple visual semantics, such as instances,\nscenes, and relations. Comprehensive evaluation with this new benchmark exposed\nmore visual vulnerabilities, particularly in the more challenging task of\nrelation reasoning. To address this issue, we propose, to the best of our\nknowledge, the first Dempster-Shafer theory (DST)-based visual hallucination\ndetection method for LVLMs through uncertainty estimation. This method aims to\nefficiently capture the degree of conflict in high-level features at the model\ninference phase. Specifically, our approach employs simple mass functions to\nmitigate the computational complexity of evidence combination on power sets. We\nconduct an extensive evaluation of state-of-the-art LVLMs, LLaVA-v1.5,\nmPLUG-Owl2 and mPLUG-Owl3, with the new PRE-HAL benchmark. Experimental results\nindicate that our method outperforms five baseline uncertainty metrics,\nachieving average AUROC improvements of 4%, 10%, and 7% across three LVLMs. Our\ncode is available at https://github.com/HT86159/Evidential-Conflict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable multimodal capabilities of Large Vision-Language\nModels (LVLMs), discrepancies often occur between visual inputs and textual\noutputs--a phenomenon we term visual hallucination. This critical reliability\ngap poses substantial risks in safety-critical Artificial Intelligence (AI)\napplications, necessitating a comprehensive evaluation benchmark and effective\ndetection methods. Firstly, we observe that existing visual-centric\nhallucination benchmarks mainly assess LVLMs from a perception perspective,\noverlooking hallucinations arising from advanced reasoning capabilities. We\ndevelop the Perception-Reasoning Evaluation Hallucination (PRE-HAL) dataset,\nwhich enables the systematic evaluation of both perception and reasoning\ncapabilities of LVLMs across multiple visual semantics, such as instances,\nscenes, and relations. Comprehensive evaluation with this new benchmark exposed\nmore visual vulnerabilities, particularly in the more challenging task of\nrelation reasoning. To address this issue, we propose, to the best of our\nknowledge, the first Dempster-Shafer theory (DST)-based visual hallucination\ndetection method for LVLMs through uncertainty estimation. This method aims to\nefficiently capture the degree of conflict in high-level features at the model\ninference phase. Specifically, our approach employs simple mass functions to\nmitigate the computational complexity of evidence combination on power sets. We\nconduct an extensive evaluation of state-of-the-art LVLMs, LLaVA-v1.5,\nmPLUG-Owl2 and mPLUG-Owl3, with the new PRE-HAL benchmark. Experimental results\nindicate that our method outperforms five baseline uncertainty metrics,\nachieving average AUROC improvements of 4%, 10%, and 7% across three LVLMs. Our\ncode is available at https://github.com/HT86159/Evidential-Conflict."
                },
                "authors": [
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Zhekun Liu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Liping Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liping Jing"
                },
                "author": "Liping Jing",
                "arxiv_doi": "10.1016/j.ijar.2025.109507",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ijar.2025.109507",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.19513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Journal of Approximate Reasoning, Volume 186,\n  November 2025, Article 109507",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19510v1",
                "updated": "2025-06-24T10:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "The H$α$ line as a probe of chromospheric magnetic fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The H$α$ line as a probe of chromospheric magnetic fields"
                },
                "summary": "We explore the diagnostic potential of the H$\\alpha$ line for probing the\nchromospheric magnetic field using a realistic 3D radiative magnetohydrodynamic\n(rMHD) model. The Stokes profiles of the H$\\alpha$ line are synthesized through\nfull 3D radiative transfer under the field-free approximation, alongside the Ca\nII 8542 {\\AA} and Fe I 6173 {\\AA} lines for comparison. The line-of-sight (LOS)\nmagnetic fields are inferred using the weak field approximation (WFA) for\ntheH$\\alpha$ and Ca II 8542 {\\AA} lines, while the Fe I 6173 {\\AA} line is\nanalyzed through Milne-Eddington inversion techniques. The comparison between\nthe inferred LOS magnetic field maps and the magnetic fields in the rMHD model\nrevealed that the H$\\alpha$ line core primarily probes the chromospheric\nmagnetic field at log tau_500 = -5.7, which corresponds to higher layers than\nthe Ca II 8542 {\\AA} line core, which is most sensitive to conditions at log\ntau_500 = -5.1. On average, the Stokes V profiles of the H$\\alpha$ line core\nform 500 km higher than those of the Ca II 8542 {\\AA} line core. The H$\\alpha$\npolarization signals persist after adding noise, and with noise at the level of\n10^-3 Ic, most simulated magnetic structures remain visible. These findings\nsuggest that spectropolarimetric observations of the H$\\alpha$ line can provide\ncomplementary insights into the stratification of the magnetic field at higher\naltitudes, especially when recorded simultaneously with widely used\nchromospheric diagnostics such as the Ca II 8542 {\\AA} line.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the diagnostic potential of the H$\\alpha$ line for probing the\nchromospheric magnetic field using a realistic 3D radiative magnetohydrodynamic\n(rMHD) model. The Stokes profiles of the H$\\alpha$ line are synthesized through\nfull 3D radiative transfer under the field-free approximation, alongside the Ca\nII 8542 {\\AA} and Fe I 6173 {\\AA} lines for comparison. The line-of-sight (LOS)\nmagnetic fields are inferred using the weak field approximation (WFA) for\ntheH$\\alpha$ and Ca II 8542 {\\AA} lines, while the Fe I 6173 {\\AA} line is\nanalyzed through Milne-Eddington inversion techniques. The comparison between\nthe inferred LOS magnetic field maps and the magnetic fields in the rMHD model\nrevealed that the H$\\alpha$ line core primarily probes the chromospheric\nmagnetic field at log tau_500 = -5.7, which corresponds to higher layers than\nthe Ca II 8542 {\\AA} line core, which is most sensitive to conditions at log\ntau_500 = -5.1. On average, the Stokes V profiles of the H$\\alpha$ line core\nform 500 km higher than those of the Ca II 8542 {\\AA} line core. The H$\\alpha$\npolarization signals persist after adding noise, and with noise at the level of\n10^-3 Ic, most simulated magnetic structures remain visible. These findings\nsuggest that spectropolarimetric observations of the H$\\alpha$ line can provide\ncomplementary insights into the stratification of the magnetic field at higher\naltitudes, especially when recorded simultaneously with widely used\nchromospheric diagnostics such as the Ca II 8542 {\\AA} line."
                },
                "authors": [
                    {
                        "name": "Harsh Mathur"
                    },
                    {
                        "name": "Jayant Joshi"
                    },
                    {
                        "name": "Thore Espedal Moe"
                    },
                    {
                        "name": "Tiago M. D. Pereira"
                    },
                    {
                        "name": "K. Nagaraju"
                    }
                ],
                "author_detail": {
                    "name": "K. Nagaraju"
                },
                "author": "K. Nagaraju",
                "arxiv_comment": "14 pages, 9 figures, accepted in ApJL on 24th June, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15961v2",
                "updated": "2025-06-24T10:50:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    50,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T02:10:06Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    10,
                    6,
                    3,
                    170,
                    0
                ],
                "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training"
                },
                "summary": "Training large language models (LLMs) at scale requires parallel execution\nacross thousands of devices, incurring enormous computational costs. Yet, these\ncostly distributed trainings are rarely verified, leaving them prone to silent\nerrors and potentially wasting millions of GPU hours. We introduce TrainVerify,\na system for verifiable distributed training of LLMs. Given a deep learning\nmodel's logical specification as the ground truth, TrainVerify formally\nverifies that a distributed parallel execution plan is mathematically\nequivalent to it. Direct verification is notoriously difficult due to the sheer\nscale of LLMs which often involves billions of variables and highly intricate\ncomputation graphs. Therefore, TrainVerify introduces shape-reduction\ntechniques and a stage-wise parallel verification algorithm that significantly\nreduces complexity while preserving formal correctness. TrainVerify scales to\nfrontier LLMs, including the successful verification of the Llama3 (405B) and\nDeepSeek-V3 (671B) training plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) at scale requires parallel execution\nacross thousands of devices, incurring enormous computational costs. Yet, these\ncostly distributed trainings are rarely verified, leaving them prone to silent\nerrors and potentially wasting millions of GPU hours. We introduce TrainVerify,\na system for verifiable distributed training of LLMs. Given a deep learning\nmodel's logical specification as the ground truth, TrainVerify formally\nverifies that a distributed parallel execution plan is mathematically\nequivalent to it. Direct verification is notoriously difficult due to the sheer\nscale of LLMs which often involves billions of variables and highly intricate\ncomputation graphs. Therefore, TrainVerify introduces shape-reduction\ntechniques and a stage-wise parallel verification algorithm that significantly\nreduces complexity while preserving formal correctness. TrainVerify scales to\nfrontier LLMs, including the successful verification of the Llama3 (405B) and\nDeepSeek-V3 (671B) training plans."
                },
                "authors": [
                    {
                        "name": "Yunchi Lu"
                    },
                    {
                        "name": "Youshan Miao"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Peng Huang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2012.11614v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2012.11614v4",
                "updated": "2025-06-24T10:40:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    40,
                    39,
                    1,
                    175,
                    0
                ],
                "published": "2020-12-21T19:00:01Z",
                "published_parsed": [
                    2020,
                    12,
                    21,
                    19,
                    0,
                    1,
                    0,
                    356,
                    0
                ],
                "title": "Testing the dark SU(N) Yang-Mills theory Confined Landscape: From the\n  Lattice to Gravitational Waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing the dark SU(N) Yang-Mills theory Confined Landscape: From the\n  Lattice to Gravitational Waves"
                },
                "summary": "We pave the way for future gravitational-wave detection experiments, such as\nthe Big Bang Observer and DECIGO, to constrain dark sectors made of SU(N)\nYang-Mills confined theories. We go beyond the state-of-the-art by combining\nfirst principle lattice results and effective field theory approaches to infer\nessential information about the non-perturbative dark deconfinement phase\ntransition driving the generation of gravitational-waves in the early universe,\nsuch as the order, duration and energy budget of the phase transition which are\nessential in establishing the strength of the resulting gravitational-wave\nsignal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We pave the way for future gravitational-wave detection experiments, such as\nthe Big Bang Observer and DECIGO, to constrain dark sectors made of SU(N)\nYang-Mills confined theories. We go beyond the state-of-the-art by combining\nfirst principle lattice results and effective field theory approaches to infer\nessential information about the non-perturbative dark deconfinement phase\ntransition driving the generation of gravitational-waves in the early universe,\nsuch as the order, duration and energy budget of the phase transition which are\nessential in establishing the strength of the resulting gravitational-wave\nsignal."
                },
                "authors": [
                    {
                        "name": "Wei-Chih Huang"
                    },
                    {
                        "name": "Manuel Reichert"
                    },
                    {
                        "name": "Francesco Sannino"
                    },
                    {
                        "name": "Zhi-Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Wei Wang"
                },
                "author": "Zhi-Wei Wang",
                "arxiv_doi": "10.1103/PhysRevD.104.035005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.104.035005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2012.11614v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2012.11614v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 13 figures; v2: corrected Fig.12 & typos, updated refs; v3:\n  matches journal version; v4: minor corrections",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19502v1",
                "updated": "2025-06-24T10:40:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    40,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:40:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    40,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications"
                },
                "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE."
                },
                "authors": [
                    {
                        "name": "Aleksandr Algazinov"
                    },
                    {
                        "name": "Matt Laing"
                    },
                    {
                        "name": "Paul Laban"
                    }
                ],
                "author_detail": {
                    "name": "Paul Laban"
                },
                "author": "Paul Laban",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19500v1",
                "updated": "2025-06-24T10:39:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:39:07Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "title": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function\n  Calling"
                },
                "summary": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration."
                },
                "authors": [
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "LiZhong GU"
                    },
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "TianLong Li"
                    }
                ],
                "author_detail": {
                    "name": "TianLong Li"
                },
                "author": "TianLong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19497v1",
                "updated": "2025-06-24T10:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    35,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:35:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    35,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "The time course of visuo-semantic representations in the human brain is\n  captured by combining vision and language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time course of visuo-semantic representations in the human brain is\n  captured by combining vision and language models"
                },
                "summary": "The human visual system provides us with a rich and meaningful percept of the\nworld, transforming retinal signals into visuo-semantic representations. For a\nmodel of these representations, here we leveraged a combination of two\ncurrently dominating approaches: vision deep neural networks (DNNs) and large\nlanguage models (LLMs). Using large-scale human electroencephalography (EEG)\ndata recorded during object image viewing, we built encoding models to predict\nEEG responses using representations from a vision DNN, an LLM, and their\nfusion. We show that the fusion encoding model outperforms encoding models\nbased on either the vision DNN or the LLM alone, as well as previous modelling\napproaches, in predicting neural responses to visual stimulation. The vision\nDNN and the LLM complemented each other in explaining stimulus-related signal\nin the EEG responses. The vision DNN uniquely captured earlier and broadband\nEEG signals, whereas the LLM uniquely captured later and low frequency signals,\nas well as detailed visuo-semantic stimulus information. Together, this\nprovides a more accurate model of the time course of visuo-semantic processing\nin the human brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human visual system provides us with a rich and meaningful percept of the\nworld, transforming retinal signals into visuo-semantic representations. For a\nmodel of these representations, here we leveraged a combination of two\ncurrently dominating approaches: vision deep neural networks (DNNs) and large\nlanguage models (LLMs). Using large-scale human electroencephalography (EEG)\ndata recorded during object image viewing, we built encoding models to predict\nEEG responses using representations from a vision DNN, an LLM, and their\nfusion. We show that the fusion encoding model outperforms encoding models\nbased on either the vision DNN or the LLM alone, as well as previous modelling\napproaches, in predicting neural responses to visual stimulation. The vision\nDNN and the LLM complemented each other in explaining stimulus-related signal\nin the EEG responses. The vision DNN uniquely captured earlier and broadband\nEEG signals, whereas the LLM uniquely captured later and low frequency signals,\nas well as detailed visuo-semantic stimulus information. Together, this\nprovides a more accurate model of the time course of visuo-semantic processing\nin the human brain."
                },
                "authors": [
                    {
                        "name": "Boyan Rong"
                    },
                    {
                        "name": "Alessandro Thomas Gifford"
                    },
                    {
                        "name": "Emrah Düzel"
                    },
                    {
                        "name": "Radoslaw Martin Cichy"
                    }
                ],
                "author_detail": {
                    "name": "Radoslaw Martin Cichy"
                },
                "author": "Radoslaw Martin Cichy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19486v1",
                "updated": "2025-06-24T10:21:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    21,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:21:10Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    21,
                    10,
                    1,
                    175,
                    0
                ],
                "title": "Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy\n  Labelers to Leak Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recalling The Forgotten Class Memberships: Unlearned Models Can Be Noisy\n  Labelers to Leak Privacy"
                },
                "summary": "Machine Unlearning (MU) technology facilitates the removal of the influence\nof specific data instances from trained models on request. Despite rapid\nadvancements in MU technology, its vulnerabilities are still underexplored,\nposing potential risks of privacy breaches through leaks of ostensibly\nunlearned information. Current limited research on MU attacks requires access\nto original models containing privacy data, which violates the critical\nprivacy-preserving objective of MU. To address this gap, we initiate an\ninnovative study on recalling the forgotten class memberships from unlearned\nmodels (ULMs) without requiring access to the original one. Specifically, we\nimplement a Membership Recall Attack (MRA) framework with a teacher-student\nknowledge distillation architecture, where ULMs serve as noisy labelers to\ntransfer knowledge to student models. Then, it is translated into a Learning\nwith Noisy Labels (LNL) problem for inferring the correct labels of the\nforgetting instances. Extensive experiments on state-of-the-art MU methods with\nmultiple real datasets demonstrate that the proposed MRA strategy exhibits high\nefficacy in recovering class memberships of unlearned instances. As a result,\nour study and evaluation have established a benchmark for future research on MU\nvulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning (MU) technology facilitates the removal of the influence\nof specific data instances from trained models on request. Despite rapid\nadvancements in MU technology, its vulnerabilities are still underexplored,\nposing potential risks of privacy breaches through leaks of ostensibly\nunlearned information. Current limited research on MU attacks requires access\nto original models containing privacy data, which violates the critical\nprivacy-preserving objective of MU. To address this gap, we initiate an\ninnovative study on recalling the forgotten class memberships from unlearned\nmodels (ULMs) without requiring access to the original one. Specifically, we\nimplement a Membership Recall Attack (MRA) framework with a teacher-student\nknowledge distillation architecture, where ULMs serve as noisy labelers to\ntransfer knowledge to student models. Then, it is translated into a Learning\nwith Noisy Labels (LNL) problem for inferring the correct labels of the\nforgetting instances. Extensive experiments on state-of-the-art MU methods with\nmultiple real datasets demonstrate that the proposed MRA strategy exhibits high\nefficacy in recovering class memberships of unlearned instances. As a result,\nour study and evaluation have established a benchmark for future research on MU\nvulnerabilities."
                },
                "authors": [
                    {
                        "name": "Zhihao Sui"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Dora D. Liu"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Zhongyuan Lai"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v4",
                "updated": "2025-06-24T10:19:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image AutoRegressive generation has emerged as a new powerful paradigm with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns regarding their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to the ones of DMs as reference points. Concretely, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images (with a True Positive Rate at False Positive\nRate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our\nnovel MIA to provide dataset inference (DI) for IARs, and show that it requires\nas few as 6 samples to detect dataset membership (compared to 200 for DI in\nDMs), confirming a higher information leakage in IARs. Finally, we are able to\nextract hundreds of training data points from an IAR (e.g., 698 from VAR-d30).\nOur results suggest a fundamental privacy-utility trade-off: while IARs excel\nin image generation quality and speed, they are empirically significantly more\nvulnerable to privacy attacks compared to DMs that achieve similar performance.\nWe release the code at https://github.com/sprintml/privacy_attacks_against_iars\nfor reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image AutoRegressive generation has emerged as a new powerful paradigm with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns regarding their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to the ones of DMs as reference points. Concretely, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images (with a True Positive Rate at False Positive\nRate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our\nnovel MIA to provide dataset inference (DI) for IARs, and show that it requires\nas few as 6 samples to detect dataset membership (compared to 200 for DI in\nDMs), confirming a higher information leakage in IARs. Finally, we are able to\nextract hundreds of training data points from an IAR (e.g., 698 from VAR-d30).\nOur results suggest a fundamental privacy-utility trade-off: while IARs excel\nin image generation quality and speed, they are empirically significantly more\nvulnerable to privacy attacks compared to DMs that achieve similar performance.\nWe release the code at https://github.com/sprintml/privacy_attacks_against_iars\nfor reproducibility."
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Accepted at ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19484v1",
                "updated": "2025-06-24T10:19:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    9,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:19:09Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    9,
                    1,
                    175,
                    0
                ],
                "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI\n  with Proven Theories of Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI\n  with Proven Theories of Learning"
                },
                "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.6; H.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19483v1",
                "updated": "2025-06-24T10:18:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    18,
                    5,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:18:05Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    18,
                    5,
                    1,
                    175,
                    0
                ],
                "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Generation and Evaluation for Dialogue Systems using Large\n  Language Models"
                },
                "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems."
                },
                "authors": [
                    {
                        "name": "Marcos Estecha-Garitagoitia"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Mario Rodríguez-Cantelar"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    }
                ],
                "author_detail": {
                    "name": "Luis Fernando D'Haro"
                },
                "author": "Luis Fernando D'Haro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19481v1",
                "updated": "2025-06-24T10:17:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    17,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:17:34Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    17,
                    34,
                    1,
                    175,
                    0
                ],
                "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code"
                },
                "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows."
                },
                "authors": [
                    {
                        "name": "Shahbaz Siddeeq"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Md Mahade Hasan"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Henri Terho"
                    },
                    {
                        "name": "Kalle Makela"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2502.07928",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07584v3",
                "updated": "2025-06-24T10:10:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    10,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-10T17:48:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    48,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Talking to GDELT Through Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking to GDELT Through Knowledge Graphs"
                },
                "summary": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end."
                },
                "authors": [
                    {
                        "name": "Audun Myers"
                    },
                    {
                        "name": "Max Vargas"
                    },
                    {
                        "name": "Sinan G. Aksoy"
                    },
                    {
                        "name": "Cliff Joslyn"
                    },
                    {
                        "name": "Benjamin Wilson"
                    },
                    {
                        "name": "Lee Burke"
                    },
                    {
                        "name": "Tom Grimes"
                    }
                ],
                "author_detail": {
                    "name": "Tom Grimes"
                },
                "author": "Tom Grimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18834v2",
                "updated": "2025-06-24T10:05:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    5,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2024-04-29T16:21:59Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    16,
                    21,
                    59,
                    0,
                    120,
                    0
                ],
                "title": "Interpolating between Optimal Transport and KL regularized Optimal\n  Transport using Rényi Divergences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpolating between Optimal Transport and KL regularized Optimal\n  Transport using Rényi Divergences"
                },
                "summary": "Regularized optimal transport (OT) has received much attention in recent\nyears starting from Cuturi's introduction of Kullback-Leibler (KL) divergence\nregularized OT. In this paper, we propose regularizing the OT problem using the\nfamily of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi\ndivergences are neither $f$-divergences nor Bregman distances, but they recover\nthe KL divergence in the limit $\\alpha \\nearrow 1$. The advantage of\nintroducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$\nwe obtain convergence to the unregularized OT problem. For the KL regularized\nOT problem, this was achieved by letting the regularization parameter\n$\\varepsilon$ tend to zero, which causes numerical instabilities. We present\ntwo different ways to obtain premetrics on probability measures, namely by\nR\\'enyi divergence constraints and by penalization. The latter premetric\ninterpolates between the unregularized and the KL regularized OT problem with\nweak convergence of the unique minimizer, generalizing the interpolation\nproperty of KL regularized OT. We use a nested mirror descent algorithm to\nsolve the primal formulation. Both on real and synthetic data sets R\\'enyi\nregularized OT plans outperform their KL and Tsallis counterparts in terms of\nbeing closer to the unregularized transport plans and recovering the ground\ntruth in inference tasks better.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized optimal transport (OT) has received much attention in recent\nyears starting from Cuturi's introduction of Kullback-Leibler (KL) divergence\nregularized OT. In this paper, we propose regularizing the OT problem using the\nfamily of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi\ndivergences are neither $f$-divergences nor Bregman distances, but they recover\nthe KL divergence in the limit $\\alpha \\nearrow 1$. The advantage of\nintroducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$\nwe obtain convergence to the unregularized OT problem. For the KL regularized\nOT problem, this was achieved by letting the regularization parameter\n$\\varepsilon$ tend to zero, which causes numerical instabilities. We present\ntwo different ways to obtain premetrics on probability measures, namely by\nR\\'enyi divergence constraints and by penalization. The latter premetric\ninterpolates between the unregularized and the KL regularized OT problem with\nweak convergence of the unique minimizer, generalizing the interpolation\nproperty of KL regularized OT. We use a nested mirror descent algorithm to\nsolve the primal formulation. Both on real and synthetic data sets R\\'enyi\nregularized OT plans outperform their KL and Tsallis counterparts in terms of\nbeing closer to the unregularized transport plans and recovering the ground\ntruth in inference tasks better."
                },
                "authors": [
                    {
                        "name": "Jonas Bresch"
                    },
                    {
                        "name": "Viktor Stein"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Stein"
                },
                "author": "Viktor Stein",
                "arxiv_comment": "40 pages, 9 figures, 3 tables, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49Q22, 46N10, 94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19468v1",
                "updated": "2025-06-24T09:53:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    53,
                    0,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:53:00Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    53,
                    0,
                    1,
                    175,
                    0
                ],
                "title": "MuBench: Assessment of Multilingual Capabilities of Large Language\n  Models Across 61 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuBench: Assessment of Multilingual Capabilities of Large Language\n  Models Across 61 Languages"
                },
                "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics."
                },
                "authors": [
                    {
                        "name": "Wenhan Han"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhixun Chen"
                    },
                    {
                        "name": "Binbin Liu"
                    },
                    {
                        "name": "Haobin Lin"
                    },
                    {
                        "name": "Bingni Zhang"
                    },
                    {
                        "name": "Taifeng Wang"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yin Zheng"
                },
                "author": "Yin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19467v1",
                "updated": "2025-06-24T09:49:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    49,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:49:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    49,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Can Large Language Models Capture Human Annotator Disagreements?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Capture Human Annotator Disagreements?"
                },
                "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "arxiv_comment": "Preprint Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19466v1",
                "updated": "2025-06-24T09:48:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models"
                },
                "summary": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Jiexiong Liu"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Qihang Zhou"
                    },
                    {
                        "name": "KunLun Meta"
                    }
                ],
                "author_detail": {
                    "name": "KunLun Meta"
                },
                "author": "KunLun Meta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.19847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19847v1",
                "updated": "2025-06-24T17:59:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:59:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "Orthogonal Finetuning Made Scalable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal Finetuning Made Scalable"
                },
                "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage."
                },
                "authors": [
                    {
                        "name": "Zeju Qiu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Schölkopf"
                },
                "author": "Bernhard Schölkopf",
                "arxiv_comment": "Technical report (17 pages, 7 figures, project page:\n  https://spherelab.ai/oftv2/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19846v1",
                "updated": "2025-06-24T17:59:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    31,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:59:31Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    59,
                    31,
                    1,
                    175,
                    0
                ],
                "title": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents\n  with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyAgents-R1: Joint Evolution Dynamics for Versatile Multi-LLM Agents\n  with Reinforcement Learning"
                },
                "summary": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm\nfor increasingly complex tasks. However, joint evolution across heterogeneous\nagents remains challenging due to cooperative inefficiency and training\ninstability. In this paper, we propose the joint evolution dynamics for MARL\ncalled JoyAgents-R1, which first applies Group Relative Policy Optimization\n(GRPO) to the joint training of heterogeneous multi-agents. By iteratively\nrefining agents' large language models (LLMs) and memories, the method achieves\nholistic equilibrium with optimal decision-making and memory capabilities.\nSpecifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on\nthe behavior of each agent across entire reasoning trajectories to enhance GRPO\nsampling efficiency while maintaining policy diversity. Then, our marginal\nbenefit-driven selection strategy identifies top-$K$ sampling groups with\nmaximal reward fluctuations, enabling targeted agent model updates that improve\ntraining stability and maximize joint benefits through cost-effective parameter\nadjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution\nmechanism that repurposes GRPO rewards as cost-free supervisory signals to\neliminate repetitive reasoning and accelerate convergence. Experiments across\ngeneral and domain-specific scenarios demonstrate that JoyAgents-R1 achieves\nperformance comparable to that of larger LLMs while built on smaller\nopen-source models."
                },
                "authors": [
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "Junxing Hu"
                    },
                    {
                        "name": "Pu Wei"
                    },
                    {
                        "name": "Zhiqian Zhang"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Jiawei Lu"
                    },
                    {
                        "name": "Zicheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zicheng Zhang"
                },
                "author": "Zicheng Zhang",
                "arxiv_comment": "33 pages, 7 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19835v1",
                "updated": "2025-06-24T17:52:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    52,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:52:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    52,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via\n  Role-Specialized Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via\n  Role-Specialized Collaboration"
                },
                "summary": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in medical Large Language Models (LLMs) have showcased\ntheir powerful reasoning and diagnostic capabilities. Despite their success,\ncurrent unified multimodal medical LLMs face limitations in knowledge update\ncosts, comprehensiveness, and flexibility. To address these challenges, we\nintroduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis\n(MAM). Inspired by our empirical findings highlighting the benefits of role\nassignment and diagnostic discernment in LLMs, MAM decomposes the medical\ndiagnostic process into specialized roles: a General Practitioner, Specialist\nTeam, Radiologist, Medical Assistant, and Director, each embodied by an\nLLM-based agent. This modular and collaborative framework enables efficient\nknowledge updates and leverages existing medical LLMs and knowledge bases.\nExtensive experimental evaluations conducted on a wide range of publicly\naccessible multimodal medical datasets, incorporating text, image, audio, and\nvideo modalities, demonstrate that MAM consistently surpasses the performance\nof modality-specific LLMs. Notably, MAM achieves significant performance\nimprovements ranging from 18% to 365% compared to baseline models. Our code is\nreleased at https://github.com/yczhou001/MAM."
                },
                "authors": [
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Lingran Song"
                    },
                    {
                        "name": "Jianbing Shen"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Shen"
                },
                "author": "Jianbing Shen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19823v1",
                "updated": "2025-06-24T17:38:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    38,
                    21,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:38:21Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    38,
                    21,
                    1,
                    175,
                    0
                ],
                "title": "Persona Features Control Emergent Misalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona Features Control Emergent Misalignment"
                },
                "summary": "Understanding how language models generalize behaviors from their training to\na broader deployment distribution is an important problem in AI safety. Betley\net al. discovered that fine-tuning GPT-4o on intentionally insecure code causes\n\"emergent misalignment,\" where models give stereotypically malicious responses\nto unrelated prompts. We extend this work, demonstrating emergent misalignment\nacross diverse conditions, including reinforcement learning on reasoning\nmodels, fine-tuning on various synthetic datasets, and in models without safety\ntraining. To investigate the mechanisms behind this generalized misalignment,\nwe apply a \"model diffing\" approach using sparse autoencoders to compare\ninternal model representations before and after fine-tuning. This approach\nreveals several \"misaligned persona\" features in activation space, including a\ntoxic persona feature which most strongly controls emergent misalignment and\ncan be used to predict whether a model will exhibit such behavior.\nAdditionally, we investigate mitigation strategies, discovering that\nfine-tuning an emergently misaligned model on just a few hundred benign samples\nefficiently restores alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how language models generalize behaviors from their training to\na broader deployment distribution is an important problem in AI safety. Betley\net al. discovered that fine-tuning GPT-4o on intentionally insecure code causes\n\"emergent misalignment,\" where models give stereotypically malicious responses\nto unrelated prompts. We extend this work, demonstrating emergent misalignment\nacross diverse conditions, including reinforcement learning on reasoning\nmodels, fine-tuning on various synthetic datasets, and in models without safety\ntraining. To investigate the mechanisms behind this generalized misalignment,\nwe apply a \"model diffing\" approach using sparse autoencoders to compare\ninternal model representations before and after fine-tuning. This approach\nreveals several \"misaligned persona\" features in activation space, including a\ntoxic persona feature which most strongly controls emergent misalignment and\ncan be used to predict whether a model will exhibit such behavior.\nAdditionally, we investigate mitigation strategies, discovering that\nfine-tuning an emergently misaligned model on just a few hundred benign samples\nefficiently restores alignment."
                },
                "authors": [
                    {
                        "name": "Miles Wang"
                    },
                    {
                        "name": "Tom Dupré la Tour"
                    },
                    {
                        "name": "Olivia Watkins"
                    },
                    {
                        "name": "Alex Makelov"
                    },
                    {
                        "name": "Ryan A. Chi"
                    },
                    {
                        "name": "Samuel Miserendino"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    },
                    {
                        "name": "Dan Mossing"
                    }
                ],
                "author_detail": {
                    "name": "Dan Mossing"
                },
                "author": "Dan Mossing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01339v2",
                "updated": "2025-06-24T17:32:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    32,
                    37,
                    1,
                    175,
                    0
                ],
                "published": "2025-05-02T15:11:22Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    15,
                    11,
                    22,
                    4,
                    122,
                    0
                ],
                "title": "Toward Teach and Repeat Across Seasonal Deep Snow Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Teach and Repeat Across Seasonal Deep Snow Accumulation"
                },
                "summary": "Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release."
                },
                "authors": [
                    {
                        "name": "Matěj Boxan"
                    },
                    {
                        "name": "Alexander Krawciw"
                    },
                    {
                        "name": "Timothy D. Barfoot"
                    },
                    {
                        "name": "François Pomerleau"
                    }
                ],
                "author_detail": {
                    "name": "François Pomerleau"
                },
                "author": "François Pomerleau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16772v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16772v5",
                "updated": "2025-06-24T17:32:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    32,
                    18,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-24T01:35:32Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    35,
                    32,
                    0,
                    55,
                    0
                ],
                "title": "Model-Based Exploration in Monitored Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Exploration in Monitored Markov Decision Processes"
                },
                "summary": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A tenet of reinforcement learning is that the agent always observes rewards.\nHowever, this is not true in many realistic settings, e.g., a human observer\nmay not always be available to provide rewards, sensors may be limited or\nmalfunctioning, or rewards may be inaccessible during deployment. Monitored\nMarkov decision processes (Mon-MDPs) have recently been proposed to model such\nsettings. However, existing Mon-MDP algorithms have several limitations: they\ndo not fully exploit the problem structure, cannot leverage a known monitor,\nlack worst-case guarantees for 'unsolvable' Mon-MDPs without specific\ninitialization, and offer only asymptotic convergence proofs. This paper makes\nthree contributions. First, we introduce a model-based algorithm for Mon-MDPs\nthat addresses these shortcomings. The algorithm employs two instances of\nmodel-based interval estimation: one to ensure that observable rewards are\nreliably captured, and another to learn the minimax-optimal policy. Second, we\nempirically demonstrate the advantages. We show faster convergence than prior\nalgorithms in over four dozen benchmarks, and even more dramatic improvement\nwhen the monitoring process is known. Third, we present the first finite-sample\nbound on performance. We show convergence to a minimax-optimal policy even when\nsome rewards are never observable."
                },
                "authors": [
                    {
                        "name": "Alireza Kazemipour"
                    },
                    {
                        "name": "Simone Parisi"
                    },
                    {
                        "name": "Matthew E. Taylor"
                    },
                    {
                        "name": "Michael Bowling"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bowling"
                },
                "author": "Michael Bowling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16772v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16772v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19807v1",
                "updated": "2025-06-24T17:17:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:17:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"
                },
                "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."
                },
                "authors": [
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19806v1",
                "updated": "2025-06-24T17:14:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    14,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:14:47Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    14,
                    47,
                    1,
                    175,
                    0
                ],
                "title": "LLM-Based Social Simulations Require a Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Social Simulations Require a Boundary"
                },
                "summary": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This position paper argues that large language model (LLM)-based social\nsimulations should establish clear boundaries to meaningfully contribute to\nsocial science research. While LLMs offer promising capabilities for modeling\nhuman-like agents compared to traditional agent-based modeling, they face\nfundamental limitations that constrain their reliability for social pattern\ndiscovery. The core issue lies in LLMs' tendency towards an ``average persona''\nthat lacks sufficient behavioral heterogeneity, a critical requirement for\nsimulating complex social dynamics. We examine three key boundary problems:\nalignment (simulated behaviors matching real-world patterns), consistency\n(maintaining coherent agent behavior over time), and robustness\n(reproducibility under varying conditions). We propose heuristic boundaries for\ndetermining when LLM-based simulations can reliably advance social science\nunderstanding. We believe that these simulations are more valuable when\nfocusing on (1) collective patterns rather than individual trajectories, (2)\nagent behaviors aligning with real population averages despite limited\nvariance, and (3) proper validation methods available for testing simulation\nrobustness. We provide a practical checklist to guide researchers in\ndetermining the appropriate scope and claims for LLM-based social simulations."
                },
                "authors": [
                    {
                        "name": "Zengqing Wu"
                    },
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Takayuki Ito"
                    },
                    {
                        "name": "Chuan Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Xiao"
                },
                "author": "Chuan Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19802v1",
                "updated": "2025-06-24T17:08:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    8,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:08:58Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    8,
                    58,
                    1,
                    175,
                    0
                ],
                "title": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs"
                },
                "summary": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive research on Machine Learning-based Network Intrusion\nDetection Systems (ML-NIDS), their capability to detect diverse attack variants\nremains uncertain. Prior studies have largely relied on homogeneous datasets,\nwhich artificially inflate performance scores and offer a false sense of\nsecurity. Designing systems that can effectively detect a wide range of attack\nvariants remains a significant challenge. The progress of ML-NIDS continues to\ndepend heavily on human expertise, which can embed subjective judgments of\nsystem designers into the model, potentially hindering its ability to\ngeneralize across diverse attack types.\n  To address this gap, we propose KnowML, a framework for knowledge-guided\nmachine learning that integrates attack knowledge into ML-NIDS. KnowML\nsystematically explores the threat landscape by leveraging Large Language\nModels (LLMs) to perform automated analysis of attack implementations. It\nconstructs a unified Knowledge Graph (KG) of attack strategies, on which it\napplies symbolic reasoning to generate KG-Augmented Input, embedding domain\nknowledge directly into the design process of ML-NIDS.\n  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly\ncollected for this study. Our findings reveal that baseline ML-NIDS models fail\nto detect several variants entirely, achieving F1 scores as low as 0 %. In\ncontrast, our knowledge-guided approach achieves up to 99 % F1 score while\nmaintaining a False Positive Rate below 0.1 %."
                },
                "authors": [
                    {
                        "name": "Xin Fan Guo"
                    },
                    {
                        "name": "Albert Merono Penuela"
                    },
                    {
                        "name": "Sergio Maffeis"
                    },
                    {
                        "name": "Fabio Pierazzi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Pierazzi"
                },
                "author": "Fabio Pierazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19794v1",
                "updated": "2025-06-24T17:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    4,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study"
                },
                "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Yi Zhong"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Ziheng Zhang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16553v2",
                "updated": "2025-06-24T16:54:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    54,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-19T15:08:37Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    15,
                    8,
                    37,
                    2,
                    78,
                    0
                ],
                "title": "A Foundational individual Mobility Prediction Model based on Open-Source\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Foundational individual Mobility Prediction Model based on Open-Source\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs."
                },
                "authors": [
                    {
                        "name": "Zhenlin Qin"
                    },
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Francisco Camara Pereira"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19783v1",
                "updated": "2025-06-24T16:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:50:51Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    50,
                    51,
                    1,
                    175,
                    0
                ],
                "title": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: Strategy-Adaptive Generation Engine for Query Rewriting"
                },
                "summary": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewriting is pivotal for enhancing dense retrieval, yet current methods\ndemand large-scale supervised data or suffer from inefficient reinforcement\nlearning (RL) exploration. In this work, we first establish that guiding Large\nLanguage Models (LLMs) with a concise set of expert-crafted strategies, such as\nsemantic expansion and entity disambiguation, substantially improves retrieval\neffectiveness on challenging benchmarks, including HotpotQA, FEVER, NFCorpus,\nand SciFact. Building on this insight, we introduce the Strategy-Adaptive\nGeneration Engine (SAGE), which operationalizes these strategies in an RL\nframework. SAGE introduces two novel reward shaping mechanisms-Strategic Credit\nShaping (SCS) and Contrastive Reward Shaping (CRS)-to deliver more informative\nlearning signals. This strategy-guided approach not only achieves new\nstate-of-the-art NDCG@10 results, but also uncovers a compelling emergent\nbehavior: the agent learns to select optimal strategies, reduces unnecessary\nexploration, and generates concise rewrites, lowering inference cost without\nsacrificing performance. Our findings demonstrate that strategy-guided RL,\nenhanced with nuanced reward shaping, offers a scalable, efficient, and more\ninterpretable paradigm for developing the next generation of robust information\nretrieval systems."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Changwang Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19780v1",
                "updated": "2025-06-24T16:47:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    47,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:47:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    47,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference\n  Alignment"
                },
                "summary": "While large-scale unsupervised language models (LMs) capture broad world\nknowledge and reasoning capabilities, steering their behavior toward desired\nobjectives remains challenging due to the lack of explicit supervision.\nExisting alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on training a reward model and performing reinforcement\nlearning to align with human preferences. However, RLHF is often\ncomputationally intensive, unstable, and sensitive to hyperparameters.\n  To address these limitations, Direct Preference Optimization (DPO) was\nintroduced as a lightweight and stable alternative, enabling direct alignment\nof language models with pairwise preference data via classification loss.\nHowever, DPO and its extensions generally assume a single static preference\ndistribution, limiting flexibility in multi-objective or dynamic alignment\nsettings.\n  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted\nListwise DPO, which extends DPO to incorporate multiple human preference\ndimensions (e.g., helpfulness, harmlessness, informativeness) and enables\ndynamic interpolation through a controllable simplex-weighted formulation. Our\nmethod supports both listwise preference feedback and flexible alignment across\nvarying user intents without re-training. Empirical and theoretical analysis\ndemonstrates that our method is as effective as traditional DPO on static\nobjectives while offering greater generality and adaptability for real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large-scale unsupervised language models (LMs) capture broad world\nknowledge and reasoning capabilities, steering their behavior toward desired\nobjectives remains challenging due to the lack of explicit supervision.\nExisting alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on training a reward model and performing reinforcement\nlearning to align with human preferences. However, RLHF is often\ncomputationally intensive, unstable, and sensitive to hyperparameters.\n  To address these limitations, Direct Preference Optimization (DPO) was\nintroduced as a lightweight and stable alternative, enabling direct alignment\nof language models with pairwise preference data via classification loss.\nHowever, DPO and its extensions generally assume a single static preference\ndistribution, limiting flexibility in multi-objective or dynamic alignment\nsettings.\n  In this paper, we propose a novel framework: Multi-Preference Lambda-weighted\nListwise DPO, which extends DPO to incorporate multiple human preference\ndimensions (e.g., helpfulness, harmlessness, informativeness) and enables\ndynamic interpolation through a controllable simplex-weighted formulation. Our\nmethod supports both listwise preference feedback and flexible alignment across\nvarying user intents without re-training. Empirical and theoretical analysis\ndemonstrates that our method is as effective as traditional DPO on static\nobjectives while offering greater generality and adaptability for real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yuhui Sun"
                    },
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Zixi Li"
                    },
                    {
                        "name": "Jinman Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinman Zhao"
                },
                "arxiv_affiliation": "University of Toronto",
                "author": "Jinman Zhao",
                "arxiv_comment": "10 pages, 4 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10326v2",
                "updated": "2025-06-24T16:45:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    45,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-01-17T17:56:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    17,
                    56,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Large language models for automated scholarly paper review: A survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models for automated scholarly paper review: A survey"
                },
                "summary": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publication, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nLLMs hold transformative potential for the full-scale implementation of\nautomated scholarly paper review (ASPR), but they also pose new issues and\nchallenges that need to be addressed. In this survey paper, we aim to provide a\nholistic view of ASPR in the era of LLMs. We begin with a survey to find out\nwhich LLMs are used to conduct ASPR. Then, we review what ASPR-related\ntechnological bottlenecks have been solved with the incorporation of LLM\ntechnology. After that, we move on to explore new methods, new datasets, new\nsource code, and new online systems that come with LLMs for ASPR. Furthermore,\nwe summarize the performance and issues of LLMs in ASPR, and investigate the\nattitudes and reactions of publishers and academia to ASPR. Lastly, we discuss\nthe challenges and future directions associated with the development of LLMs\nfor ASPR. This survey serves as an inspirational reference for the researchers\nand can promote the progress of ASPR for its actual implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly impacted human society,\ninfluencing various domains. Among them, academia is not simply a domain\naffected by LLMs, but it is also the pivotal force in the development of LLMs.\nIn academic publication, this phenomenon is represented during the\nincorporation of LLMs into the peer review mechanism for reviewing manuscripts.\nLLMs hold transformative potential for the full-scale implementation of\nautomated scholarly paper review (ASPR), but they also pose new issues and\nchallenges that need to be addressed. In this survey paper, we aim to provide a\nholistic view of ASPR in the era of LLMs. We begin with a survey to find out\nwhich LLMs are used to conduct ASPR. Then, we review what ASPR-related\ntechnological bottlenecks have been solved with the incorporation of LLM\ntechnology. After that, we move on to explore new methods, new datasets, new\nsource code, and new online systems that come with LLMs for ASPR. Furthermore,\nwe summarize the performance and issues of LLMs in ASPR, and investigate the\nattitudes and reactions of publishers and academia to ASPR. Lastly, we discuss\nthe challenges and future directions associated with the development of LLMs\nfor ASPR. This survey serves as an inspirational reference for the researchers\nand can promote the progress of ASPR for its actual implementation."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Zhuang"
                    },
                    {
                        "name": "Jiandong Chen"
                    },
                    {
                        "name": "Hongfeng Xu"
                    },
                    {
                        "name": "Yuwen Jiang"
                    },
                    {
                        "name": "Jialiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jialiang Lin"
                },
                "author": "Jialiang Lin",
                "arxiv_doi": "10.1016/j.inffus.2025.103332",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.103332",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please cite the version of Information Fusion",
                "arxiv_journal_ref": "Information Fusion, Vol. 124, 103332 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19775v1",
                "updated": "2025-06-24T16:40:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    40,
                    40,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:40:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    40,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Canary in the Mine: An LLM Augmented Survey of Disciplinary Complaints\n  to the Ordre des ingénieurs du Québec (OIQ)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary in the Mine: An LLM Augmented Survey of Disciplinary Complaints\n  to the Ordre des ingénieurs du Québec (OIQ)"
                },
                "summary": "This study uses pre-trained LLMs to conduct thematic analysis to investigate\ndisciplinary incidents involving engineers in Quebec, shedding light on\ncritical gaps in engineering education. Through a comprehensive review of the\ndisciplinary register of the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)'s\ndisciplinary register for 2010 to 2024, researchers from engineering education\nand human resources management in technological development laboratories\nconducted a thematic analysis of reported incidents to identify patterns,\ntrends, and areas for improvement. The analysis aims to uncover the most common\ntypes of disciplinary incidents, underlying causes, and implications for the\nfield in how engineering education addresses (or fails to address) these\nissues. Our findings identify recurring themes, analyze root causes, and offer\nrecommendations for engineering educators and students to mitigate similar\nincidents. This research has implications for informing curriculum development,\nprofessional development, and performance evaluation, ultimately fostering a\nculture of professionalism and ethical responsibility in engineering. By\nproviding empirical evidence of disciplinary incidents and their causes, this\nstudy contributes to evidence-based practices for engineering education and\nprofessional development, enhancing the engineering education community's\nunderstanding of professionalism and ethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study uses pre-trained LLMs to conduct thematic analysis to investigate\ndisciplinary incidents involving engineers in Quebec, shedding light on\ncritical gaps in engineering education. Through a comprehensive review of the\ndisciplinary register of the Ordre des ing\\'enieurs du Qu\\'ebec (OIQ)'s\ndisciplinary register for 2010 to 2024, researchers from engineering education\nand human resources management in technological development laboratories\nconducted a thematic analysis of reported incidents to identify patterns,\ntrends, and areas for improvement. The analysis aims to uncover the most common\ntypes of disciplinary incidents, underlying causes, and implications for the\nfield in how engineering education addresses (or fails to address) these\nissues. Our findings identify recurring themes, analyze root causes, and offer\nrecommendations for engineering educators and students to mitigate similar\nincidents. This research has implications for informing curriculum development,\nprofessional development, and performance evaluation, ultimately fostering a\nculture of professionalism and ethical responsibility in engineering. By\nproviding empirical evidence of disciplinary incidents and their causes, this\nstudy contributes to evidence-based practices for engineering education and\nprofessional development, enhancing the engineering education community's\nunderstanding of professionalism and ethics."
                },
                "authors": [
                    {
                        "name": "Tammy Mackenzie"
                    },
                    {
                        "name": "Varsha Kesavan"
                    },
                    {
                        "name": "Thomas Mekhael"
                    },
                    {
                        "name": "Animesh Paul"
                    },
                    {
                        "name": "Branislav Radeljic"
                    },
                    {
                        "name": "Sara Kodeiri"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    }
                ],
                "author_detail": {
                    "name": "Sreyoshi Bhaduri"
                },
                "author": "Sreyoshi Bhaduri",
                "arxiv_comment": "22 pages, accepted at the American Society of Engineering Education\n  annual conference 2025, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19773v1",
                "updated": "2025-06-24T16:38:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    38,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:38:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    38,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights\n  from an Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization for Knowledge Graph Construction: Insights\n  from an Empirical Study"
                },
                "summary": "A KG represents a network of entities and illustrates relationships between\nthem. KGs are used for various applications, including semantic search and\ndiscovery, reasoning, decision-making, natural language processing, machine\nlearning, and recommendation systems. Triple (subject-relation-object)\nextraction from text is the fundamental building block of KG construction and\nhas been widely studied, for example, in early benchmarks such as ACE 2002 to\nmore recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs\nis explored for KG construction, handcrafting reasonable task-specific prompts\nfor LLMs is a labour-intensive exercise and can be brittle due to subtle\nchanges in the LLM models employed. Recent work in NLP tasks (e.g. autonomy\ngeneration) uses automatic prompt optimization/engineering to address this\nchallenge by generating optimal or near-optimal task-specific prompts given\ninput-output examples.\n  This empirical study explores the application of automatic prompt\noptimization for the triple extraction task using experimental benchmarking. We\nevaluate different settings by changing (a) the prompting strategy, (b) the LLM\nbeing used for prompt optimization and task execution, (c) the number of\ncanonical relations in the schema (schema complexity), (d) the length and\ndiversity of input text, (e) the metric used to drive the prompt optimization,\nand (f) the dataset being used for training and testing. We evaluate three\ndifferent automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use\ntwo different triple extraction datasets, SynthIE and REBEL. Through rigorous\nempirical evaluation, our main contribution highlights that automatic prompt\noptimization techniques can generate reasonable prompts similar to humans for\ntriple extraction. In turn, these optimized prompts achieve improved results,\nparticularly with increasing schema complexity and text size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A KG represents a network of entities and illustrates relationships between\nthem. KGs are used for various applications, including semantic search and\ndiscovery, reasoning, decision-making, natural language processing, machine\nlearning, and recommendation systems. Triple (subject-relation-object)\nextraction from text is the fundamental building block of KG construction and\nhas been widely studied, for example, in early benchmarks such as ACE 2002 to\nmore recent ones, such as WebNLG 2020, REBEL and SynthIE. While the use of LLMs\nis explored for KG construction, handcrafting reasonable task-specific prompts\nfor LLMs is a labour-intensive exercise and can be brittle due to subtle\nchanges in the LLM models employed. Recent work in NLP tasks (e.g. autonomy\ngeneration) uses automatic prompt optimization/engineering to address this\nchallenge by generating optimal or near-optimal task-specific prompts given\ninput-output examples.\n  This empirical study explores the application of automatic prompt\noptimization for the triple extraction task using experimental benchmarking. We\nevaluate different settings by changing (a) the prompting strategy, (b) the LLM\nbeing used for prompt optimization and task execution, (c) the number of\ncanonical relations in the schema (schema complexity), (d) the length and\ndiversity of input text, (e) the metric used to drive the prompt optimization,\nand (f) the dataset being used for training and testing. We evaluate three\ndifferent automatic prompt optimizers, namely, DSPy, APE, and TextGrad and use\ntwo different triple extraction datasets, SynthIE and REBEL. Through rigorous\nempirical evaluation, our main contribution highlights that automatic prompt\noptimization techniques can generate reasonable prompts similar to humans for\ntriple extraction. In turn, these optimized prompts achieve improved results,\nparticularly with increasing schema complexity and text size."
                },
                "authors": [
                    {
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "name": "Niharika S. D'Souza"
                    },
                    {
                        "name": "Faisal Chowdhury"
                    },
                    {
                        "name": "Horst Samulowitz"
                    }
                ],
                "author_detail": {
                    "name": "Horst Samulowitz"
                },
                "author": "Horst Samulowitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19769v1",
                "updated": "2025-06-24T16:34:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    34,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:34:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    34,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background,\n  Methods, Challenges and Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background,\n  Methods, Challenges and Prospects"
                },
                "summary": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,\nwhich can serve a variety of downstream tasks (e.g., 3D object detection and\nsemantic segmentation) and application scenarios (e.g., autonomous driving and\nswarm robotics). Recently, impressive achievements on AI-based MSFP methods\nhave been reviewed in relevant surveys. However, we observe that the existing\nsurveys have some limitations after a rigorous and detailed investigation. For\none thing, most surveys are oriented to a single task or research field, such\nas 3D object detection or autonomous driving. Therefore, researchers in other\nrelated tasks often find it difficult to benefit directly. For another, most\nsurveys only introduce MSFP from a single perspective of multi-modal fusion,\nwhile lacking consideration of the diversity of MSFP methods, such as\nmulti-view fusion and time-series fusion. To this end, in this paper, we hope\nto organize MSFP research from a task-agnostic perspective, where methods are\nreported from various technical views. Specifically, we first introduce the\nbackground of MSFP. Next, we review multi-modal and multi-agent fusion methods.\nA step further, time-series fusion methods are analyzed. In the era of LLM, we\nalso investigate multimodal LLM fusion methods. Finally, we discuss open\nchallenges and future directions for MSFP. We hope this survey can help\nresearchers understand the important progress in MSFP and provide possible\ninsights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,\nwhich can serve a variety of downstream tasks (e.g., 3D object detection and\nsemantic segmentation) and application scenarios (e.g., autonomous driving and\nswarm robotics). Recently, impressive achievements on AI-based MSFP methods\nhave been reviewed in relevant surveys. However, we observe that the existing\nsurveys have some limitations after a rigorous and detailed investigation. For\none thing, most surveys are oriented to a single task or research field, such\nas 3D object detection or autonomous driving. Therefore, researchers in other\nrelated tasks often find it difficult to benefit directly. For another, most\nsurveys only introduce MSFP from a single perspective of multi-modal fusion,\nwhile lacking consideration of the diversity of MSFP methods, such as\nmulti-view fusion and time-series fusion. To this end, in this paper, we hope\nto organize MSFP research from a task-agnostic perspective, where methods are\nreported from various technical views. Specifically, we first introduce the\nbackground of MSFP. Next, we review multi-modal and multi-agent fusion methods.\nA step further, time-series fusion methods are analyzed. In the era of LLM, we\nalso investigate multimodal LLM fusion methods. Finally, we discuss open\nchallenges and future directions for MSFP. We hope this survey can help\nresearchers understand the important progress in MSFP and provide possible\ninsights for future research."
                },
                "authors": [
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Rongwei Wang"
                    },
                    {
                        "name": "Xuchen Shen"
                    },
                    {
                        "name": "Huijie Liu"
                    },
                    {
                        "name": "Baihui Xiao"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "You He"
                    }
                ],
                "author_detail": {
                    "name": "You He"
                },
                "author": "You He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19767v1",
                "updated": "2025-06-24T16:31:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    37,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:31:37Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    37,
                    1,
                    175,
                    0
                ],
                "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks."
                },
                "authors": [
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Tinghong Chen"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Xihuai Wang"
                    },
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Yuanheng Zhu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19832v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19832v3",
                "updated": "2025-06-24T16:31:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    31,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-29T16:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    16,
                    44,
                    2,
                    4,
                    334,
                    0
                ],
                "title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation"
                },
                "summary": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others."
                },
                "authors": [
                    {
                        "name": "Dimosthenis Antypas"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Carla Perez-Almendros"
                    },
                    {
                        "name": "Jose Camacho-Collados"
                    },
                    {
                        "name": "Francesco Barbieri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Barbieri"
                },
                "author": "Francesco Barbieri",
                "arxiv_comment": "Accepted at the 9th Workshop on Online Abuse and Harms (WOAH)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19832v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19832v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19753v1",
                "updated": "2025-06-24T16:06:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T16:06:58Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    58,
                    1,
                    175,
                    0
                ],
                "title": "Arabic Dialect Classification using RNNs, Transformers, and Large\n  Language Models: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arabic Dialect Classification using RNNs, Transformers, and Large\n  Language Models: A Comparative Analysis"
                },
                "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities."
                },
                "authors": [
                    {
                        "name": "Omar A. Essameldin"
                    },
                    {
                        "name": "Ali O. Elbeih"
                    },
                    {
                        "name": "Wael H. Gomaa"
                    },
                    {
                        "name": "Wael F. Elsersy"
                    }
                ],
                "author_detail": {
                    "name": "Wael F. Elsersy"
                },
                "author": "Wael F. Elsersy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19750v2",
                "updated": "2025-06-25T11:56:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    11,
                    56,
                    15,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-24T16:06:37Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    6,
                    37,
                    1,
                    175,
                    0
                ],
                "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A\n  Synthetic Vignette Simulation Approach"
                },
                "summary": "Symptom Checkers (SCs) provide users with personalized medical information.\nTo prevent performance degradation from algorithm updates, SC developers must\nevaluate diagnostic performance changes for individual diseases before\ndeployment. However, acquiring sufficient evaluation data for rare diseases is\ndifficult, and manually creating numerous clinical vignettes is costly and\nimpractical. This study proposes and validates a novel Synthetic Vignette\nSimulation Approach to evaluate diagnostic performance changes for individual\nrare diseases following SC algorithm updates. We used disease-phenotype\nannotations from the Human Phenotype Ontology (HPO), a knowledge database for\nrare diseases, to generate synthetic vignettes. With these, we simulated SC\ninterviews to estimate the impact of algorithm updates on real-world diagnostic\nperformance. The method's effectiveness was evaluated retrospectively by\ncomparing estimated values with actual metric changes using the $R^2$\ncoefficient. The experiment included eight past SC algorithm updates. For\nupdates on diseases with frequency information in HPO (n=5), the $R^2$ for\nRecall@8 change was 0.831 ($p$=0.031), and for Precision@8 change, it was 0.78\n($p$=0.047), indicating the method can predict post-deployment performance. In\ncontrast, large prediction errors occurred for diseases without frequency\ninformation (n=3), highlighting its importance. Our method enables\npre-deployment evaluation of SC algorithm changes for individual rare diseases\nusing a publicly available, expert-created knowledge base. This transparent and\nlow-cost approach allows developers to efficiently improve diagnostic\nperformance for rare diseases, potentially enhancing support for early\ndiagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symptom Checkers (SCs) provide users with personalized medical information.\nTo prevent performance degradation from algorithm updates, SC developers must\nevaluate diagnostic performance changes for individual diseases before\ndeployment. However, acquiring sufficient evaluation data for rare diseases is\ndifficult, and manually creating numerous clinical vignettes is costly and\nimpractical. This study proposes and validates a novel Synthetic Vignette\nSimulation Approach to evaluate diagnostic performance changes for individual\nrare diseases following SC algorithm updates. We used disease-phenotype\nannotations from the Human Phenotype Ontology (HPO), a knowledge database for\nrare diseases, to generate synthetic vignettes. With these, we simulated SC\ninterviews to estimate the impact of algorithm updates on real-world diagnostic\nperformance. The method's effectiveness was evaluated retrospectively by\ncomparing estimated values with actual metric changes using the $R^2$\ncoefficient. The experiment included eight past SC algorithm updates. For\nupdates on diseases with frequency information in HPO (n=5), the $R^2$ for\nRecall@8 change was 0.831 ($p$=0.031), and for Precision@8 change, it was 0.78\n($p$=0.047), indicating the method can predict post-deployment performance. In\ncontrast, large prediction errors occurred for diseases without frequency\ninformation (n=3), highlighting its importance. Our method enables\npre-deployment evaluation of SC algorithm changes for individual rare diseases\nusing a publicly available, expert-created knowledge base. This transparent and\nlow-cost approach allows developers to efficiently improve diagnostic\nperformance for rare diseases, potentially enhancing support for early\ndiagnosis."
                },
                "authors": [
                    {
                        "name": "Takashi Nishibayashi"
                    },
                    {
                        "name": "Seiji Kanazawa"
                    },
                    {
                        "name": "Kumpei Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Kumpei Yamada"
                },
                "author": "Kumpei Yamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20185v2",
                "updated": "2025-06-24T16:03:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    3,
                    30,
                    1,
                    175,
                    0
                ],
                "published": "2024-12-28T15:51:02Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    15,
                    51,
                    2,
                    5,
                    363,
                    0
                ],
                "title": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecDEC: A Systems Approach to Advancing Low-Bit LLM Quantization"
                },
                "summary": "Quantization of Large Language Models (LLMs) has recently gained popularity,\nparticularly for on-device settings with limited hardware resources. While\nefficient, quantization inevitably degrades model quality, especially in\naggressive low-bit settings such as 3-bit and 4-bit precision. In this paper,\nwe propose DecDEC, an inference scheme that improves the quality of low-bit\nLLMs while preserving the key benefits of quantization: GPU memory savings and\nlatency reduction. DecDEC stores the residual matrix -- the difference between\nfull-precision and quantized weights -- in CPU, and dynamically fetches the\nresiduals for only a small portion of the weights. This portion corresponds to\nthe salient channels, marked by activation outliers, with the fetched residuals\nhelping to correct quantization errors in these channels. Salient channels are\nidentified dynamically at each decoding step by analyzing the input activations\n-- this enables adaptation to the dynamic nature of activation distribution,\nthus maximizing the effectiveness of error compensation. We demonstrate the\neffectiveness of DecDEC by augmenting state-of-the-art quantization methods.\nFor example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model\nfrom 10.15 to 9.12 -- outperforming its 3.5-bit counterpart -- while adding\nless than 0.0003\\% to GPU memory usage and incurring only a 1.7\\% inference\nslowdown on NVIDIA RTX 4050 Mobile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization of Large Language Models (LLMs) has recently gained popularity,\nparticularly for on-device settings with limited hardware resources. While\nefficient, quantization inevitably degrades model quality, especially in\naggressive low-bit settings such as 3-bit and 4-bit precision. In this paper,\nwe propose DecDEC, an inference scheme that improves the quality of low-bit\nLLMs while preserving the key benefits of quantization: GPU memory savings and\nlatency reduction. DecDEC stores the residual matrix -- the difference between\nfull-precision and quantized weights -- in CPU, and dynamically fetches the\nresiduals for only a small portion of the weights. This portion corresponds to\nthe salient channels, marked by activation outliers, with the fetched residuals\nhelping to correct quantization errors in these channels. Salient channels are\nidentified dynamically at each decoding step by analyzing the input activations\n-- this enables adaptation to the dynamic nature of activation distribution,\nthus maximizing the effectiveness of error compensation. We demonstrate the\neffectiveness of DecDEC by augmenting state-of-the-art quantization methods.\nFor example, DecDEC reduces the perplexity of a 3-bit Llama-3-8B-Instruct model\nfrom 10.15 to 9.12 -- outperforming its 3.5-bit counterpart -- while adding\nless than 0.0003\\% to GPU memory usage and incurring only a 1.7\\% inference\nslowdown on NVIDIA RTX 4050 Mobile."
                },
                "authors": [
                    {
                        "name": "Yeonhong Park"
                    },
                    {
                        "name": "Jake Hyun"
                    },
                    {
                        "name": "Hojoon Kim"
                    },
                    {
                        "name": "Jae W. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae W. Lee"
                },
                "author": "Jae W. Lee",
                "arxiv_comment": "OSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12743v2",
                "updated": "2025-06-24T16:03:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    16,
                    3,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-18T11:00:28Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    0,
                    28,
                    1,
                    49,
                    0
                ],
                "title": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect\n  and Explain LLM-Generated Texts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I know myself better, but not really greatly\": How Well Can LLMs Detect\n  and Explain LLM-Generated Texts?"
                },
                "summary": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing between human- and LLM-generated texts is crucial given the\nrisks associated with misuse of LLMs. This paper investigates detection and\nexplanation capabilities of current LLMs across two settings: binary (human vs.\nLLM-generated) and ternary classification (including an ``undecided'' class).\nWe evaluate 6 close- and open-source LLMs of varying sizes and find that\nself-detection (LLMs identifying their own outputs) consistently outperforms\ncross-detection (identifying outputs from other LLMs), though both remain\nsuboptimal. Introducing a ternary classification framework improves both\ndetection accuracy and explanation quality across all models. Through\ncomprehensive quantitative and qualitative analyses using our human-annotated\ndataset, we identify key explanation failures, primarily reliance on inaccurate\nfeatures, hallucinations, and flawed reasoning. Our findings underscore the\nlimitations of current LLMs in self-detection and self-explanation,\nhighlighting the need for further research to address overfitting and enhance\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Jiazhou Ji"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Weidong Qiu"
                    },
                    {
                        "name": "Zheng Huang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Xinru Lu"
                    },
                    {
                        "name": "Xiaoyu Jiang"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19741v1",
                "updated": "2025-06-24T15:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    58,
                    55,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:58:55Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    58,
                    55,
                    1,
                    175,
                    0
                ],
                "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls"
                },
                "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT"
                },
                "authors": [
                    {
                        "name": "Yihong Luo"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Tianyang Hu"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02978v2",
                "updated": "2025-06-24T15:54:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    54,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2024-08-06T06:24:10Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    6,
                    24,
                    10,
                    1,
                    219,
                    0
                ],
                "title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval"
                },
                "summary": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval."
                },
                "authors": [
                    {
                        "name": "Ruixiang Zhao"
                    },
                    {
                        "name": "Jian Jia"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Xuehan Bai"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Xirong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xirong Li"
                },
                "author": "Xirong Li",
                "arxiv_comment": "accepted for publication as a REGULAR paper in the IEEE Transactions\n  on Multimedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19733v1",
                "updated": "2025-06-24T15:53:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    53,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:53:10Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    53,
                    10,
                    1,
                    175,
                    0
                ],
                "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To\n  Unseen Domains?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To\n  Unseen Domains?"
                },
                "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns."
                },
                "authors": [
                    {
                        "name": "Chuxuan Hu"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Antony Kellermann"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Suppakit Waiwitlikhit"
                    },
                    {
                        "name": "Jason Benn"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19732v1",
                "updated": "2025-06-24T15:50:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    50,
                    35,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:50:35Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    50,
                    35,
                    1,
                    175,
                    0
                ],
                "title": "Who Does What in Deep Learning? Multidimensional Game-Theoretic\n  Attribution of Function of Neural Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Does What in Deep Learning? Multidimensional Game-Theoretic\n  Attribution of Function of Neural Units"
                },
                "summary": "Neural networks now generate text, images, and speech with billions of\nparameters, producing a need to know how each neural unit contributes to these\nhigh-dimensional outputs. Existing explainable-AI methods, such as SHAP,\nattribute importance to inputs, but cannot quantify the contributions of neural\nunits across thousands of output pixels, tokens, or logits. Here we close that\ngap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic\ngame-theoretic framework. By systematically lesioning combinations of units,\nMSA yields Shapley Modes, unit-wise contribution maps that share the exact\ndimensionality of the model's output. We apply MSA across scales, from\nmulti-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative\nAdversarial Networks (GAN). The approach demonstrates how regularisation\nconcentrates computation in a few hubs, exposes language-specific experts\ninside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.\nTogether, these results showcase MSA as a powerful approach for interpreting,\nediting, and compressing deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks now generate text, images, and speech with billions of\nparameters, producing a need to know how each neural unit contributes to these\nhigh-dimensional outputs. Existing explainable-AI methods, such as SHAP,\nattribute importance to inputs, but cannot quantify the contributions of neural\nunits across thousands of output pixels, tokens, or logits. Here we close that\ngap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic\ngame-theoretic framework. By systematically lesioning combinations of units,\nMSA yields Shapley Modes, unit-wise contribution maps that share the exact\ndimensionality of the model's output. We apply MSA across scales, from\nmulti-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative\nAdversarial Networks (GAN). The approach demonstrates how regularisation\nconcentrates computation in a few hubs, exposes language-specific experts\ninside the LLM, and reveals an inverted pixel-generation hierarchy in GANs.\nTogether, these results showcase MSA as a powerful approach for interpreting,\nediting, and compressing deep neural networks."
                },
                "authors": [
                    {
                        "name": "Shrey Dixit"
                    },
                    {
                        "name": "Kayson Fakhar"
                    },
                    {
                        "name": "Fatemeh Hadaeghi"
                    },
                    {
                        "name": "Patrick Mineault"
                    },
                    {
                        "name": "Konrad P. Kording"
                    },
                    {
                        "name": "Claus C. Hilgetag"
                    }
                ],
                "author_detail": {
                    "name": "Claus C. Hilgetag"
                },
                "author": "Claus C. Hilgetag",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18259v2",
                "updated": "2025-06-24T15:45:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    45,
                    5,
                    1,
                    175,
                    0
                ],
                "published": "2024-06-26T11:11:47Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    11,
                    47,
                    2,
                    178,
                    0
                ],
                "title": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated"
                },
                "summary": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power."
                },
                "authors": [
                    {
                        "name": "Jiazhou Ji"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Shujun Li"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Weidong Qiu"
                    },
                    {
                        "name": "Zheng Huang"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Xiaoyu Jiang"
                    },
                    {
                        "name": "Xinru Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xinru Lu"
                },
                "author": "Xinru Lu",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09730v2",
                "updated": "2025-06-24T15:42:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    42,
                    55,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-12T18:20:47Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    20,
                    47,
                    2,
                    71,
                    0
                ],
                "title": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem\n  Proving"
                },
                "summary": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the LLMs,\neven for the step-wise rewards, or large quantities of human-annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which, unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most promising recent methods for AI reasoning require applying variants\nof reinforcement learning (RL) either on rolled out trajectories from the LLMs,\neven for the step-wise rewards, or large quantities of human-annotated\ntrajectory data. The reliance on the rolled-out trajectory renders the compute\ncost and time prohibitively high. In particular, the correctness of a reasoning\ntrajectory can typically only be judged at its completion, leading to sparse\nrewards in RL or requiring expensive synthetic data generation in expert\niteration-like methods. In this work, we focus on the Automatic Theorem Proving\n(ATP) task and propose a novel verifier-in-the-loop design, which, unlike\nexisting approaches that leverage feedback on the entire reasoning trajectory,\nemploys an automated verifier to give intermediate feedback at each step of the\nreasoning process. Using Lean as the verifier, we empirically show that the\nstep-by-step local verification produces a global improvement in the model's\nreasoning accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Sara Rajaee"
                    },
                    {
                        "name": "Kumar Pratik"
                    },
                    {
                        "name": "Gabriele Cesa"
                    },
                    {
                        "name": "Arash Behboodi"
                    }
                ],
                "author_detail": {
                    "name": "Arash Behboodi"
                },
                "author": "Arash Behboodi",
                "arxiv_comment": "Accepted at the Findings of ACL 2025, Accepted at ICLR 2025 Workshop\n  on Reasoning and Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19702v1",
                "updated": "2025-06-24T15:12:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    12,
                    42,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:12:42Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    12,
                    42,
                    1,
                    175,
                    0
                ],
                "title": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology\n  and Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology\n  and Differential Diagnosis"
                },
                "summary": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical document analysis plays a crucial role in extracting essential\nclinical insights from unstructured healthcare records, supporting critical\ntasks such as differential diagnosis. Determining the most probable condition\namong overlapping symptoms requires precise evaluation and deep medical\nexpertise. While recent advancements in large language models (LLMs) have\nsignificantly enhanced performance in medical document analysis, privacy\nconcerns related to sensitive patient data limit the use of online LLMs\nservices in clinical settings. To address these challenges, we propose a\ntrustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using\nlow-rank adaptation, specifically optimized for differential diagnosis tasks.\nOur approach utilizes DDXPlus, the largest benchmark dataset for differential\ndiagnosis, and demonstrates superior performance in pathology prediction and\nvariable-length differential diagnosis compared to existing methods. The\ndeveloped web-based platform allows users to submit their own unstructured\nmedical documents and receive accurate, explainable diagnostic results. By\nincorporating advanced explainability techniques, the system ensures\ntransparent and reliable predictions, fostering user trust and confidence.\nExtensive evaluations confirm that the proposed method surpasses current\nstate-of-the-art models in predictive accuracy while offering practical utility\nin clinical settings. This work addresses the urgent need for reliable,\nexplainable, and privacy-preserving artificial intelligence solutions,\nrepresenting a significant advancement in intelligent medical document analysis\nfor real-world healthcare applications. The code can be found at\n\\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}."
                },
                "authors": [
                    {
                        "name": "Lei Kang"
                    },
                    {
                        "name": "Xuanshuo Fu"
                    },
                    {
                        "name": "Oriol Ramos Terrades"
                    },
                    {
                        "name": "Javier Vazquez-Corral"
                    },
                    {
                        "name": "Ernest Valveny"
                    },
                    {
                        "name": "Dimosthenis Karatzas"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Karatzas"
                },
                "author": "Dimosthenis Karatzas",
                "arxiv_comment": "Accepted at ICDAR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19697v1",
                "updated": "2025-06-24T15:03:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    3,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T15:03:57Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    15,
                    3,
                    57,
                    1,
                    175,
                    0
                ],
                "title": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models"
                },
                "summary": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme activation outliers in Large Language Models (LLMs) critically\ndegrade quantization performance, hindering efficient on-device deployment.\nWhile channel-wise operations and adaptive gradient scaling are recognized\ncauses, practical mitigation remains challenging. We introduce Outlier-Safe\nPre-Training (OSP), a practical guideline that proactively prevents outlier\nformation rather than relying on post-hoc mitigation. OSP combines three key\ninnovations: (1) the Muon optimizer, eliminating privileged bases while\nmaintaining training efficiency; (2) Single-Scale RMSNorm, preventing\nchannel-wise amplification; and (3) a learnable embedding projection,\nredistributing activation magnitudes originating from embedding matrices. We\nvalidate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is\nthe first production-scale LLM trained without such outliers. Under aggressive\n4-bit quantization, our OSP model achieves a 35.7 average score across 10\nbenchmarks (compared to 26.5 for an Adam-trained model), with only a 2%\ntraining overhead. Remarkably, OSP models exhibit near-zero excess kurtosis\n(0.04) compared to extreme values (1818.56) in standard models, fundamentally\naltering LLM quantization behavior. Our work demonstrates that outliers are not\ninherent to LLMs but are consequences of training strategies, paving the way\nfor more efficient LLM deployment. The source code and pretrained checkpoints\nare available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."
                },
                "authors": [
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19687v1",
                "updated": "2025-06-24T14:56:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    56,
                    55,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:56:55Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    56,
                    55,
                    1,
                    175,
                    0
                ],
                "title": "ReCoGNet: Recurrent Context-Guided Network for 3D MRI Prostate\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCoGNet: Recurrent Context-Guided Network for 3D MRI Prostate\n  Segmentation"
                },
                "summary": "Prostate gland segmentation from T2-weighted MRI is a critical yet\nchallenging task in clinical prostate cancer assessment. While deep\nlearning-based methods have significantly advanced automated segmentation, most\nconventional approaches-particularly 2D convolutional neural networks\n(CNNs)-fail to leverage inter-slice anatomical continuity, limiting their\naccuracy and robustness. Fully 3D models offer improved spatial coherence but\nrequire large amounts of annotated data, which is often impractical in clinical\nsettings. To address these limitations, we propose a hybrid architecture that\nmodels MRI sequences as spatiotemporal data. Our method uses a deep, pretrained\nDeepLabV3 backbone to extract high-level semantic features from each MRI slice\nand a recurrent convolutional head, built with ConvLSTM layers, to integrate\ninformation across slices while preserving spatial structure. This combination\nenables context-aware segmentation with improved consistency, particularly in\ndata-limited and noisy imaging conditions. We evaluate our method on the\nPROMISE12 benchmark under both clean and contrast-degraded test settings.\nCompared to state-of-the-art 2D and 3D segmentation models, our approach\ndemonstrates superior performance in terms of precision, recall, Intersection\nover Union (IoU), and Dice Similarity Coefficient (DSC), highlighting its\npotential for robust clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prostate gland segmentation from T2-weighted MRI is a critical yet\nchallenging task in clinical prostate cancer assessment. While deep\nlearning-based methods have significantly advanced automated segmentation, most\nconventional approaches-particularly 2D convolutional neural networks\n(CNNs)-fail to leverage inter-slice anatomical continuity, limiting their\naccuracy and robustness. Fully 3D models offer improved spatial coherence but\nrequire large amounts of annotated data, which is often impractical in clinical\nsettings. To address these limitations, we propose a hybrid architecture that\nmodels MRI sequences as spatiotemporal data. Our method uses a deep, pretrained\nDeepLabV3 backbone to extract high-level semantic features from each MRI slice\nand a recurrent convolutional head, built with ConvLSTM layers, to integrate\ninformation across slices while preserving spatial structure. This combination\nenables context-aware segmentation with improved consistency, particularly in\ndata-limited and noisy imaging conditions. We evaluate our method on the\nPROMISE12 benchmark under both clean and contrast-degraded test settings.\nCompared to state-of-the-art 2D and 3D segmentation models, our approach\ndemonstrates superior performance in terms of precision, recall, Intersection\nover Union (IoU), and Dice Similarity Coefficient (DSC), highlighting its\npotential for robust clinical deployment."
                },
                "authors": [
                    {
                        "name": "Ahmad Mustafa"
                    },
                    {
                        "name": "Reza Rastegar"
                    },
                    {
                        "name": "Ghassan AlRegib"
                    }
                ],
                "author_detail": {
                    "name": "Ghassan AlRegib"
                },
                "author": "Ghassan AlRegib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19683v1",
                "updated": "2025-06-24T14:49:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:49:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    49,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Scene Graph for Ultrasound Image Explanation and Scanning\n  Guidance"
                },
                "summary": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding medical ultrasound imaging remains a long-standing challenge\ndue to significant visual variability caused by differences in imaging and\nacquisition parameters. Recent advancements in large language models (LLMs)\nhave been used to automatically generate terminology-rich summaries orientated\nto clinicians with sufficient physiological knowledge. Nevertheless, the\nincreasing demand for improved ultrasound interpretability and basic scanning\nguidance among non-expert users, e.g., in point-of-care settings, has not yet\nbeen explored. In this study, we first introduce the scene graph (SG) for\nultrasound images to explain image content to ordinary and provide guidance for\nultrasound scanning. The ultrasound SG is first computed using a\ntransformer-based one-stage method, eliminating the need for explicit object\ndetection. To generate a graspable image explanation for ordinary, the user\nquery is then used to further refine the abstract SG representation through\nLLMs. Additionally, the predicted SG is explored for its potential in guiding\nultrasound scanning toward missing anatomies within the current imaging view,\nassisting ordinary users in achieving more standardized and complete anatomical\nexploration. The effectiveness of this SG-based image explanation and scanning\nguidance has been validated on images from the left and right neck regions,\nincluding the carotid and thyroid, across five volunteers. The results\ndemonstrate the potential of the method to maximally democratize ultrasound by\nenhancing its interpretability and usability for ordinaries."
                },
                "authors": [
                    {
                        "name": "Xuesong Li"
                    },
                    {
                        "name": "Dianye Huang"
                    },
                    {
                        "name": "Yameng Zhang"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Zhongliang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongliang Jiang"
                },
                "author": "Zhongliang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15196v2",
                "updated": "2025-06-24T14:48:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    48,
                    12,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-18T07:20:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    20,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges"
                },
                "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix."
                },
                "authors": [
                    {
                        "name": "Xianliang Yang"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Haolong Qian"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "27 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19677v1",
                "updated": "2025-06-24T14:44:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:44:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees"
                },
                "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."
                },
                "authors": [
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v1",
                "updated": "2025-06-24T14:44:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence, flexibility, and adaptability, and are rapidly\nchanging human production and lifestyle. Nowadays, agents are undergoing a new\nround of evolution. They no longer act as an isolated island like LLMs.\nInstead, they start to communicate with diverse external entities, such as\nother agents and tools, to collectively perform more complex tasks. Under this\ntrend, agent communication is regarded as a foundational pillar of the future\nAI ecosystem, and many organizations intensively begin to design related\ncommunication protocols (e.g., Anthropic's MCP and Google's A2A) within the\nrecent few months. However, this new field exposes significant security hazard,\nwhich can cause severe damage to real-world scenarios. To help researchers to\nquickly figure out this promising topic and benefit the future agent\ncommunication development, this paper presents a comprehensive survey of agent\ncommunication security. More precisely, we first present a clear definition of\nagent communication and categorize the entire lifecyle of agent communication\ninto three stages: user-agent interaction, agent-agent communication, and\nagent-environment communication. Next, for each communication phase, we dissect\nrelated protocols and analyze its security risks according to the communication\ncharacteristics. Then, we summarize and outlook on the possible defense\ncountermeasures for each risk. Finally, we discuss open issues and future\ndirections in this promising research field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19665v1",
                "updated": "2025-06-24T14:29:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    29,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:29:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    29,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recurrent Visual Feature Extraction and Stereo Attentions for CT Report\n  Generation"
                },
                "summary": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating reports for computed tomography (CT) images is a challenging task,\nwhile similar to existing studies for medical image report generation, yet has\nits unique characteristics, such as spatial encoding of multiple images,\nalignment between image volume and texts, etc. Existing solutions typically use\ngeneral 2D or 3D image processing techniques to extract features from a CT\nvolume, where they firstly compress the volume and then divide the compressed\nCT slices into patches for visual encoding. These approaches do not explicitly\naccount for the transformations among CT slices, nor do they effectively\nintegrate multi-level image features, particularly those containing specific\norgan lesions, to instruct CT report generation (CTRG). In considering the\nstrong correlation among consecutive slices in CT scans, in this paper, we\npropose a large language model (LLM) based CTRG method with recurrent visual\nfeature extraction and stereo attentions for hierarchical feature modeling.\nSpecifically, we use a vision Transformer to recurrently process each slice in\na CT volume, and employ a set of attentions over the encoded slices from\ndifferent perspectives to selectively obtain important visual information and\nalign them with textual features, so as to better instruct an LLM for CTRG.\nExperiment results and further analysis on the benchmark M3D-Cap dataset show\nthat our method outperforms strong baseline models and achieves\nstate-of-the-art results, demonstrating its validity and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuanhe Tian"
                    },
                    {
                        "name": "Lei Mao"
                    },
                    {
                        "name": "Yan Song"
                    }
                ],
                "author_detail": {
                    "name": "Yan Song"
                },
                "author": "Yan Song",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v3",
                "updated": "2025-06-24T14:21:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    21,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19652v1",
                "updated": "2025-06-24T14:15:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:15:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    15,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager"
                },
                "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals."
                },
                "authors": [
                    {
                        "name": "Lucie Galland"
                    },
                    {
                        "name": "Catherine Pelachaud"
                    },
                    {
                        "name": "Florian Pecune"
                    }
                ],
                "author_detail": {
                    "name": "Florian Pecune"
                },
                "author": "Florian Pecune",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19646v1",
                "updated": "2025-06-24T14:11:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    11,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:11:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    11,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "Learning to Solve Parametric Mixed-Integer Optimal Control Problems via\n  Differentiable Predictive Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Solve Parametric Mixed-Integer Optimal Control Problems via\n  Differentiable Predictive Control"
                },
                "summary": "We propose a novel approach to solving input- and state-constrained\nparametric mixed-integer optimal control problems using Differentiable\nPredictive Control (DPC). Our approach follows the differentiable programming\nparadigm by learning an explicit neural policy that maps control parameters to\ninteger- and continuous-valued decision variables. This policy is optimized via\nstochastic gradient descent by differentiating the quadratic model predictive\ncontrol objective through the closed-loop finite-horizon response of the system\ndynamics. To handle integrality constraints, we incorporate three\ndifferentiable rounding strategies. The approach is evaluated on a conceptual\nthermal energy system, comparing its performance with the optimal solution for\ndifferent lengths of the prediction horizon. The simulation results indicate\nthat our self-supervised learning approach can achieve near-optimal control\nperformance while significantly reducing inference time by avoiding online\noptimization, thus implying its potential for embedded deployment even on edge\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach to solving input- and state-constrained\nparametric mixed-integer optimal control problems using Differentiable\nPredictive Control (DPC). Our approach follows the differentiable programming\nparadigm by learning an explicit neural policy that maps control parameters to\ninteger- and continuous-valued decision variables. This policy is optimized via\nstochastic gradient descent by differentiating the quadratic model predictive\ncontrol objective through the closed-loop finite-horizon response of the system\ndynamics. To handle integrality constraints, we incorporate three\ndifferentiable rounding strategies. The approach is evaluated on a conceptual\nthermal energy system, comparing its performance with the optimal solution for\ndifferent lengths of the prediction horizon. The simulation results indicate\nthat our self-supervised learning approach can achieve near-optimal control\nperformance while significantly reducing inference time by avoiding online\noptimization, thus implying its potential for embedded deployment even on edge\ndevices."
                },
                "authors": [
                    {
                        "name": "Ján Boldocký"
                    },
                    {
                        "name": "Shahriar Dadras Javan"
                    },
                    {
                        "name": "Martin Gulan"
                    },
                    {
                        "name": "Martin Mönnigmann"
                    },
                    {
                        "name": "Ján Drgoňa"
                    }
                ],
                "author_detail": {
                    "name": "Ján Drgoňa"
                },
                "author": "Ján Drgoňa",
                "arxiv_comment": "7 pages, 2 figures, 1 algorithm, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19645v1",
                "updated": "2025-06-24T14:09:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    9,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T14:09:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    9,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "Tensor-Parallelism with Partially Synchronized Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor-Parallelism with Partially Synchronized Activations"
                },
                "summary": "Training and inference of Large Language Models (LLMs) with\ntensor-parallelism requires substantial communication to synchronize\nactivations. Our findings suggest that with a few minor adjustments to current\npractices, LLMs can be trained without fully synchronizing activations,\nreducing bandwidth demands. We name this \"Communication-Aware Architecture for\nTensor-parallelism\" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,\nwith a 50% reduction in tensor-parallel communication and no significant drop\nin pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates\nboth training and inference workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and inference of Large Language Models (LLMs) with\ntensor-parallelism requires substantial communication to synchronize\nactivations. Our findings suggest that with a few minor adjustments to current\npractices, LLMs can be trained without fully synchronizing activations,\nreducing bandwidth demands. We name this \"Communication-Aware Architecture for\nTensor-parallelism\" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,\nwith a 50% reduction in tensor-parallel communication and no significant drop\nin pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates\nboth training and inference workloads."
                },
                "authors": [
                    {
                        "name": "Itay Lamprecht"
                    },
                    {
                        "name": "Asaf Karnieli"
                    },
                    {
                        "name": "Yair Hanani"
                    },
                    {
                        "name": "Niv Giladi"
                    },
                    {
                        "name": "Daniel Soudry"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Soudry"
                },
                "author": "Daniel Soudry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19636v1",
                "updated": "2025-06-24T13:58:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    58,
                    12,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    58,
                    12,
                    1,
                    175,
                    0
                ],
                "title": "Resilience assessment framework for cyber-physical distribution power\n  system based on coordinated cyber-physical attacks under dynamic game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resilience assessment framework for cyber-physical distribution power\n  system based on coordinated cyber-physical attacks under dynamic game"
                },
                "summary": "Owing to the advanced communication networks and intelligent electronic\ndevices, the cyber-physical distribution systems (CPDSs) possess the capability\nto perform flexible economic dispatch and achieve rapid self-healing from\nextreme events. Meanwhile, the deep integration of cyber and physical systems\nmakes CPDS vulnerable to coordinated cyber-physical attacks. In this paper, a\nresilience assessment framework for the CPDS under coordinated cyber-physical\nattacks is proposed to investigate the impact of the coordinated attacks on\nload loss and service restoration in CPDS. First, a three-stage\ndefender-attacker-defender dynamic game model considering fake base station\n(FBS) and physical attacks for CPDS is established, aiming at seeking the\noptimal defense resource deployment strategy to enhance the resilience of the\nCPDS. The physical attack is launched to cause faults on the power lines, and\nthe FBS attack is employed to interrupt the service of wireless cellular\nnetwork to hinder the self-healing process of the CPDS. The lognormal shadowing\nmodel and search theory are applied to quantitatively describe the process of\nthe coordinated cyber-physical attacks. Further, the constructed three-stage\ndynamic game model is equivalently recast as a tri-level max-min-max\noptimization model, which is solved using column-and-constraint generation\ncombined with enumeration method. Finally, the effectiveness of the proposed\nresilience assessment framework and solution strategy is demonstrated by\nconducting simulation analysis on the modified IEEE 33-node CPDS and a\nreal-world 47-node CPDS in China.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the advanced communication networks and intelligent electronic\ndevices, the cyber-physical distribution systems (CPDSs) possess the capability\nto perform flexible economic dispatch and achieve rapid self-healing from\nextreme events. Meanwhile, the deep integration of cyber and physical systems\nmakes CPDS vulnerable to coordinated cyber-physical attacks. In this paper, a\nresilience assessment framework for the CPDS under coordinated cyber-physical\nattacks is proposed to investigate the impact of the coordinated attacks on\nload loss and service restoration in CPDS. First, a three-stage\ndefender-attacker-defender dynamic game model considering fake base station\n(FBS) and physical attacks for CPDS is established, aiming at seeking the\noptimal defense resource deployment strategy to enhance the resilience of the\nCPDS. The physical attack is launched to cause faults on the power lines, and\nthe FBS attack is employed to interrupt the service of wireless cellular\nnetwork to hinder the self-healing process of the CPDS. The lognormal shadowing\nmodel and search theory are applied to quantitatively describe the process of\nthe coordinated cyber-physical attacks. Further, the constructed three-stage\ndynamic game model is equivalently recast as a tri-level max-min-max\noptimization model, which is solved using column-and-constraint generation\ncombined with enumeration method. Finally, the effectiveness of the proposed\nresilience assessment framework and solution strategy is demonstrated by\nconducting simulation analysis on the modified IEEE 33-node CPDS and a\nreal-world 47-node CPDS in China."
                },
                "authors": [
                    {
                        "name": "Yulin Liu"
                    },
                    {
                        "name": "Zhaojun Ruan"
                    },
                    {
                        "name": "Libao Shi"
                    }
                ],
                "author_detail": {
                    "name": "Libao Shi"
                },
                "author": "Libao Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06877v2",
                "updated": "2025-06-24T13:55:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    55,
                    38,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-07T17:54:56Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    17,
                    54,
                    56,
                    5,
                    158,
                    0
                ],
                "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training\n  LLMs for Math Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training\n  LLMs for Math Reasoning"
                },
                "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Jiaxing Guo"
                    },
                    {
                        "name": "Wenjie Yang"
                    },
                    {
                        "name": "Shengzhong Zhang"
                    },
                    {
                        "name": "Tongshan Xu"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Zengfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zengfeng Huang"
                },
                "author": "Zengfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19624v1",
                "updated": "2025-06-24T13:42:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    42,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:42:59Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    42,
                    59,
                    1,
                    175,
                    0
                ],
                "title": "Decompiling Smart Contracts with a Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiling Smart Contracts with a Large Language Model"
                },
                "summary": "The widespread lack of broad source code verification on blockchain explorers\nsuch as Etherscan, where despite 78,047,845 smart contracts deployed on\nEthereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents\na severe impediment to blockchain security. This opacity necessitates the\nautomated semantic analysis of on-chain smart contract bytecode, a fundamental\nresearch challenge with direct implications for identifying vulnerabilities and\nunderstanding malicious behavior. Prevailing decompilers struggle to reverse\nbytecode in a readable manner, often yielding convoluted code that critically\nhampers vulnerability analysis and thwarts efforts to dissect contract\nfunctionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation\npipeline that, for the first time, successfully leverages Large Language Models\n(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable\nand semantically faithful Solidity code. Our novel methodology first employs\nrigorous static program analysis to convert bytecode into a structured\nthree-address code (TAC) representation. This intermediate representation then\nguides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset\nof 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.\nThis approach uniquely recovers meaningful variable names, intricate control\nflow, and precise function signatures. Our extensive empirical evaluation\ndemonstrates a significant leap beyond traditional decompilers, achieving an\naverage semantic similarity of 0.82 with original source and markedly superior\nreadability. The practical viability and effectiveness of our research are\ndemonstrated through its implementation in a publicly accessible system,\navailable at https://evmdecompiler.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread lack of broad source code verification on blockchain explorers\nsuch as Etherscan, where despite 78,047,845 smart contracts deployed on\nEthereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents\na severe impediment to blockchain security. This opacity necessitates the\nautomated semantic analysis of on-chain smart contract bytecode, a fundamental\nresearch challenge with direct implications for identifying vulnerabilities and\nunderstanding malicious behavior. Prevailing decompilers struggle to reverse\nbytecode in a readable manner, often yielding convoluted code that critically\nhampers vulnerability analysis and thwarts efforts to dissect contract\nfunctionalities for security auditing.\n  This paper addresses this challenge by introducing a pioneering decompilation\npipeline that, for the first time, successfully leverages Large Language Models\n(LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable\nand semantically faithful Solidity code. Our novel methodology first employs\nrigorous static program analysis to convert bytecode into a structured\nthree-address code (TAC) representation. This intermediate representation then\nguides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset\nof 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity.\nThis approach uniquely recovers meaningful variable names, intricate control\nflow, and precise function signatures. Our extensive empirical evaluation\ndemonstrates a significant leap beyond traditional decompilers, achieving an\naverage semantic similarity of 0.82 with original source and markedly superior\nreadability. The practical viability and effectiveness of our research are\ndemonstrated through its implementation in a publicly accessible system,\navailable at https://evmdecompiler.com."
                },
                "authors": [
                    {
                        "name": "Isaac David"
                    },
                    {
                        "name": "Liyi Zhou"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Arthur Gervais"
                    },
                    {
                        "name": "Kaihua Qin"
                    }
                ],
                "author_detail": {
                    "name": "Kaihua Qin"
                },
                "author": "Kaihua Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19620v1",
                "updated": "2025-06-24T13:39:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    39,
                    32,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:39:32Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    39,
                    32,
                    1,
                    175,
                    0
                ],
                "title": "Probabilistic modelling and safety assurance of an agriculture robot\n  providing light-treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic modelling and safety assurance of an agriculture robot\n  providing light-treatment"
                },
                "summary": "Continued adoption of agricultural robots postulates the farmer's trust in\nthe reliability, robustness and safety of the new technology. This motivates\nour work on safety assurance of agricultural robots, particularly their ability\nto detect, track and avoid obstacles and humans. This paper considers a\nprobabilistic modelling and risk analysis framework for use in the early\ndevelopment phases. Starting off with hazard identification and a risk\nassessment matrix, the behaviour of the mobile robot platform, sensor and\nperception system, and any humans present are captured using three state\nmachines. An auto-generated probabilistic model is then solved and analysed\nusing the probabilistic model checker PRISM. The result provides unique insight\ninto fundamental development and engineering aspects by quantifying the effect\nof the risk mitigation actions and risk reduction associated with distinct\ndesign concepts. These include implications of adopting a higher performance\nand more expensive Object Detection System or opting for a more elaborate\nwarning system to increase human awareness. Although this paper mainly focuses\non the initial concept-development phase, the proposed safety assurance\nframework can also be used during implementation, and subsequent deployment and\noperation phases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continued adoption of agricultural robots postulates the farmer's trust in\nthe reliability, robustness and safety of the new technology. This motivates\nour work on safety assurance of agricultural robots, particularly their ability\nto detect, track and avoid obstacles and humans. This paper considers a\nprobabilistic modelling and risk analysis framework for use in the early\ndevelopment phases. Starting off with hazard identification and a risk\nassessment matrix, the behaviour of the mobile robot platform, sensor and\nperception system, and any humans present are captured using three state\nmachines. An auto-generated probabilistic model is then solved and analysed\nusing the probabilistic model checker PRISM. The result provides unique insight\ninto fundamental development and engineering aspects by quantifying the effect\nof the risk mitigation actions and risk reduction associated with distinct\ndesign concepts. These include implications of adopting a higher performance\nand more expensive Object Detection System or opting for a more elaborate\nwarning system to increase human awareness. Although this paper mainly focuses\non the initial concept-development phase, the proposed safety assurance\nframework can also be used during implementation, and subsequent deployment and\noperation phases."
                },
                "authors": [
                    {
                        "name": "Mustafa Adam"
                    },
                    {
                        "name": "Kangfeng Ye"
                    },
                    {
                        "name": "David A. Anisi"
                    },
                    {
                        "name": "Ana Cavalcanti"
                    },
                    {
                        "name": "Jim Woodcock"
                    },
                    {
                        "name": "Robert Morris"
                    }
                ],
                "author_detail": {
                    "name": "Robert Morris"
                },
                "author": "Robert Morris",
                "arxiv_doi": "10.1109/CASE56687.2023.10260395",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CASE56687.2023.10260395",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.19620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19612v1",
                "updated": "2025-06-24T13:31:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    31,
                    5,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:31:05Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    31,
                    5,
                    1,
                    175,
                    0
                ],
                "title": "A Wireless Self-Calibrating Ultrasound Microphone Array with\n  Sub-Microsecond Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Wireless Self-Calibrating Ultrasound Microphone Array with\n  Sub-Microsecond Synchronization"
                },
                "summary": "We present a novel system architecture for a distributed wireless,\nself-calibrating ultrasound microphone network for synchronized in-air acoustic\nsensing. Once deployed the embedded nodes determine their position in the\nenvironment using the infrared optical tracking system found in the HTC Vive\nLighthouses. After self-calibration, the nodes start sampling the ultrasound\nmicrophone while embedding a synchronization signal in the data which is\nestablished using a wireless Sub-1GHz RF link. Data transmission is handled via\nthe Wi-Fi 6 radio that is embedded in the nodes' SoC, decoupling\nsynchronization from payload transport. A prototype system with a limited\namount of network nodes was used to verify the proposed distributed microphone\narray's wireless data acquisition and synchronization capabilities. This\narchitecture lays the groundwork for scalable, deployable ultrasound arrays for\nsound source localization applications in bio-acoustic research and industrial\nacoustic monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel system architecture for a distributed wireless,\nself-calibrating ultrasound microphone network for synchronized in-air acoustic\nsensing. Once deployed the embedded nodes determine their position in the\nenvironment using the infrared optical tracking system found in the HTC Vive\nLighthouses. After self-calibration, the nodes start sampling the ultrasound\nmicrophone while embedding a synchronization signal in the data which is\nestablished using a wireless Sub-1GHz RF link. Data transmission is handled via\nthe Wi-Fi 6 radio that is embedded in the nodes' SoC, decoupling\nsynchronization from payload transport. A prototype system with a limited\namount of network nodes was used to verify the proposed distributed microphone\narray's wireless data acquisition and synchronization capabilities. This\narchitecture lays the groundwork for scalable, deployable ultrasound arrays for\nsound source localization applications in bio-acoustic research and industrial\nacoustic monitoring."
                },
                "authors": [
                    {
                        "name": "Dennis Laurijssen"
                    },
                    {
                        "name": "Rens Baeyens"
                    },
                    {
                        "name": "Walter Daems"
                    },
                    {
                        "name": "Jan Steckel"
                    }
                ],
                "author_detail": {
                    "name": "Jan Steckel"
                },
                "author": "Jan Steckel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05153v2",
                "updated": "2025-06-24T13:24:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    24,
                    58,
                    1,
                    175,
                    0
                ],
                "published": "2024-12-06T16:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    10,
                    40,
                    4,
                    341,
                    0
                ],
                "title": "A text-to-tabular approach to generate synthetic patient data using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A text-to-tabular approach to generate synthetic patient data using LLMs"
                },
                "summary": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to large-scale high-quality healthcare databases is key to accelerate\nmedical research and make insightful discoveries about diseases. However,\naccess to such data is often limited by patient privacy concerns, data sharing\nrestrictions and high costs. To overcome these limitations, synthetic patient\ndata has emerged as an alternative. However, synthetic data generation (SDG)\nmethods typically rely on machine learning (ML) models trained on original\ndata, leading back to the data scarcity problem. We propose an approach to\ngenerate synthetic tabular patient data that does not require access to the\noriginal data, but only a description of the desired database. We leverage\nprior medical knowledge and in-context learning capabilities of large language\nmodels (LLMs) to generate realistic patient data, even in a low-resource\nsetting. We quantitatively evaluate our approach against state-of-the-art SDG\nmodels, using fidelity, privacy, and utility metrics. Our results show that\nwhile LLMs may not match the performance of state-of-the-art models trained on\nthe original data, they effectively generate realistic patient data with\nwell-preserved clinical correlations. An ablation study highlights key elements\nof our prompt contributing to high-quality synthetic patient data generation.\nThis approach, which is easy to use and does not require original data or\nadvanced ML skills, is particularly valuable for quickly generating\ncustom-designed patient data, supporting project implementation and providing\neducational resources."
                },
                "authors": [
                    {
                        "name": "Margaux Tornqvist"
                    },
                    {
                        "name": "Jean-Daniel Zucker"
                    },
                    {
                        "name": "Tristan Fauvel"
                    },
                    {
                        "name": "Nicolas Lambert"
                    },
                    {
                        "name": "Mathilde Berthelot"
                    },
                    {
                        "name": "Antoine Movschin"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Movschin"
                },
                "author": "Antoine Movschin",
                "arxiv_comment": "12 pages, 3 figures. Accepted to the 2025 IEEE International\n  Conference on Healthcare Informatics (IEEE ICHI 2025), 2025, Rende (CS),\n  Calabria, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19607v1",
                "updated": "2025-06-24T13:20:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    20,
                    31,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:20:31Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    20,
                    31,
                    1,
                    175,
                    0
                ],
                "title": "Correcting Hallucinations in News Summaries: Exploration of\n  Self-Correcting LLM Methods with External Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correcting Hallucinations in News Summaries: Exploration of\n  Self-Correcting LLM Methods with External Knowledge"
                },
                "summary": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation."
                },
                "authors": [
                    {
                        "name": "Juraj Vladika"
                    },
                    {
                        "name": "Ihsan Soydemir"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_comment": "Accepted to FEVER @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01799v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01799v3",
                "updated": "2025-06-24T13:11:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    11,
                    54,
                    1,
                    175,
                    0
                ],
                "published": "2024-04-02T09:58:57Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    9,
                    58,
                    57,
                    1,
                    93,
                    0
                ],
                "title": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language\n  Models against Human Populations: A Case Study of Proficiency in 8th Grade\n  Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language\n  Models against Human Populations: A Case Study of Proficiency in 8th Grade\n  Mathematics"
                },
                "summary": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers'. While such benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics -- a field dedicated to the measurement of latent variables like\nacademic proficiency -- into LLM benchmarking. We make four primary\ncontributions. First, we reflect on current LLM benchmark developments and\ncontrast them with psychometrics-based test development. Second, we introduce\nPATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of\nLLMs. PATCH addresses the aforementioned limitations. In particular, PATCH\nenables valid comparison between LLMs and human populations. Third, we\ndemonstrate PATCH by measuring several LLMs' proficiency in 8th grade\nmathematics against 56 human populations. We show that adopting a\npsychometrics-based approach yields evaluation outcomes that diverge from those\nbased on current benchmarking practices. Fourth, we release 4 high-quality\ndatasets to support measuring and comparing LLM proficiency in grade school\nmathematics and science with human populations."
                },
                "authors": [
                    {
                        "name": "Qixiang Fang"
                    },
                    {
                        "name": "Daniel L. Oberski"
                    },
                    {
                        "name": "Dong Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dong Nguyen"
                },
                "author": "Dong Nguyen",
                "arxiv_comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01799v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01799v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08697v2",
                "updated": "2025-06-24T13:11:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    11,
                    18,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-11T17:04:51Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "title": "Large Language Models as Span Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Span Annotators"
                },
                "summary": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Span annotation is the task of localizing and classifying text spans\naccording to custom guidelines. Annotated spans can be used to analyze and\nevaluate high-quality texts for which single-score metrics fail to provide\nactionable feedback. Until recently, span annotation was limited to human\nannotators or fine-tuned models. In this study, we show that large language\nmodels (LLMs) can serve as flexible and cost-effective span annotation\nbackbones. To demonstrate their utility, we compare LLMs to skilled human\nannotators on three diverse span annotation tasks: evaluating data-to-text\ngeneration, identifying translation errors, and detecting propaganda\ntechniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA)\ncomparable to human annotators at a fraction of a cost per output annotation.\nWe also manually analyze model outputs, finding that LLMs make errors at a\nsimilar rate to human annotators. We release the dataset of more than 40k model\nand human annotations for further research."
                },
                "authors": [
                    {
                        "name": "Zdeněk Kasner"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Patrícia Schmidtová"
                    },
                    {
                        "name": "Ivan Kartáč"
                    },
                    {
                        "name": "Kristýna Onderková"
                    },
                    {
                        "name": "Ondřej Plátek"
                    },
                    {
                        "name": "Dimitra Gkatzia"
                    },
                    {
                        "name": "Saad Mahamood"
                    },
                    {
                        "name": "Ondřej Dušek"
                    },
                    {
                        "name": "Simone Balloccu"
                    }
                ],
                "author_detail": {
                    "name": "Simone Balloccu"
                },
                "author": "Simone Balloccu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19599v1",
                "updated": "2025-06-24T13:09:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    9,
                    53,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:09:53Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    9,
                    53,
                    1,
                    175,
                    0
                ],
                "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of\n  Thought in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of\n  Thought in Large Language Model"
                },
                "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git."
                },
                "authors": [
                    {
                        "name": "Zhenke Duan"
                    },
                    {
                        "name": "Jiqun Pan"
                    },
                    {
                        "name": "Jiani Tu"
                    },
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Yanqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Wang"
                },
                "author": "Yanqing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19592v1",
                "updated": "2025-06-24T13:02:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning"
                },
                "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."
                },
                "authors": [
                    {
                        "name": "Harisankar Babu"
                    },
                    {
                        "name": "Philipp Schillinger"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17728v2",
                "updated": "2025-06-24T12:50:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    50,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-21T14:58:53Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    14,
                    58,
                    53,
                    5,
                    172,
                    0
                ],
                "title": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAG-Thinker: Interactive Thinking and Deep Reasoning in LLMs via\n  Knowledge-Augmented Generation"
                },
                "summary": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce KAG-Thinker, which upgrade KAG to a multi-turn\ninteractive thinking and deep reasoning framework powered by a dedicated\nparameter-light large language model (LLM). Our approach constructs a\nstructured thinking process for solving complex problems, enhancing the the\nlogical coherence and contextual consistency of the reasoning process in\nquestion-answering (Q&A) tasks on domain-specific knowledge bases (KBs) within\nLLMs. Following the \\textbf{Logical Form} guided retrieval and reasoning\ntechnology route of KAG, this framework first decomposes complex questions into\nindependently solvable sub-problems (which are also referred to as logical\nforms) through \\textbf{breadth decomposition}. Each such logical form is\nrepresented in two equivalent forms-natural language and logical function-and\nsubsequently classified as either a Knowledge Retrieval or Reasoning Analysis\ntask. Dependencies and parameter passing between these tasks are explicitly\nmodeled via logical function interfaces. In the solving process, the Retrieval\nfunction performs retrieval tasks. It retrieves one-hop structured and\nunstructured information of specified knowledge unit. While the Math and Deduce\nfunctions are used to perform reasoning analysis tasks. Secondly, it is worth\nnoting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external\nknowledge sources are regarded as equivalent KBs. We use the \\textbf{knowledge\nboundary} module to determine the optimal source using self-regulatory\nmechanisms such as confidence calibration and reflective reasoning, and use the\n\\textbf{depth solving} module to enhance the comprehensiveness of knowledge\nacquisition..."
                },
                "authors": [
                    {
                        "name": "Dalong Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Lin Yuan"
                    },
                    {
                        "name": "Ling Zhong"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Peilong Zhao"
                    },
                    {
                        "name": "QiWei Wang"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Xinkai Du"
                    },
                    {
                        "name": "YangYang Hou"
                    },
                    {
                        "name": "Yu Ao"
                    },
                    {
                        "name": "ZhaoYang Wang"
                    },
                    {
                        "name": "Zhengke Gui"
                    },
                    {
                        "name": "ZhiYing Yi"
                    },
                    {
                        "name": "Zhongpu Bo"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpu Bo"
                },
                "author": "Zhongpu Bo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v2",
                "updated": "2025-06-24T12:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    36,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelièvre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valdés Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "María José Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19563v1",
                "updated": "2025-06-24T12:22:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    22,
                    59,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T12:22:59Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    22,
                    59,
                    1,
                    175,
                    0
                ],
                "title": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic\n  Consistency and Probability Certainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyXray: Detecting Privacy Breaches in LLMs through Semantic\n  Consistency and Probability Certainty"
                },
                "summary": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in sensitive domains, including\nhealthcare, finance, and legal services, raising concerns about potential\nprivate information leaks during inference. Privacy extraction attacks, such as\njailbreaking, expose vulnerabilities in LLMs by crafting inputs that force the\nmodels to output sensitive information. However, these attacks cannot verify\nwhether the extracted private information is accurate, as no public datasets\nexist for cross-validation, leaving a critical gap in private information\ndetection during inference. To address this, we propose PrivacyXray, a novel\nframework detecting privacy breaches by analyzing LLM inner states. Our\nanalysis reveals that LLMs exhibit higher semantic coherence and probabilistic\ncertainty when generating correct private outputs. Based on this, PrivacyXray\ndetects privacy breaches using four metrics: intra-layer and inter-layer\nsemantic similarity, token-level and sentence-level probability distributions.\nPrivacyXray addresses critical challenges in private information detection by\novercoming the lack of open-source private datasets and eliminating reliance on\nexternal data for validation. It achieves this through the synthesis of\nrealistic private data and a detection mechanism based on the inner states of\nLLMs. Experiments show that PrivacyXray achieves consistent performance, with\nan average accuracy of 92.69% across five LLMs. Compared to state-of-the-art\nmethods, PrivacyXray achieves significant improvements, with an average\naccuracy increase of 20.06%, highlighting its stability and practical utility\nin real-world applications."
                },
                "authors": [
                    {
                        "name": "Jinwen He"
                    },
                    {
                        "name": "Yiyang Lu"
                    },
                    {
                        "name": "Zijin Lin"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23254v2",
                "updated": "2025-06-24T12:14:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    14,
                    6,
                    1,
                    175,
                    0
                ],
                "published": "2025-05-29T09:00:35Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    0,
                    35,
                    3,
                    149,
                    0
                ],
                "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning"
                },
                "summary": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "15 pages, 19 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11558v2",
                "updated": "2025-06-24T11:59:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    59,
                    30,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-13T08:13:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs"
                },
                "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."
                },
                "authors": [
                    {
                        "name": "Bo-Cheng Chiu"
                    },
                    {
                        "name": "Jen-Jee Chen"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Feng-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Chi Chen"
                },
                "author": "Feng-Chi Chen",
                "arxiv_comment": "I would like to request the withdrawal of this submission because the\n  current version contains significant errors and incomplete results. I intend\n  to revise the manuscript thoroughly before resubmitting. I apologize for the\n  oversight and appreciate your understanding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19536v1",
                "updated": "2025-06-24T11:45:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    45,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:45:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    45,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "Programming Geotechnical Reliability Algorithms using Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Geotechnical Reliability Algorithms using Generative AI"
                },
                "summary": "Programming reliability algorithms is crucial for risk assessment in\ngeotechnical engineering. This study explores the possibility of automating and\naccelerating this task using Generative AI based on Large Language Models\n(LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the\nability to generate MATLAB codes for four classical reliability algorithms. The\nfour specific examples considered in this study are: (1) First Order\nReliability Method (FORM); (2) Subset simulation; (3) Random field simulation;\nand (4) Bayesian update using Gibbs sampling. The results obtained using the\ngenerated codes are compared with benchmark methods. It is found that the use\nof LLMs can be promising for generating reliability codes. Failure,\nlimitations, and challenges of adopting LLMs are also discussed. Overall, this\nstudy demonstrates that existing LLMs can be leveraged powerfully and can\ncontribute toward accelerating the adoption of reliability techniques in\nroutine geotechnical engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming reliability algorithms is crucial for risk assessment in\ngeotechnical engineering. This study explores the possibility of automating and\naccelerating this task using Generative AI based on Large Language Models\n(LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the\nability to generate MATLAB codes for four classical reliability algorithms. The\nfour specific examples considered in this study are: (1) First Order\nReliability Method (FORM); (2) Subset simulation; (3) Random field simulation;\nand (4) Bayesian update using Gibbs sampling. The results obtained using the\ngenerated codes are compared with benchmark methods. It is found that the use\nof LLMs can be promising for generating reliability codes. Failure,\nlimitations, and challenges of adopting LLMs are also discussed. Overall, this\nstudy demonstrates that existing LLMs can be leveraged powerfully and can\ncontribute toward accelerating the adoption of reliability techniques in\nroutine geotechnical engineering."
                },
                "authors": [
                    {
                        "name": "Atma Sharma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Meng Lu"
                    },
                    {
                        "name": "Shuangyi Wu"
                    },
                    {
                        "name": "Baoxiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Baoxiang Li"
                },
                "author": "Baoxiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19527v1",
                "updated": "2025-06-24T11:30:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    30,
                    38,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:30:38Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    30,
                    38,
                    1,
                    175,
                    0
                ],
                "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs"
                },
                "summary": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Kelin Fu"
                    },
                    {
                        "name": "Kaigui Bian"
                    }
                ],
                "author_detail": {
                    "name": "Kaigui Bian"
                },
                "author": "Kaigui Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19526v1",
                "updated": "2025-06-24T11:28:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    28,
                    9,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:28:09Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    28,
                    9,
                    1,
                    175,
                    0
                ],
                "title": "Reconfigurable Intelligent Surfaces for 6G and Beyond: A Comprehensive\n  Survey from Theory to Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable Intelligent Surfaces for 6G and Beyond: A Comprehensive\n  Survey from Theory to Deployment"
                },
                "summary": "As the wireless research community moves toward shaping the vision of\nsixth-generation (6G) networks, reconfigurable intelligent surfaces (RIS) have\nemerged as a promising technology for controlling the propagation environment.\nAlthough RIS has not yet been standardized, its versatile applications and\nenabling capabilities have attracted growing attention in both academia and\nindustry. This survey presents a comprehensive review of RIS technology\nspanning theoretical foundations, design aspects, and practical deployment\nconsiderations. In contrast to existing surveys that focus on isolated aspects,\nthis work offers an integrated view covering use cases, control mechanisms,\nchannel sounding methodologies, and channel estimation strategies. Each of\nthese topics is reviewed through the lens of recent literature, synthesizing\nthe latest advancements to provide updated insights for both academic\nresearchers and industry practitioners. It further addresses emerging topics\nsuch as standardization activities and industrial perspectives, which are often\noverlooked in prior literature. By bridging theoretical insights with practical\nchallenges, this survey aims to provide a holistic understanding of RIS and\nsupport its evolution from a research concept toward real-world implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the wireless research community moves toward shaping the vision of\nsixth-generation (6G) networks, reconfigurable intelligent surfaces (RIS) have\nemerged as a promising technology for controlling the propagation environment.\nAlthough RIS has not yet been standardized, its versatile applications and\nenabling capabilities have attracted growing attention in both academia and\nindustry. This survey presents a comprehensive review of RIS technology\nspanning theoretical foundations, design aspects, and practical deployment\nconsiderations. In contrast to existing surveys that focus on isolated aspects,\nthis work offers an integrated view covering use cases, control mechanisms,\nchannel sounding methodologies, and channel estimation strategies. Each of\nthese topics is reviewed through the lens of recent literature, synthesizing\nthe latest advancements to provide updated insights for both academic\nresearchers and industry practitioners. It further addresses emerging topics\nsuch as standardization activities and industrial perspectives, which are often\noverlooked in prior literature. By bridging theoretical insights with practical\nchallenges, this survey aims to provide a holistic understanding of RIS and\nsupport its evolution from a research concept toward real-world implementation."
                },
                "authors": [
                    {
                        "name": "Prasetyo Putranto"
                    },
                    {
                        "name": "Anis Amazigh Hamza"
                    },
                    {
                        "name": "Sameh Mabrouki"
                    },
                    {
                        "name": "Nasrullah Armi"
                    },
                    {
                        "name": "Iyad Dayoub"
                    }
                ],
                "author_detail": {
                    "name": "Iyad Dayoub"
                },
                "author": "Iyad Dayoub",
                "arxiv_comment": "39 page, 21 figures, submitted to IEEE Communications Surveys &\n  Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19525v1",
                "updated": "2025-06-24T11:25:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    25,
                    21,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:25:21Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    25,
                    21,
                    1,
                    175,
                    0
                ],
                "title": "Automatic Posology Structuration : What role for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Posology Structuration : What role for LLMs?"
                },
                "summary": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use."
                },
                "authors": [
                    {
                        "name": "Natalia Bobkova"
                    },
                    {
                        "name": "Laura Zanella-Calzada"
                    },
                    {
                        "name": "Anyes Tafoughalt"
                    },
                    {
                        "name": "Raphaël Teboul"
                    },
                    {
                        "name": "François Plesse"
                    },
                    {
                        "name": "Félix Gaschi"
                    }
                ],
                "author_detail": {
                    "name": "Félix Gaschi"
                },
                "author": "Félix Gaschi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06102v2",
                "updated": "2025-06-24T11:23:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    23,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-09T07:32:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    7,
                    32,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiriusBI: A Comprehensive LLM-Powered Solution for Data Analytics in\n  Business Intelligence"
                },
                "summary": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the proliferation of Large Language Models (LLMs) in Business\nIntelligence (BI), existing solutions face critical challenges in industrial\ndeployments: functionality deficiencies from legacy systems failing to meet\nevolving LLM-era user demands, interaction limitations from single-round SQL\ngeneration paradigms inadequate for multi-round clarification, and cost for\ndomain adaptation arising from cross-domain methods migration.\n  We present SiriusBI, a practical LLM-powered BI system addressing the\nchallenges of industrial deployments through three key innovations: (a) An\nend-to-end architecture integrating multi-module coordination to overcome\nfunctionality gaps in legacy systems; (b) A multi-round dialogue with querying\nmechanism, consisting of semantic completion, knowledge-guided clarification,\nand proactive querying processes, to resolve interaction constraints in SQL\ngeneration; (c) A data-conditioned SQL generation method selection strategy\nthat supports both an efficient one-step Fine-Tuning approach and a two-step\nmethod leveraging Semantic Intermediate Representation for low-cost\ncross-domain applications. Experiments on both real-world datasets and public\nbenchmarks demonstrate the effectiveness of SiriusBI. User studies further\nconfirm that SiriusBI enhances both productivity and user experience.\n  As an independent service on Tencent's data platform, SiriusBI is deployed\nacross finance, advertising, and cloud sectors, serving dozens of enterprise\nclients. It achieves over 93% accuracy in SQL generation and reduces data\nanalysts' query time from minutes to seconds in real-world applications."
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Haining Xie"
                    },
                    {
                        "name": "Siqishen"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Meng Lei"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chunyou Li"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yinjun Wu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Peng Chen"
                },
                "author": "Peng Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15961v2",
                "updated": "2025-06-24T10:50:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    50,
                    28,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T02:10:06Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    10,
                    6,
                    3,
                    170,
                    0
                ],
                "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training"
                },
                "summary": "Training large language models (LLMs) at scale requires parallel execution\nacross thousands of devices, incurring enormous computational costs. Yet, these\ncostly distributed trainings are rarely verified, leaving them prone to silent\nerrors and potentially wasting millions of GPU hours. We introduce TrainVerify,\na system for verifiable distributed training of LLMs. Given a deep learning\nmodel's logical specification as the ground truth, TrainVerify formally\nverifies that a distributed parallel execution plan is mathematically\nequivalent to it. Direct verification is notoriously difficult due to the sheer\nscale of LLMs which often involves billions of variables and highly intricate\ncomputation graphs. Therefore, TrainVerify introduces shape-reduction\ntechniques and a stage-wise parallel verification algorithm that significantly\nreduces complexity while preserving formal correctness. TrainVerify scales to\nfrontier LLMs, including the successful verification of the Llama3 (405B) and\nDeepSeek-V3 (671B) training plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) at scale requires parallel execution\nacross thousands of devices, incurring enormous computational costs. Yet, these\ncostly distributed trainings are rarely verified, leaving them prone to silent\nerrors and potentially wasting millions of GPU hours. We introduce TrainVerify,\na system for verifiable distributed training of LLMs. Given a deep learning\nmodel's logical specification as the ground truth, TrainVerify formally\nverifies that a distributed parallel execution plan is mathematically\nequivalent to it. Direct verification is notoriously difficult due to the sheer\nscale of LLMs which often involves billions of variables and highly intricate\ncomputation graphs. Therefore, TrainVerify introduces shape-reduction\ntechniques and a stage-wise parallel verification algorithm that significantly\nreduces complexity while preserving formal correctness. TrainVerify scales to\nfrontier LLMs, including the successful verification of the Llama3 (405B) and\nDeepSeek-V3 (671B) training plans."
                },
                "authors": [
                    {
                        "name": "Yunchi Lu"
                    },
                    {
                        "name": "Youshan Miao"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Peng Huang"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19502v1",
                "updated": "2025-06-24T10:40:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    40,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:40:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    40,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications"
                },
                "summary": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE."
                },
                "authors": [
                    {
                        "name": "Aleksandr Algazinov"
                    },
                    {
                        "name": "Matt Laing"
                    },
                    {
                        "name": "Paul Laban"
                    }
                ],
                "author_detail": {
                    "name": "Paul Laban"
                },
                "author": "Paul Laban",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19500v1",
                "updated": "2025-06-24T10:39:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:39:07Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "title": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function\n  Calling"
                },
                "summary": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' reliance on static knowledge and fragile tool invocation severely\nhinders the orchestration of complex, heterogeneous toolchains, particularly at\nlarge scales. Existing methods typically use rigid single-path execution,\nresulting in poor error recovery and exponentially growing search spaces. We\nintroduce NaviAgent, a graph-navigated bilevel planning architecture for robust\nfunction calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.\nAs an LLM-powered agent, the Multi-Path Decider defines a four-dimensional\ndecision space and continuously perceives environmental states, dynamically\nselecting the optimal action to fully cover all tool invocation scenarios. The\nGraph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph\n(TDHG), where node embeddings explicitly fuse API schema structure with\nhistorical invocation behavior. It also integrates a novel heuristic search\nstrategy that guides the Decider toward efficient and highly successful\ntoolchains, even for unseen tool combinations. Experiments show that NaviAgent\nconsistently achieves the highest task success rate (TSR) across all foundation\nmodels and task complexities, outperforming the average baselines (ReAct,\nToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,\nand Deepseek-V3, respectively. Its execution steps are typically within one\nstep of the most efficient baseline, ensuring a strong balance between quality\nand efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of\n49.5%, surpassing the much larger 32B model (44.9%) under our architecture.\nIncorporating the Graph-Encoded Navigator further boosts TSR by an average of\n2.4 points, with gains up over 9 points on complex tasks for larger models\n(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain\norchestration."
                },
                "authors": [
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "LiZhong GU"
                    },
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "TianLong Li"
                    }
                ],
                "author_detail": {
                    "name": "TianLong Li"
                },
                "author": "TianLong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19497v1",
                "updated": "2025-06-24T10:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    35,
                    52,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:35:52Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    35,
                    52,
                    1,
                    175,
                    0
                ],
                "title": "The time course of visuo-semantic representations in the human brain is\n  captured by combining vision and language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time course of visuo-semantic representations in the human brain is\n  captured by combining vision and language models"
                },
                "summary": "The human visual system provides us with a rich and meaningful percept of the\nworld, transforming retinal signals into visuo-semantic representations. For a\nmodel of these representations, here we leveraged a combination of two\ncurrently dominating approaches: vision deep neural networks (DNNs) and large\nlanguage models (LLMs). Using large-scale human electroencephalography (EEG)\ndata recorded during object image viewing, we built encoding models to predict\nEEG responses using representations from a vision DNN, an LLM, and their\nfusion. We show that the fusion encoding model outperforms encoding models\nbased on either the vision DNN or the LLM alone, as well as previous modelling\napproaches, in predicting neural responses to visual stimulation. The vision\nDNN and the LLM complemented each other in explaining stimulus-related signal\nin the EEG responses. The vision DNN uniquely captured earlier and broadband\nEEG signals, whereas the LLM uniquely captured later and low frequency signals,\nas well as detailed visuo-semantic stimulus information. Together, this\nprovides a more accurate model of the time course of visuo-semantic processing\nin the human brain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human visual system provides us with a rich and meaningful percept of the\nworld, transforming retinal signals into visuo-semantic representations. For a\nmodel of these representations, here we leveraged a combination of two\ncurrently dominating approaches: vision deep neural networks (DNNs) and large\nlanguage models (LLMs). Using large-scale human electroencephalography (EEG)\ndata recorded during object image viewing, we built encoding models to predict\nEEG responses using representations from a vision DNN, an LLM, and their\nfusion. We show that the fusion encoding model outperforms encoding models\nbased on either the vision DNN or the LLM alone, as well as previous modelling\napproaches, in predicting neural responses to visual stimulation. The vision\nDNN and the LLM complemented each other in explaining stimulus-related signal\nin the EEG responses. The vision DNN uniquely captured earlier and broadband\nEEG signals, whereas the LLM uniquely captured later and low frequency signals,\nas well as detailed visuo-semantic stimulus information. Together, this\nprovides a more accurate model of the time course of visuo-semantic processing\nin the human brain."
                },
                "authors": [
                    {
                        "name": "Boyan Rong"
                    },
                    {
                        "name": "Alessandro Thomas Gifford"
                    },
                    {
                        "name": "Emrah Düzel"
                    },
                    {
                        "name": "Radoslaw Martin Cichy"
                    }
                ],
                "author_detail": {
                    "name": "Radoslaw Martin Cichy"
                },
                "author": "Radoslaw Martin Cichy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19491v1",
                "updated": "2025-06-24T10:25:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    25,
                    17,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:25:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    25,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based\n  Applications"
                },
                "summary": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has\nexpanded their deployment potential to indoor and hard-to-reach areas. However,\nthis trend introduces distinct challenges, particularly in terms of flight\ndynamics and power consumption, which limit the UAVs' autonomy and mission\ncapabilities. This paper presents a novel approach to overcoming these\nlimitations by integrating Neural 3D Reconstruction (N3DR) with small UAV\nsystems for fine-grained 3-Dimensional (3D) digital reconstruction of small\nstatic objects. Specifically, we design, implement, and evaluate an N3DR-based\npipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and\nSplatfacto, to improve the quality of 3D reconstructions using images of the\nobject captured by a fleet of small UAVs. We assess the performance of the\nconsidered models using various imagery and pointcloud metrics, comparing them\nagainst the baseline Structure from Motion (SfM) algorithm. The experimental\nresults demonstrate that the N3DR-enhanced pipeline significantly improves\nreconstruction quality, making it feasible for small UAVs to support\nhigh-precision 3D mapping and anomaly detection in constrained environments. In\nmore general terms, our results highlight the potential of N3DR in advancing\nthe capabilities of miniaturized UAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has\nexpanded their deployment potential to indoor and hard-to-reach areas. However,\nthis trend introduces distinct challenges, particularly in terms of flight\ndynamics and power consumption, which limit the UAVs' autonomy and mission\ncapabilities. This paper presents a novel approach to overcoming these\nlimitations by integrating Neural 3D Reconstruction (N3DR) with small UAV\nsystems for fine-grained 3-Dimensional (3D) digital reconstruction of small\nstatic objects. Specifically, we design, implement, and evaluate an N3DR-based\npipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and\nSplatfacto, to improve the quality of 3D reconstructions using images of the\nobject captured by a fleet of small UAVs. We assess the performance of the\nconsidered models using various imagery and pointcloud metrics, comparing them\nagainst the baseline Structure from Motion (SfM) algorithm. The experimental\nresults demonstrate that the N3DR-enhanced pipeline significantly improves\nreconstruction quality, making it feasible for small UAVs to support\nhigh-precision 3D mapping and anomaly detection in constrained environments. In\nmore general terms, our results highlight the potential of N3DR in advancing\nthe capabilities of miniaturized UAV systems."
                },
                "authors": [
                    {
                        "name": "Genís Castillo Gómez-Raya"
                    },
                    {
                        "name": "Álmos Veres-Vitályos"
                    },
                    {
                        "name": "Filip Lemic"
                    },
                    {
                        "name": "Pablo Royo"
                    },
                    {
                        "name": "Mario Montagud"
                    },
                    {
                        "name": "Sergi Fernández"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Xavier Costa-Pérez"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Costa-Pérez"
                },
                "author": "Xavier Costa-Pérez",
                "arxiv_comment": "6 pages, 7 figures, 2 tables, accepted at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v4",
                "updated": "2025-06-24T10:19:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image AutoRegressive generation has emerged as a new powerful paradigm with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns regarding their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to the ones of DMs as reference points. Concretely, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images (with a True Positive Rate at False Positive\nRate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our\nnovel MIA to provide dataset inference (DI) for IARs, and show that it requires\nas few as 6 samples to detect dataset membership (compared to 200 for DI in\nDMs), confirming a higher information leakage in IARs. Finally, we are able to\nextract hundreds of training data points from an IAR (e.g., 698 from VAR-d30).\nOur results suggest a fundamental privacy-utility trade-off: while IARs excel\nin image generation quality and speed, they are empirically significantly more\nvulnerable to privacy attacks compared to DMs that achieve similar performance.\nWe release the code at https://github.com/sprintml/privacy_attacks_against_iars\nfor reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image AutoRegressive generation has emerged as a new powerful paradigm with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns regarding their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to the ones of DMs as reference points. Concretely, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images (with a True Positive Rate at False Positive\nRate = 1% of 86.38% vs. 6.38% for DMs with comparable attacks). We leverage our\nnovel MIA to provide dataset inference (DI) for IARs, and show that it requires\nas few as 6 samples to detect dataset membership (compared to 200 for DI in\nDMs), confirming a higher information leakage in IARs. Finally, we are able to\nextract hundreds of training data points from an IAR (e.g., 698 from VAR-d30).\nOur results suggest a fundamental privacy-utility trade-off: while IARs excel\nin image generation quality and speed, they are empirically significantly more\nvulnerable to privacy attacks compared to DMs that achieve similar performance.\nWe release the code at https://github.com/sprintml/privacy_attacks_against_iars\nfor reproducibility."
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Accepted at ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19484v1",
                "updated": "2025-06-24T10:19:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    9,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:19:09Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    19,
                    9,
                    1,
                    175,
                    0
                ],
                "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI\n  with Proven Theories of Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI\n  with Proven Theories of Learning"
                },
                "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned."
                },
                "authors": [
                    {
                        "name": "Russell Beale"
                    }
                ],
                "author_detail": {
                    "name": "Russell Beale"
                },
                "author": "Russell Beale",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.6; H.4.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19483v1",
                "updated": "2025-06-24T10:18:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    18,
                    5,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:18:05Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    18,
                    5,
                    1,
                    175,
                    0
                ],
                "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense Generation and Evaluation for Dialogue Systems using Large\n  Language Models"
                },
                "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems."
                },
                "authors": [
                    {
                        "name": "Marcos Estecha-Garitagoitia"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Mario Rodríguez-Cantelar"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    }
                ],
                "author_detail": {
                    "name": "Luis Fernando D'Haro"
                },
                "author": "Luis Fernando D'Haro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19481v1",
                "updated": "2025-06-24T10:17:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    17,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:17:34Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    17,
                    34,
                    1,
                    175,
                    0
                ],
                "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code"
                },
                "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows."
                },
                "authors": [
                    {
                        "name": "Shahbaz Siddeeq"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Zeeshan Rasheed"
                    },
                    {
                        "name": "Md Mahade Hasan"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Mika Saari"
                    },
                    {
                        "name": "Henri Terho"
                    },
                    {
                        "name": "Kalle Makela"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2502.07928",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07584v3",
                "updated": "2025-06-24T10:10:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    10,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-10T17:48:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    48,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Talking to GDELT Through Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking to GDELT Through Knowledge Graphs"
                },
                "summary": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end."
                },
                "authors": [
                    {
                        "name": "Audun Myers"
                    },
                    {
                        "name": "Max Vargas"
                    },
                    {
                        "name": "Sinan G. Aksoy"
                    },
                    {
                        "name": "Cliff Joslyn"
                    },
                    {
                        "name": "Benjamin Wilson"
                    },
                    {
                        "name": "Lee Burke"
                    },
                    {
                        "name": "Tom Grimes"
                    }
                ],
                "author_detail": {
                    "name": "Tom Grimes"
                },
                "author": "Tom Grimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19474v1",
                "updated": "2025-06-24T10:00:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    0,
                    23,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:00:23Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    0,
                    23,
                    1,
                    175,
                    0
                ],
                "title": "HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for\n  Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HMSViT: A Hierarchical Masked Self-Supervised Vision Transformer for\n  Corneal Nerve Segmentation and Diabetic Neuropathy Diagnosis"
                },
                "summary": "Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes\npatients, requiring early detection. Corneal Confocal Microscopy (CCM) enables\nnon-invasive diagnosis, but automated methods suffer from inefficient feature\nextraction, reliance on handcrafted priors, and data limitations. We propose\nHMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)\ndesigned for corneal nerve segmentation and DPN diagnosis. Unlike existing\nmethods, HMSViT employs pooling-based hierarchical and dual attention\nmechanisms with absolute positional encoding, enabling efficient multi-scale\nfeature extraction by capturing fine-grained local details in early layers and\nintegrating global context in deeper layers, all at a lower computational cost.\nA block-masked self supervised learning framework is designed for the HMSViT\nthat reduces reliance on labelled data, enhancing feature robustness, while a\nmulti-scale decoder is used for segmentation and classification by fusing\nhierarchical features. Experiments on clinical CCM datasets showed HMSViT\nachieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation\nand 70.40% diagnostic accuracy, outperforming leading hierarchical models like\nthe Swin Transformer and HiViT by margins of up to 6.39% in segmentation\naccuracy while using fewer parameters. Detailed ablation studies further reveal\nthat integrating block-masked SSL with hierarchical multi-scale feature\nextraction substantially enhances performance compared to conventional\nsupervised training. Overall, these comprehensive experiments confirm that\nHMSViT delivers excellent, robust, and clinically viable results, demonstrating\nits potential for scalable deployment in real-world diagnostic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diabetic Peripheral Neuropathy (DPN) affects nearly half of diabetes\npatients, requiring early detection. Corneal Confocal Microscopy (CCM) enables\nnon-invasive diagnosis, but automated methods suffer from inefficient feature\nextraction, reliance on handcrafted priors, and data limitations. We propose\nHMSViT, a novel Hierarchical Masked Self-Supervised Vision Transformer (HMSViT)\ndesigned for corneal nerve segmentation and DPN diagnosis. Unlike existing\nmethods, HMSViT employs pooling-based hierarchical and dual attention\nmechanisms with absolute positional encoding, enabling efficient multi-scale\nfeature extraction by capturing fine-grained local details in early layers and\nintegrating global context in deeper layers, all at a lower computational cost.\nA block-masked self supervised learning framework is designed for the HMSViT\nthat reduces reliance on labelled data, enhancing feature robustness, while a\nmulti-scale decoder is used for segmentation and classification by fusing\nhierarchical features. Experiments on clinical CCM datasets showed HMSViT\nachieves state-of-the-art performance, with 61.34% mIoU for nerve segmentation\nand 70.40% diagnostic accuracy, outperforming leading hierarchical models like\nthe Swin Transformer and HiViT by margins of up to 6.39% in segmentation\naccuracy while using fewer parameters. Detailed ablation studies further reveal\nthat integrating block-masked SSL with hierarchical multi-scale feature\nextraction substantially enhances performance compared to conventional\nsupervised training. Overall, these comprehensive experiments confirm that\nHMSViT delivers excellent, robust, and clinically viable results, demonstrating\nits potential for scalable deployment in real-world diagnostic applications."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Liangxiu Han"
                    },
                    {
                        "name": "Yue Shi"
                    },
                    {
                        "name": "Yanlin Zheng"
                    },
                    {
                        "name": "Alam Uazman"
                    },
                    {
                        "name": "Maryam Ferdousi"
                    },
                    {
                        "name": "Rayaz Malik"
                    }
                ],
                "author_detail": {
                    "name": "Rayaz Malik"
                },
                "author": "Rayaz Malik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18807v2",
                "updated": "2025-06-24T09:56:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    56,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T16:16:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    16,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision\n  Applications"
                },
                "summary": "Real-time, on-device segmentation is critical for latency-sensitive and\nprivacy-aware applications like smart glasses and IoT devices. We introduce\nPicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation\nmodel optimized for edge and in-sensor execution, including the Sony IMX500. It\nbuilds on a depthwise separable U-Net, with knowledge distillation and\nfixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).\nOn COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized\nmodel (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it\nthe only model meeting both memory and compute constraints for in-sensor\ndeployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.\nThese results demonstrate that efficient, promptable segmentation is feasible\ndirectly on-camera, enabling privacy-preserving vision without cloud or host\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time, on-device segmentation is critical for latency-sensitive and\nprivacy-aware applications like smart glasses and IoT devices. We introduce\nPicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation\nmodel optimized for edge and in-sensor execution, including the Sony IMX500. It\nbuilds on a depthwise separable U-Net, with knowledge distillation and\nfixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).\nOn COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized\nmodel (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it\nthe only model meeting both memory and compute constraints for in-sensor\ndeployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.\nThese results demonstrate that efficient, promptable segmentation is feasible\ndirectly on-camera, enabling privacy-preserving vision without cloud or host\nprocessing."
                },
                "authors": [
                    {
                        "name": "Pietro Bonazzi"
                    },
                    {
                        "name": "Nicola Farronato"
                    },
                    {
                        "name": "Stefan Zihlmann"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19468v1",
                "updated": "2025-06-24T09:53:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    53,
                    0,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:53:00Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    53,
                    0,
                    1,
                    175,
                    0
                ],
                "title": "MuBench: Assessment of Multilingual Capabilities of Large Language\n  Models Across 61 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuBench: Assessment of Multilingual Capabilities of Large Language\n  Models Across 61 Languages"
                },
                "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics."
                },
                "authors": [
                    {
                        "name": "Wenhan Han"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhixun Chen"
                    },
                    {
                        "name": "Binbin Liu"
                    },
                    {
                        "name": "Haobin Lin"
                    },
                    {
                        "name": "Bingni Zhang"
                    },
                    {
                        "name": "Taifeng Wang"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yin Zheng"
                },
                "author": "Yin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19467v1",
                "updated": "2025-06-24T09:49:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    49,
                    26,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:49:26Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    49,
                    26,
                    1,
                    175,
                    0
                ],
                "title": "Can Large Language Models Capture Human Annotator Disagreements?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Capture Human Annotator Disagreements?"
                },
                "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Donya Rooein"
                    },
                    {
                        "name": "Alexander Hoyle"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Dirk Hovy"
                    },
                    {
                        "name": "Elliott Ash"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Ash"
                },
                "author": "Elliott Ash",
                "arxiv_comment": "Preprint Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19466v1",
                "updated": "2025-06-24T09:48:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    48,
                    1,
                    1,
                    175,
                    0
                ],
                "title": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunLunBaizeRAG: Reinforcement Learning Driven Inference Performance Leap\n  for Large Language Models"
                },
                "summary": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KunLunBaizeRAG, a reinforcement learning-driven\nreasoning framework designed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in complex multi-hop question-answering tasks. The\nframework addresses key limitations of traditional RAG, such as retrieval\ndrift, information redundancy, and strategy rigidity. Key innovations include\nthe RAG-driven Reasoning Alignment (RDRA) mechanism, the Search-Think Iterative\nEnhancement (STIE) mechanism, the Network-Local Intelligent Routing (NLR)\nmechanism, and a progressive hybrid training strategy. Experimental results\ndemonstrate significant improvements in exact match (EM) and LLM-judged score\n(LJ) across four benchmarks, highlighting the framework's robustness and\neffectiveness in complex reasoning scenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Jiexiong Liu"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Qihang Zhou"
                    },
                    {
                        "name": "KunLun Meta"
                    }
                ],
                "author_detail": {
                    "name": "KunLun Meta"
                },
                "author": "KunLun Meta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19456v1",
                "updated": "2025-06-24T09:37:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    37,
                    3,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:37:03Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    37,
                    3,
                    1,
                    175,
                    0
                ],
                "title": "Can Movable Antenna-enabled Micro-Mobility Replace UAV-enabled\n  Macro-Mobility? A Physical Layer Security Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Movable Antenna-enabled Micro-Mobility Replace UAV-enabled\n  Macro-Mobility? A Physical Layer Security Perspective"
                },
                "summary": "This paper investigates the potential of movable antenna (MA)-enabled\nmicro-mobility to replace UAV-enabled macro-mobility for enhancing physical\nlayer security (PLS) in air-to-ground communications. While UAV trajectory\noptimization offers high flexibility and Line-of-Sight (LoS) advantages, it\nsuffers from significant energy consumption, latency, and complex trajectory\noptimization. Conversely, MA technology provides fine-grained spatial\nreconfiguration (antenna positioning within a confined area) with ultra-low\nenergy overhead and millisecond-scale response, enabling real-time channel\nmanipulation and covert beam steering. To systematically compare these\nparadigms, we establish a dual-scale mobility framework where a UAV-mounted\nuniform linear array (ULA) serves as a base station transmitting confidential\ninformation to a legitimate user (Bob) in the presence of an eavesdropper\n(Eve). We formulate non-convex average secrecy rate (ASR) maximization problems\nfor both schemes: 1) MA-based micro-mobility: Jointly optimizing antenna\npositions and beamforming (BF) vectors under positioning constraints; 2)\nUAV-based macro-mobility: Jointly optimizing the UAV's trajectory and BF\nvectors under kinematic constraints. Extensive simulations reveal distinct\noperational regimes: MA micro-mobility demonstrates significant ASR advantages\nin low-transmit-power scenarios or under antenna constraints due to its\nenergy-efficient spatial control. Conversely, UAV macro-mobility excels under\nresource-sufficient conditions (higher power, larger antenna arrays) by\nleveraging global mobility for optimal positioning. The findings highlight the\ncomplementary strengths of both approaches, suggesting hybrid micro-macro\nmobility as a promising direction for balancing security, energy efficiency,\nand deployment complexity in future wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the potential of movable antenna (MA)-enabled\nmicro-mobility to replace UAV-enabled macro-mobility for enhancing physical\nlayer security (PLS) in air-to-ground communications. While UAV trajectory\noptimization offers high flexibility and Line-of-Sight (LoS) advantages, it\nsuffers from significant energy consumption, latency, and complex trajectory\noptimization. Conversely, MA technology provides fine-grained spatial\nreconfiguration (antenna positioning within a confined area) with ultra-low\nenergy overhead and millisecond-scale response, enabling real-time channel\nmanipulation and covert beam steering. To systematically compare these\nparadigms, we establish a dual-scale mobility framework where a UAV-mounted\nuniform linear array (ULA) serves as a base station transmitting confidential\ninformation to a legitimate user (Bob) in the presence of an eavesdropper\n(Eve). We formulate non-convex average secrecy rate (ASR) maximization problems\nfor both schemes: 1) MA-based micro-mobility: Jointly optimizing antenna\npositions and beamforming (BF) vectors under positioning constraints; 2)\nUAV-based macro-mobility: Jointly optimizing the UAV's trajectory and BF\nvectors under kinematic constraints. Extensive simulations reveal distinct\noperational regimes: MA micro-mobility demonstrates significant ASR advantages\nin low-transmit-power scenarios or under antenna constraints due to its\nenergy-efficient spatial control. Conversely, UAV macro-mobility excels under\nresource-sufficient conditions (higher power, larger antenna arrays) by\nleveraging global mobility for optimal positioning. The findings highlight the\ncomplementary strengths of both approaches, suggesting hybrid micro-macro\nmobility as a promising direction for balancing security, energy efficiency,\nand deployment complexity in future wireless networks."
                },
                "authors": [
                    {
                        "name": "Kaixuan Li"
                    },
                    {
                        "name": "Kan Yu"
                    },
                    {
                        "name": "Dingyou Ma"
                    },
                    {
                        "name": "Yujia Zhao"
                    },
                    {
                        "name": "Xiaowu Liu"
                    },
                    {
                        "name": "Qixun Zhang"
                    },
                    {
                        "name": "ZHiyong Feng"
                    }
                ],
                "author_detail": {
                    "name": "ZHiyong Feng"
                },
                "author": "ZHiyong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19453v1",
                "updated": "2025-06-24T09:30:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    30,
                    40,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:30:40Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    30,
                    40,
                    1,
                    175,
                    0
                ],
                "title": "FuncVul: An Effective Function Level Vulnerability Detection Model using\n  LLM and Code Chunk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuncVul: An Effective Function Level Vulnerability Detection Model using\n  LLM and Code Chunk"
                },
                "summary": "Software supply chain vulnerabilities arise when attackers exploit weaknesses\nby injecting vulnerable code into widely used packages or libraries within\nsoftware repositories. While most existing approaches focus on identifying\nvulnerable packages or libraries, they often overlook the specific functions\nresponsible for these vulnerabilities. Pinpointing vulnerable functions within\npackages or libraries is critical, as it can significantly reduce the risks\nassociated with using open-source software. Identifying vulnerable patches is\nchallenging because developers often submit code changes that are unrelated to\nvulnerability fixes. To address this issue, this paper introduces FuncVul, an\ninnovative code chunk-based model for function-level vulnerability detection in\nC/C++ and Python, designed to identify multiple vulnerabilities within a\nfunction by focusing on smaller, critical code segments. To assess the model's\neffectiveness, we construct six code and generic code chunk based datasets\nusing two approaches: (1) integrating patch information with large language\nmodels to label vulnerable samples and (2) leveraging large language models\nalone to detect vulnerabilities in function-level code. To design FuncVul\nvulnerability model, we utilise GraphCodeBERT fine tune model that captures\nboth the syntactic and semantic aspects of code. Experimental results show that\nFuncVul outperforms existing state-of-the-art models, achieving an average\naccuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore,\nwe have demonstrated that our code-chunk-based FuncVul model improves 53.9%\naccuracy and 42.0% F1-score than the full function-based vulnerability\nprediction. The FuncVul code and datasets are publicly available on GitHub at\nhttps://github.com/sajalhalder/FuncVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software supply chain vulnerabilities arise when attackers exploit weaknesses\nby injecting vulnerable code into widely used packages or libraries within\nsoftware repositories. While most existing approaches focus on identifying\nvulnerable packages or libraries, they often overlook the specific functions\nresponsible for these vulnerabilities. Pinpointing vulnerable functions within\npackages or libraries is critical, as it can significantly reduce the risks\nassociated with using open-source software. Identifying vulnerable patches is\nchallenging because developers often submit code changes that are unrelated to\nvulnerability fixes. To address this issue, this paper introduces FuncVul, an\ninnovative code chunk-based model for function-level vulnerability detection in\nC/C++ and Python, designed to identify multiple vulnerabilities within a\nfunction by focusing on smaller, critical code segments. To assess the model's\neffectiveness, we construct six code and generic code chunk based datasets\nusing two approaches: (1) integrating patch information with large language\nmodels to label vulnerable samples and (2) leveraging large language models\nalone to detect vulnerabilities in function-level code. To design FuncVul\nvulnerability model, we utilise GraphCodeBERT fine tune model that captures\nboth the syntactic and semantic aspects of code. Experimental results show that\nFuncVul outperforms existing state-of-the-art models, achieving an average\naccuracy of 87-92% and an F1 score of 86-92% across all datasets. Furthermore,\nwe have demonstrated that our code-chunk-based FuncVul model improves 53.9%\naccuracy and 42.0% F1-score than the full function-based vulnerability\nprediction. The FuncVul code and datasets are publicly available on GitHub at\nhttps://github.com/sajalhalder/FuncVul."
                },
                "authors": [
                    {
                        "name": "Sajal Halder"
                    },
                    {
                        "name": "Muhammad Ejaz Ahmed"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "In The 30th European Symposium on Research in Computer Security\n  (ESORICS), 22 Sep - 26 Sep, 2025, Toulouse, France",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19451v1",
                "updated": "2025-06-24T09:25:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    25,
                    44,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:25:44Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    25,
                    44,
                    1,
                    175,
                    0
                ],
                "title": "Low-Complexity Semantic Packet Aggregation for Token Communication via\n  Lookahead Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity Semantic Packet Aggregation for Token Communication via\n  Lookahead Search"
                },
                "summary": "Tokens are fundamental processing units of generative AI (GenAI) and large\nlanguage models (LLMs), and token communication (TC) is essential for enabling\nremote AI-generate content (AIGC) and wireless LLM applications. Unlike\ntraditional bits, each of which is independently treated, the semantics of each\ntoken depends on its surrounding context tokens. This inter-token dependency\nmakes TC vulnerable to outage channels, where the loss of a single token can\nsignificantly distort the original message semantics. Motivated by this, this\npaper focuses on optimizing token packetization to maximize the average token\nsimilarity (ATS) between the original and received token messages under outage\nchannels. Due to inter-token dependency, this token grouping problem is\ncombinatorial, with complexity growing exponentially with message length. To\naddress this, we propose a novel framework of semantic packet aggregation with\nlookahead search (SemPA-Look), built on two core ideas. First, it introduces\nthe residual semantic score (RSS) as a token-level surrogate for the\nmessage-level ATS, allowing robust semantic preservation even when a certain\ntoken packet is lost. Second, instead of full search, SemPA-Look applies a\nlookahead search-inspired algorithm that samples intra-packet token candidates\nwithout replacement (fixed depth), conditioned on inter-packet token candidates\nsampled with replacement (fixed width), thereby achieving linear complexity.\nExperiments on a remote AIGC task with the MS-COCO dataset (text captioned\nimages) demonstrate that SemPA-Look achieves high ATS and LPIPS scores\ncomparable to exhaustive search, while reducing computational complexity by up\nto 40$\\times$. Compared to other linear-complexity algorithms such as the\ngenetic algorithm (GA), SemPA-Look achieves 10$\\times$ lower complexity,\ndemonstrating its practicality for remote AIGC and other TC applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens are fundamental processing units of generative AI (GenAI) and large\nlanguage models (LLMs), and token communication (TC) is essential for enabling\nremote AI-generate content (AIGC) and wireless LLM applications. Unlike\ntraditional bits, each of which is independently treated, the semantics of each\ntoken depends on its surrounding context tokens. This inter-token dependency\nmakes TC vulnerable to outage channels, where the loss of a single token can\nsignificantly distort the original message semantics. Motivated by this, this\npaper focuses on optimizing token packetization to maximize the average token\nsimilarity (ATS) between the original and received token messages under outage\nchannels. Due to inter-token dependency, this token grouping problem is\ncombinatorial, with complexity growing exponentially with message length. To\naddress this, we propose a novel framework of semantic packet aggregation with\nlookahead search (SemPA-Look), built on two core ideas. First, it introduces\nthe residual semantic score (RSS) as a token-level surrogate for the\nmessage-level ATS, allowing robust semantic preservation even when a certain\ntoken packet is lost. Second, instead of full search, SemPA-Look applies a\nlookahead search-inspired algorithm that samples intra-packet token candidates\nwithout replacement (fixed depth), conditioned on inter-packet token candidates\nsampled with replacement (fixed width), thereby achieving linear complexity.\nExperiments on a remote AIGC task with the MS-COCO dataset (text captioned\nimages) demonstrate that SemPA-Look achieves high ATS and LPIPS scores\ncomparable to exhaustive search, while reducing computational complexity by up\nto 40$\\times$. Compared to other linear-complexity algorithms such as the\ngenetic algorithm (GA), SemPA-Look achieves 10$\\times$ lower complexity,\ndemonstrating its practicality for remote AIGC and other TC applications."
                },
                "authors": [
                    {
                        "name": "Seunghun Lee"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    },
                    {
                        "name": "Hyuncheol Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyuncheol Park"
                },
                "author": "Hyuncheol Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18269v2",
                "updated": "2025-06-24T09:12:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    12,
                    31,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T03:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    54,
                    30,
                    0,
                    174,
                    0
                ],
                "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User\n  Personas through Social Media Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User\n  Personas through Social Media Data Analysis"
                },
                "summary": "This study introduces Co-Persona, a methodological framework bridging\nlarge-scale social media analysis with authentic user understanding through\nsystematic integration of Large Language Models and expert validation. Through\na case study of B.Co, a Chinese manufacturer, we investigated Co-Persona\napplication in bedside lamp development. Our methodology analyzed over 38\nmillion posts from Xiao Hongshu, employing multi-stage data processing\ncombining advanced NLP with expert validation. Analysis revealed five user\npersonas derived from bedtime behaviors: Health Aficionados, Night Owls,\nInterior Decorators, Child-care Workers, and Workaholics-each showing unique\npre-sleep activities and product preferences. Findings demonstrate Co-Persona\nenhances manufacturers' ability to process large datasets while maintaining\nuser understanding. The methodology provides structured approaches for targeted\nmarketing and product strategies. Research contributes to theoretical\nunderstanding of data-driven persona development and practical applications in\nconsumer-driven innovation. Code and data available at\nhttps://github.com/INFPa/LLMwithPersona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Co-Persona, a methodological framework bridging\nlarge-scale social media analysis with authentic user understanding through\nsystematic integration of Large Language Models and expert validation. Through\na case study of B.Co, a Chinese manufacturer, we investigated Co-Persona\napplication in bedside lamp development. Our methodology analyzed over 38\nmillion posts from Xiao Hongshu, employing multi-stage data processing\ncombining advanced NLP with expert validation. Analysis revealed five user\npersonas derived from bedtime behaviors: Health Aficionados, Night Owls,\nInterior Decorators, Child-care Workers, and Workaholics-each showing unique\npre-sleep activities and product preferences. Findings demonstrate Co-Persona\nenhances manufacturers' ability to process large datasets while maintaining\nuser understanding. The methodology provides structured approaches for targeted\nmarketing and product strategies. Research contributes to theoretical\nunderstanding of data-driven persona development and practical applications in\nconsumer-driven innovation. Code and data available at\nhttps://github.com/INFPa/LLMwithPersona."
                },
                "authors": [
                    {
                        "name": "Min Yin"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Boyi Lian"
                    },
                    {
                        "name": "Chunlei Chai"
                    }
                ],
                "author_detail": {
                    "name": "Chunlei Chai"
                },
                "author": "Chunlei Chai",
                "arxiv_comment": "17pages,5figures,8tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19426v1",
                "updated": "2025-06-24T08:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    44,
                    4,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T08:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    44,
                    4,
                    1,
                    175,
                    0
                ],
                "title": "A Stochastic Electric Vehicle Routing Problem under Uncertain Energy\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stochastic Electric Vehicle Routing Problem under Uncertain Energy\n  Consumption"
                },
                "summary": "The increasing adoption of Electric Vehicles (EVs) for service and goods\ndistribution operations has led to the emergence of Electric Vehicle Routing\nProblems (EVRPs), a class of vehicle routing problems addressing the unique\nchallenges posed by the limited driving range and recharging needs of EVs.\nWhile the majority of EVRP variants have considered deterministic energy\nconsumption, this paper focuses on the Stochastic Electric Vehicle Routing\nProblem with a Threshold recourse policy (SEVRP-T), where the uncertainty in\nenergy consumption is considered, and a recourse policy is employed to ensure\nthat EVs recharge at Charging Stations (CSs) whenever their State of Charge\n(SoC) falls below a specified threshold. We formulate the SEVRP-T as a\ntwo-stage stochastic mixed-integer second-order cone model, where the first\nstage determines the sequences of customers to be visited, and the second stage\nincorporates charging activities. The objective is to minimize the expected\ntotal duration of the routes, composed by travel times and recharging\noperations. To cope with the computational complexity of the model, we propose\na heuristic based on an Iterated Local Search (ILS) procedure coupled with a\nSet Partitioning problem. To further speed up the heuristic, we develop two\nlower bounds on the corresponding first-stage customer sequences. Furthermore,\nto handle a large number of energy consumption scenarios, we employ a scenario\nreduction technique. Extensive computational experiments are conducted to\nvalidate the effectiveness of the proposed solution strategy and to assess the\nimportance of considering the stochastic nature of the energy consumption. The\nresearch presented in this paper contributes to the growing body of literature\non EVRP and provides insights into managing the operational deployment of EVs\nin logistics activities under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Electric Vehicles (EVs) for service and goods\ndistribution operations has led to the emergence of Electric Vehicle Routing\nProblems (EVRPs), a class of vehicle routing problems addressing the unique\nchallenges posed by the limited driving range and recharging needs of EVs.\nWhile the majority of EVRP variants have considered deterministic energy\nconsumption, this paper focuses on the Stochastic Electric Vehicle Routing\nProblem with a Threshold recourse policy (SEVRP-T), where the uncertainty in\nenergy consumption is considered, and a recourse policy is employed to ensure\nthat EVs recharge at Charging Stations (CSs) whenever their State of Charge\n(SoC) falls below a specified threshold. We formulate the SEVRP-T as a\ntwo-stage stochastic mixed-integer second-order cone model, where the first\nstage determines the sequences of customers to be visited, and the second stage\nincorporates charging activities. The objective is to minimize the expected\ntotal duration of the routes, composed by travel times and recharging\noperations. To cope with the computational complexity of the model, we propose\na heuristic based on an Iterated Local Search (ILS) procedure coupled with a\nSet Partitioning problem. To further speed up the heuristic, we develop two\nlower bounds on the corresponding first-stage customer sequences. Furthermore,\nto handle a large number of energy consumption scenarios, we employ a scenario\nreduction technique. Extensive computational experiments are conducted to\nvalidate the effectiveness of the proposed solution strategy and to assess the\nimportance of considering the stochastic nature of the energy consumption. The\nresearch presented in this paper contributes to the growing body of literature\non EVRP and provides insights into managing the operational deployment of EVs\nin logistics activities under uncertainty."
                },
                "authors": [
                    {
                        "name": "Andrea Spinelli"
                    },
                    {
                        "name": "Dario Bezzi"
                    },
                    {
                        "name": "Ola Jabali"
                    },
                    {
                        "name": "Francesca Maggioni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Maggioni"
                },
                "author": "Francesca Maggioni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19420v1",
                "updated": "2025-06-24T08:38:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    38,
                    32,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T08:38:32Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    38,
                    32,
                    1,
                    175,
                    0
                ],
                "title": "Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commander-GPT: Dividing and Routing for Multimodal Sarcasm Detection"
                },
                "summary": "Multimodal sarcasm understanding is a high-order cognitive task. Although\nlarge language models (LLMs) have shown impressive performance on many\ndownstream NLP tasks, growing evidence suggests that they struggle with sarcasm\nunderstanding. In this paper, we propose Commander-GPT, a modular decision\nrouting framework inspired by military command theory. Rather than relying on a\nsingle LLM's capability, Commander-GPT orchestrates a team of specialized LLM\nagents where each agent will be selectively assigned to a focused sub-task such\nas context modeling, sentiment analysis, etc. Their outputs are then routed\nback to the commander, which integrates the information and performs the final\nsarcasm judgment. To coordinate these agents, we introduce three types of\ncentralized commanders: (1) a trained lightweight encoder-based commander\n(e.g., multi-modal BERT); (2) four small autoregressive language models,\nserving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large\nLLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output\naggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate\nCommander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting\nstrategies. Experimental results show that our framework achieves 4.4% and\n11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on\naverage, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal sarcasm understanding is a high-order cognitive task. Although\nlarge language models (LLMs) have shown impressive performance on many\ndownstream NLP tasks, growing evidence suggests that they struggle with sarcasm\nunderstanding. In this paper, we propose Commander-GPT, a modular decision\nrouting framework inspired by military command theory. Rather than relying on a\nsingle LLM's capability, Commander-GPT orchestrates a team of specialized LLM\nagents where each agent will be selectively assigned to a focused sub-task such\nas context modeling, sentiment analysis, etc. Their outputs are then routed\nback to the commander, which integrates the information and performs the final\nsarcasm judgment. To coordinate these agents, we introduce three types of\ncentralized commanders: (1) a trained lightweight encoder-based commander\n(e.g., multi-modal BERT); (2) four small autoregressive language models,\nserving as moderately capable commanders (e.g., DeepSeek-VL); (3) two large\nLLM-based commander (Gemini Pro and GPT-4o) that performs task routing, output\naggregation, and sarcasm decision-making in a zero-shot fashion. We evaluate\nCommander-GPT on the MMSD and MMSD 2.0 benchmarks, comparing five prompting\nstrategies. Experimental results show that our framework achieves 4.4% and\n11.7% improvement in F1 score over state-of-the-art (SoTA) baselines on\naverage, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Yazhou Zhang"
                    },
                    {
                        "name": "Chunwang Zou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19918v3",
                "updated": "2025-06-24T08:27:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    27,
                    42,
                    1,
                    175,
                    0
                ],
                "published": "2025-02-27T09:40:13Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    9,
                    40,
                    13,
                    3,
                    58,
                    0
                ],
                "title": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Reasoner: Dynamic Guidance for Optimized Inference-time Reasoning\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\ncontextual multi-armed bandits to iteratively evaluate reasoning progress and\nselect optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on prolonged reasoning chains\nto solve complex tasks. However, this trial-and-error approach often leads to\nhigh computational overhead and error propagation, where early mistakes can\nderail subsequent steps. To address these issues, we introduce Meta-Reasoner, a\nframework that dynamically optimizes inference-time reasoning by enabling LLMs\nto \"think about how to think.\" Drawing inspiration from human meta-cognition\nand dual-process theory, Meta-Reasoner operates as a strategic advisor,\ndecoupling high-level guidance from step-by-step generation. It employs\ncontextual multi-armed bandits to iteratively evaluate reasoning progress and\nselect optimal strategies (e.g., backtrack, clarify ambiguity, restart from\nscratch, or propose alternative approaches), and reallocates computational\nresources toward the most promising paths. Our evaluations on mathematical\nreasoning and puzzles highlight the potential of dynamic reasoning chains to\novercome inherent challenges in the LLM reasoning process and also show promise\nin broader applications, offering a scalable and adaptable solution for\nreasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Simeng Han"
                    },
                    {
                        "name": "Yulin Chen"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19399v1",
                "updated": "2025-06-24T08:08:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    8,
                    15,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T08:08:15Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    8,
                    15,
                    1,
                    175,
                    0
                ],
                "title": "Automated Detection of Pre-training Text in Black-box LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Detection of Pre-training Text in Black-box LLMs"
                },
                "summary": "Detecting whether a given text is a member of the pre-training data of Large\nLanguage Models (LLMs) is crucial for ensuring data privacy and copyright\nprotection. Most existing methods rely on the LLM's hidden information (e.g.,\nmodel parameters or token probabilities), making them ineffective in the\nblack-box setting, where only input and output texts are accessible. Although\nsome methods have been proposed for the black-box setting, they rely on massive\nmanual efforts such as designing complicated questions or instructions. To\naddress these issues, we propose VeilProbe, the first framework for\nautomatically detecting LLMs' pre-training texts in a black-box setting without\nhuman intervention. VeilProbe utilizes a sequence-to-sequence mapping model to\ninfer the latent mapping feature between the input text and the corresponding\noutput suffix generated by the LLM. Then it performs the key token\nperturbations to obtain more distinguishable membership features. Additionally,\nconsidering real-world scenarios where the ground-truth training text samples\nare limited, a prototype-based membership classifier is introduced to alleviate\nthe overfitting issue. Extensive evaluations on three widely used datasets\ndemonstrate that our framework is effective and superior in the black-box\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting whether a given text is a member of the pre-training data of Large\nLanguage Models (LLMs) is crucial for ensuring data privacy and copyright\nprotection. Most existing methods rely on the LLM's hidden information (e.g.,\nmodel parameters or token probabilities), making them ineffective in the\nblack-box setting, where only input and output texts are accessible. Although\nsome methods have been proposed for the black-box setting, they rely on massive\nmanual efforts such as designing complicated questions or instructions. To\naddress these issues, we propose VeilProbe, the first framework for\nautomatically detecting LLMs' pre-training texts in a black-box setting without\nhuman intervention. VeilProbe utilizes a sequence-to-sequence mapping model to\ninfer the latent mapping feature between the input text and the corresponding\noutput suffix generated by the LLM. Then it performs the key token\nperturbations to obtain more distinguishable membership features. Additionally,\nconsidering real-world scenarios where the ground-truth training text samples\nare limited, a prototype-based membership classifier is introduced to alleviate\nthe overfitting issue. Extensive evaluations on three widely used datasets\ndemonstrate that our framework is effective and superior in the black-box\nsetting."
                },
                "authors": [
                    {
                        "name": "Ruihan Hu"
                    },
                    {
                        "name": "Yu-Ming Shang"
                    },
                    {
                        "name": "Jiankun Peng"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Yazhe Wang"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18813v2",
                "updated": "2025-06-24T08:05:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    5,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-24T15:54:10Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    54,
                    10,
                    0,
                    83,
                    0
                ],
                "title": "Defeating Prompt Injections by Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeating Prompt Injections by Design"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an untrusted environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models are susceptible to attacks. To\noperate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL uses a notion of a\ncapability to prevent the exfiltration of private data over unauthorized data\nflows by enforcing security policies when tools are called. We demonstrate\neffectiveness of CaMeL by solving $77\\%$ of tasks with provable security\n(compared to $84\\%$ with an undefended system) in AgentDojo. We release CaMeL\nat https://github.com/google-research/camel-prompt-injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an untrusted environment. However, LLM agents are vulnerable\nto prompt injection attacks when handling untrusted data. In this paper we\npropose CaMeL, a robust defense that creates a protective system layer around\nthe LLM, securing it even when underlying models are susceptible to attacks. To\noperate, CaMeL explicitly extracts the control and data flows from the\n(trusted) query; therefore, the untrusted data retrieved by the LLM can never\nimpact the program flow. To further improve security, CaMeL uses a notion of a\ncapability to prevent the exfiltration of private data over unauthorized data\nflows by enforcing security policies when tools are called. We demonstrate\neffectiveness of CaMeL by solving $77\\%$ of tasks with provable security\n(compared to $84\\%$ with an undefended system) in AgentDojo. We release CaMeL\nat https://github.com/google-research/camel-prompt-injection."
                },
                "authors": [
                    {
                        "name": "Edoardo Debenedetti"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Tianqi Fan"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Daniel Fabian"
                    },
                    {
                        "name": "Christoph Kern"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Andreas Terzis"
                    },
                    {
                        "name": "Florian Tramèr"
                    }
                ],
                "author_detail": {
                    "name": "Florian Tramèr"
                },
                "author": "Florian Tramèr",
                "arxiv_comment": "Updated version with newer models and link to the code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19398v1",
                "updated": "2025-06-24T08:01:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    1,
                    33,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T08:01:33Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    8,
                    1,
                    33,
                    1,
                    175,
                    0
                ],
                "title": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and\n  Practical Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClearerVoice-Studio: Bridging Advanced Speech Processing Research and\n  Practical Deployment"
                },
                "summary": "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech\nprocessing toolkit designed to bridge cutting-edge research and practical\napplication. Unlike broad platforms like SpeechBrain and ESPnet,\nClearerVoice-Studio focuses on interconnected speech tasks of speech\nenhancement, separation, super-resolution, and multimodal target speaker\nextraction. A key advantage is its state-of-the-art pretrained models,\nincluding FRCRN with 3 million uses and MossFormer with 2.5 million uses,\noptimized for real-world scenarios. It also offers model optimization tools,\nmulti-format audio support, the SpeechScore evaluation toolkit, and\nuser-friendly interfaces, catering to researchers, developers, and end-users.\nIts rapid adoption attracting 3000 GitHub stars and 239 forks highlights its\nacademic and industrial impact. This paper details ClearerVoice-Studio's\ncapabilities, architectures, training strategies, benchmarks, community impact,\nand future plan. Source code is available at\nhttps://github.com/modelscope/ClearerVoice-Studio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ClearerVoice-Studio, an open-source, AI-powered speech\nprocessing toolkit designed to bridge cutting-edge research and practical\napplication. Unlike broad platforms like SpeechBrain and ESPnet,\nClearerVoice-Studio focuses on interconnected speech tasks of speech\nenhancement, separation, super-resolution, and multimodal target speaker\nextraction. A key advantage is its state-of-the-art pretrained models,\nincluding FRCRN with 3 million uses and MossFormer with 2.5 million uses,\noptimized for real-world scenarios. It also offers model optimization tools,\nmulti-format audio support, the SpeechScore evaluation toolkit, and\nuser-friendly interfaces, catering to researchers, developers, and end-users.\nIts rapid adoption attracting 3000 GitHub stars and 239 forks highlights its\nacademic and industrial impact. This paper details ClearerVoice-Studio's\ncapabilities, architectures, training strategies, benchmarks, community impact,\nand future plan. Source code is available at\nhttps://github.com/modelscope/ClearerVoice-Studio."
                },
                "authors": [
                    {
                        "name": "Shengkui Zhao"
                    },
                    {
                        "name": "Zexu Pan"
                    },
                    {
                        "name": "Bin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Bin Ma"
                },
                "author": "Bin Ma",
                "arxiv_comment": "accepted by Interspeech 2025, 5 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18082v2",
                "updated": "2025-06-24T07:59:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    59,
                    45,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-22T16:08:44Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    16,
                    8,
                    44,
                    6,
                    173,
                    0
                ],
                "title": "Statistical Multicriteria Evaluation of LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Multicriteria Evaluation of LLM-Generated Text"
                },
                "summary": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design."
                },
                "authors": [
                    {
                        "name": "Esteban Garces Arias"
                    },
                    {
                        "name": "Hannah Blocher"
                    },
                    {
                        "name": "Julian Rodemann"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Christoph Jansen"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Jansen"
                },
                "author": "Christoph Jansen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15066v2",
                "updated": "2025-06-24T07:31:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    31,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-18T02:15:02Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    15,
                    2,
                    2,
                    169,
                    0
                ],
                "title": "ChatModel: Automating Reference Model Design and Verification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatModel: Automating Reference Model Design and Verification with LLMs"
                },
                "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation."
                },
                "authors": [
                    {
                        "name": "Jianmin Ye"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Shengchu Su"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Xi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Wang"
                },
                "author": "Xi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08890v2",
                "updated": "2025-06-24T07:24:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    24,
                    29,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-10T15:17:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    15,
                    17,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and\n  Non-verbal Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and\n  Non-verbal Communication"
                },
                "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."
                },
                "authors": [
                    {
                        "name": "Tauhid Tanjim"
                    },
                    {
                        "name": "Promise Ekpo"
                    },
                    {
                        "name": "Huajie Cao"
                    },
                    {
                        "name": "Jonathan St. George"
                    },
                    {
                        "name": "Kevin Ching"
                    },
                    {
                        "name": "Hee Rin Lee"
                    },
                    {
                        "name": "Angelique Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Angelique Taylor"
                },
                "author": "Angelique Taylor",
                "arxiv_comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19385v1",
                "updated": "2025-06-24T07:20:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    20,
                    45,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T07:20:45Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    20,
                    45,
                    1,
                    175,
                    0
                ],
                "title": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue\n  Systems through Adaptive Dual-Retrieval of Flow Patterns and Context\n  Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue\n  Systems through Adaptive Dual-Retrieval of Flow Patterns and Context\n  Semantics"
                },
                "summary": "We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval\nAugmented Generation), a novel framework that addresses the limitations of\nexisting dialogue systems in maintaining both contextual coherence and\ngoal-oriented progression in multi-turn customer service conversations. Unlike\ntraditional RAG systems that rely solely on semantic similarity (Conversation\nRAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic\nintent transition graphs from goal achieved historical dialogues and implements\na dual-retrieval mechanism that adaptively balances intent-based graph\ntraversal with semantic search. This approach enables the system to\nsimultaneously leverage both conversional intent flow patterns and contextual\nsemantics, significantly improving retrieval quality and response quality. In\nextensive experiments on real-world customer service dialogues, we employ both\nautomatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG\nsignificantly outperforms both semantic-based Conversation RAG and intent-based\nGraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG\ndemonstrates substantial improvements over Conversation RAG across automatic\nmetrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and\nmost notably, a 58% improvement in response quality according to LLM-as-judge\nevaluations. These results demonstrate that the integration of intent\ntransition structures with semantic retrieval creates a synergistic effect that\nneither approach achieves independently, establishing CID-GraphRAG as an\neffective framework for addressing the challenges of maintaining contextual\ncoherence and goal-oriented progression in knowledge-intensive multi-turn\ndialogues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval\nAugmented Generation), a novel framework that addresses the limitations of\nexisting dialogue systems in maintaining both contextual coherence and\ngoal-oriented progression in multi-turn customer service conversations. Unlike\ntraditional RAG systems that rely solely on semantic similarity (Conversation\nRAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic\nintent transition graphs from goal achieved historical dialogues and implements\na dual-retrieval mechanism that adaptively balances intent-based graph\ntraversal with semantic search. This approach enables the system to\nsimultaneously leverage both conversional intent flow patterns and contextual\nsemantics, significantly improving retrieval quality and response quality. In\nextensive experiments on real-world customer service dialogues, we employ both\nautomatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG\nsignificantly outperforms both semantic-based Conversation RAG and intent-based\nGraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG\ndemonstrates substantial improvements over Conversation RAG across automatic\nmetrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and\nmost notably, a 58% improvement in response quality according to LLM-as-judge\nevaluations. These results demonstrate that the integration of intent\ntransition structures with semantic retrieval creates a synergistic effect that\nneither approach achieves independently, establishing CID-GraphRAG as an\neffective framework for addressing the challenges of maintaining contextual\ncoherence and goal-oriented progression in knowledge-intensive multi-turn\ndialogues."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhu"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Honglong Zhang"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "HanGeng Chen"
                    },
                    {
                        "name": "Mengran Zhang"
                    },
                    {
                        "name": "Xilun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilun Chen"
                },
                "author": "Xilun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19382v1",
                "updated": "2025-06-24T07:18:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    18,
                    20,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T07:18:20Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    18,
                    20,
                    1,
                    175,
                    0
                ],
                "title": "Measuring and Guiding Monosemanticity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Guiding Monosemanticity"
                },
                "summary": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Ruben Härle"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Stephan Wäldchen"
                    },
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18631v2",
                "updated": "2025-06-24T07:07:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    7,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T13:36:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    36,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
                },
                "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Jiarui Yu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "10 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13040v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13040v7",
                "updated": "2025-06-24T07:06:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    6,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2023-05-22T13:47:51Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    13,
                    47,
                    51,
                    0,
                    142,
                    0
                ],
                "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented\n  Dialogue Agents"
                },
                "summary": "Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) models have made significant progress in recent\nyears. However, previous studies primarily focus on datasets written by\nannotators, which has resulted in a gap between academic research and\nreal-world spoken conversation scenarios. While several small-scale spoken TOD\ndatasets are proposed to address robustness issues such as ASR errors, they\nignore the unique challenges in spoken conversation. To tackle the limitations,\nwe introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,\ncontaining 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from\nhuman-to-human spoken conversations. SpokenWOZ further incorporates common\nspoken characteristics such as word-by-word processing and reasoning in spoken\nlanguage. Based on these characteristics, we present cross-turn slot and\nreasoning slot detection as new challenges. We conduct experiments on various\nbaselines, including text-modal models, newly proposed dual-modal models, and\nLLMs, e.g., ChatGPT. The results show that the current models still have\nsubstantial room for improvement in spoken conversation, where the most\nadvanced dialogue state tracker only achieves 25.65% in joint goal accuracy and\nthe SOTA end-to-end model only correctly completes the user request in 52.1% of\ndialogues. The dataset, code, and leaderboard are available:\nhttps://spokenwoz.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Haoyu Gao"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Yinpei Dai"
                    },
                    {
                        "name": "Hangyu Li"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "NeurIPS 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13040v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13040v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19366v1",
                "updated": "2025-06-24T06:53:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    53,
                    20,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T06:53:20Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    53,
                    20,
                    1,
                    175,
                    0
                ],
                "title": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network\n  Performance"
                },
                "summary": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, and scale-free network models reveal that high-dimensional\nfractal topologies achieve enhanced resilience and throughput under equivalent\nconditions. These findings demonstrate the potential of fractal geometry as a\ndesign paradigm for scalable and efficient WMN architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, and scale-free network models reveal that high-dimensional\nfractal topologies achieve enhanced resilience and throughput under equivalent\nconditions. These findings demonstrate the potential of fractal geometry as a\ndesign paradigm for scalable and efficient WMN architectures."
                },
                "authors": [
                    {
                        "name": "Marat Zaidyn"
                    },
                    {
                        "name": "Sayat Akhtanov"
                    },
                    {
                        "name": "Dana Turlykozhayeva"
                    },
                    {
                        "name": "Symbat Temesheva"
                    },
                    {
                        "name": "Almat Akhmetali"
                    },
                    {
                        "name": "Alisher Skabylov"
                    },
                    {
                        "name": "Nurzhan Ussipov"
                    }
                ],
                "author_detail": {
                    "name": "Nurzhan Ussipov"
                },
                "author": "Nurzhan Ussipov",
                "arxiv_comment": "11 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19353v1",
                "updated": "2025-06-24T06:33:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    33,
                    41,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T06:33:41Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    33,
                    41,
                    1,
                    175,
                    0
                ],
                "title": "Partially Observable Residual Reinforcement Learning for\n  PV-Inverter-Based Voltage Control in Distribution Grids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observable Residual Reinforcement Learning for\n  PV-Inverter-Based Voltage Control in Distribution Grids"
                },
                "summary": "This paper introduces an efficient Residual Reinforcement Learning (RRL)\nframework for voltage control in active distribution grids. Voltage control\nremains a critical challenge in distribution grids, where conventional\nReinforcement Learning (RL) methods often suffer from slow training convergence\nand inefficient exploration. To overcome these challenges, the proposed RRL\napproach learns a residual policy on top of a modified Sequential Droop Control\n(SDC) mechanism, ensuring faster convergence. Additionally, the framework\nintroduces a Local Shared Linear (LSL) architecture for the Q-network and a\nTransformer-Encoder actor network, which collectively enhance overall\nperformance. Unlike several existing approaches, the proposed method relies\nsolely on inverters' measurements without requiring full state information of\nthe power grid, rendering it more practical for real-world deployment.\nSimulation results validate the effectiveness of the RRL framework in achieving\nrapid convergence, minimizing active power curtailment, and ensuring reliable\nvoltage regulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an efficient Residual Reinforcement Learning (RRL)\nframework for voltage control in active distribution grids. Voltage control\nremains a critical challenge in distribution grids, where conventional\nReinforcement Learning (RL) methods often suffer from slow training convergence\nand inefficient exploration. To overcome these challenges, the proposed RRL\napproach learns a residual policy on top of a modified Sequential Droop Control\n(SDC) mechanism, ensuring faster convergence. Additionally, the framework\nintroduces a Local Shared Linear (LSL) architecture for the Q-network and a\nTransformer-Encoder actor network, which collectively enhance overall\nperformance. Unlike several existing approaches, the proposed method relies\nsolely on inverters' measurements without requiring full state information of\nthe power grid, rendering it more practical for real-world deployment.\nSimulation results validate the effectiveness of the RRL framework in achieving\nrapid convergence, minimizing active power curtailment, and ensuring reliable\nvoltage regulation."
                },
                "authors": [
                    {
                        "name": "Sarra Bouchkati"
                    },
                    {
                        "name": "Ramil Sabirov"
                    },
                    {
                        "name": "Steffen Kortmann"
                    },
                    {
                        "name": "Andreas Ulbig"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Ulbig"
                },
                "author": "Andreas Ulbig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19352v1",
                "updated": "2025-06-24T06:33:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    33,
                    10,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T06:33:10Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    33,
                    10,
                    1,
                    175,
                    0
                ],
                "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona\n  Fidelity in Open-Ended Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona\n  Fidelity in Open-Ended Generation"
                },
                "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression."
                },
                "authors": [
                    {
                        "name": "Jisu Shin"
                    },
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Hoyun Song"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]