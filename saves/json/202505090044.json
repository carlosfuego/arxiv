[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v2",
                "updated": "2025-05-07T13:07:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    7,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v6",
                "updated": "2025-05-07T01:29:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    29,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v6",
                "updated": "2025-05-06T19:28:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    19,
                    28,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02082v2",
                "updated": "2025-05-08T07:37:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    37,
                    11,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-04T12:21:16Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "title": "Performance Characterization of Containers in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization of Containers in Edge Computing"
                },
                "summary": "Edge computing addresses critical limitations of cloud computing such as high\nlatency and network congestion by decentralizing processing from cloud to the\nedge. However, the need for software replication across heterogeneous edge\ndevices introduces dependency and portability challenges, driving the adoption\nof containerization technologies like Docker. While containers offer\nlightweight isolation and deployment advantages, they introduce new bottlenecks\nin edge environments, including cold-start delays, memory constraints, network\nthroughput variability, and inefficient IO handling when interfacing with\nembedded peripherals. This paper presents an empirical evaluation of Docker\ncontainers on resource-constrained edge devices, using Raspberry Pi as a\nrepresentative platform. We benchmark performance across diverse workloads,\nincluding microbenchmarks (CPU, memory, network profiling) and macrobenchmarks\n(AI inference, sensor IO operations), to quantify the overheads of\ncontainerization in real-world edge scenarios. Our testbed comprises physical\nRaspberry Pi nodes integrated with environmental sensors and camera modules,\nenabling measurements of latency, memory faults, IO throughput, and cold start\ndelays under varying loads. Key findings reveal trade-offs between container\nisolation and edge-specific resource limitations, with performance degradation\nobserved in IO heavy and latency sensitive tasks. We identify configuration\noptimizations to mitigate these issues, providing actionable insights for\ndeploying containers in edge environments while meeting real time and\nreliability requirements. This work advances the understanding of containerized\nedge computing by systematically evaluating its feasibility and pitfalls on\nlow-power embedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing addresses critical limitations of cloud computing such as high\nlatency and network congestion by decentralizing processing from cloud to the\nedge. However, the need for software replication across heterogeneous edge\ndevices introduces dependency and portability challenges, driving the adoption\nof containerization technologies like Docker. While containers offer\nlightweight isolation and deployment advantages, they introduce new bottlenecks\nin edge environments, including cold-start delays, memory constraints, network\nthroughput variability, and inefficient IO handling when interfacing with\nembedded peripherals. This paper presents an empirical evaluation of Docker\ncontainers on resource-constrained edge devices, using Raspberry Pi as a\nrepresentative platform. We benchmark performance across diverse workloads,\nincluding microbenchmarks (CPU, memory, network profiling) and macrobenchmarks\n(AI inference, sensor IO operations), to quantify the overheads of\ncontainerization in real-world edge scenarios. Our testbed comprises physical\nRaspberry Pi nodes integrated with environmental sensors and camera modules,\nenabling measurements of latency, memory faults, IO throughput, and cold start\ndelays under varying loads. Key findings reveal trade-offs between container\nisolation and edge-specific resource limitations, with performance degradation\nobserved in IO heavy and latency sensitive tasks. We identify configuration\noptimizations to mitigate these issues, providing actionable insights for\ndeploying containers in edge environments while meeting real time and\nreliability requirements. This work advances the understanding of containerized\nedge computing by systematically evaluating its feasibility and pitfalls on\nlow-power embedded systems."
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03763v1",
                "updated": "2025-04-21T00:21:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:21:08Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    21,
                    8,
                    0,
                    111,
                    0
                ],
                "title": "Splitwiser: Efficient LM inference with constrained resources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Splitwiser: Efficient LM inference with constrained resources"
                },
                "summary": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml)."
                },
                "authors": [
                    {
                        "name": "Asad Aali"
                    },
                    {
                        "name": "Adney Cardoza"
                    },
                    {
                        "name": "Melissa Capo"
                    }
                ],
                "author_detail": {
                    "name": "Melissa Capo"
                },
                "author": "Melissa Capo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Côme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "César Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03756v1",
                "updated": "2025-04-19T13:17:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    17,
                    34,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T13:17:34Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    13,
                    17,
                    34,
                    5,
                    109,
                    0
                ],
                "title": "Improving the Serving Performance of Multi-LoRA Large Language Models\n  via Efficient LoRA and KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Serving Performance of Multi-LoRA Large Language Models\n  via Efficient LoRA and KV Cache Management"
                },
                "summary": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Jiuchen Shi"
                    },
                    {
                        "name": "Yixiao Wang"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.04623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04623v1",
                "updated": "2025-05-07T17:59:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:59:49Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    49,
                    2,
                    127,
                    0
                ],
                "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning"
                },
                "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04620v1",
                "updated": "2025-05-07T17:59:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:59:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Path to Multimodal Generalist: General-Level and General-Bench"
                },
                "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/"
                },
                "authors": [
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Qingshan Xu"
                    },
                    {
                        "name": "Bobo Li"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Yaoting Wang"
                    },
                    {
                        "name": "Junbao Zhou"
                    },
                    {
                        "name": "Jiahao Meng"
                    },
                    {
                        "name": "Qingyu Shi"
                    },
                    {
                        "name": "Zhiyuan Zhou"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Weiming Wu"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Yaobo Ye"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zixiang Meng"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Liyu Jia"
                    },
                    {
                        "name": "Wentao Hu"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Hanwang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanwang Zhang"
                },
                "author": "Hanwang Zhang",
                "arxiv_comment": "ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04613v1",
                "updated": "2025-05-07T17:56:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    56,
                    19,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:56:19Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    56,
                    19,
                    2,
                    127,
                    0
                ],
                "title": "From Two Sample Testing to Singular Gaussian Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Two Sample Testing to Singular Gaussian Discrimination"
                },
                "summary": "We establish that testing for the equality of two probability measures on a\ngeneral separable and compact metric space is equivalent to testing for the\nsingularity between two corresponding Gaussian measures on a suitable\nReproducing Kernel Hilbert Space. The corresponding Gaussians are defined via\nthe notion of kernel mean and covariance embedding of a probability measure.\nDiscerning two singular Gaussians is fundamentally simpler from an\ninformation-theoretic perspective than non-parametric two-sample testing,\nparticularly in high-dimensional settings. Our proof leverages the\nFeldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert\nspaces, and shows that discrepancies between distributions are heavily\nmagnified through their corresponding Gaussian embeddings: at a population\nlevel, distinct probability measures lead to essentially separated Gaussian\nembeddings. This appears to be a new instance of the blessing of dimensionality\nthat can be harnessed for the design of efficient inference tools in great\ngenerality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We establish that testing for the equality of two probability measures on a\ngeneral separable and compact metric space is equivalent to testing for the\nsingularity between two corresponding Gaussian measures on a suitable\nReproducing Kernel Hilbert Space. The corresponding Gaussians are defined via\nthe notion of kernel mean and covariance embedding of a probability measure.\nDiscerning two singular Gaussians is fundamentally simpler from an\ninformation-theoretic perspective than non-parametric two-sample testing,\nparticularly in high-dimensional settings. Our proof leverages the\nFeldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert\nspaces, and shows that discrepancies between distributions are heavily\nmagnified through their corresponding Gaussian embeddings: at a population\nlevel, distinct probability measures lead to essentially separated Gaussian\nembeddings. This appears to be a new instance of the blessing of dimensionality\nthat can be harnessed for the design of efficient inference tools in great\ngenerality."
                },
                "authors": [
                    {
                        "name": "Leonardo V. Santoro"
                    },
                    {
                        "name": "Kartik G. Waghmare"
                    },
                    {
                        "name": "Victor M. Panaretos"
                    }
                ],
                "author_detail": {
                    "name": "Victor M. Panaretos"
                },
                "author": "Victor M. Panaretos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G10, 46E22, 60G15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04611v2",
                "updated": "2025-05-08T11:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    56,
                    7,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T17:55:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    55,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "Particle Gibbs without the Gibbs bit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle Gibbs without the Gibbs bit"
                },
                "summary": "Exact parameter and trajectory inference in state-space models is typically\nachieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or\nparticle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly\nproposes a new trajectory and parameter, and accepts or rejects both at once.\nPGibbs instead alternates between sampling from the trajectory, using an\nalgorithm known as conditional sequential Monte Carlo (CSMC) and the parameter\nin a Hastings-within-Gibbs fashion. While particle independent Metropolis\nHastings (PIMH), the parameter-free version of PMMH, is known to be\nstatistically worse than CSMC, PGibbs can induce a slow mixing if the parameter\nand the state trajectory are very correlated. This has made PMMH the method of\nchoice for many practitioners, despite theory and experiments favouring CSMC\nover PIMH for the parameter-free problem. In this article, we describe a\nformulation of PGibbs which bypasses the Gibbs step, essentially marginalizing\nover the trajectory distribution in a fashion similar to PMMH. This is achieved\nby considering the implementation of a CSMC algortihm for the state-space model\nintegrated over the joint distribution of the current parameter and the\nparameter proposal. We illustrate the benefits of method on a simple example\nknown to be challenging for PMMH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact parameter and trajectory inference in state-space models is typically\nachieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or\nparticle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly\nproposes a new trajectory and parameter, and accepts or rejects both at once.\nPGibbs instead alternates between sampling from the trajectory, using an\nalgorithm known as conditional sequential Monte Carlo (CSMC) and the parameter\nin a Hastings-within-Gibbs fashion. While particle independent Metropolis\nHastings (PIMH), the parameter-free version of PMMH, is known to be\nstatistically worse than CSMC, PGibbs can induce a slow mixing if the parameter\nand the state trajectory are very correlated. This has made PMMH the method of\nchoice for many practitioners, despite theory and experiments favouring CSMC\nover PIMH for the parameter-free problem. In this article, we describe a\nformulation of PGibbs which bypasses the Gibbs step, essentially marginalizing\nover the trajectory distribution in a fashion similar to PMMH. This is achieved\nby considering the implementation of a CSMC algortihm for the state-space model\nintegrated over the joint distribution of the current parameter and the\nparameter proposal. We illustrate the benefits of method on a simple example\nknown to be challenging for PMMH."
                },
                "authors": [
                    {
                        "name": "Adrien Corenflos"
                    }
                ],
                "author_detail": {
                    "name": "Adrien Corenflos"
                },
                "author": "Adrien Corenflos",
                "arxiv_comment": "Feedback most welcome. 12 pages, 1 figure. Difference with previous\n  version: fixed a couple of typos + longer simulations to remove noise in the\n  figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04608v1",
                "updated": "2025-05-07T17:53:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    53,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    53,
                    47,
                    2,
                    127,
                    0
                ],
                "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\n  Weighted-Conformal Martingales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\n  Weighted-Conformal Martingales"
                },
                "summary": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Drew Prinster"
                    },
                    {
                        "name": "Xing Han"
                    },
                    {
                        "name": "Anqi Liu"
                    },
                    {
                        "name": "Suchi Saria"
                    }
                ],
                "author_detail": {
                    "name": "Suchi Saria"
                },
                "author": "Suchi Saria",
                "arxiv_comment": "To be published in The International Conference on Machine Learning\n  (ICML), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04606v1",
                "updated": "2025-05-07T17:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    51,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:51:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    51,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution"
                },
                "summary": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Lianghong Guo"
                    },
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Runhan Jiang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Mingzhi Mao"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "To appear at ISSTA'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04603v1",
                "updated": "2025-05-07T17:50:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    50,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:50:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    50,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "Likelihood-Free Adaptive Bayesian Inference via Nonparametric\n  Distribution Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-Free Adaptive Bayesian Inference via Nonparametric\n  Distribution Matching"
                },
                "summary": "When the likelihood is analytically unavailable and computationally\nintractable, approximate Bayesian computation (ABC) has emerged as a widely\nused methodology for approximate posterior inference; however, it suffers from\nsevere computational inefficiency in high-dimensional settings or under diffuse\npriors. To overcome these limitations, we propose Adaptive Bayesian Inference\n(ABI), a framework that bypasses traditional data-space discrepancies and\ninstead compares distributions directly in posterior space through\nnonparametric distribution matching. By leveraging a novel Marginally-augmented\nSliced Wasserstein (MSW) distance on posterior measures and exploiting its\nquantile representation, ABI transforms the challenging problem of measuring\ndivergence between posterior distributions into a tractable sequence of\none-dimensional conditional quantile regression tasks. Moreover, we introduce a\nnew adaptive rejection sampling scheme that iteratively refines the posterior\napproximation by updating the proposal distribution via generative density\nestimation. Theoretically, we establish parametric convergence rates for the\ntrimmed MSW distance and prove that the ABI posterior converges to the true\nposterior as the tolerance threshold vanishes. Through extensive empirical\nevaluation, we demonstrate that ABI significantly outperforms data-based\nWasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free\nsimulators, especially in high-dimensional or dependent observation regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the likelihood is analytically unavailable and computationally\nintractable, approximate Bayesian computation (ABC) has emerged as a widely\nused methodology for approximate posterior inference; however, it suffers from\nsevere computational inefficiency in high-dimensional settings or under diffuse\npriors. To overcome these limitations, we propose Adaptive Bayesian Inference\n(ABI), a framework that bypasses traditional data-space discrepancies and\ninstead compares distributions directly in posterior space through\nnonparametric distribution matching. By leveraging a novel Marginally-augmented\nSliced Wasserstein (MSW) distance on posterior measures and exploiting its\nquantile representation, ABI transforms the challenging problem of measuring\ndivergence between posterior distributions into a tractable sequence of\none-dimensional conditional quantile regression tasks. Moreover, we introduce a\nnew adaptive rejection sampling scheme that iteratively refines the posterior\napproximation by updating the proposal distribution via generative density\nestimation. Theoretically, we establish parametric convergence rates for the\ntrimmed MSW distance and prove that the ABI posterior converges to the true\nposterior as the tolerance threshold vanishes. Through extensive empirical\nevaluation, we demonstrate that ABI significantly outperforms data-based\nWasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free\nsimulators, especially in high-dimensional or dependent observation regimes."
                },
                "authors": [
                    {
                        "name": "Wenhui Sophia Lu"
                    },
                    {
                        "name": "Wing Hung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Wing Hung Wong"
                },
                "author": "Wing Hung Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04602v1",
                "updated": "2025-05-07T17:48:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    48,
                    56,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:48:56Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    48,
                    56,
                    2,
                    127,
                    0
                ],
                "title": "Extracting local velocity from cosmic dipole using simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting local velocity from cosmic dipole using simulations"
                },
                "summary": "Our velocity with respect to the cosmic frame of rest leads to a dipole in\nthe number count distribution of galaxies. The dipole depends on the source\nspectrum, which is usually assumed to be a power law, $S(\\nu) \\propto\n\\nu^{-\\alpha}$ and on the flux dependence of the number density of sources. The\nlatter is also generally assumed to be a power law, parametrised with exponent\n$x$. The velocity can be extracted from the observed dipole once the two\nparameters $x$ and $\\alpha$ are known. The standard procedure uses the mean\nvalue of $\\alpha$ across the entire sample, and the parameter $x$ is inferred\nby fitting the cumulative number count, $\\frac{dN}{d\\Omega}(>S_*) \\propto\nS_*^{-x}$, near the flux limit $S_*$ of the survey. Here, we introduce a\nsimulation procedure to extract the velocity which directly uses the $\\alpha$\nvalues of each source rather than their mean and does not rely on the\nfunctional form of the cumulative number count near the flux limit. We apply\nthis to the quasar sample in CatWISE2020 data and find that the final results\ndiffer from the standard procedure by approximately one sigma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our velocity with respect to the cosmic frame of rest leads to a dipole in\nthe number count distribution of galaxies. The dipole depends on the source\nspectrum, which is usually assumed to be a power law, $S(\\nu) \\propto\n\\nu^{-\\alpha}$ and on the flux dependence of the number density of sources. The\nlatter is also generally assumed to be a power law, parametrised with exponent\n$x$. The velocity can be extracted from the observed dipole once the two\nparameters $x$ and $\\alpha$ are known. The standard procedure uses the mean\nvalue of $\\alpha$ across the entire sample, and the parameter $x$ is inferred\nby fitting the cumulative number count, $\\frac{dN}{d\\Omega}(>S_*) \\propto\nS_*^{-x}$, near the flux limit $S_*$ of the survey. Here, we introduce a\nsimulation procedure to extract the velocity which directly uses the $\\alpha$\nvalues of each source rather than their mean and does not rely on the\nfunctional form of the cumulative number count near the flux limit. We apply\nthis to the quasar sample in CatWISE2020 data and find that the final results\ndiffer from the standard procedure by approximately one sigma."
                },
                "authors": [
                    {
                        "name": "Mohit Panwar"
                    },
                    {
                        "name": "Akash Gandhi"
                    },
                    {
                        "name": "Pankaj Jain"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Jain"
                },
                "author": "Pankaj Jain",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04594v2",
                "updated": "2025-05-08T06:18:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    18,
                    31,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T17:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    37,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"
                },
                "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Girish Chandar Ganesan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04588v1",
                "updated": "2025-05-07T17:30:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    30,
                    22,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:30:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    30,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching"
                },
                "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Xuanbo Fan"
                    },
                    {
                        "name": "Yingyan Hou"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04586v1",
                "updated": "2025-05-07T17:27:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    27,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:27:51Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    27,
                    51,
                    2,
                    127,
                    0
                ],
                "title": "Active Sampling for MRI-based Sequential Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Sampling for MRI-based Sequential Decision Making"
                },
                "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling"
                },
                "authors": [
                    {
                        "name": "Yuning Du"
                    },
                    {
                        "name": "Jingshuai Liu"
                    },
                    {
                        "name": "Rohan Dharmakumar"
                    },
                    {
                        "name": "Sotirios A. Tsaftaris"
                    }
                ],
                "author_detail": {
                    "name": "Sotirios A. Tsaftaris"
                },
                "author": "Sotirios A. Tsaftaris",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20984v2",
                "updated": "2025-05-07T17:26:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    26,
                    46,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-29T17:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE: A Security Architecture for LLM-Integrated App Systems"
                },
                "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
                },
                "authors": [
                    {
                        "name": "Evan Li"
                    },
                    {
                        "name": "Tushin Mallick"
                    },
                    {
                        "name": "Evan Rose"
                    },
                    {
                        "name": "William Robertson"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Nita-Rotaru"
                },
                "author": "Cristina Nita-Rotaru",
                "arxiv_comment": "21 pages, 13 figures; clarify relation to indirect prompt injection\n  attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04584v1",
                "updated": "2025-05-07T17:24:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    24,
                    40,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:24:40Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    24,
                    40,
                    2,
                    127,
                    0
                ],
                "title": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for\n  Open-Ended Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for\n  Open-Ended Questions"
                },
                "summary": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback."
                },
                "authors": [
                    {
                        "name": "Chloe Qianhui Zhao"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    },
                    {
                        "name": "Jionghao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jionghao Lin"
                },
                "author": "Jionghao Lin",
                "arxiv_comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04575v1",
                "updated": "2025-05-07T17:12:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    12,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:12:15Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    12,
                    15,
                    2,
                    127,
                    0
                ],
                "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning"
                },
                "summary": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt"
                },
                "authors": [
                    {
                        "name": "Kunlun Xu"
                    },
                    {
                        "name": "Xu Zou"
                    },
                    {
                        "name": "Gang Hua"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiahuan Zhou"
                },
                "author": "Jiahuan Zhou",
                "arxiv_comment": "Accpted by ICML2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00846v2",
                "updated": "2025-05-07T17:06:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    6,
                    46,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-02T16:39:37Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    39,
                    37,
                    6,
                    33,
                    0
                ],
                "title": "Federated Generalised Variational Inference: A Robust Probabilistic\n  Federated Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Generalised Variational Inference: A Robust Probabilistic\n  Federated Learning Framework"
                },
                "summary": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that\nis robust to both prior and likelihood misspecification. FedGVI addresses\nlimitations in both frequentist and Bayesian FL by providing unbiased\npredictions under model misspecification, with calibrated uncertainty\nquantification. Our approach generalises previous FL approaches, specifically\nPartitioned Variational Inference (Ashman et al., 2022), by allowing robust and\nconjugate updates, decreasing computational complexity at the clients. We offer\ntheoretical analysis in terms of fixed-point convergence, optimality of the\ncavity distribution, and provable robustness to likelihood misspecification.\nFurther, we empirically demonstrate the effectiveness of FedGVI in terms of\nimproved robustness and predictive performance on multiple synthetic and real\nworld classification data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FedGVI, a probabilistic Federated Learning (FL) framework that\nis robust to both prior and likelihood misspecification. FedGVI addresses\nlimitations in both frequentist and Bayesian FL by providing unbiased\npredictions under model misspecification, with calibrated uncertainty\nquantification. Our approach generalises previous FL approaches, specifically\nPartitioned Variational Inference (Ashman et al., 2022), by allowing robust and\nconjugate updates, decreasing computational complexity at the clients. We offer\ntheoretical analysis in terms of fixed-point convergence, optimality of the\ncavity distribution, and provable robustness to likelihood misspecification.\nFurther, we empirically demonstrate the effectiveness of FedGVI in terms of\nimproved robustness and predictive performance on multiple synthetic and real\nworld classification data sets."
                },
                "authors": [
                    {
                        "name": "Terje Mildner"
                    },
                    {
                        "name": "Oliver Hamelijnck"
                    },
                    {
                        "name": "Paris Giampouras"
                    },
                    {
                        "name": "Theodoros Damoulas"
                    }
                ],
                "author_detail": {
                    "name": "Theodoros Damoulas"
                },
                "author": "Theodoros Damoulas",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04568v1",
                "updated": "2025-05-07T17:03:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    3,
                    22,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:03:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    3,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "Conformal Survival Bands for Risk Screening under Right-Censoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Survival Bands for Risk Screening under Right-Censoring"
                },
                "summary": "We propose a method to quantify uncertainty around individual survival\ndistribution estimates using right-censored data, compatible with any survival\nmodel. Unlike classical confidence intervals, the survival bands produced by\nthis method offer predictive rather than population-level inference, making\nthem useful for personalized risk screening. For example, in a low-risk\nscreening scenario, they can be applied to flag patients whose survival band at\n12 months lies entirely above 50\\%, while ensuring that at least half of\nflagged individuals will survive past that time on average. Our approach builds\non recent advances in conformal inference and integrates ideas from inverse\nprobability of censoring weighting and multiple testing with false discovery\nrate control. We provide asymptotic guarantees and show promising performance\nin finite samples with both simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method to quantify uncertainty around individual survival\ndistribution estimates using right-censored data, compatible with any survival\nmodel. Unlike classical confidence intervals, the survival bands produced by\nthis method offer predictive rather than population-level inference, making\nthem useful for personalized risk screening. For example, in a low-risk\nscreening scenario, they can be applied to flag patients whose survival band at\n12 months lies entirely above 50\\%, while ensuring that at least half of\nflagged individuals will survive past that time on average. Our approach builds\non recent advances in conformal inference and integrates ideas from inverse\nprobability of censoring weighting and multiple testing with false discovery\nrate control. We provide asymptotic guarantees and show promising performance\nin finite samples with both simulated and real data."
                },
                "authors": [
                    {
                        "name": "Matteo Sesia"
                    },
                    {
                        "name": "Vladimir Svetnik"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Svetnik"
                },
                "author": "Vladimir Svetnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04565v1",
                "updated": "2025-05-07T16:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    57,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:57:51Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    57,
                    51,
                    2,
                    127,
                    0
                ],
                "title": "Hierarchical Task Decomposition for Execution Monitoring and Error\n  Recovery: Understanding the Rationale Behind Task Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Task Decomposition for Execution Monitoring and Error\n  Recovery: Understanding the Rationale Behind Task Demonstrations"
                },
                "summary": "Multi-step manipulation tasks where robots interact with their environment\nand must apply process forces based on the perceived situation remain\nchallenging to learn and prone to execution errors. Accurately simulating these\ntasks is also difficult. Hence, it is crucial for robust task performance to\nlearn how to coordinate end-effector pose and applied force, monitor execution,\nand react to deviations. To address these challenges, we propose a learning\napproach that directly infers both low- and high-level task representations\nfrom user demonstrations on the real system. We developed an unsupervised task\nsegmentation algorithm that combines intention recognition and feature\nclustering to infer the skills of a task. We leverage the inferred\ncharacteristic features of each skill in a novel unsupervised anomaly detection\napproach to identify deviations from the intended task execution. Together,\nthese components form a comprehensive framework capable of incrementally\nlearning task decisions and new behaviors as new situations arise. Compared to\nstate-of-the-art learning techniques, our approach significantly reduces the\nrequired amount of training data and computational complexity while efficiently\nlearning complex in-contact behaviors and recovery strategies. Our proposed\ntask segmentation and anomaly detection approaches outperform state-of-the-art\nmethods on force-based tasks evaluated on two different robotic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-step manipulation tasks where robots interact with their environment\nand must apply process forces based on the perceived situation remain\nchallenging to learn and prone to execution errors. Accurately simulating these\ntasks is also difficult. Hence, it is crucial for robust task performance to\nlearn how to coordinate end-effector pose and applied force, monitor execution,\nand react to deviations. To address these challenges, we propose a learning\napproach that directly infers both low- and high-level task representations\nfrom user demonstrations on the real system. We developed an unsupervised task\nsegmentation algorithm that combines intention recognition and feature\nclustering to infer the skills of a task. We leverage the inferred\ncharacteristic features of each skill in a novel unsupervised anomaly detection\napproach to identify deviations from the intended task execution. Together,\nthese components form a comprehensive framework capable of incrementally\nlearning task decisions and new behaviors as new situations arise. Compared to\nstate-of-the-art learning techniques, our approach significantly reduces the\nrequired amount of training data and computational complexity while efficiently\nlearning complex in-contact behaviors and recovery strategies. Our proposed\ntask segmentation and anomaly detection approaches outperform state-of-the-art\nmethods on force-based tasks evaluated on two different robotic systems."
                },
                "authors": [
                    {
                        "name": "Christoph Willibald"
                    },
                    {
                        "name": "Dongheui Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongheui Lee"
                },
                "author": "Dongheui Lee",
                "arxiv_comment": "The paper has been accepted for publication by the International\n  Journal of Robotics Research (IJRR), 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06661v2",
                "updated": "2025-05-07T16:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    57,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-09T17:00:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion"
                },
                "summary": "Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality."
                },
                "authors": [
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Rongtao Deng"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Haibin Shen"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v4",
                "updated": "2025-05-07T16:51:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    51,
                    33,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04558v1",
                "updated": "2025-05-07T16:46:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    46,
                    48,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:46:48Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    46,
                    48,
                    2,
                    127,
                    0
                ],
                "title": "Purity Law for Generalizable Neural TSP Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purity Law for Generalizable Neural TSP Solvers"
                },
                "summary": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference."
                },
                "authors": [
                    {
                        "name": "Wenzhao Liu"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Congying Han"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Anqi Li"
                    },
                    {
                        "name": "Tiande Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tiande Guo"
                },
                "author": "Tiande Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04534v1",
                "updated": "2025-05-07T16:09:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    9,
                    22,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:09:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    9,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "PANCAKE: Python bAsed Numerical Color-magnitude-diagram Analysis pacKagE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANCAKE: Python bAsed Numerical Color-magnitude-diagram Analysis pacKagE"
                },
                "summary": "Stellar populations serve as a fossil record of galaxy formation and\nevolution, providing crucial information about the history of star formation\nand galaxy evolution. The color-magnitude diagram (CMD) stands out as the most\naccurate tool currently available for inferring the star formation histories\n(SFHs) of nearby galaxies with stellar-resolved multiband data. The launch of\nnew space telescopes, including JWST, EUCLID, and the upcoming CSST and Roman,\nwill significantly increase the number of stellar-resolved galaxies over the\nnext decade. A user-friendly and customizable CMD fitting package would be\nvaluable for galaxy evolution studies with these data. We develop an\nopen-source Python-based package named \\textsc{pancake}, which is fast and\naccurate in determining SFHs and stellar population parameters in nearby\ngalaxies. We have validated our method via a series of comprehensive tests.\nFirst, \\textsc{pancake} performs well on mock data, meanwhile the random and\nsystematic uncertainties are quantified. Second, \\textsc{pancake} performs well\non observational data containing a star cluster and 38 dwarf galaxies (50\nfields). Third, the star formation rate (SFR) from \\textsc{pancake} is\nconsistent with the SFR from FUV photometry. To ensure compatibility and\naccuracy, we have included isochrone libraries generated using PARSEC for most\nof the optical and near-infrared filters used in space telescopes such as HST,\nJWST, and the upcoming CSST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar populations serve as a fossil record of galaxy formation and\nevolution, providing crucial information about the history of star formation\nand galaxy evolution. The color-magnitude diagram (CMD) stands out as the most\naccurate tool currently available for inferring the star formation histories\n(SFHs) of nearby galaxies with stellar-resolved multiband data. The launch of\nnew space telescopes, including JWST, EUCLID, and the upcoming CSST and Roman,\nwill significantly increase the number of stellar-resolved galaxies over the\nnext decade. A user-friendly and customizable CMD fitting package would be\nvaluable for galaxy evolution studies with these data. We develop an\nopen-source Python-based package named \\textsc{pancake}, which is fast and\naccurate in determining SFHs and stellar population parameters in nearby\ngalaxies. We have validated our method via a series of comprehensive tests.\nFirst, \\textsc{pancake} performs well on mock data, meanwhile the random and\nsystematic uncertainties are quantified. Second, \\textsc{pancake} performs well\non observational data containing a star cluster and 38 dwarf galaxies (50\nfields). Third, the star formation rate (SFR) from \\textsc{pancake} is\nconsistent with the SFR from FUV photometry. To ensure compatibility and\naccuracy, we have included isochrone libraries generated using PARSEC for most\nof the optical and near-infrared filters used in space telescopes such as HST,\nJWST, and the upcoming CSST."
                },
                "authors": [
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yujiao Yang"
                    },
                    {
                        "name": "Yong-kun Zhang"
                    },
                    {
                        "name": "Zheng Zheng"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Lister Staveley-Smith"
                    },
                    {
                        "name": "Chao-Wei Tsai"
                    },
                    {
                        "name": "Di Li"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Jingjing Hu"
                    },
                    {
                        "name": "Huaxi Chen"
                    },
                    {
                        "name": "Donghui Quan"
                    },
                    {
                        "name": "Yinghui Zheng"
                    },
                    {
                        "name": "Hangyuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Hangyuan Li"
                },
                "author": "Hangyuan Li",
                "arxiv_comment": "26 pages, 24 figures, accepted by APJS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03161v2",
                "updated": "2025-05-07T16:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    4,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-06T04:14:13Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    13,
                    1,
                    126,
                    0
                ],
                "title": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks"
                },
                "summary": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community."
                },
                "authors": [
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Rushan Li"
                    },
                    {
                        "name": "Li Su"
                    },
                    {
                        "name": "Haitao Du"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Pengxuan Mao"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v3",
                "updated": "2025-05-07T16:03:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    3,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04521v1",
                "updated": "2025-05-07T15:52:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    52,
                    6,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:52:06Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    52,
                    6,
                    2,
                    127,
                    0
                ],
                "title": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development"
                },
                "summary": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches."
                },
                "authors": [
                    {
                        "name": "Kuen Sum Cheung"
                    },
                    {
                        "name": "Mayuri Kaul"
                    },
                    {
                        "name": "Gunel Jahangirova"
                    },
                    {
                        "name": "Mohammad Reza Mousavi"
                    },
                    {
                        "name": "Eric Zie"
                    }
                ],
                "author_detail": {
                    "name": "Eric Zie"
                },
                "author": "Eric Zie",
                "arxiv_doi": "10.1145/3711919.3728678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711919.3728678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04519v1",
                "updated": "2025-05-07T15:46:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    46,
                    36,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:46:36Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    46,
                    36,
                    2,
                    127,
                    0
                ],
                "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs"
                },
                "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."
                },
                "authors": [
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Miao Rang"
                    },
                    {
                        "name": "Fangcheng Liu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Binghan Li"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Youliang Yan"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Botian Huang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Da Kuang"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jie Ran"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liang Dai"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Peifeng Qin"
                    },
                    {
                        "name": "Pengyuan Zeng"
                    },
                    {
                        "name": "Qiang Gu"
                    },
                    {
                        "name": "Shaohua Tang"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tao Gao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianshu Li"
                    },
                    {
                        "name": "Tianyu Bi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Weikai Mao"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xiabing Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Yangkai Du"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Yimeng Wu"
                    },
                    {
                        "name": "Yongbing Huang"
                    },
                    {
                        "name": "Yong Tian"
                    },
                    {
                        "name": "Yong Zhu"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Yusen Sun"
                    },
                    {
                        "name": "Zelin Chen"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Zhipeng Tu"
                    },
                    {
                        "name": "Zilin Ding"
                    },
                    {
                        "name": "Zongyuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Zongyuan Zhan"
                },
                "author": "Zongyuan Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08177v2",
                "updated": "2025-05-07T15:39:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    39,
                    44,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-11T08:07:40Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    40,
                    2,
                    346,
                    0
                ],
                "title": "SecureNT: Smart Topology Obfuscation for Privacy-Aware Network\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecureNT: Smart Topology Obfuscation for Privacy-Aware Network\n  Monitoring"
                },
                "summary": "Network tomography plays a crucial role in network monitoring and management,\nwhere network topology serves as the fundamental basis for various tomography\ntasks including traffic matrix estimation and link performance inference. The\ntopology information, however, can be inferred through end-to-end measurements\nusing various inference algorithms, posing significant security risks to\nnetwork infrastructure. While existing protection methods attempt to secure\ntopology information by modifying end-to-end measurements, they often require\ncomplex computation and sophisticated modification strategies, making real-time\nprotection challenging. Moreover, these modifications typically render the\nmeasurements unusable for network monitoring, even by trusted users. This paper\npresents a novel privacy-preserving framework that addresses these limitations.\nOur approach provides efficient topology protection while maintaining the\nutility of measurements for authorized network monitoring. Through extensive\nevaluation on both simulated and real-world networks, we demonstrate that our\nframework achieves superior privacy protection compared to existing methods\nwhile enabling trusted users to effectively monitor network performance. Our\nsolution offers a practical approach for organizations to protect sensitive\ntopology information without sacrificing their network monitoring capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network tomography plays a crucial role in network monitoring and management,\nwhere network topology serves as the fundamental basis for various tomography\ntasks including traffic matrix estimation and link performance inference. The\ntopology information, however, can be inferred through end-to-end measurements\nusing various inference algorithms, posing significant security risks to\nnetwork infrastructure. While existing protection methods attempt to secure\ntopology information by modifying end-to-end measurements, they often require\ncomplex computation and sophisticated modification strategies, making real-time\nprotection challenging. Moreover, these modifications typically render the\nmeasurements unusable for network monitoring, even by trusted users. This paper\npresents a novel privacy-preserving framework that addresses these limitations.\nOur approach provides efficient topology protection while maintaining the\nutility of measurements for authorized network monitoring. Through extensive\nevaluation on both simulated and real-world networks, we demonstrate that our\nframework achieves superior privacy protection compared to existing methods\nwhile enabling trusted users to effectively monitor network performance. Our\nsolution offers a practical approach for organizations to protect sensitive\ntopology information without sacrificing their network monitoring capabilities."
                },
                "authors": [
                    {
                        "name": "Chengze Du"
                    },
                    {
                        "name": "Jibin Shi"
                    },
                    {
                        "name": "Hui Xu"
                    },
                    {
                        "name": "Guangzhen Yao"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhen Yao"
                },
                "author": "Guangzhen Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11134v2",
                "updated": "2025-05-07T15:34:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    34,
                    5,
                    2,
                    127,
                    0
                ],
                "published": "2024-02-16T23:47:47Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    23,
                    47,
                    47,
                    4,
                    47,
                    0
                ],
                "title": "Functional Partial Least-Squares: Adaptive Estimation and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Partial Least-Squares: Adaptive Estimation and Inference"
                },
                "summary": "We study the functional linear regression model with a scalar response and a\nHilbert space-valued predictor, a canonical example of an ill-posed inverse\nproblem. We show that the functional partial least squares (PLS) estimator\nattains nearly minimax-optimal convergence rates over a class of ellipsoids and\npropose an adaptive early stopping procedure for selecting the number of PLS\ncomponents. In addition, we develop new test that can detect local alternatives\nconverging at the parametric rate which can be inverted to construct confidence\nsets. Simulation results demonstrate that the estimator performs favorably\nrelative to several existing methods and the proposed test exhibits good power\nproperties. We apply our methodology to evaluate the nonlinear effects of\ntemperature on corn and soybean yields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the functional linear regression model with a scalar response and a\nHilbert space-valued predictor, a canonical example of an ill-posed inverse\nproblem. We show that the functional partial least squares (PLS) estimator\nattains nearly minimax-optimal convergence rates over a class of ellipsoids and\npropose an adaptive early stopping procedure for selecting the number of PLS\ncomponents. In addition, we develop new test that can detect local alternatives\nconverging at the parametric rate which can be inverted to construct confidence\nsets. Simulation results demonstrate that the estimator performs favorably\nrelative to several existing methods and the proposed test exhibits good power\nproperties. We apply our methodology to evaluate the nonlinear effects of\ntemperature on corn and soybean yields."
                },
                "authors": [
                    {
                        "name": "Andrii Babii"
                    },
                    {
                        "name": "Marine Carrasco"
                    },
                    {
                        "name": "Idriss Tsafack"
                    }
                ],
                "author_detail": {
                    "name": "Idriss Tsafack"
                },
                "author": "Idriss Tsafack",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05, 62G08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06397v2",
                "updated": "2025-05-07T15:32:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "published": "2024-10-08T22:03:41Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    22,
                    3,
                    41,
                    1,
                    282,
                    0
                ],
                "title": "Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling"
                },
                "summary": "Analog dynamical accelerators (DXs) are a growing sub-field in computer\narchitecture research, offering order-of-magnitude gains in power efficiency\nand latency over traditional digital methods in several machine learning,\noptimization, and sampling tasks. However, limited-capacity accelerators\nrequire hybrid analog/digital algorithms to solve real-world problems, commonly\nusing large-neighborhood local search (LNLS) frameworks. Unlike fully digital\nalgorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no\nprincipled hyperparameter selection schemes, particularly limiting cross-device\ntraining and inference.\n  In this work, we provide non-asymptotic convergence guarantees for hybrid\nLNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools\nfrom classical sampling theory, we prove exponential KL-divergence convergence\nfor randomized and cyclic block selection strategies using ideal DXs. With\nfinite device variation, we provide explicit bounds on the 2-Wasserstein bias\nin terms of step duration, noise strength, and function parameters. Our BLD\nmodel provides a key link between established theory and novel computing\nplatforms, and our theoretical results provide a closed-form expression linking\ndevice variation, algorithm hyperparameters, and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog dynamical accelerators (DXs) are a growing sub-field in computer\narchitecture research, offering order-of-magnitude gains in power efficiency\nand latency over traditional digital methods in several machine learning,\noptimization, and sampling tasks. However, limited-capacity accelerators\nrequire hybrid analog/digital algorithms to solve real-world problems, commonly\nusing large-neighborhood local search (LNLS) frameworks. Unlike fully digital\nalgorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no\nprincipled hyperparameter selection schemes, particularly limiting cross-device\ntraining and inference.\n  In this work, we provide non-asymptotic convergence guarantees for hybrid\nLNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools\nfrom classical sampling theory, we prove exponential KL-divergence convergence\nfor randomized and cyclic block selection strategies using ideal DXs. With\nfinite device variation, we provide explicit bounds on the 2-Wasserstein bias\nin terms of step duration, noise strength, and function parameters. Our BLD\nmodel provides a key link between established theory and novel computing\nplatforms, and our theoretical results provide a closed-form expression linking\ndevice variation, algorithm hyperparameters, and performance."
                },
                "authors": [
                    {
                        "name": "Matthew X. Burns"
                    },
                    {
                        "name": "Qingyuan Hou"
                    },
                    {
                        "name": "Michael C. Huang"
                    }
                ],
                "author_detail": {
                    "name": "Michael C. Huang"
                },
                "author": "Michael C. Huang",
                "arxiv_comment": "33 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60J60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21813v2",
                "updated": "2025-05-07T15:02:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    2,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-25T18:20:04Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    20,
                    4,
                    1,
                    84,
                    0
                ],
                "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangcheng Qiang"
                },
                "author": "Zhangcheng Qiang",
                "arxiv_comment": "15 pages, 4 figures, 5 tables, 2 prompt templates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04487v1",
                "updated": "2025-05-07T15:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    1,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:01:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    1,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "A Design Space for the Critical Validation of LLM-Generated Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Design Space for the Critical Validation of LLM-Generated Tabular Data"
                },
                "summary": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power."
                },
                "authors": [
                    {
                        "name": "Madhav Sachdeva"
                    },
                    {
                        "name": "Christopher Narayanan"
                    },
                    {
                        "name": "Marvin Wiedenkeller"
                    },
                    {
                        "name": "Jana Sedlakova"
                    },
                    {
                        "name": "Jürgen Bernard"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Bernard"
                },
                "author": "Jürgen Bernard",
                "arxiv_comment": "To appear at the 16th International EuroVis Workshop on Visual\n  Analytics (EuroVA'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04486v1",
                "updated": "2025-05-07T14:59:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    59,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:59:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    59,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "Efficient Flow Matching using Latent Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Flow Matching using Latent Variables"
                },
                "summary": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features."
                },
                "authors": [
                    {
                        "name": "Anirban Samaddar"
                    },
                    {
                        "name": "Yixuan Sun"
                    },
                    {
                        "name": "Viktor Nilsson"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Madireddy"
                },
                "author": "Sandeep Madireddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04481v1",
                "updated": "2025-05-07T14:52:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    52,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:52:02Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    52,
                    2,
                    2,
                    127,
                    0
                ],
                "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines."
                },
                "authors": [
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Weijian Ma"
                    },
                    {
                        "name": "Xueyang Li"
                    },
                    {
                        "name": "Yunzhong Lou"
                    },
                    {
                        "name": "Guichun Zhou"
                    },
                    {
                        "name": "Xiangdong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiangdong Zhou"
                },
                "author": "Xiangdong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04480v1",
                "updated": "2025-05-07T14:51:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    51,
                    43,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:51:43Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    51,
                    43,
                    2,
                    127,
                    0
                ],
                "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution"
                },
                "summary": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo."
                },
                "authors": [
                    {
                        "name": "Zhikai Zhao"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04475v1",
                "updated": "2025-05-07T14:46:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    46,
                    22,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:46:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    46,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda\n  XXIII"
                },
                "summary": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. In order to better\nunderstand the nature of dark matter, a more comprehensive study of Local Group\ndwarfs is necessary. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r < r$_{\\rm{h}}$)\n= (4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a\nmass-to-light ratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2)\nM$_{\\odot}$/L$_{\\odot}$. We infer an enclosed mass for And XXIII of M(r <\nr$_{\\rm{h}}$) = (3.1 $\\pm$ 1.9) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to\na mass-to-light ratio of $[M/L]_{\\rm{r_{h}}}$ = (90.2 $\\pm$ 53.9)\nM$_{\\odot}$/L$_{\\odot}$. Using the dynamical Jeans modeling tool, GravSphere,\nwe determine And VI and And XXIII's dark matter density at 150 pc, finding\n$\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$ 0.5) $\\times$ 10$^{8}$ M$_{\\odot}$\nkpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc) = 0.5$\\substack{+0.4 \\\\ -0.3}\n\\times$ 10$^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our results make And VI the first\nmass-modeled M31 satellite to fall into the cuspy regime, while And XXIII has a\nlower density, implying either a more cored central dark matter density, or a\nlowering of the density through tides. This adds And XXIII to a growing list of\nM31 dwarfs with a central density lower than most MW dwarfs and lower than\nexpected for isolated dwarfs in the Standard Cosmology. This could be explained\nby the M31 dwarfs having experienced stronger tides than their Milky Way\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph)\ngalaxies allows us to test predictions made by dark matter (DM) models. To\ndate, such analyses have primarily been performed on Milky Way (MW) satellites.\nMeanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet\nonly two have been successfully mass-modeled so far. In order to better\nunderstand the nature of dark matter, a more comprehensive study of Local Group\ndwarfs is necessary. In this study, we have undertaken a dynamical study of two\nhigher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda\nXXIII (And XXIII). We infer an enclosed mass for And VI of M(r < r$_{\\rm{h}}$)\n= (4.9 $\\pm$ 1.5) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to a\nmass-to-light ratio of $[M/L]_{r_{\\rm{h}}}$ = (27.1 $\\pm$ 8.2)\nM$_{\\odot}$/L$_{\\odot}$. We infer an enclosed mass for And XXIII of M(r <\nr$_{\\rm{h}}$) = (3.1 $\\pm$ 1.9) $\\times$ 10$^{7}$ M$_{\\odot}$, corresponding to\na mass-to-light ratio of $[M/L]_{\\rm{r_{h}}}$ = (90.2 $\\pm$ 53.9)\nM$_{\\odot}$/L$_{\\odot}$. Using the dynamical Jeans modeling tool, GravSphere,\nwe determine And VI and And XXIII's dark matter density at 150 pc, finding\n$\\rho_{\\rm{DM,VI}}$(150 pc) = (1.4 $\\pm$ 0.5) $\\times$ 10$^{8}$ M$_{\\odot}$\nkpc$^{-3}$ and $\\rho_{\\rm{DM,XXIII}}$(150 pc) = 0.5$\\substack{+0.4 \\\\ -0.3}\n\\times$ 10$^{8}$ M$_{\\odot}$ kpc$^{-3}$. Our results make And VI the first\nmass-modeled M31 satellite to fall into the cuspy regime, while And XXIII has a\nlower density, implying either a more cored central dark matter density, or a\nlowering of the density through tides. This adds And XXIII to a growing list of\nM31 dwarfs with a central density lower than most MW dwarfs and lower than\nexpected for isolated dwarfs in the Standard Cosmology. This could be explained\nby the M31 dwarfs having experienced stronger tides than their Milky Way\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Connor S. Pickett"
                    },
                    {
                        "name": "Michelle L. M. Collins"
                    },
                    {
                        "name": "R. Michael Rich"
                    },
                    {
                        "name": "Justin I. Read"
                    },
                    {
                        "name": "Emily J. E. Charles"
                    },
                    {
                        "name": "Nicolas Martin"
                    },
                    {
                        "name": "Scott Chapman"
                    },
                    {
                        "name": "Alan McConnachie"
                    },
                    {
                        "name": "Alessandro Savino"
                    },
                    {
                        "name": "Daniel R. Weisz"
                    }
                ],
                "author_detail": {
                    "name": "Daniel R. Weisz"
                },
                "author": "Daniel R. Weisz",
                "arxiv_comment": "19 pages, 16 figures. Submitted to MNRAS for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11713v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11713v3",
                "updated": "2025-05-07T14:40:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    40,
                    4,
                    2,
                    127,
                    0
                ],
                "published": "2024-10-15T15:47:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing"
                },
                "summary": "External controls from historical trials or observational data can augment\nrandomized controlled trials when large-scale randomization is impractical or\nunethical, such as in drug evaluation for rare diseases. However,\nnon-randomized external controls can introduce biases, and existing Bayesian\nand frequentist methods may inflate the type I error rate, particularly in\nsmall-sample trials where external data borrowing is most critical. To address\nthese challenges, we propose a randomization inference framework that ensures\nfinite-sample exact and model-free type I error rate control, adhering to the\n\"analyze as you randomize\" principle to safeguard against hidden biases.\nRecognizing that biased external controls reduce the power of randomization\ntests, we leverage conformal inference to develop an individualized\ntest-then-pool procedure that selectively borrows comparable external controls\nto improve power. Our approach incorporates selection uncertainty into\nrandomization tests, providing valid post-selection inference. Additionally, we\npropose an adaptive procedure to optimize the selection threshold by minimizing\nthe mean squared error across a class of estimators encompassing both\nno-borrowing and full-borrowing approaches. The proposed methods are supported\nby non-asymptotic theoretical analysis, validated through simulations, and\napplied to a randomized lung cancer trial that integrates external controls\nfrom the National Cancer Database.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External controls from historical trials or observational data can augment\nrandomized controlled trials when large-scale randomization is impractical or\nunethical, such as in drug evaluation for rare diseases. However,\nnon-randomized external controls can introduce biases, and existing Bayesian\nand frequentist methods may inflate the type I error rate, particularly in\nsmall-sample trials where external data borrowing is most critical. To address\nthese challenges, we propose a randomization inference framework that ensures\nfinite-sample exact and model-free type I error rate control, adhering to the\n\"analyze as you randomize\" principle to safeguard against hidden biases.\nRecognizing that biased external controls reduce the power of randomization\ntests, we leverage conformal inference to develop an individualized\ntest-then-pool procedure that selectively borrows comparable external controls\nto improve power. Our approach incorporates selection uncertainty into\nrandomization tests, providing valid post-selection inference. Additionally, we\npropose an adaptive procedure to optimize the selection threshold by minimizing\nthe mean squared error across a class of estimators encompassing both\nno-borrowing and full-borrowing approaches. The proposed methods are supported\nby non-asymptotic theoretical analysis, validated through simulations, and\napplied to a randomized lung cancer trial that integrates external controls\nfrom the National Cancer Database."
                },
                "authors": [
                    {
                        "name": "Ke Zhu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xiaofei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofei Wang"
                },
                "author": "Xiaofei Wang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11713v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11713v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07971v2",
                "updated": "2025-05-07T14:26:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    26,
                    9,
                    2,
                    127,
                    0
                ],
                "published": "2023-06-13T17:59:59Z",
                "published_parsed": [
                    2023,
                    6,
                    13,
                    17,
                    59,
                    59,
                    1,
                    164,
                    0
                ],
                "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models"
                },
                "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT."
                },
                "authors": [
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Jorma Laaksonen"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Accepted at ACL 2024-BIONLP Workshop. Code:\n  https://github.com/mbzuai-oryx/XrayGPT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04453v1",
                "updated": "2025-05-07T14:25:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    25,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:25:47Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    25,
                    47,
                    2,
                    127,
                    0
                ],
                "title": "Meta-Learning Driven Lightweight Phase Shift Compression for\n  IRS-Assisted Wireless Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Learning Driven Lightweight Phase Shift Compression for\n  IRS-Assisted Wireless Systems"
                },
                "summary": "The phase shift information (PSI) overhead poses a critical challenge to\nenabling real-time intelligent reflecting surface (IRS)-assisted wireless\nsystems, particularly under dynamic and resource-constrained conditions. In\nthis paper, we propose a lightweight PSI compression framework, termed\nmeta-learning-driven compression and reconstruction network (MCRNet). By\nleveraging a few-shot adaptation strategy via model-agnostic meta-learning\n(MAML), MCRNet enables rapid generalization across diverse IRS configurations\nwith minimal retraining overhead. Furthermore, a novel depthwise convolutional\ngating (DWCG) module is incorporated into the decoder to achieve adaptive local\nfeature modulation with low computational cost, significantly improving\ndecoding efficiency. Extensive simulations demonstrate that MCRNet achieves\ncompetitive normalized mean square error performance compared to\nstate-of-the-art baselines across various compression ratios, while\nsubstantially reducing model size and inference latency. These results validate\nthe effectiveness of the proposed asymmetric architecture and highlight the\npractical scalability and real-time applicability of MCRNet for dynamic\nIRS-assisted wireless deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The phase shift information (PSI) overhead poses a critical challenge to\nenabling real-time intelligent reflecting surface (IRS)-assisted wireless\nsystems, particularly under dynamic and resource-constrained conditions. In\nthis paper, we propose a lightweight PSI compression framework, termed\nmeta-learning-driven compression and reconstruction network (MCRNet). By\nleveraging a few-shot adaptation strategy via model-agnostic meta-learning\n(MAML), MCRNet enables rapid generalization across diverse IRS configurations\nwith minimal retraining overhead. Furthermore, a novel depthwise convolutional\ngating (DWCG) module is incorporated into the decoder to achieve adaptive local\nfeature modulation with low computational cost, significantly improving\ndecoding efficiency. Extensive simulations demonstrate that MCRNet achieves\ncompetitive normalized mean square error performance compared to\nstate-of-the-art baselines across various compression ratios, while\nsubstantially reducing model size and inference latency. These results validate\nthe effectiveness of the proposed asymmetric architecture and highlight the\npractical scalability and real-time applicability of MCRNet for dynamic\nIRS-assisted wireless deployments."
                },
                "authors": [
                    {
                        "name": "Xianhua Yu"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Bowen Gu"
                    },
                    {
                        "name": "Xiaoye Jing"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Tuo Wu"
                    },
                    {
                        "name": "Kan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kan Yu"
                },
                "author": "Kan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04445v1",
                "updated": "2025-05-07T14:14:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    14,
                    29,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:14:29Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    14,
                    29,
                    2,
                    127,
                    0
                ],
                "title": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation"
                },
                "summary": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec."
                },
                "authors": [
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Honggang Wen"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Siu-Ming Yiu"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04441v1",
                "updated": "2025-05-07T14:12:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    12,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:12:41Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    12,
                    41,
                    2,
                    127,
                    0
                ],
                "title": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs"
                },
                "summary": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mirazul Haque"
                    },
                    {
                        "name": "Petr Babkin"
                    },
                    {
                        "name": "Farima Farmahinifarahani"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12093v3",
                "updated": "2025-05-07T14:05:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    5,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-21T12:38:04Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    38,
                    4,
                    1,
                    21,
                    0
                ],
                "title": "Checkification: A Practical Approach for Testing Static Analysis Truths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checkification: A Practical Approach for Testing Static Analysis Truths"
                },
                "summary": "Static analysis is an essential component of many modern software development\ntools. Unfortunately, the ever-increasing complexity of static analyzers makes\ntheir coding error-prone. Even analysis tools based on rigorous mathematical\ntechniques, such as abstract interpretation, are not immune to bugs. Ensuring\nthe correctness and reliability of software analyzers is critical if they are\nto be inserted in production compilers and development environments. While\ncompiler validation has seen notable success, formal validation of static\nanalysis tools remains relatively unexplored. In this paper, we propose a\nmethod for testing abstract interpretation-based static analyzers. Broadly, it\nconsists in checking, over a suite of benchmarks, that the properties inferred\nstatically are satisfied dynamically. The main advantage of our approach lies\nin its simplicity, which stems directly from framing it within the Ciao\nassertion-based validation framework, and its blended static/dynamic assertion\nchecking approach. We demonstrate that in this setting, the analysis can be\ntested with little effort by combining the following components already present\nin the framework: 1) the static analyzer, which outputs its results as the\noriginal program source with assertions interspersed; 2) the assertion run-time\nchecking mechanism, which instruments a program to ensure that no assertion is\nviolated at run time; 3) the random test case generator, which generates random\ntest cases satisfying the properties present in assertion preconditions; and 4)\nthe unit-test framework, which executes those test cases. We have applied our\napproach to the CiaoPP static analyzer, resulting in the identification of many\nbugs with reasonable overhead. Most of these bugs have been either fixed or\nconfirmed, helping us detect a range of errors not only related to analysis\nsoundness but also within other aspects of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis is an essential component of many modern software development\ntools. Unfortunately, the ever-increasing complexity of static analyzers makes\ntheir coding error-prone. Even analysis tools based on rigorous mathematical\ntechniques, such as abstract interpretation, are not immune to bugs. Ensuring\nthe correctness and reliability of software analyzers is critical if they are\nto be inserted in production compilers and development environments. While\ncompiler validation has seen notable success, formal validation of static\nanalysis tools remains relatively unexplored. In this paper, we propose a\nmethod for testing abstract interpretation-based static analyzers. Broadly, it\nconsists in checking, over a suite of benchmarks, that the properties inferred\nstatically are satisfied dynamically. The main advantage of our approach lies\nin its simplicity, which stems directly from framing it within the Ciao\nassertion-based validation framework, and its blended static/dynamic assertion\nchecking approach. We demonstrate that in this setting, the analysis can be\ntested with little effort by combining the following components already present\nin the framework: 1) the static analyzer, which outputs its results as the\noriginal program source with assertions interspersed; 2) the assertion run-time\nchecking mechanism, which instruments a program to ensure that no assertion is\nviolated at run time; 3) the random test case generator, which generates random\ntest cases satisfying the properties present in assertion preconditions; and 4)\nthe unit-test framework, which executes those test cases. We have applied our\napproach to the CiaoPP static analyzer, resulting in the identification of many\nbugs with reasonable overhead. Most of these bugs have been either fixed or\nconfirmed, helping us detect a range of errors not only related to analysis\nsoundness but also within other aspects of the framework."
                },
                "authors": [
                    {
                        "name": "Daniela Ferreiro"
                    },
                    {
                        "name": "Ignacio Casso"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP). Extended, revised version of our work published in LOPSTR 2020",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04551v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04551v4",
                "updated": "2025-05-07T14:04:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    4,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2024-02-07T03:16:16Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    3,
                    16,
                    16,
                    2,
                    38,
                    0
                ],
                "title": "Gamma-ray Bursts as Distance Indicators by a Statistical Learning\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray Bursts as Distance Indicators by a Statistical Learning\n  Approach"
                },
                "summary": "Gamma-ray bursts (GRBs) can be probes of the early universe, but currently,\nonly 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known\nredshifts ($z$) due to observational limitations. To address this, we estimated\nthe GRB redshift (distance) via a supervised statistical learning model that\nuses optical afterglow observed by Swift and ground-based telescopes. The\ninferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with\nthe observed redshifts, thus proving the reliability of this method. The\ninferred and observed redshifts allow us to estimate the number of GRBs\noccurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for\n$1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared\nthis rate with the star formation rate highlighting a discrepancy of a factor\nof 3 at $z<1$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts (GRBs) can be probes of the early universe, but currently,\nonly 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known\nredshifts ($z$) due to observational limitations. To address this, we estimated\nthe GRB redshift (distance) via a supervised statistical learning model that\nuses optical afterglow observed by Swift and ground-based telescopes. The\ninferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with\nthe observed redshifts, thus proving the reliability of this method. The\ninferred and observed redshifts allow us to estimate the number of GRBs\noccurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for\n$1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared\nthis rate with the star formation rate highlighting a discrepancy of a factor\nof 3 at $z<1$."
                },
                "authors": [
                    {
                        "name": "Maria Giovanna Dainotti"
                    },
                    {
                        "name": "Aditya Narendra"
                    },
                    {
                        "name": "Agnieszka Pollo"
                    },
                    {
                        "name": "Vahe Petrosian"
                    },
                    {
                        "name": "Malgorzata Bogdan"
                    },
                    {
                        "name": "Kazunari Iwasaki"
                    },
                    {
                        "name": "Jason Xavier Prochaska"
                    },
                    {
                        "name": "Enrico Rinaldi"
                    },
                    {
                        "name": "David Zhou"
                    }
                ],
                "author_detail": {
                    "name": "David Zhou"
                },
                "author": "David Zhou",
                "arxiv_doi": "10.3847/2041-8213/ad4970",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad4970",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.04551v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04551v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 figures. Published in The Astrophysical Journal Letters. arXiv\n  admin note: text overlap with arXiv:1907.05074",
                "arxiv_journal_ref": "ApJL, 967(2), p.L30 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.10643v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.10643v3",
                "updated": "2025-05-07T13:52:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    52,
                    53,
                    2,
                    127,
                    0
                ],
                "published": "2023-01-25T15:26:18Z",
                "published_parsed": [
                    2023,
                    1,
                    25,
                    15,
                    26,
                    18,
                    2,
                    25,
                    0
                ],
                "title": "Automatic Debiased Estimation with Machine Learning-Generated Regressors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Debiased Estimation with Machine Learning-Generated Regressors"
                },
                "summary": "Many parameters of interest in economics and other social sciences depend on\ngenerated regressors. Examples in economics include structural parameters in\nmodels with endogenous variables estimated by control functions and in models\nwith sample selection, treatment effect estimation with propensity score\nmatching, and marginal treatment effects. More recently, Machine Learning (ML)\ngenerated regressors are becoming ubiquitous for these and other applications\nsuch as imputation with missing regressors, dimension reduction, including\nautoencoders, learned proxies, confounders and treatments, and for feature\nengineering with unstructured data, among others. We provide the first general\nmethod for valid inference with regressors generated from ML. Inference with\ngenerated regressors is complicated by the very complex expression for\ninfluence functions and asymptotic variances. Additionally, ML-generated\nregressors may lead to large biases in downstream inferences. To address these\nproblems, we propose Automatic Locally Robust/debiased GMM estimators in a\ngeneral three-step setting with ML-generated regressors. We illustrate our\nresults with treatment effects and counterfactual parameters in the partially\nlinear and nonparametric models with ML-generated regressors. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite-sample performance through Monte Carlo\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many parameters of interest in economics and other social sciences depend on\ngenerated regressors. Examples in economics include structural parameters in\nmodels with endogenous variables estimated by control functions and in models\nwith sample selection, treatment effect estimation with propensity score\nmatching, and marginal treatment effects. More recently, Machine Learning (ML)\ngenerated regressors are becoming ubiquitous for these and other applications\nsuch as imputation with missing regressors, dimension reduction, including\nautoencoders, learned proxies, confounders and treatments, and for feature\nengineering with unstructured data, among others. We provide the first general\nmethod for valid inference with regressors generated from ML. Inference with\ngenerated regressors is complicated by the very complex expression for\ninfluence functions and asymptotic variances. Additionally, ML-generated\nregressors may lead to large biases in downstream inferences. To address these\nproblems, we propose Automatic Locally Robust/debiased GMM estimators in a\ngeneral three-step setting with ML-generated regressors. We illustrate our\nresults with treatment effects and counterfactual parameters in the partially\nlinear and nonparametric models with ML-generated regressors. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite-sample performance through Monte Carlo\nsimulations."
                },
                "authors": [
                    {
                        "name": "Juan Carlos Escanciano"
                    },
                    {
                        "name": "Telmo Pérez-Izquierdo"
                    }
                ],
                "author_detail": {
                    "name": "Telmo Pérez-Izquierdo"
                },
                "author": "Telmo Pérez-Izquierdo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.10643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.10643v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04416v1",
                "updated": "2025-05-07T13:51:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    51,
                    42,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:51:42Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    51,
                    42,
                    2,
                    127,
                    0
                ],
                "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Minxin Du"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "18 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04406v1",
                "updated": "2025-05-07T13:42:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    42,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:42:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    42,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YABLoCo: Yet Another Benchmark for Long Context Code Generation"
                },
                "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."
                },
                "authors": [
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Roman Garaev"
                    },
                    {
                        "name": "Vadim Lomshakov"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    },
                    {
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "name": "Israel Adewuyi"
                    }
                ],
                "author_detail": {
                    "name": "Israel Adewuyi"
                },
                "arxiv_affiliation": "Research Center of the Artificial Intelligence Institute, Innopolis University, Russia",
                "author": "Israel Adewuyi",
                "arxiv_comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19251v3",
                "updated": "2025-05-08T11:09:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    9,
                    33,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-27T14:21:48Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    21,
                    48,
                    6,
                    117,
                    0
                ],
                "title": "Probing the Bounce Energy Scale in Bouncing Cosmologies with Pulsar\n  Timing Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Bounce Energy Scale in Bouncing Cosmologies with Pulsar\n  Timing Arrays"
                },
                "summary": "In this work we constrain the bounce energy scale $\\rho_{s\\downarrow}^{1/4}$\nin a generic framework of bouncing cosmologies using the nanohertz stochastic\ngravitational-wave background recently detected by pulsar timing arrays\n(NANOGrav 15-yr, EPTA DR2, PPTA DR3, IPTA DR2). A full Bayesian fit of the\nanalytic SGWB spectrum for this bounce scenario reveals, for the first time,\ntwo distinct posterior branches in $(\\rho_{s\\downarrow}^{1/4},w_1)$: one near\n$w_1\\approx0.3$ and one at $w_1\\gg1$, where $w_1$ is the contraction phase\nequation of state. We find that the bouncing model attains larger Bayes factors\nagainst each of six conventional SGWB sources (SMBHBs, inflationary GWs, cosmic\nstrings, domain walls, first order phase transitions, scalar induced GWs),\ndemonstrating strong preference of current PTA data for the bounce hypothesis.\nCompared to the more generic dual inflation bounce scenario, the concrete\nbounce realization yields smaller Bayes factors, indicating that PTA\nmeasurements impose tighter constraints when the bounce scale is explicit.\nMoreover, the two posterior branches illuminate distinct theoretical frontiers.\nThe right branch ($w_1\\gg1$) violates the dominant energy condition (DEC),\nthereby providing direct empirical impetus for models with novel early Universe\nphysics, e.g. ghost condensates, higher-derivative or modified gravity\noperators, and extra dimensional effects. Independently, both branches infer\n$\\rho_{s\\downarrow}^{1/4}$ above the Planck scale $M_\\mathrm{pl}$,\ndemonstrating that current PTAs already probe trans-Planckian regimes.\nTogether, these findings offer a rare observational window into UV completions\nof cosmology. We further describe how normalizing flow based machine learning\ncan accelerate such Bayesian analyses as PTA data volumes increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we constrain the bounce energy scale $\\rho_{s\\downarrow}^{1/4}$\nin a generic framework of bouncing cosmologies using the nanohertz stochastic\ngravitational-wave background recently detected by pulsar timing arrays\n(NANOGrav 15-yr, EPTA DR2, PPTA DR3, IPTA DR2). A full Bayesian fit of the\nanalytic SGWB spectrum for this bounce scenario reveals, for the first time,\ntwo distinct posterior branches in $(\\rho_{s\\downarrow}^{1/4},w_1)$: one near\n$w_1\\approx0.3$ and one at $w_1\\gg1$, where $w_1$ is the contraction phase\nequation of state. We find that the bouncing model attains larger Bayes factors\nagainst each of six conventional SGWB sources (SMBHBs, inflationary GWs, cosmic\nstrings, domain walls, first order phase transitions, scalar induced GWs),\ndemonstrating strong preference of current PTA data for the bounce hypothesis.\nCompared to the more generic dual inflation bounce scenario, the concrete\nbounce realization yields smaller Bayes factors, indicating that PTA\nmeasurements impose tighter constraints when the bounce scale is explicit.\nMoreover, the two posterior branches illuminate distinct theoretical frontiers.\nThe right branch ($w_1\\gg1$) violates the dominant energy condition (DEC),\nthereby providing direct empirical impetus for models with novel early Universe\nphysics, e.g. ghost condensates, higher-derivative or modified gravity\noperators, and extra dimensional effects. Independently, both branches infer\n$\\rho_{s\\downarrow}^{1/4}$ above the Planck scale $M_\\mathrm{pl}$,\ndemonstrating that current PTAs already probe trans-Planckian regimes.\nTogether, these findings offer a rare observational window into UV completions\nof cosmology. We further describe how normalizing flow based machine learning\ncan accelerate such Bayesian analyses as PTA data volumes increase."
                },
                "authors": [
                    {
                        "name": "Junrong Lai"
                    },
                    {
                        "name": "Changhong Li"
                    }
                ],
                "author_detail": {
                    "name": "Changhong Li"
                },
                "author": "Changhong Li",
                "arxiv_comment": "30 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04394v1",
                "updated": "2025-05-07T13:18:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    43,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:18:43Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    43,
                    2,
                    127,
                    0
                ],
                "title": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin\n  Transformer"
                },
                "summary": "This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model."
                },
                "authors": [
                    {
                        "name": "Young-Hu Park"
                    },
                    {
                        "name": "Rae-Hong Park"
                    },
                    {
                        "name": "Hyung-Min Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyung-Min Park"
                },
                "author": "Hyung-Min Park",
                "arxiv_doi": "10.1016/j.neucom.2025.130289",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2025.130289",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Neurocomputing, Volume 639, 28 July 2025, 130289",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04393v1",
                "updated": "2025-05-07T13:18:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:18:41Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    41,
                    2,
                    127,
                    0
                ],
                "title": "Large Means Left: Political Bias in Large Language Models Increases with\n  Their Number of Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Means Left: Political Bias in Large Language Models Increases with\n  Their Number of Parameters"
                },
                "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."
                },
                "authors": [
                    {
                        "name": "David Exler"
                    },
                    {
                        "name": "Mark Schutera"
                    },
                    {
                        "name": "Markus Reischl"
                    },
                    {
                        "name": "Luca Rettenberger"
                    }
                ],
                "author_detail": {
                    "name": "Luca Rettenberger"
                },
                "author": "Luca Rettenberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00290v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00290v4",
                "updated": "2025-05-07T13:13:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-01T03:18:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    18,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "Estimating LLM Uncertainty with Logits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Uncertainty with Logits"
                },
                "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Fixed some data errors in Table 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00290v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00290v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04388v1",
                "updated": "2025-05-07T13:13:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:13:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs"
                },
                "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Enrique Lopez-Cuena"
                    },
                    {
                        "name": "Adrian Tormos"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Pablo Bernabeu-Perez"
                    },
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Pablo Agustin Martin-Torres"
                    },
                    {
                        "name": "Marta Gonzalez-Mallo"
                    },
                    {
                        "name": "Sergio Alvarez-Napagao"
                    },
                    {
                        "name": "Eduard Ayguadé-Parra"
                    },
                    {
                        "name": "Ulises Cortés"
                    }
                ],
                "author_detail": {
                    "name": "Ulises Cortés"
                },
                "author": "Ulises Cortés",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09353v2",
                "updated": "2025-05-07T12:46:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    46,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-12T22:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    22,
                    5,
                    50,
                    5,
                    102,
                    0
                ],
                "title": "Breaking the Lens of the Telescope: Online Relevance Estimation over\n  Large Retrieval Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Lens of the Telescope: Online Relevance Estimation over\n  Large Retrieval Sets"
                },
                "summary": "Advanced relevance models, such as those that use large language models\n(LLMs), provide highly accurate relevance estimations. However, their\ncomputational costs make them infeasible for processing large document corpora.\nTo address this, retrieval systems often employ a telescoping approach, where\ncomputationally efficient but less precise lexical and semantic retrievers\nfilter potential candidates for further ranking. However, this approach heavily\ndepends on the quality of early-stage retrieval, which can potentially exclude\nrelevant documents early in the process. In this work, we propose a novel\nparadigm for re-ranking called online relevance estimation that continuously\nupdates relevance estimates for a query throughout the ranking process. Instead\nof re-ranking a fixed set of top-k documents in a single step, online relevance\nestimation iteratively re-scores smaller subsets of the most promising\ndocuments while adjusting relevance scores for the remaining pool based on the\nestimations from the final model using an online bandit-based algorithm. This\ndynamic process mitigates the recall limitations of telescoping systems by\nre-prioritizing documents initially deemed less relevant by earlier stages --\nincluding those completely excluded by earlier-stage retrievers. We validate\nour approach on TREC benchmarks under two scenarios: hybrid retrieval and\nadaptive retrieval. Experimental results demonstrate that our method is\nsample-efficient and significantly improves recall, highlighting the\neffectiveness of our online relevance estimation framework for modern search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced relevance models, such as those that use large language models\n(LLMs), provide highly accurate relevance estimations. However, their\ncomputational costs make them infeasible for processing large document corpora.\nTo address this, retrieval systems often employ a telescoping approach, where\ncomputationally efficient but less precise lexical and semantic retrievers\nfilter potential candidates for further ranking. However, this approach heavily\ndepends on the quality of early-stage retrieval, which can potentially exclude\nrelevant documents early in the process. In this work, we propose a novel\nparadigm for re-ranking called online relevance estimation that continuously\nupdates relevance estimates for a query throughout the ranking process. Instead\nof re-ranking a fixed set of top-k documents in a single step, online relevance\nestimation iteratively re-scores smaller subsets of the most promising\ndocuments while adjusting relevance scores for the remaining pool based on the\nestimations from the final model using an online bandit-based algorithm. This\ndynamic process mitigates the recall limitations of telescoping systems by\nre-prioritizing documents initially deemed less relevant by earlier stages --\nincluding those completely excluded by earlier-stage retrievers. We validate\nour approach on TREC benchmarks under two scenarios: hybrid retrieval and\nadaptive retrieval. Experimental results demonstrate that our method is\nsample-efficient and significantly improves recall, highlighting the\neffectiveness of our online relevance estimation framework for modern search\nsystems."
                },
                "authors": [
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "Accepted for publication at SIGIR'25 . 11 pages,5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16867v2",
                "updated": "2025-05-07T12:44:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    44,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2023-05-26T12:17:59Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    12,
                    17,
                    59,
                    4,
                    146,
                    0
                ],
                "title": "Playing repeated games with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing repeated games with Large Language Models"
                },
                "summary": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines."
                },
                "authors": [
                    {
                        "name": "Elif Akata"
                    },
                    {
                        "name": "Lion Schulz"
                    },
                    {
                        "name": "Julian Coda-Forno"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v3",
                "updated": "2025-05-07T12:33:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    33,
                    27,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-based Bi-level Multi-interest Learning Framework for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Bi-level Multi-interest Learning Framework for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation (SR) leverages users' dynamic preferences, with\nrecent advances incorporating multi-interest learning to model diverse user\ninterests. However, most multi-interest SR models rely on noisy, sparse\nimplicit feedback, limiting recommendation accuracy. Large language models\n(LLMs) offer robust reasoning on low-quality data but face high computational\ncosts and latency challenges for SR integration. We propose a novel LLM-based\nmulti-interest SR framework combining implicit behavioral and explicit semantic\nperspectives. It includes two modules: the Implicit Behavioral Interest Module\n(IBIM), which learns from user behavior using a traditional SR model, and the\nExplicit Semantic Interest Module (ESIM), which uses clustering and\nprompt-engineered LLMs to extract semantic multi-interest representations from\ninformative samples. Semantic insights from ESIM enhance IBIM's behavioral\nrepresentations via modality alignment and semantic prediction tasks. During\ninference, only IBIM is used, ensuring efficient, LLM-free recommendations.\nExperiments on four real-world datasets validate the framework's effectiveness\nand practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) leverages users' dynamic preferences, with\nrecent advances incorporating multi-interest learning to model diverse user\ninterests. However, most multi-interest SR models rely on noisy, sparse\nimplicit feedback, limiting recommendation accuracy. Large language models\n(LLMs) offer robust reasoning on low-quality data but face high computational\ncosts and latency challenges for SR integration. We propose a novel LLM-based\nmulti-interest SR framework combining implicit behavioral and explicit semantic\nperspectives. It includes two modules: the Implicit Behavioral Interest Module\n(IBIM), which learns from user behavior using a traditional SR model, and the\nExplicit Semantic Interest Module (ESIM), which uses clustering and\nprompt-engineered LLMs to extract semantic multi-interest representations from\ninformative samples. Semantic insights from ESIM enhance IBIM's behavioral\nrepresentations via modality alignment and semantic prediction tasks. During\ninference, only IBIM is used, ensuring efficient, LLM-free recommendations.\nExperiments on four real-world datasets validate the framework's effectiveness\nand practicality."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04364v1",
                "updated": "2025-05-07T12:32:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T12:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLMs' Swarm intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Swarm intelligence"
                },
                "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Mowen Huang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11646v2",
                "updated": "2025-05-07T11:54:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    54,
                    19,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-16T10:47:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    47,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "Information-Geometric Barycenters for Bayesian Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Geometric Barycenters for Bayesian Federated Learning"
                },
                "summary": "Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration."
                },
                "authors": [
                    {
                        "name": "Nour Jamoussi"
                    },
                    {
                        "name": "Giuseppe Serra"
                    },
                    {
                        "name": "Photios A. Stavrou"
                    },
                    {
                        "name": "Marios Kountouris"
                    }
                ],
                "author_detail": {
                    "name": "Marios Kountouris"
                },
                "author": "Marios Kountouris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15175v3",
                "updated": "2025-05-07T11:48:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    48,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2024-03-22T12:59:03Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    12,
                    59,
                    3,
                    4,
                    82,
                    0
                ],
                "title": "Double Cross-fit Doubly Robust Estimators: Beyond Series Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Cross-fit Doubly Robust Estimators: Beyond Series Regression"
                },
                "summary": "Doubly robust estimators with cross-fitting have gained popularity in causal\ninference due to their favorable structure-agnostic error guarantees. However,\nwhen additional structure, such as H\\\"{o}lder smoothness, is available then\nmore accurate \"double cross-fit doubly robust\" (DCDR) estimators can be\nconstructed by splitting the training data and undersmoothing nuisance function\nestimators on independent samples. We study a DCDR estimator of the Expected\nConditional Covariance, a functional of interest in causal inference and\nconditional independence testing. We first provide a structure-agnostic error\nanalysis for the DCDR estimator with no assumptions on the nuisance functions\nor their estimators. Then, assuming the nuisance functions are H\\\"{o}lder\nsmooth, but without assuming knowledge of the true smoothness level or the\ncovariate density, we establish that DCDR estimators with several linear\nsmoothers are $\\sqrt{n}$-consistent and asymptotically normal under minimal\nconditions and achieve fast convergence rates in the non-$\\sqrt{n}$ regime.\nWhen the covariate density and smoothnesses are known, we propose a minimax\nrate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover,\nwe show an undersmoothed DCDR estimator satisfies a slower-than-$\\sqrt{n}$\ncentral limit theorem, and that inference is possible even in the\nnon-$\\sqrt{n}$ regime. Finally, we support our theoretical results with\nsimulations, providing intuition for double cross-fitting and undersmoothing,\ndemonstrating where our estimator achieves $\\sqrt{n}$-consistency while the\nusual \"single cross-fit\" estimator fails, and illustrating asymptotic normality\nfor the undersmoothed DCDR estimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly robust estimators with cross-fitting have gained popularity in causal\ninference due to their favorable structure-agnostic error guarantees. However,\nwhen additional structure, such as H\\\"{o}lder smoothness, is available then\nmore accurate \"double cross-fit doubly robust\" (DCDR) estimators can be\nconstructed by splitting the training data and undersmoothing nuisance function\nestimators on independent samples. We study a DCDR estimator of the Expected\nConditional Covariance, a functional of interest in causal inference and\nconditional independence testing. We first provide a structure-agnostic error\nanalysis for the DCDR estimator with no assumptions on the nuisance functions\nor their estimators. Then, assuming the nuisance functions are H\\\"{o}lder\nsmooth, but without assuming knowledge of the true smoothness level or the\ncovariate density, we establish that DCDR estimators with several linear\nsmoothers are $\\sqrt{n}$-consistent and asymptotically normal under minimal\nconditions and achieve fast convergence rates in the non-$\\sqrt{n}$ regime.\nWhen the covariate density and smoothnesses are known, we propose a minimax\nrate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover,\nwe show an undersmoothed DCDR estimator satisfies a slower-than-$\\sqrt{n}$\ncentral limit theorem, and that inference is possible even in the\nnon-$\\sqrt{n}$ regime. Finally, we support our theoretical results with\nsimulations, providing intuition for double cross-fitting and undersmoothing,\ndemonstrating where our estimator achieves $\\sqrt{n}$-consistency while the\nusual \"single cross-fit\" estimator fails, and illustrating asymptotic normality\nfor the undersmoothed DCDR estimator."
                },
                "authors": [
                    {
                        "name": "Alec McClean"
                    },
                    {
                        "name": "Sivaraman Balakrishnan"
                    },
                    {
                        "name": "Edward H. Kennedy"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18884v2",
                "updated": "2025-05-07T11:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    31,
                    37,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T10:10:26Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    10,
                    10,
                    26,
                    5,
                    116,
                    0
                ],
                "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification"
                },
                "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "arxiv_comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language \\& Information Systems (NLDB 2025) and will\n  appear in Springer Lecture Notes in Computer Science (LNCS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04321v1",
                "updated": "2025-05-07T11:12:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    12,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:12:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    12,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "Generic Two-Mode Gaussian States as Quantum Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generic Two-Mode Gaussian States as Quantum Sensors"
                },
                "summary": "Gaussian quantum channels constitute a cornerstone of continuous-variable\nquantum information science, underpinning a wide array of protocols in quantum\noptics and quantum metrology. While the action of such channels on arbitrary\nstates is well-characterized under full channel knowledge, we address the\ninverse problem, namely, the precise estimation of fundamental channel\nparameters, including the beam splitter transmissivity and the two-mode\nsqueezing amplitude. Employing the quantum Fisher information (QFI) as a\nbenchmark for metrological sensitivity, we demonstrate that the symmetry\ninherent in mode mixing critically governs the amplification of QFI, thereby\nenabling high-precision parameter estimation. In addition, we investigate\nquantum thermometry by estimating the average photon number of thermal states,\nrevealing that the transmissivity parameter significantly modulates estimation\nprecision. Our results underscore the metrological utility of two-mode Gaussian\nstates and establish a robust framework for parameter inference in noisy and\ndynamically evolving quantum systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian quantum channels constitute a cornerstone of continuous-variable\nquantum information science, underpinning a wide array of protocols in quantum\noptics and quantum metrology. While the action of such channels on arbitrary\nstates is well-characterized under full channel knowledge, we address the\ninverse problem, namely, the precise estimation of fundamental channel\nparameters, including the beam splitter transmissivity and the two-mode\nsqueezing amplitude. Employing the quantum Fisher information (QFI) as a\nbenchmark for metrological sensitivity, we demonstrate that the symmetry\ninherent in mode mixing critically governs the amplification of QFI, thereby\nenabling high-precision parameter estimation. In addition, we investigate\nquantum thermometry by estimating the average photon number of thermal states,\nrevealing that the transmissivity parameter significantly modulates estimation\nprecision. Our results underscore the metrological utility of two-mode Gaussian\nstates and establish a robust framework for parameter inference in noisy and\ndynamically evolving quantum systems."
                },
                "authors": [
                    {
                        "name": "Pritam Chattopadhyay"
                    },
                    {
                        "name": "Saikat Sur"
                    },
                    {
                        "name": "Jonas F. G. Santos"
                    }
                ],
                "author_detail": {
                    "name": "Jonas F. G. Santos"
                },
                "author": "Jonas F. G. Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04318v1",
                "updated": "2025-05-07T11:04:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    4,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:04:47Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    4,
                    47,
                    2,
                    127,
                    0
                ],
                "title": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of\n  Fit Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of\n  Fit Testing"
                },
                "summary": "As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions."
                },
                "authors": [
                    {
                        "name": "Jacob Glenn Ayers"
                    },
                    {
                        "name": "Buvaneswari A. Ramanan"
                    },
                    {
                        "name": "Manzoor A. Khan"
                    }
                ],
                "author_detail": {
                    "name": "Manzoor A. Khan"
                },
                "author": "Manzoor A. Khan",
                "arxiv_comment": "8 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04313v1",
                "updated": "2025-05-07T10:56:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    56,
                    5,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T10:56:05Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    56,
                    5,
                    2,
                    127,
                    0
                ],
                "title": "KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge\n  Representation and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge\n  Representation and Reasoning"
                },
                "summary": "In this paper, we introduce KERAIA, a novel framework and software platform\nfor symbolic knowledge engineering designed to address the persistent\nchallenges of representing, reasoning with, and executing knowledge in dynamic,\ncomplex, and context-sensitive environments. The central research question that\nmotivates this work is: How can unstructured, often tacit, human expertise be\neffectively transformed into computationally tractable algorithms that AI\nsystems can efficiently utilise? KERAIA seeks to bridge this gap by building on\nfoundational concepts such as Minsky's frame-based reasoning and K-lines, while\nintroducing significant innovations. These include Clouds of Knowledge for\ndynamic aggregation, Dynamic Relations (DRels) for context-sensitive\ninheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and\nCloud Elaboration for adaptive knowledge transformation. This approach moves\nbeyond the limitations of traditional, often static, knowledge representation\nparadigms. KERAIA is designed with Explainable AI (XAI) as a core principle,\nensuring transparency and interpretability, particularly through the use of\nLoTs. The paper details the framework's architecture, the KSYNTH representation\nlanguage, and the General Purpose Paradigm Builder (GPPB) to integrate diverse\ninference methods within a unified structure. We validate KERAIA's versatility,\nexpressiveness, and practical applicability through detailed analysis of\nmultiple case studies spanning naval warfare simulation, industrial diagnostics\nin water treatment plants, and strategic decision-making in the game of RISK.\nFurthermore, we provide a comparative analysis against established knowledge\nrepresentation paradigms (including ontologies, rule-based systems, and\nknowledge graphs) and discuss the implementation aspects and computational\nconsiderations of the KERAIA platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce KERAIA, a novel framework and software platform\nfor symbolic knowledge engineering designed to address the persistent\nchallenges of representing, reasoning with, and executing knowledge in dynamic,\ncomplex, and context-sensitive environments. The central research question that\nmotivates this work is: How can unstructured, often tacit, human expertise be\neffectively transformed into computationally tractable algorithms that AI\nsystems can efficiently utilise? KERAIA seeks to bridge this gap by building on\nfoundational concepts such as Minsky's frame-based reasoning and K-lines, while\nintroducing significant innovations. These include Clouds of Knowledge for\ndynamic aggregation, Dynamic Relations (DRels) for context-sensitive\ninheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and\nCloud Elaboration for adaptive knowledge transformation. This approach moves\nbeyond the limitations of traditional, often static, knowledge representation\nparadigms. KERAIA is designed with Explainable AI (XAI) as a core principle,\nensuring transparency and interpretability, particularly through the use of\nLoTs. The paper details the framework's architecture, the KSYNTH representation\nlanguage, and the General Purpose Paradigm Builder (GPPB) to integrate diverse\ninference methods within a unified structure. We validate KERAIA's versatility,\nexpressiveness, and practical applicability through detailed analysis of\nmultiple case studies spanning naval warfare simulation, industrial diagnostics\nin water treatment plants, and strategic decision-making in the game of RISK.\nFurthermore, we provide a comparative analysis against established knowledge\nrepresentation paradigms (including ontologies, rule-based systems, and\nknowledge graphs) and discuss the implementation aspects and computational\nconsiderations of the KERAIA platform."
                },
                "authors": [
                    {
                        "name": "Stephen Richard Varey"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    }
                ],
                "author_detail": {
                    "name": "The Anh Han"
                },
                "author": "The Anh Han",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04312v1",
                "updated": "2025-05-07T10:55:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    55,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T10:55:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    55,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "Beyond entropic regularization: Debiased Gaussian estimators for\n  discrete optimal transport and general linear programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond entropic regularization: Debiased Gaussian estimators for\n  discrete optimal transport and general linear programs"
                },
                "summary": "This work proposes new estimators for discrete optimal transport plans that\nenjoy Gaussian limits centered at the true solution. This behavior stands in\nstark contrast with the performance of existing estimators, including those\nbased on entropic regularization, which are asymptotically biased and only\nsatisfy a CLT centered at a regularized version of the population-level plan.\nWe develop a new regularization approach based on a different class of penalty\nfunctions, which can be viewed as the duals of those previously considered in\nthe literature. The key feature of these penalty schemes it that they give rise\nto preliminary estimates that are asymptotically linear in the penalization\nstrength. Our final estimator is obtained by constructing an appropriate linear\ncombination of two penalized solutions corresponding to two different tuning\nparameters so that the bias introduced by the penalization cancels out. Unlike\nclassical debiasing procedures, therefore, our proposal entirely avoids the\ndelicate problem of estimating and then subtracting the estimated bias term.\nOur proofs, which apply beyond the case of optimal transport, are based on a\nnovel asymptotic analysis of penalization schemes for linear programs. As a\ncorollary of our results, we obtain the consistency of the naive bootstrap for\nfully data-driven inference on the true optimal solution. Simulation results\nand two data analyses support strongly the benefits of our approach relative to\nexisting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes new estimators for discrete optimal transport plans that\nenjoy Gaussian limits centered at the true solution. This behavior stands in\nstark contrast with the performance of existing estimators, including those\nbased on entropic regularization, which are asymptotically biased and only\nsatisfy a CLT centered at a regularized version of the population-level plan.\nWe develop a new regularization approach based on a different class of penalty\nfunctions, which can be viewed as the duals of those previously considered in\nthe literature. The key feature of these penalty schemes it that they give rise\nto preliminary estimates that are asymptotically linear in the penalization\nstrength. Our final estimator is obtained by constructing an appropriate linear\ncombination of two penalized solutions corresponding to two different tuning\nparameters so that the bias introduced by the penalization cancels out. Unlike\nclassical debiasing procedures, therefore, our proposal entirely avoids the\ndelicate problem of estimating and then subtracting the estimated bias term.\nOur proofs, which apply beyond the case of optimal transport, are based on a\nnovel asymptotic analysis of penalization schemes for linear programs. As a\ncorollary of our results, we obtain the consistency of the naive bootstrap for\nfully data-driven inference on the true optimal solution. Simulation results\nand two data analyses support strongly the benefits of our approach relative to\nexisting techniques."
                },
                "authors": [
                    {
                        "name": "Shuyu Liu"
                    },
                    {
                        "name": "Florentina Bunea"
                    },
                    {
                        "name": "Jonathan Niles-Weed"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Niles-Weed"
                },
                "author": "Jonathan Niles-Weed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07547v2",
                "updated": "2025-05-07T10:08:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    8,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2024-10-10T02:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    39,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption."
                },
                "authors": [
                    {
                        "name": "Zecheng Hao"
                    },
                    {
                        "name": "Yifan Huang"
                    },
                    {
                        "name": "Zijie Xu"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Yuanhong Tang"
                    },
                    {
                        "name": "Zhaofei Yu"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v3",
                "updated": "2025-05-07T10:00:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    0,
                    50,
                    2,
                    127,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Accepted by ICIC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20752v2",
                "updated": "2025-05-07T09:47:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    47,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-29T13:33:29Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    33,
                    29,
                    1,
                    119,
                    0
                ],
                "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers"
                },
                "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models."
                },
                "authors": [
                    {
                        "name": "Roman Abramov"
                    },
                    {
                        "name": "Felix Steinbauer"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "arxiv_comment": "Accepted to the International Conference on Machine Learning (ICML)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.3; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04284v1",
                "updated": "2025-05-07T09:40:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    40,
                    18,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:40:18Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    40,
                    18,
                    2,
                    127,
                    0
                ],
                "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance"
                },
                "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Aryan Dabad"
                    },
                    {
                        "name": "Bollampalli Areen Reddy"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Rajiv Misra"
                    },
                    {
                        "name": "Adil A. Shakur"
                    }
                ],
                "author_detail": {
                    "name": "Adil A. Shakur"
                },
                "author": "Adil A. Shakur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18504v3",
                "updated": "2025-05-07T09:39:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    39,
                    42,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-30T17:13:32Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "title": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction"
                },
                "summary": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_doi": "10.1145/3712256.3726317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712256.3726317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.18504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages plus 2 pages of supplemental material",
                "arxiv_journal_ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  2025 (GECCO 25). ACM, Malaga, Spain",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18827v2",
                "updated": "2025-05-07T09:29:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    29,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T07:29:12Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    29,
                    12,
                    5,
                    116,
                    0
                ],
                "title": "Test It Before You Trust It: Applying Software Testing for Trustworthy\n  In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test It Before You Trust It: Applying Software Testing for Trustworthy\n  In-context Learning"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Teeradaj Racharak"
                    },
                    {
                        "name": "Chaiyong Ragkhitwetsagul"
                    },
                    {
                        "name": "Chommakorn Sontesadisai"
                    },
                    {
                        "name": "Thanwadee Sunetnanta"
                    }
                ],
                "author_detail": {
                    "name": "Thanwadee Sunetnanta"
                },
                "author": "Thanwadee Sunetnanta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04265v1",
                "updated": "2025-05-07T09:14:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    14,
                    55,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:14:55Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    14,
                    55,
                    2,
                    127,
                    0
                ],
                "title": "Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper"
                },
                "summary": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly."
                },
                "authors": [
                    {
                        "name": "Abdulrahman S Almuhaidib"
                    },
                    {
                        "name": "Azlan Mohd Zain"
                    },
                    {
                        "name": "Zalmiyah Zakaria"
                    },
                    {
                        "name": "Izyan Izzati Kamsani"
                    },
                    {
                        "name": "Abdulaziz S Almuhaidib"
                    }
                ],
                "author_detail": {
                    "name": "Abdulaziz S Almuhaidib"
                },
                "author": "Abdulaziz S Almuhaidib",
                "arxiv_comment": "Pre-print - Accepted for publication in the Proceedings of the\n  International Computer Sciences and Informatics Conference (ICSIC-2024),\n  published by AIP Publishing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04260v1",
                "updated": "2025-05-07T09:10:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    10,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:10:51Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    10,
                    51,
                    2,
                    127,
                    0
                ],
                "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering"
                },
                "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Tianyu Xu"
                    },
                    {
                        "name": "Ishan Chatterjee"
                    },
                    {
                        "name": "Katrina Passarella-Ward"
                    },
                    {
                        "name": "Achin Kulshrestha"
                    },
                    {
                        "name": "D Shin"
                    }
                ],
                "author_detail": {
                    "name": "D Shin"
                },
                "author": "D Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04254v1",
                "updated": "2025-05-07T08:59:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    59,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:59:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    59,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "CompileAgent: Automated Real-World Repo-Level Compilation with\n  Tool-Integrated LLM-based Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompileAgent: Automated Real-World Repo-Level Compilation with\n  Tool-Integrated LLM-based Agent System"
                },
                "summary": "With open-source projects growing in size and complexity, manual compilation\nbecomes tedious and error-prone, highlighting the need for automation to\nimprove efficiency and accuracy. However, the complexity of compilation\ninstruction search and error resolution makes automatic compilation\nchallenging. Inspired by the success of LLM-based agents in various fields, we\npropose CompileAgent, the first LLM-based agent framework dedicated to\nrepo-level compilation. CompileAgent integrates five tools and a flow-based\nagent strategy, enabling interaction with software artifacts for compilation\ninstruction search and error resolution. To measure the effectiveness of our\nmethod, we design a public repo-level benchmark CompileAgentBench, and we also\ndesign two baselines for comparison by combining two compilation-friendly\nschemes. The performance on this benchmark shows that our method significantly\nimproves the compilation success rate, ranging from 10% to 71%. Meanwhile, we\nevaluate the performance of CompileAgent under different agent strategies and\nverify the effectiveness of the flow-based strategy. Additionally, we emphasize\nthe scalability of CompileAgent, further expanding its application prospects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With open-source projects growing in size and complexity, manual compilation\nbecomes tedious and error-prone, highlighting the need for automation to\nimprove efficiency and accuracy. However, the complexity of compilation\ninstruction search and error resolution makes automatic compilation\nchallenging. Inspired by the success of LLM-based agents in various fields, we\npropose CompileAgent, the first LLM-based agent framework dedicated to\nrepo-level compilation. CompileAgent integrates five tools and a flow-based\nagent strategy, enabling interaction with software artifacts for compilation\ninstruction search and error resolution. To measure the effectiveness of our\nmethod, we design a public repo-level benchmark CompileAgentBench, and we also\ndesign two baselines for comparison by combining two compilation-friendly\nschemes. The performance on this benchmark shows that our method significantly\nimproves the compilation success rate, ranging from 10% to 71%. Meanwhile, we\nevaluate the performance of CompileAgent under different agent strategies and\nverify the effectiveness of the flow-based strategy. Additionally, we emphasize\nthe scalability of CompileAgent, further expanding its application prospects."
                },
                "authors": [
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Benlong Wu"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Xu Zhu"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04253v1",
                "updated": "2025-05-07T08:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    58,
                    52,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    58,
                    52,
                    2,
                    127,
                    0
                ],
                "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself"
                },
                "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."
                },
                "authors": [
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Viktor Moskvoretskii"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Moskvoretskii"
                },
                "author": "Viktor Moskvoretskii",
                "arxiv_comment": "11 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04251v1",
                "updated": "2025-05-07T08:55:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    55,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    55,
                    15,
                    2,
                    127,
                    0
                ],
                "title": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering"
                },
                "summary": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented."
                },
                "authors": [
                    {
                        "name": "Krishna Ronanki"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Ronanki"
                },
                "author": "Krishna Ronanki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04231v1",
                "updated": "2025-05-07T08:27:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    27,
                    52,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    27,
                    52,
                    2,
                    127,
                    0
                ],
                "title": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving\n  in Smart Intersections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving\n  in Smart Intersections"
                },
                "summary": "Unsignalized intersections pose significant safety and efficiency challenges\ndue to complex traffic flows. This paper proposes a novel roadside unit\n(RSU)-centric cooperative driving system leveraging global perception and\nvehicle-to-infrastructure (V2I) communication. The core of the system is an\nRSU-based decision-making module using a two-stage hybrid reinforcement\nlearning (RL) framework. At first, policies are pre-trained offline using\nconservative Q-learning (CQL) combined with behavior cloning (BC) on collected\ndataset. Subsequently, these policies are fine-tuned in the simulation using\nmulti-agent proximal policy optimization (MAPPO), aligned with a self-attention\nmechanism to effectively solve inter-agent dependencies. RSUs perform real-time\ninference based on the trained models to realize vehicle control via V2I\ncommunications. Extensive experiments in CARLA environment demonstrate high\neffectiveness of the proposed system, by: \\textit{(i)} achieving failure rates\nbelow 0.03\\% in coordinating three connected and autonomous vehicles (CAVs)\nthrough complex intersection scenarios, significantly outperforming the\ntraditional Autoware control method, and \\textit{(ii)} exhibiting strong\nrobustness across varying numbers of controlled agents and shows promising\ngeneralization capabilities on other maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsignalized intersections pose significant safety and efficiency challenges\ndue to complex traffic flows. This paper proposes a novel roadside unit\n(RSU)-centric cooperative driving system leveraging global perception and\nvehicle-to-infrastructure (V2I) communication. The core of the system is an\nRSU-based decision-making module using a two-stage hybrid reinforcement\nlearning (RL) framework. At first, policies are pre-trained offline using\nconservative Q-learning (CQL) combined with behavior cloning (BC) on collected\ndataset. Subsequently, these policies are fine-tuned in the simulation using\nmulti-agent proximal policy optimization (MAPPO), aligned with a self-attention\nmechanism to effectively solve inter-agent dependencies. RSUs perform real-time\ninference based on the trained models to realize vehicle control via V2I\ncommunications. Extensive experiments in CARLA environment demonstrate high\neffectiveness of the proposed system, by: \\textit{(i)} achieving failure rates\nbelow 0.03\\% in coordinating three connected and autonomous vehicles (CAVs)\nthrough complex intersection scenarios, significantly outperforming the\ntraditional Autoware control method, and \\textit{(ii)} exhibiting strong\nrobustness across varying numbers of controlled agents and shows promising\ngeneralization capabilities on other maps."
                },
                "authors": [
                    {
                        "name": "Taoyuan Yu"
                    },
                    {
                        "name": "Kui Wang"
                    },
                    {
                        "name": "Zongdian Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Kei Sakaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Kei Sakaguchi"
                },
                "author": "Kei Sakaguchi",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04209v1",
                "updated": "2025-05-07T08:03:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    3,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:03:25Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    3,
                    25,
                    2,
                    127,
                    0
                ],
                "title": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase\n  Relevance at eBay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase\n  Relevance at eBay"
                },
                "summary": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics."
                },
                "authors": [
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Binbin Li"
                },
                "author": "Binbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03885v3",
                "updated": "2025-05-07T08:02:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    2,
                    44,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-06T09:01:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    1,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers"
                },
                "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node)."
                },
                "authors": [
                    {
                        "name": "Chenchen Shou"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Huaiyu Meng"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Wenqing Lv"
                    },
                    {
                        "name": "Yelong Xu"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yichen Shen"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12697v2",
                "updated": "2025-05-07T07:56:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    56,
                    43,
                    2,
                    127,
                    0
                ],
                "published": "2024-01-23T12:00:07Z",
                "published_parsed": [
                    2024,
                    1,
                    23,
                    12,
                    0,
                    7,
                    1,
                    23,
                    0
                ],
                "title": "A Computationally Efficient Approach to False Discovery Rate Control and\n  Power Maximisation via Randomisation and Mirror Statistic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Computationally Efficient Approach to False Discovery Rate Control and\n  Power Maximisation via Randomisation and Mirror Statistic"
                },
                "summary": "Simultaneously performing variable selection and inference in\nhigh-dimensional regression models is an open challenge in statistics and\nmachine learning. The increasing availability of vast amounts of variables\nrequires the adoption of specific statistical procedures to accurately select\nthe most important predictors in a high-dimensional space, while controlling\nthe false discovery rate (FDR) associated with the variable selection\nprocedure. In this paper, we propose the joint adoption of the Mirror Statistic\napproach to FDR control, coupled with outcome randomisation to maximise the\nstatistical power of the variable selection procedure, measured through the\ntrue positive rate. Through extensive simulations, we show how our proposed\nstrategy allows us to combine the benefits of the two techniques. The Mirror\nStatistic is a flexible method to control FDR, which only requires mild model\nassumptions, but requires two sets of independent regression coefficient\nestimates, usually obtained after splitting the original dataset. Outcome\nrandomisation is an alternative to data splitting that allows to generate two\nindependent outcomes, which can then be used to estimate the coefficients that\ngo into the construction of the Mirror Statistic. The combination of these two\napproaches provides increased testing power in a number of scenarios, such as\nhighly correlated covariates and high percentages of active variables.\nMoreover, it is scalable to very high-dimensional problems, since the algorithm\nhas a low memory footprint and only requires a single run on the full dataset,\nas opposed to iterative alternatives such as multiple data splitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously performing variable selection and inference in\nhigh-dimensional regression models is an open challenge in statistics and\nmachine learning. The increasing availability of vast amounts of variables\nrequires the adoption of specific statistical procedures to accurately select\nthe most important predictors in a high-dimensional space, while controlling\nthe false discovery rate (FDR) associated with the variable selection\nprocedure. In this paper, we propose the joint adoption of the Mirror Statistic\napproach to FDR control, coupled with outcome randomisation to maximise the\nstatistical power of the variable selection procedure, measured through the\ntrue positive rate. Through extensive simulations, we show how our proposed\nstrategy allows us to combine the benefits of the two techniques. The Mirror\nStatistic is a flexible method to control FDR, which only requires mild model\nassumptions, but requires two sets of independent regression coefficient\nestimates, usually obtained after splitting the original dataset. Outcome\nrandomisation is an alternative to data splitting that allows to generate two\nindependent outcomes, which can then be used to estimate the coefficients that\ngo into the construction of the Mirror Statistic. The combination of these two\napproaches provides increased testing power in a number of scenarios, such as\nhighly correlated covariates and high percentages of active variables.\nMoreover, it is scalable to very high-dimensional problems, since the algorithm\nhas a low memory footprint and only requires a single run on the full dataset,\nas opposed to iterative alternatives such as multiple data splitting."
                },
                "authors": [
                    {
                        "name": "Marco Molinari"
                    },
                    {
                        "name": "Magne Thoresen"
                    }
                ],
                "author_detail": {
                    "name": "Magne Thoresen"
                },
                "author": "Magne Thoresen",
                "arxiv_doi": "10.1177/09622802251329768",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/09622802251329768",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.12697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Statistical Methods in Medical Research. 2025;0(0)",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04196v1",
                "updated": "2025-05-07T07:50:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    50,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:50:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    50,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "A Large Language Model for Feasible and Diverse Population Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model for Feasible and Diverse Population Synthesis"
                },
                "summary": "Generating a synthetic population that is both feasible and diverse is\ncrucial for ensuring the validity of downstream activity schedule simulation in\nactivity-based models (ABMs). While deep generative models (DGMs), such as\nvariational autoencoders and generative adversarial networks, have been applied\nto this task, they often struggle to balance the inclusion of rare but\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\nones (i.e., structural zeros). To improve feasibility while maintaining\ndiversity, we propose a fine-tuning method for large language models (LLMs)\nthat explicitly controls the autoregressive generation process through\ntopological orderings derived from a Bayesian Network (BN). Experimental\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\nour approach achieves approximately 95% feasibility, significantly higher than\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\nwell-suited for practical applications. Importantly, the method is based on a\nlightweight open-source LLM, enabling fine-tuning and inference on standard\npersonal computing environments. This makes the approach cost-effective and\nscalable for large-scale applications, such as synthesizing populations in\nmegacities, without relying on expensive infrastructure. By initiating the ABM\npipeline with high-quality synthetic populations, our method improves overall\nsimulation reliability and reduces downstream error propagation. The source\ncode for these methods is available for research and practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating a synthetic population that is both feasible and diverse is\ncrucial for ensuring the validity of downstream activity schedule simulation in\nactivity-based models (ABMs). While deep generative models (DGMs), such as\nvariational autoencoders and generative adversarial networks, have been applied\nto this task, they often struggle to balance the inclusion of rare but\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\nones (i.e., structural zeros). To improve feasibility while maintaining\ndiversity, we propose a fine-tuning method for large language models (LLMs)\nthat explicitly controls the autoregressive generation process through\ntopological orderings derived from a Bayesian Network (BN). Experimental\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\nour approach achieves approximately 95% feasibility, significantly higher than\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\nwell-suited for practical applications. Importantly, the method is based on a\nlightweight open-source LLM, enabling fine-tuning and inference on standard\npersonal computing environments. This makes the approach cost-effective and\nscalable for large-scale applications, such as synthesizing populations in\nmegacities, without relying on expensive infrastructure. By initiating the ABM\npipeline with high-quality synthetic populations, our method improves overall\nsimulation reliability and reduces downstream error propagation. The source\ncode for these methods is available for research and practical application."
                },
                "authors": [
                    {
                        "name": "Sung Yoo Lim"
                    },
                    {
                        "name": "Hyunsoo Yun"
                    },
                    {
                        "name": "Prateek Bansal"
                    },
                    {
                        "name": "Dong-Kyu Kim"
                    },
                    {
                        "name": "Eui-Jin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Jin Kim"
                },
                "author": "Eui-Jin Kim",
                "arxiv_comment": "28 pages, 7 figures, 6 tables. Submitted to Transportation Research\n  Part C: Emerging Technologies. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04195v1",
                "updated": "2025-05-07T07:49:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    49,
                    5,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:49:05Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    49,
                    5,
                    2,
                    127,
                    0
                ],
                "title": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE\n  Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising tools in software\ndevelopment, enabling automated code generation and analysis. However, their\nknowledge is limited to a fixed cutoff date, making them prone to generating\ncode vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets\nis costly, and existing LLM-based approaches focus on oversimplified CWE\nexamples and require providing explicit bug locations to LLMs, limiting their\nability to patch complex real-world vulnerabilities. To address these\nlimitations, we propose AutoPatch, a multi-agent framework designed to patch\nvulnerable LLM-generated code, particularly those introduced after the LLMs'\nknowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)\nwith a structured database of recently disclosed vulnerabilities, comprising\n525 code snippets derived from 75 high-severity CVEs across real-world systems\nsuch as the Linux kernel and Chrome. AutoPatch combines semantic and taint\nanalysis to identify the most relevant CVE and leverages enhanced\nChain-of-Thought (CoT) reasoning to construct enriched prompts for verification\nand patching. Our unified similarity model, which selects the most relevant\nvulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch\nattains 89.5 percent F1-score for vulnerability verification and 95.0 percent\naccuracy in patching, while being over 50x more cost-efficient than traditional\nfine-tuning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising tools in software\ndevelopment, enabling automated code generation and analysis. However, their\nknowledge is limited to a fixed cutoff date, making them prone to generating\ncode vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets\nis costly, and existing LLM-based approaches focus on oversimplified CWE\nexamples and require providing explicit bug locations to LLMs, limiting their\nability to patch complex real-world vulnerabilities. To address these\nlimitations, we propose AutoPatch, a multi-agent framework designed to patch\nvulnerable LLM-generated code, particularly those introduced after the LLMs'\nknowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)\nwith a structured database of recently disclosed vulnerabilities, comprising\n525 code snippets derived from 75 high-severity CVEs across real-world systems\nsuch as the Linux kernel and Chrome. AutoPatch combines semantic and taint\nanalysis to identify the most relevant CVE and leverages enhanced\nChain-of-Thought (CoT) reasoning to construct enriched prompts for verification\nand patching. Our unified similarity model, which selects the most relevant\nvulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch\nattains 89.5 percent F1-score for vulnerability verification and 95.0 percent\naccuracy in patching, while being over 50x more cost-efficient than traditional\nfine-tuning approaches."
                },
                "authors": [
                    {
                        "name": "Minjae Seo"
                    },
                    {
                        "name": "Wonwoo Choi"
                    },
                    {
                        "name": "Myoungsung You"
                    },
                    {
                        "name": "Seungwon Shin"
                    }
                ],
                "author_detail": {
                    "name": "Seungwon Shin"
                },
                "author": "Seungwon Shin",
                "arxiv_comment": "16 pages, single column, 7 figures. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01496v2",
                "updated": "2025-05-07T07:42:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    42,
                    11,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-03T13:08:00Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    13,
                    8,
                    0,
                    0,
                    62,
                    0
                ],
                "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liger: Linearizing Large Language Models to Gated Recurrent Structures"
                },
                "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization."
                },
                "authors": [
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Accepted by ICML 2025, 15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18837v3",
                "updated": "2025-05-07T07:37:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    37,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T07:48:35Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    48,
                    35,
                    5,
                    116,
                    0
                ],
                "title": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events"
                },
                "summary": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era."
                },
                "authors": [
                    {
                        "name": "Pouya Shaeri"
                    },
                    {
                        "name": "Yasaman Mohammadpour"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Ariane Middel"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "13 Pages, 1 figure, Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18681v3",
                "updated": "2025-05-07T07:22:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    22,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2023-11-30T16:28:40Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    28,
                    40,
                    3,
                    334,
                    0
                ],
                "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance"
                },
                "summary": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog."
                },
                "authors": [
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Özsoy"
                    },
                    {
                        "name": "Benjamin Busam"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Matthias Keicher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Keicher"
                },
                "author": "Matthias Keicher",
                "arxiv_comment": "Accepted for publication at MIDL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04174v1",
                "updated": "2025-05-07T07:04:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    4,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:04:49Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    4,
                    49,
                    2,
                    127,
                    0
                ],
                "title": "On-Device LLM for Context-Aware Wi-Fi Roaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device LLM for Context-Aware Wi-Fi Roaming"
                },
                "summary": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems."
                },
                "authors": [
                    {
                        "name": "Ju-Hyung Lee"
                    },
                    {
                        "name": "Yanqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Lu"
                },
                "author": "Yanqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03397v2",
                "updated": "2025-05-07T06:55:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    55,
                    17,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-06T10:25:50Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    25,
                    50,
                    1,
                    126,
                    0
                ],
                "title": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath"
                },
                "summary": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space."
                },
                "authors": [
                    {
                        "name": "Chris Wise"
                    },
                    {
                        "name": "Akram Youssry"
                    },
                    {
                        "name": "Alberto Peruzzo"
                    },
                    {
                        "name": "Jo Plested"
                    },
                    {
                        "name": "Matt Woolley"
                    }
                ],
                "author_detail": {
                    "name": "Matt Woolley"
                },
                "author": "Matt Woolley",
                "arxiv_comment": "19 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04171v1",
                "updated": "2025-05-07T06:53:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    53,
                    59,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T06:53:59Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    53,
                    59,
                    2,
                    127,
                    0
                ],
                "title": "Large Language Models are often politically extreme, usually\n  ideologically inconsistent, and persuasive even in informational contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are often politically extreme, usually\n  ideologically inconsistent, and persuasive even in informational contexts"
                },
                "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."
                },
                "authors": [
                    {
                        "name": "Nouar Aldahoul"
                    },
                    {
                        "name": "Hazem Ibrahim"
                    },
                    {
                        "name": "Matteo Varvello"
                    },
                    {
                        "name": "Aaron Kaufman"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Yasir Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Zaki"
                },
                "author": "Yasir Zaki",
                "arxiv_comment": "61 pages, 29 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04168v1",
                "updated": "2025-05-07T06:44:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    44,
                    34,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T06:44:34Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    44,
                    34,
                    2,
                    127,
                    0
                ],
                "title": "Principal Curves In Metric Spaces And The Space Of Probability Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal Curves In Metric Spaces And The Space Of Probability Measures"
                },
                "summary": "We introduce principal curves in Wasserstein space, and in general compact\nmetric spaces. Our motivation for the Wasserstein case comes from\noptimal-transport-based trajectory inference, where a developing population of\ncells traces out a curve in Wasserstein space. Our framework enables new\nexperimental procedures for collecting high-density time-courses of developing\npopulations of cells: time-points can be processed in parallel (making it\neasier to collect more time-points). However, then the time of collection is\nunknown, and must be recovered by solving a seriation problem (or\none-dimensional manifold learning problem).\n  We propose an estimator based on Wasserstein principal curves, and prove it\nis consistent for recovering a curve of probability measures in Wasserstein\nspace from empirical samples. This consistency theorem is obtained via a series\nof results regarding principal curves in compact metric spaces. In particular,\nwe establish the validity of certain numerical discretization schemes for\nprincipal curves, which is a new result even in the Euclidean setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce principal curves in Wasserstein space, and in general compact\nmetric spaces. Our motivation for the Wasserstein case comes from\noptimal-transport-based trajectory inference, where a developing population of\ncells traces out a curve in Wasserstein space. Our framework enables new\nexperimental procedures for collecting high-density time-courses of developing\npopulations of cells: time-points can be processed in parallel (making it\neasier to collect more time-points). However, then the time of collection is\nunknown, and must be recovered by solving a seriation problem (or\none-dimensional manifold learning problem).\n  We propose an estimator based on Wasserstein principal curves, and prove it\nis consistent for recovering a curve of probability measures in Wasserstein\nspace from empirical samples. This consistency theorem is obtained via a series\nof results regarding principal curves in compact metric spaces. In particular,\nwe establish the validity of certain numerical discretization schemes for\nprincipal curves, which is a new result even in the Euclidean setting."
                },
                "authors": [
                    {
                        "name": "Andrew Warren"
                    },
                    {
                        "name": "Anton Afanassiev"
                    },
                    {
                        "name": "Forest Kobayashi"
                    },
                    {
                        "name": "Young-Heon Kim"
                    },
                    {
                        "name": "Geoffrey Schiebinger"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Schiebinger"
                },
                "author": "Geoffrey Schiebinger",
                "arxiv_comment": "53 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary: 62G05, 49Q20, Secondary: 62P10, 62R20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07158v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07158v5",
                "updated": "2025-05-07T06:35:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    35,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-10T10:33:31Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    33,
                    31,
                    0,
                    69,
                    0
                ],
                "title": "Generative AI in Transportation Planning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI in Transportation Planning: A Survey"
                },
                "summary": "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning."
                },
                "authors": [
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Tiejin Chen"
                    },
                    {
                        "name": "Zhuoheng Li"
                    },
                    {
                        "name": "Shreyas Bachiraju"
                    },
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Dongjie Wang"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Ram Pendyala"
                    },
                    {
                        "name": "Benjamin Stabler"
                    },
                    {
                        "name": "Yezhou Yang"
                    },
                    {
                        "name": "Xuesong Zhou"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "55 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07158v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07158v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T99, 90B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.6.3; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04152v1",
                "updated": "2025-05-07T06:03:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    3,
                    37,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T06:03:37Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    3,
                    37,
                    2,
                    127,
                    0
                ],
                "title": "Can Language Models Understand Social Behavior in Clinical\n  Conversations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Understand Social Behavior in Clinical\n  Conversations?"
                },
                "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Manas Satish Bedmutha"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Andrea Hartzler"
                    },
                    {
                        "name": "Trevor Cohen"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; H.1.2; I.2.7; I.2.m; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04147v1",
                "updated": "2025-05-07T05:55:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    55,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:55:45Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    55,
                    45,
                    2,
                    127,
                    0
                ],
                "title": "R^3-VQA: \"Read the Room\" by Video Social Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R^3-VQA: \"Read the Room\" by Video Social Reasoning"
                },
                "summary": "\"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Lixing Niu"
                    },
                    {
                        "name": "Jiapeng Li"
                    },
                    {
                        "name": "Xingping Yu"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Ruining Feng"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Ping Wei"
                    },
                    {
                        "name": "Yisen Wang"
                    },
                    {
                        "name": "Lifeng Fan"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Fan"
                },
                "author": "Lifeng Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04146v1",
                "updated": "2025-05-07T05:54:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    54,
                    4,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:54:04Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    54,
                    4,
                    2,
                    127,
                    0
                ],
                "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation\n  Jailbreaking and LLM Content Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation\n  Jailbreaking and LLM Content Safety"
                },
                "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."
                },
                "authors": [
                    {
                        "name": "Variath Madhupal Gautham Nair"
                    },
                    {
                        "name": "Vishal Varma Dantuluri"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Varma Dantuluri"
                },
                "author": "Vishal Varma Dantuluri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04141v1",
                "updated": "2025-05-07T05:45:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    45,
                    33,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:45:33Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    45,
                    33,
                    2,
                    127,
                    0
                ],
                "title": "NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance"
                },
                "summary": "Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality."
                },
                "authors": [
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04135v1",
                "updated": "2025-05-07T05:13:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    13,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:13:15Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    13,
                    15,
                    2,
                    127,
                    0
                ],
                "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought\n  Prompting in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Granular Sentiment Classification with Chain-of-Thought\n  Prompting in Large Language Models"
                },
                "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."
                },
                "authors": [
                    {
                        "name": "Vihaan Miriyala"
                    },
                    {
                        "name": "Smrithi Bukkapatnam"
                    },
                    {
                        "name": "Lavanya Prahallad"
                    }
                ],
                "author_detail": {
                    "name": "Lavanya Prahallad"
                },
                "author": "Lavanya Prahallad",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07261v2",
                "updated": "2025-05-07T05:11:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    11,
                    35,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-13T12:21:33Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    21,
                    33,
                    0,
                    13,
                    0
                ],
                "title": "Inference accuracy about an aircraft crash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accuracy about an aircraft crash"
                },
                "summary": "Problem-based learning benefits from situations taken from real life, which\narouse student interest. The shooting of Rwanda president aircraft on April\n6th, 1994 is still unsolved. We discuss the methods to infer information and\nconclusions about where the aircraft was shot and its trajectory during its\nfall, as well as about the place from which the missiles were launched, and\ntheir trajectory and type. To this goal, we compiled expert reports, witness\nindications and other public sources, then translated plain language sentences\ninto quantitative equalities and inequalities applied to geometry and mechanics\nat undergraduate level. The accuracy of each result is discussed and propagated\nin order to ensure a proper assessment of the hypotheses and a traceability of\ntheir consequences. Overall, the accuracy discussion can train the students\ncritical mind, and teach inference methods which are routinely used in several\nfields of physics research. In addition, it demonstrates the importance and\nlimits of scientific expertise during a judiciary process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-based learning benefits from situations taken from real life, which\narouse student interest. The shooting of Rwanda president aircraft on April\n6th, 1994 is still unsolved. We discuss the methods to infer information and\nconclusions about where the aircraft was shot and its trajectory during its\nfall, as well as about the place from which the missiles were launched, and\ntheir trajectory and type. To this goal, we compiled expert reports, witness\nindications and other public sources, then translated plain language sentences\ninto quantitative equalities and inequalities applied to geometry and mechanics\nat undergraduate level. The accuracy of each result is discussed and propagated\nin order to ensure a proper assessment of the hypotheses and a traceability of\ntheir consequences. Overall, the accuracy discussion can train the students\ncritical mind, and teach inference methods which are routinely used in several\nfields of physics research. In addition, it demonstrates the importance and\nlimits of scientific expertise during a judiciary process."
                },
                "authors": [
                    {
                        "name": "François Graner"
                    },
                    {
                        "name": "Stefano Matthias Panebianco"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Matthias Panebianco"
                },
                "arxiv_affiliation": "DPHN",
                "author": "Stefano Matthias Panebianco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01495v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01495v3",
                "updated": "2025-05-07T05:01:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    1,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2024-06-03T16:21:38Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    21,
                    38,
                    0,
                    155,
                    0
                ],
                "title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents"
                },
                "summary": "Finetuning language agents with reasoning-action trajectories is effective,\nbut obtaining these trajectories from human annotations or stronger models is\ncostly and sometimes impractical. In this paper, we investigate the use of\nself-training in language agents, which can generate supervision from the agent\nitself, offering a promising alternative without relying on human or stronger\nmodel demonstrations. Self-training, however, requires high-quality\nmodel-generated samples, which are hard to obtain for challenging language\nagent tasks. To address this, we present Reflection-Reinforced Self-Training\n(Re-ReST), which uses a \\textit{reflector} to refine low-quality generated\nsamples during self-training. The reflector takes the agent's output and\nfeedback from an external environment (e.g., unit test results in code\ngeneration) to produce improved samples. This technique enhances the quality of\ninferior samples and efficiently enriches the self-training dataset with\nhigher-quality samples. We conduct extensive experiments on open-source\nlanguage agents across tasks, including multi-hop question answering,\nsequential decision-making, code generation, visual question answering, and\ntext-to-image generation. The results demonstrate the effectiveness of\nself-training and Re-ReST in language agent tasks, with self-training improving\nbaselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further\nboosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also\nconfirm the efficiency of using a reflector to generate high-quality samples\nfor self-training. Moreover, we demonstrate a method to employ reflection\nduring inference without ground-truth feedback, addressing the limitation of\nprevious reflection work. Our code is released at\nhttps://github.com/PlusLabNLP/Re-ReST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning language agents with reasoning-action trajectories is effective,\nbut obtaining these trajectories from human annotations or stronger models is\ncostly and sometimes impractical. In this paper, we investigate the use of\nself-training in language agents, which can generate supervision from the agent\nitself, offering a promising alternative without relying on human or stronger\nmodel demonstrations. Self-training, however, requires high-quality\nmodel-generated samples, which are hard to obtain for challenging language\nagent tasks. To address this, we present Reflection-Reinforced Self-Training\n(Re-ReST), which uses a \\textit{reflector} to refine low-quality generated\nsamples during self-training. The reflector takes the agent's output and\nfeedback from an external environment (e.g., unit test results in code\ngeneration) to produce improved samples. This technique enhances the quality of\ninferior samples and efficiently enriches the self-training dataset with\nhigher-quality samples. We conduct extensive experiments on open-source\nlanguage agents across tasks, including multi-hop question answering,\nsequential decision-making, code generation, visual question answering, and\ntext-to-image generation. The results demonstrate the effectiveness of\nself-training and Re-ReST in language agent tasks, with self-training improving\nbaselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further\nboosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also\nconfirm the efficiency of using a reflector to generate high-quality samples\nfor self-training. Moreover, we demonstrate a method to employ reflection\nduring inference without ground-truth feedback, addressing the limitation of\nprevious reflection work. Our code is released at\nhttps://github.com/PlusLabNLP/Re-ReST."
                },
                "authors": [
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Cheng-Fu Yang"
                    },
                    {
                        "name": "Xueqing Wu"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01495v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01495v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22240v2",
                "updated": "2025-05-07T04:53:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    53,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-28T08:42:54Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    8,
                    42,
                    54,
                    4,
                    87,
                    0
                ],
                "title": "Bimanual Regrasp Planning and Control for Active Reduction of Object\n  Pose Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimanual Regrasp Planning and Control for Active Reduction of Object\n  Pose Uncertainty"
                },
                "summary": "Precisely grasping an object is a challenging task due to pose uncertainties.\nConventional methods have used cameras and fixtures to reduce object\nuncertainty. They are effective but require intensive preparation, such as\ndesigning jigs based on the object geometry and calibrating cameras with\nhigh-precision tools fabricated using lasers. In this study, we propose a\nmethod to reduce the uncertainty of the position and orientation of a grasped\nobject without using a fixture or a camera. Our method is based on the concept\nthat the flat finger pads of a parallel gripper can reduce uncertainty along\nits opening/closing direction through flat surface contact. Three orthogonal\ngrasps by parallel grippers with flat finger pads collectively constrain an\nobject's position and orientation to a unique state. Guided by the concepts, we\ndevelop a regrasp planning and admittance control approach that sequentially\nfinds and leverages three orthogonal grasps of two robotic arms to actively\nreduce uncertainties in the object pose. We evaluated the proposed method on\ndifferent initial object uncertainties and verified that it had good\nrepeatability. The deviation levels of the experimental trials were on the same\norder of magnitude as those of an optical tracking system, demonstrating strong\nrelative inference performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precisely grasping an object is a challenging task due to pose uncertainties.\nConventional methods have used cameras and fixtures to reduce object\nuncertainty. They are effective but require intensive preparation, such as\ndesigning jigs based on the object geometry and calibrating cameras with\nhigh-precision tools fabricated using lasers. In this study, we propose a\nmethod to reduce the uncertainty of the position and orientation of a grasped\nobject without using a fixture or a camera. Our method is based on the concept\nthat the flat finger pads of a parallel gripper can reduce uncertainty along\nits opening/closing direction through flat surface contact. Three orthogonal\ngrasps by parallel grippers with flat finger pads collectively constrain an\nobject's position and orientation to a unique state. Guided by the concepts, we\ndevelop a regrasp planning and admittance control approach that sequentially\nfinds and leverages three orthogonal grasps of two robotic arms to actively\nreduce uncertainties in the object pose. We evaluated the proposed method on\ndifferent initial object uncertainties and verified that it had good\nrepeatability. The deviation levels of the experimental trials were on the same\norder of magnitude as those of an optical tracking system, demonstrating strong\nrelative inference performance."
                },
                "authors": [
                    {
                        "name": "Ryuta Nagahama"
                    },
                    {
                        "name": "Weiwei Wan"
                    },
                    {
                        "name": "Zhengtao Hu"
                    },
                    {
                        "name": "Kensuke Harada"
                    }
                ],
                "author_detail": {
                    "name": "Kensuke Harada"
                },
                "author": "Kensuke Harada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v3",
                "updated": "2025-05-07T04:50:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    50,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07550v2",
                "updated": "2025-05-07T04:26:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    26,
                    40,
                    2,
                    127,
                    0
                ],
                "published": "2024-10-10T02:46:28Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    46,
                    28,
                    3,
                    284,
                    0
                ],
                "title": "Conditional Lagrangian Wasserstein Flow for Time Series Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Lagrangian Wasserstein Flow for Time Series Imputation"
                },
                "summary": "Time series imputation is important for numerous real-world applications. To\novercome the limitations of diffusion model-based imputation methods, e.g.,\nslow convergence in inference, we propose a novel method for time series\nimputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF).\nFollowing the principle of least action in Lagrangian mechanics, we learn the\nvelocity by minimizing the corresponding kinetic energy. Moreover, to enhance\nthe model's performance, we estimate the gradient of a task-specific potential\nfunction using a time-dependent denoising autoencoder and integrate it into the\nbase estimator to reduce the sampling variance. Finally, the proposed method\ndemonstrates competitive performance compared to other state-of-the-art\nimputation approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series imputation is important for numerous real-world applications. To\novercome the limitations of diffusion model-based imputation methods, e.g.,\nslow convergence in inference, we propose a novel method for time series\nimputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF).\nFollowing the principle of least action in Lagrangian mechanics, we learn the\nvelocity by minimizing the corresponding kinetic energy. Moreover, to enhance\nthe model's performance, we estimate the gradient of a task-specific potential\nfunction using a time-dependent denoising autoencoder and integrate it into the\nbase estimator to reduce the sampling variance. Finally, the proposed method\ndemonstrates competitive performance compared to other state-of-the-art\nimputation approaches."
                },
                "authors": [
                    {
                        "name": "Weizhu Qian"
                    },
                    {
                        "name": "Dalin Zhang"
                    },
                    {
                        "name": "Yan Zhao"
                    },
                    {
                        "name": "Yunyao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yunyao Cheng"
                },
                "author": "Yunyao Cheng",
                "arxiv_comment": "20 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04115v1",
                "updated": "2025-05-07T04:14:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    14,
                    3,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T04:14:03Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    14,
                    3,
                    2,
                    127,
                    0
                ],
                "title": "Polynomial-Time Relational Probabilistic Inference in Open Universes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polynomial-Time Relational Probabilistic Inference in Open Universes"
                },
                "summary": "Reasoning under uncertainty is a fundamental challenge in Artificial\nIntelligence. As with most of these challenges, there is a harsh dilemma\nbetween the expressive power of the language used, and the tractability of the\ncomputational problem posed by reasoning. Inspired by human reasoning, we\nintroduce a method of first-order relational probabilistic inference that\nsatisfies both criteria, and can handle hybrid (discrete and continuous)\nvariables. Specifically, we extend sum-of-squares logic of expectation to\nrelational settings, demonstrating that lifted reasoning in the bounded-degree\nfragment for knowledge bases of bounded quantifier rank can be performed in\npolynomial time, even with an a priori unknown and/or countably infinite set of\nobjects. Crucially, our notion of tractability is framed in proof-theoretic\nterms, which extends beyond the syntactic properties of the language or\nqueries. We are able to derive the tightest bounds provable by proofs of a\ngiven degree and size and establish completeness in our sum-of-squares\nrefutations for fixed degrees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning under uncertainty is a fundamental challenge in Artificial\nIntelligence. As with most of these challenges, there is a harsh dilemma\nbetween the expressive power of the language used, and the tractability of the\ncomputational problem posed by reasoning. Inspired by human reasoning, we\nintroduce a method of first-order relational probabilistic inference that\nsatisfies both criteria, and can handle hybrid (discrete and continuous)\nvariables. Specifically, we extend sum-of-squares logic of expectation to\nrelational settings, demonstrating that lifted reasoning in the bounded-degree\nfragment for knowledge bases of bounded quantifier rank can be performed in\npolynomial time, even with an a priori unknown and/or countably infinite set of\nobjects. Crucially, our notion of tractability is framed in proof-theoretic\nterms, which extends beyond the syntactic properties of the language or\nqueries. We are able to derive the tightest bounds provable by proofs of a\ngiven degree and size and establish completeness in our sum-of-squares\nrefutations for fixed degrees."
                },
                "authors": [
                    {
                        "name": "Luise Ge"
                    },
                    {
                        "name": "Brendan Juba"
                    },
                    {
                        "name": "Kris Nilsson"
                    }
                ],
                "author_detail": {
                    "name": "Kris Nilsson"
                },
                "author": "Kris Nilsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05040v3",
                "updated": "2025-05-07T04:06:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    6,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-09T07:54:24Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04110v1",
                "updated": "2025-05-07T03:56:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    56,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T03:56:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    56,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "Alpha Excel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha Excel Benchmark"
                },
                "summary": "This study presents a novel benchmark for evaluating Large Language Models\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\nchallenges into programmatically evaluable JSON formats and use this dataset to\ncompare the performance of several leading LLMs. Our findings demonstrate\nsignificant variations in performance across different challenge categories,\nwith models showing specific strengths in pattern recognition tasks but\nstruggling with complex numerical reasoning. The benchmark provides a\nstandardized framework for assessing LLM capabilities in realistic\nbusiness-oriented tasks rather than abstract academic problems. This research\ncontributes to the growing field of AI benchmarking by establishing proficiency\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\nevaluation metric that bridges the gap between academic AI benchmarks and\npractical business applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel benchmark for evaluating Large Language Models\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\nchallenges into programmatically evaluable JSON formats and use this dataset to\ncompare the performance of several leading LLMs. Our findings demonstrate\nsignificant variations in performance across different challenge categories,\nwith models showing specific strengths in pattern recognition tasks but\nstruggling with complex numerical reasoning. The benchmark provides a\nstandardized framework for assessing LLM capabilities in realistic\nbusiness-oriented tasks rather than abstract academic problems. This research\ncontributes to the growing field of AI benchmarking by establishing proficiency\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\nevaluation metric that bridges the gap between academic AI benchmarks and\npractical business applications."
                },
                "authors": [
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Forrest McKee"
                    }
                ],
                "author_detail": {
                    "name": "Forrest McKee"
                },
                "author": "Forrest McKee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21123v2",
                "updated": "2025-05-07T03:48:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    48,
                    31,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-30T17:52:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation"
                },
                "summary": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns arise over their potential to generate verbatim training content with\ncopyrighted or private information. However, current protections against web\ncrawling or sample-specific memorization are inherently limited, as they\nrequire compliance from crawlers (e.g., respecting robots.txt) or model\ntrainers (e.g., applying differential privacy). To empower data owners with\ndirect control, we propose ExpShiled, a proactive self-defense mechanism that\nmitigates sample-specific memorization via imperceptible text perturbations.\nThis approach requires no external collaboration while maintaining original\nreadability. To evaluate individual-level defense efficacy, we first propose\nthe metric of instance exploitation: a zero value indicates perfect defense,\nachieved when a protected text's log-perplexity ranking aligns with its\ncounterfactual untrained ranking. We then reveal and validate the memorization\ntrigger hypothesis, demonstrating that a model's memorization of a specific\ntext sample stems primarily from its outlier tokens. Leveraging this insight,\nwe design targeted perturbations that (1) prioritize inherent trigger tokens\nand (2) introduce artificial trigger tokens as pitfalls to disrupt memorization\non the protected sample. Experiments validate our defense across model scales,\nlanguages, vision-to-language tasks, and fine-tuning methods. Even with privacy\nbackdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55,\nand instance exploitation approaches zero. This suggests that compared to the\nideal no-misuse scenario, the risk of exposing a text instance remains nearly\nunchanged despite its inclusion in training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns arise over their potential to generate verbatim training content with\ncopyrighted or private information. However, current protections against web\ncrawling or sample-specific memorization are inherently limited, as they\nrequire compliance from crawlers (e.g., respecting robots.txt) or model\ntrainers (e.g., applying differential privacy). To empower data owners with\ndirect control, we propose ExpShiled, a proactive self-defense mechanism that\nmitigates sample-specific memorization via imperceptible text perturbations.\nThis approach requires no external collaboration while maintaining original\nreadability. To evaluate individual-level defense efficacy, we first propose\nthe metric of instance exploitation: a zero value indicates perfect defense,\nachieved when a protected text's log-perplexity ranking aligns with its\ncounterfactual untrained ranking. We then reveal and validate the memorization\ntrigger hypothesis, demonstrating that a model's memorization of a specific\ntext sample stems primarily from its outlier tokens. Leveraging this insight,\nwe design targeted perturbations that (1) prioritize inherent trigger tokens\nand (2) introduce artificial trigger tokens as pitfalls to disrupt memorization\non the protected sample. Experiments validate our defense across model scales,\nlanguages, vision-to-language tasks, and fine-tuning methods. Even with privacy\nbackdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55,\nand instance exploitation approaches zero. This suggests that compared to the\nideal no-misuse scenario, the risk of exposing a text instance remains nearly\nunchanged despite its inclusion in training data."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.04623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04623v1",
                "updated": "2025-05-07T17:59:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:59:49Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    49,
                    2,
                    127,
                    0
                ],
                "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning"
                },
                "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research."
                },
                "authors": [
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04620v1",
                "updated": "2025-05-07T17:59:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:59:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Path to Multimodal Generalist: General-Level and General-Bench"
                },
                "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/"
                },
                "authors": [
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Qingshan Xu"
                    },
                    {
                        "name": "Bobo Li"
                    },
                    {
                        "name": "Shengqiong Wu"
                    },
                    {
                        "name": "Yaoting Wang"
                    },
                    {
                        "name": "Junbao Zhou"
                    },
                    {
                        "name": "Jiahao Meng"
                    },
                    {
                        "name": "Qingyu Shi"
                    },
                    {
                        "name": "Zhiyuan Zhou"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Weiming Wu"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Yaobo Ye"
                    },
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zixiang Meng"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Liyu Jia"
                    },
                    {
                        "name": "Wentao Hu"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Hanwang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanwang Zhang"
                },
                "author": "Hanwang Zhang",
                "arxiv_comment": "ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04619v1",
                "updated": "2025-05-07T17:59:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    28,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:59:28Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    28,
                    2,
                    127,
                    0
                ],
                "title": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation"
                },
                "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"
                },
                "authors": [
                    {
                        "name": "Abdulaziz Almuzairee"
                    },
                    {
                        "name": "Rohan Patil"
                    },
                    {
                        "name": "Dwait Bhatt"
                    },
                    {
                        "name": "Henrik I. Christensen"
                    }
                ],
                "author_detail": {
                    "name": "Henrik I. Christensen"
                },
                "author": "Henrik I. Christensen",
                "arxiv_comment": "For project website and code, see https://aalmuzairee.github.io/mad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15091v2",
                "updated": "2025-05-07T17:55:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    55,
                    19,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-22T17:40:16Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    17,
                    40,
                    16,
                    4,
                    327,
                    0
                ],
                "title": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting\n  Content Creators From AI Crawlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting\n  Content Creators From AI Crawlers"
                },
                "summary": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n203 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by critical hurdles in technical awareness, agency in\ndeploying them, and limited efficacy against unresponsive crawlers. We further\ntest and evaluate network-level crawler blockers provided by reverse proxies.\nDespite relatively limited deployment today, they offer stronger protections\nagainst AI crawlers, but still come with their own set of limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n203 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by critical hurdles in technical awareness, agency in\ndeploying them, and limited efficacy against unresponsive crawlers. We further\ntest and evaluate network-level crawler blockers provided by reverse proxies.\nDespite relatively limited deployment today, they offer stronger protections\nagainst AI crawlers, but still come with their own set of limitations."
                },
                "authors": [
                    {
                        "name": "Enze Liu"
                    },
                    {
                        "name": "Elisa Luo"
                    },
                    {
                        "name": "Shawn Shan"
                    },
                    {
                        "name": "Geoffrey M. Voelker"
                    },
                    {
                        "name": "Ben Y. Zhao"
                    },
                    {
                        "name": "Stefan Savage"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Savage"
                },
                "author": "Stefan Savage",
                "arxiv_doi": "10.1145/3730567.3732913",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3730567.3732913",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.15091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IMC 25. Please cite the conference version",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04608v1",
                "updated": "2025-05-07T17:53:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    53,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:53:47Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    53,
                    47,
                    2,
                    127,
                    0
                ],
                "title": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\n  Weighted-Conformal Martingales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\n  Weighted-Conformal Martingales"
                },
                "summary": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Drew Prinster"
                    },
                    {
                        "name": "Xing Han"
                    },
                    {
                        "name": "Anqi Liu"
                    },
                    {
                        "name": "Suchi Saria"
                    }
                ],
                "author_detail": {
                    "name": "Suchi Saria"
                },
                "author": "Suchi Saria",
                "arxiv_comment": "To be published in The International Conference on Machine Learning\n  (ICML), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04606v1",
                "updated": "2025-05-07T17:51:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    51,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:51:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    51,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution"
                },
                "summary": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Lianghong Guo"
                    },
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Runhan Jiang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Mingzhi Mao"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "To appear at ISSTA'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04601v1",
                "updated": "2025-05-07T17:48:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    48,
                    35,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:48:35Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    48,
                    35,
                    2,
                    127,
                    0
                ],
                "title": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning"
                },
                "summary": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments."
                },
                "authors": [
                    {
                        "name": "Xianhang Li"
                    },
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Hongru Zhu"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04594v2",
                "updated": "2025-05-08T06:18:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    6,
                    18,
                    31,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T17:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    37,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"
                },
                "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Girish Chandar Ganesan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04592v1",
                "updated": "2025-05-07T17:35:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    35,
                    36,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:35:36Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    35,
                    36,
                    2,
                    127,
                    0
                ],
                "title": "AI Governance to Avoid Extinction: The Strategic Landscape and\n  Actionable Research Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Governance to Avoid Extinction: The Strategic Landscape and\n  Actionable Research Questions"
                },
                "summary": "Humanity appears to be on course to soon develop AI systems that\nsubstantially outperform human experts in all cognitive domains and activities.\nWe believe the default trajectory has a high likelihood of catastrophe,\nincluding human extinction. Risks come from failure to control powerful AI\nsystems, misuse of AI by malicious rogue actors, war between great powers, and\nauthoritarian lock-in. This research agenda has two aims: to describe the\nstrategic landscape of AI development and to catalog important governance\nresearch questions. These questions, if answered, would provide important\ninsight on how to successfully reduce catastrophic risks.\n  We describe four high-level scenarios for the geopolitical response to\nadvanced AI development, cataloging the research questions most relevant to\neach. Our favored scenario involves building the technical, legal, and\ninstitutional infrastructure required to internationally restrict dangerous AI\ndevelopment and deployment (which we refer to as an Off Switch), which leads\ninto an internationally coordinated Halt on frontier AI activities at some\npoint in the future. The second scenario we describe is a US National Project\nfor AI, in which the US Government races to develop advanced AI systems and\nestablish unilateral control over global AI development. We also describe two\nadditional scenarios: a Light-Touch world similar to that of today and a Threat\nof Sabotage situation where countries use sabotage and deterrence to slow AI\ndevelopment.\n  In our view, apart from the Off Switch and Halt scenario, all of these\ntrajectories appear to carry an unacceptable risk of catastrophic harm. Urgent\naction is needed from the US National Security community and AI governance\necosystem to answer key research questions, build the capability to halt\ndangerous AI activities, and prepare for international AI agreements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity appears to be on course to soon develop AI systems that\nsubstantially outperform human experts in all cognitive domains and activities.\nWe believe the default trajectory has a high likelihood of catastrophe,\nincluding human extinction. Risks come from failure to control powerful AI\nsystems, misuse of AI by malicious rogue actors, war between great powers, and\nauthoritarian lock-in. This research agenda has two aims: to describe the\nstrategic landscape of AI development and to catalog important governance\nresearch questions. These questions, if answered, would provide important\ninsight on how to successfully reduce catastrophic risks.\n  We describe four high-level scenarios for the geopolitical response to\nadvanced AI development, cataloging the research questions most relevant to\neach. Our favored scenario involves building the technical, legal, and\ninstitutional infrastructure required to internationally restrict dangerous AI\ndevelopment and deployment (which we refer to as an Off Switch), which leads\ninto an internationally coordinated Halt on frontier AI activities at some\npoint in the future. The second scenario we describe is a US National Project\nfor AI, in which the US Government races to develop advanced AI systems and\nestablish unilateral control over global AI development. We also describe two\nadditional scenarios: a Light-Touch world similar to that of today and a Threat\nof Sabotage situation where countries use sabotage and deterrence to slow AI\ndevelopment.\n  In our view, apart from the Off Switch and Halt scenario, all of these\ntrajectories appear to carry an unacceptable risk of catastrophic harm. Urgent\naction is needed from the US National Security community and AI governance\necosystem to answer key research questions, build the capability to halt\ndangerous AI activities, and prepare for international AI agreements."
                },
                "authors": [
                    {
                        "name": "Peter Barnett"
                    },
                    {
                        "name": "Aaron Scher"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Scher"
                },
                "author": "Aaron Scher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04588v1",
                "updated": "2025-05-07T17:30:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    30,
                    22,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:30:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    30,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching"
                },
                "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Xuanbo Fan"
                    },
                    {
                        "name": "Yingyan Hou"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20984v2",
                "updated": "2025-05-07T17:26:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    26,
                    46,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-29T17:55:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    55,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "ACE: A Security Architecture for LLM-Integrated App Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACE: A Security Architecture for LLM-Integrated App Systems"
                },
                "summary": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."
                },
                "authors": [
                    {
                        "name": "Evan Li"
                    },
                    {
                        "name": "Tushin Mallick"
                    },
                    {
                        "name": "Evan Rose"
                    },
                    {
                        "name": "William Robertson"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Nita-Rotaru"
                },
                "author": "Cristina Nita-Rotaru",
                "arxiv_comment": "21 pages, 13 figures; clarify relation to indirect prompt injection\n  attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04584v1",
                "updated": "2025-05-07T17:24:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    24,
                    40,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T17:24:40Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    24,
                    40,
                    2,
                    127,
                    0
                ],
                "title": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for\n  Open-Ended Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for\n  Open-Ended Questions"
                },
                "summary": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback."
                },
                "authors": [
                    {
                        "name": "Chloe Qianhui Zhao"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Eason Chen"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    },
                    {
                        "name": "Jionghao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jionghao Lin"
                },
                "author": "Jionghao Lin",
                "arxiv_comment": "14 pages, to be published at the 26th International Conference on\n  Artificial Intelligence in Education (AIED '25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06661v2",
                "updated": "2025-05-07T16:57:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    57,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-09T17:00:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    17,
                    0,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion"
                },
                "summary": "Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality."
                },
                "authors": [
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Rongtao Deng"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Haibin Shen"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05820v2",
                "updated": "2025-05-07T16:55:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    55,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-10T09:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    9,
                    59,
                    46,
                    4,
                    10,
                    0
                ],
                "title": "User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays"
                },
                "summary": "Modular Arrays (MAs) are a promising architecture to enable multi-user\ncommunications in next-generation multiple-input multiple-output (MIMO) systems\nbased on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off\nimproved spatial resolution with characteristic interference patterns\nassociated with grating lobes. In this work, we analyze whether MAs can\noutperform conventional collocated deployments, in terms of achievable\nsum-spectral efficiency (SE) and served users in a multi-user downlink set-up.\nFirst, we provide a rigorous analytical characterization of the inter-user\ninterference for modular gMIMO systems operating in the near field. Then, we\nleverage these results to optimize the user selection and precoding mechanisms,\ndesigning two algorithms that largely outperform existing alternatives in the\nliterature, with different algorithmic complexities. Results show that the\nproposed algorithms yield over 70% improvements in achievable sum-spectral\nefficiencies compared to the state of the art. We also illustrate how MAs\nallows us to serve a larger number of users thanks to their improved spatial\nresolution, compared to the collocated counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Arrays (MAs) are a promising architecture to enable multi-user\ncommunications in next-generation multiple-input multiple-output (MIMO) systems\nbased on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off\nimproved spatial resolution with characteristic interference patterns\nassociated with grating lobes. In this work, we analyze whether MAs can\noutperform conventional collocated deployments, in terms of achievable\nsum-spectral efficiency (SE) and served users in a multi-user downlink set-up.\nFirst, we provide a rigorous analytical characterization of the inter-user\ninterference for modular gMIMO systems operating in the near field. Then, we\nleverage these results to optimize the user selection and precoding mechanisms,\ndesigning two algorithms that largely outperform existing alternatives in the\nliterature, with different algorithmic complexities. Results show that the\nproposed algorithms yield over 70% improvements in achievable sum-spectral\nefficiencies compared to the state of the art. We also illustrate how MAs\nallows us to serve a larger number of users thanks to their improved spatial\nresolution, compared to the collocated counterpart."
                },
                "authors": [
                    {
                        "name": "José P. González-Coma"
                    },
                    {
                        "name": "Santiago Fernández"
                    },
                    {
                        "name": "F. Javier López-Martínez"
                    }
                ],
                "author_detail": {
                    "name": "F. Javier López-Martínez"
                },
                "author": "F. Javier López-Martínez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v4",
                "updated": "2025-05-07T16:51:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    51,
                    33,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03161v2",
                "updated": "2025-05-07T16:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    4,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-06T04:14:13Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    13,
                    1,
                    126,
                    0
                ],
                "title": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks"
                },
                "summary": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community."
                },
                "authors": [
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Rushan Li"
                    },
                    {
                        "name": "Li Su"
                    },
                    {
                        "name": "Haitao Du"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Pengxuan Mao"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v3",
                "updated": "2025-05-07T16:03:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    3,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04529v1",
                "updated": "2025-05-07T16:02:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    2,
                    46,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:02:46Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    2,
                    46,
                    2,
                    127,
                    0
                ],
                "title": "RAFT: Robust Augmentation of FeaTures for Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAFT: Robust Augmentation of FeaTures for Image Segmentation"
                },
                "summary": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU."
                },
                "authors": [
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04521v1",
                "updated": "2025-05-07T15:52:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    52,
                    6,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:52:06Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    52,
                    6,
                    2,
                    127,
                    0
                ],
                "title": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development"
                },
                "summary": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches."
                },
                "authors": [
                    {
                        "name": "Kuen Sum Cheung"
                    },
                    {
                        "name": "Mayuri Kaul"
                    },
                    {
                        "name": "Gunel Jahangirova"
                    },
                    {
                        "name": "Mohammad Reza Mousavi"
                    },
                    {
                        "name": "Eric Zie"
                    }
                ],
                "author_detail": {
                    "name": "Eric Zie"
                },
                "author": "Eric Zie",
                "arxiv_doi": "10.1145/3711919.3728678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711919.3728678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04519v1",
                "updated": "2025-05-07T15:46:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    46,
                    36,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:46:36Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    46,
                    36,
                    2,
                    127,
                    0
                ],
                "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs"
                },
                "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference."
                },
                "authors": [
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Miao Rang"
                    },
                    {
                        "name": "Fangcheng Liu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Binghan Li"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Youliang Yan"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Botian Huang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Da Kuang"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jie Ran"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liang Dai"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Peifeng Qin"
                    },
                    {
                        "name": "Pengyuan Zeng"
                    },
                    {
                        "name": "Qiang Gu"
                    },
                    {
                        "name": "Shaohua Tang"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tao Gao"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Tianshu Li"
                    },
                    {
                        "name": "Tianyu Bi"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Weikai Mao"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xiabing Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Yangkai Du"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Yimeng Wu"
                    },
                    {
                        "name": "Yongbing Huang"
                    },
                    {
                        "name": "Yong Tian"
                    },
                    {
                        "name": "Yong Zhu"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yu Luo"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Yusen Sun"
                    },
                    {
                        "name": "Zelin Chen"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhicheng Liu"
                    },
                    {
                        "name": "Zhipeng Tu"
                    },
                    {
                        "name": "Zilin Ding"
                    },
                    {
                        "name": "Zongyuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Zongyuan Zhan"
                },
                "author": "Zongyuan Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21813v2",
                "updated": "2025-05-07T15:02:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    2,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-25T18:20:04Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    18,
                    20,
                    4,
                    1,
                    84,
                    0
                ],
                "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language\n  Model Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e.\nschema-matching) datasets in the Ontology Alignment Evaluation Initiative\n(OAEI), capturing hallucinations of different LLMs performing OM tasks. These\nOM-specific hallucinations are carefully classified into two primary categories\nand six sub-categories. We showcase the usefulness of the dataset in\nconstructing the LLM leaderboard and fine-tuning foundational LLMs for\nLLM-based OM systems."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangcheng Qiang"
                },
                "author": "Zhangcheng Qiang",
                "arxiv_comment": "15 pages, 4 figures, 5 tables, 2 prompt templates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04487v1",
                "updated": "2025-05-07T15:01:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    1,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T15:01:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    15,
                    1,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "A Design Space for the Critical Validation of LLM-Generated Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Design Space for the Critical Validation of LLM-Generated Tabular Data"
                },
                "summary": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power."
                },
                "authors": [
                    {
                        "name": "Madhav Sachdeva"
                    },
                    {
                        "name": "Christopher Narayanan"
                    },
                    {
                        "name": "Marvin Wiedenkeller"
                    },
                    {
                        "name": "Jana Sedlakova"
                    },
                    {
                        "name": "Jürgen Bernard"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Bernard"
                },
                "author": "Jürgen Bernard",
                "arxiv_comment": "To appear at the 16th International EuroVis Workshop on Visual\n  Analytics (EuroVA'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04481v1",
                "updated": "2025-05-07T14:52:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    52,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:52:02Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    52,
                    2,
                    2,
                    127,
                    0
                ],
                "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines."
                },
                "authors": [
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Weijian Ma"
                    },
                    {
                        "name": "Xueyang Li"
                    },
                    {
                        "name": "Yunzhong Lou"
                    },
                    {
                        "name": "Guichun Zhou"
                    },
                    {
                        "name": "Xiangdong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiangdong Zhou"
                },
                "author": "Xiangdong Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04480v1",
                "updated": "2025-05-07T14:51:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    51,
                    43,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:51:43Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    51,
                    43,
                    2,
                    127,
                    0
                ],
                "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution"
                },
                "summary": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo."
                },
                "authors": [
                    {
                        "name": "Zhikai Zhao"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07971v2",
                "updated": "2025-05-07T14:26:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    26,
                    9,
                    2,
                    127,
                    0
                ],
                "published": "2023-06-13T17:59:59Z",
                "published_parsed": [
                    2023,
                    6,
                    13,
                    17,
                    59,
                    59,
                    1,
                    164,
                    0
                ],
                "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models"
                },
                "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT."
                },
                "authors": [
                    {
                        "name": "Omkar Thawakar"
                    },
                    {
                        "name": "Abdelrahman Shaker"
                    },
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Jorma Laaksonen"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Accepted at ACL 2024-BIONLP Workshop. Code:\n  https://github.com/mbzuai-oryx/XrayGPT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04453v1",
                "updated": "2025-05-07T14:25:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    25,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:25:47Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    25,
                    47,
                    2,
                    127,
                    0
                ],
                "title": "Meta-Learning Driven Lightweight Phase Shift Compression for\n  IRS-Assisted Wireless Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Learning Driven Lightweight Phase Shift Compression for\n  IRS-Assisted Wireless Systems"
                },
                "summary": "The phase shift information (PSI) overhead poses a critical challenge to\nenabling real-time intelligent reflecting surface (IRS)-assisted wireless\nsystems, particularly under dynamic and resource-constrained conditions. In\nthis paper, we propose a lightweight PSI compression framework, termed\nmeta-learning-driven compression and reconstruction network (MCRNet). By\nleveraging a few-shot adaptation strategy via model-agnostic meta-learning\n(MAML), MCRNet enables rapid generalization across diverse IRS configurations\nwith minimal retraining overhead. Furthermore, a novel depthwise convolutional\ngating (DWCG) module is incorporated into the decoder to achieve adaptive local\nfeature modulation with low computational cost, significantly improving\ndecoding efficiency. Extensive simulations demonstrate that MCRNet achieves\ncompetitive normalized mean square error performance compared to\nstate-of-the-art baselines across various compression ratios, while\nsubstantially reducing model size and inference latency. These results validate\nthe effectiveness of the proposed asymmetric architecture and highlight the\npractical scalability and real-time applicability of MCRNet for dynamic\nIRS-assisted wireless deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The phase shift information (PSI) overhead poses a critical challenge to\nenabling real-time intelligent reflecting surface (IRS)-assisted wireless\nsystems, particularly under dynamic and resource-constrained conditions. In\nthis paper, we propose a lightweight PSI compression framework, termed\nmeta-learning-driven compression and reconstruction network (MCRNet). By\nleveraging a few-shot adaptation strategy via model-agnostic meta-learning\n(MAML), MCRNet enables rapid generalization across diverse IRS configurations\nwith minimal retraining overhead. Furthermore, a novel depthwise convolutional\ngating (DWCG) module is incorporated into the decoder to achieve adaptive local\nfeature modulation with low computational cost, significantly improving\ndecoding efficiency. Extensive simulations demonstrate that MCRNet achieves\ncompetitive normalized mean square error performance compared to\nstate-of-the-art baselines across various compression ratios, while\nsubstantially reducing model size and inference latency. These results validate\nthe effectiveness of the proposed asymmetric architecture and highlight the\npractical scalability and real-time applicability of MCRNet for dynamic\nIRS-assisted wireless deployments."
                },
                "authors": [
                    {
                        "name": "Xianhua Yu"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Bowen Gu"
                    },
                    {
                        "name": "Xiaoye Jing"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Tuo Wu"
                    },
                    {
                        "name": "Kan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kan Yu"
                },
                "author": "Kan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04445v1",
                "updated": "2025-05-07T14:14:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    14,
                    29,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:14:29Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    14,
                    29,
                    2,
                    127,
                    0
                ],
                "title": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation"
                },
                "summary": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec."
                },
                "authors": [
                    {
                        "name": "Qianru Zhang"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Honggang Wen"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Siu-Ming Yiu"
                    },
                    {
                        "name": "Nguyen Quoc Viet Hung"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04441v1",
                "updated": "2025-05-07T14:12:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    12,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:12:41Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    12,
                    41,
                    2,
                    127,
                    0
                ],
                "title": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs"
                },
                "summary": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mirazul Haque"
                    },
                    {
                        "name": "Petr Babkin"
                    },
                    {
                        "name": "Farima Farmahinifarahani"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04416v1",
                "updated": "2025-05-07T13:51:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    51,
                    42,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:51:42Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    51,
                    42,
                    2,
                    127,
                    0
                ],
                "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Minxin Du"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "18 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04406v1",
                "updated": "2025-05-07T13:42:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    42,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:42:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    42,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YABLoCo: Yet Another Benchmark for Long Context Code Generation"
                },
                "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++."
                },
                "authors": [
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Roman Garaev"
                    },
                    {
                        "name": "Vadim Lomshakov"
                    },
                    {
                        "name": "Irina Piontkovskaya"
                    },
                    {
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "name": "Israel Adewuyi"
                    }
                ],
                "author_detail": {
                    "name": "Israel Adewuyi"
                },
                "arxiv_affiliation": "Research Center of the Artificial Intelligence Institute, Innopolis University, Russia",
                "author": "Israel Adewuyi",
                "arxiv_comment": "Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04393v1",
                "updated": "2025-05-07T13:18:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:18:41Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    18,
                    41,
                    2,
                    127,
                    0
                ],
                "title": "Large Means Left: Political Bias in Large Language Models Increases with\n  Their Number of Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Means Left: Political Bias in Large Language Models Increases with\n  Their Number of Parameters"
                },
                "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale."
                },
                "authors": [
                    {
                        "name": "David Exler"
                    },
                    {
                        "name": "Mark Schutera"
                    },
                    {
                        "name": "Markus Reischl"
                    },
                    {
                        "name": "Luca Rettenberger"
                    }
                ],
                "author_detail": {
                    "name": "Luca Rettenberger"
                },
                "author": "Luca Rettenberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00290v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00290v4",
                "updated": "2025-05-07T13:13:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-01T03:18:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    18,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "Estimating LLM Uncertainty with Logits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating LLM Uncertainty with Logits"
                },
                "summary": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise."
                },
                "authors": [
                    {
                        "name": "Huan Ma"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Fixed some data errors in Table 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00290v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00290v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04388v1",
                "updated": "2025-05-07T13:13:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:13:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    13,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs"
                },
                "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Jordi Bayarri-Planas"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Enrique Lopez-Cuena"
                    },
                    {
                        "name": "Adrian Tormos"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Pablo Bernabeu-Perez"
                    },
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Pablo Agustin Martin-Torres"
                    },
                    {
                        "name": "Marta Gonzalez-Mallo"
                    },
                    {
                        "name": "Sergio Alvarez-Napagao"
                    },
                    {
                        "name": "Eduard Ayguadé-Parra"
                    },
                    {
                        "name": "Ulises Cortés"
                    }
                ],
                "author_detail": {
                    "name": "Ulises Cortés"
                },
                "author": "Ulises Cortés",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.01886",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04379v1",
                "updated": "2025-05-07T12:59:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    59,
                    59,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T12:59:59Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    59,
                    59,
                    2,
                    127,
                    0
                ],
                "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and\n  Performance in Mixed Urban Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and\n  Performance in Mixed Urban Traffic"
                },
                "summary": "Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis."
                },
                "authors": [
                    {
                        "name": "Mohammad Elayan"
                    },
                    {
                        "name": "Wissam Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Wissam Kontar"
                },
                "author": "Wissam Kontar",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09353v2",
                "updated": "2025-05-07T12:46:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    46,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-12T22:05:50Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    22,
                    5,
                    50,
                    5,
                    102,
                    0
                ],
                "title": "Breaking the Lens of the Telescope: Online Relevance Estimation over\n  Large Retrieval Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Lens of the Telescope: Online Relevance Estimation over\n  Large Retrieval Sets"
                },
                "summary": "Advanced relevance models, such as those that use large language models\n(LLMs), provide highly accurate relevance estimations. However, their\ncomputational costs make them infeasible for processing large document corpora.\nTo address this, retrieval systems often employ a telescoping approach, where\ncomputationally efficient but less precise lexical and semantic retrievers\nfilter potential candidates for further ranking. However, this approach heavily\ndepends on the quality of early-stage retrieval, which can potentially exclude\nrelevant documents early in the process. In this work, we propose a novel\nparadigm for re-ranking called online relevance estimation that continuously\nupdates relevance estimates for a query throughout the ranking process. Instead\nof re-ranking a fixed set of top-k documents in a single step, online relevance\nestimation iteratively re-scores smaller subsets of the most promising\ndocuments while adjusting relevance scores for the remaining pool based on the\nestimations from the final model using an online bandit-based algorithm. This\ndynamic process mitigates the recall limitations of telescoping systems by\nre-prioritizing documents initially deemed less relevant by earlier stages --\nincluding those completely excluded by earlier-stage retrievers. We validate\nour approach on TREC benchmarks under two scenarios: hybrid retrieval and\nadaptive retrieval. Experimental results demonstrate that our method is\nsample-efficient and significantly improves recall, highlighting the\neffectiveness of our online relevance estimation framework for modern search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced relevance models, such as those that use large language models\n(LLMs), provide highly accurate relevance estimations. However, their\ncomputational costs make them infeasible for processing large document corpora.\nTo address this, retrieval systems often employ a telescoping approach, where\ncomputationally efficient but less precise lexical and semantic retrievers\nfilter potential candidates for further ranking. However, this approach heavily\ndepends on the quality of early-stage retrieval, which can potentially exclude\nrelevant documents early in the process. In this work, we propose a novel\nparadigm for re-ranking called online relevance estimation that continuously\nupdates relevance estimates for a query throughout the ranking process. Instead\nof re-ranking a fixed set of top-k documents in a single step, online relevance\nestimation iteratively re-scores smaller subsets of the most promising\ndocuments while adjusting relevance scores for the remaining pool based on the\nestimations from the final model using an online bandit-based algorithm. This\ndynamic process mitigates the recall limitations of telescoping systems by\nre-prioritizing documents initially deemed less relevant by earlier stages --\nincluding those completely excluded by earlier-stage retrievers. We validate\nour approach on TREC benchmarks under two scenarios: hybrid retrieval and\nadaptive retrieval. Experimental results demonstrate that our method is\nsample-efficient and significantly improves recall, highlighting the\neffectiveness of our online relevance estimation framework for modern search\nsystems."
                },
                "authors": [
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "Accepted for publication at SIGIR'25 . 11 pages,5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16867v2",
                "updated": "2025-05-07T12:44:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    44,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2023-05-26T12:17:59Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    12,
                    17,
                    59,
                    4,
                    146,
                    0
                ],
                "title": "Playing repeated games with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing repeated games with Large Language Models"
                },
                "summary": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used in applications where they interact with humans\nand other agents. We propose to use behavioural game theory to study LLM's\ncooperation and coordination behaviour. We let different LLMs play finitely\nrepeated $2\\times2$ games with each other, with human-like strategies, and\nactual human players. Our results show that LLMs perform particularly well at\nself-interested games like the iterated Prisoner's Dilemma family. However,\nthey behave sub-optimally in games that require coordination, like the Battle\nof the Sexes. We verify that these behavioural signatures are stable across\nrobustness checks. We additionally show how GPT-4's behaviour can be modulated\nby providing additional information about its opponent and by using a \"social\nchain-of-thought\" (SCoT) strategy. This also leads to better scores and more\nsuccessful coordination when interacting with human players. These results\nenrich our understanding of LLM's social behaviour and pave the way for a\nbehavioural game theory for machines."
                },
                "authors": [
                    {
                        "name": "Elif Akata"
                    },
                    {
                        "name": "Lion Schulz"
                    },
                    {
                        "name": "Julian Coda-Forno"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v3",
                "updated": "2025-05-07T12:33:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    33,
                    27,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-based Bi-level Multi-interest Learning Framework for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Bi-level Multi-interest Learning Framework for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation (SR) leverages users' dynamic preferences, with\nrecent advances incorporating multi-interest learning to model diverse user\ninterests. However, most multi-interest SR models rely on noisy, sparse\nimplicit feedback, limiting recommendation accuracy. Large language models\n(LLMs) offer robust reasoning on low-quality data but face high computational\ncosts and latency challenges for SR integration. We propose a novel LLM-based\nmulti-interest SR framework combining implicit behavioral and explicit semantic\nperspectives. It includes two modules: the Implicit Behavioral Interest Module\n(IBIM), which learns from user behavior using a traditional SR model, and the\nExplicit Semantic Interest Module (ESIM), which uses clustering and\nprompt-engineered LLMs to extract semantic multi-interest representations from\ninformative samples. Semantic insights from ESIM enhance IBIM's behavioral\nrepresentations via modality alignment and semantic prediction tasks. During\ninference, only IBIM is used, ensuring efficient, LLM-free recommendations.\nExperiments on four real-world datasets validate the framework's effectiveness\nand practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) leverages users' dynamic preferences, with\nrecent advances incorporating multi-interest learning to model diverse user\ninterests. However, most multi-interest SR models rely on noisy, sparse\nimplicit feedback, limiting recommendation accuracy. Large language models\n(LLMs) offer robust reasoning on low-quality data but face high computational\ncosts and latency challenges for SR integration. We propose a novel LLM-based\nmulti-interest SR framework combining implicit behavioral and explicit semantic\nperspectives. It includes two modules: the Implicit Behavioral Interest Module\n(IBIM), which learns from user behavior using a traditional SR model, and the\nExplicit Semantic Interest Module (ESIM), which uses clustering and\nprompt-engineered LLMs to extract semantic multi-interest representations from\ninformative samples. Semantic insights from ESIM enhance IBIM's behavioral\nrepresentations via modality alignment and semantic prediction tasks. During\ninference, only IBIM is used, ensuring efficient, LLM-free recommendations.\nExperiments on four real-world datasets validate the framework's effectiveness\nand practicality."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04364v1",
                "updated": "2025-05-07T12:32:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T12:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLMs' Swarm intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Swarm intelligence"
                },
                "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Mowen Huang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00205v2",
                "updated": "2025-05-07T11:40:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    40,
                    20,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-31T22:46:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    46,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "EcoWeedNet: A Lightweight and Automated Weed Detection Method for\n  Sustainable Next-Generation Agricultural Consumer Electronics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoWeedNet: A Lightweight and Automated Weed Detection Method for\n  Sustainable Next-Generation Agricultural Consumer Electronics"
                },
                "summary": "Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies."
                },
                "authors": [
                    {
                        "name": "Omar H. Khater"
                    },
                    {
                        "name": "Abdul Jabbar Siddiqui"
                    },
                    {
                        "name": "M. Shamim Hossain"
                    },
                    {
                        "name": "Aiman El-Maleh"
                    }
                ],
                "author_detail": {
                    "name": "Aiman El-Maleh"
                },
                "author": "Aiman El-Maleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18884v2",
                "updated": "2025-05-07T11:31:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    31,
                    37,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T10:10:26Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    10,
                    10,
                    26,
                    5,
                    116,
                    0
                ],
                "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification"
                },
                "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."
                },
                "authors": [
                    {
                        "name": "Junichiro Niimi"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Niimi"
                },
                "author": "Junichiro Niimi",
                "arxiv_comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language \\& Information Systems (NLDB 2025) and will\n  appear in Springer Lecture Notes in Computer Science (LNCS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17667v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17667v2",
                "updated": "2025-05-07T11:25:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    25,
                    3,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-22T06:27:30Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    27,
                    30,
                    5,
                    81,
                    0
                ],
                "title": "DGAR: A Unified Domain Generalization Framework for RF-Enabled Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DGAR: A Unified Domain Generalization Framework for RF-Enabled Human\n  Activity Recognition"
                },
                "summary": "Radio-frequency (RF)-based human activity recognition (HAR) provides a\ncontactless and privacy-preserving solution for monitoring human behavior in\napplications such as personalized healthcare, ambient assisted living, and\ntelemedicine. However, real-world deployment is frequently challenged by domain\nshifts arising from inter-subject variability, heterogeneous physical\nenvironments, and unseen activity patterns, resulting in significant\nperformance degradation. To address this issue, we propose DGAR, a\ndomain-generalized activity recognition framework that learns transferable\nrepresentations without access to target-domain data. DGAR integrates\ninstance-adaptive feature modulation with cross-domain distribution alignment\nto enhance both personalization and generalization. Specifically, it\nincorporates a squeeze-and-excitation (SE) block to extract salient\nspatiotemporal features and employs correlation alignment to mitigate\ninter-domain discrepancies. Extensive experiments on three public RF-based\ndatasets -- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR\nconsistently outperforms state-of-the-art baselines, achieving up to a 5.81%\nimprovement in weighted F1-score. These results underscore DGAR's potential to\nenable generalizable, real-time RF sensing systems in dynamic and personalized\nhealthcare scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio-frequency (RF)-based human activity recognition (HAR) provides a\ncontactless and privacy-preserving solution for monitoring human behavior in\napplications such as personalized healthcare, ambient assisted living, and\ntelemedicine. However, real-world deployment is frequently challenged by domain\nshifts arising from inter-subject variability, heterogeneous physical\nenvironments, and unseen activity patterns, resulting in significant\nperformance degradation. To address this issue, we propose DGAR, a\ndomain-generalized activity recognition framework that learns transferable\nrepresentations without access to target-domain data. DGAR integrates\ninstance-adaptive feature modulation with cross-domain distribution alignment\nto enhance both personalization and generalization. Specifically, it\nincorporates a squeeze-and-excitation (SE) block to extract salient\nspatiotemporal features and employs correlation alignment to mitigate\ninter-domain discrepancies. Extensive experiments on three public RF-based\ndatasets -- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR\nconsistently outperforms state-of-the-art baselines, achieving up to a 5.81%\nimprovement in weighted F1-score. These results underscore DGAR's potential to\nenable generalizable, real-time RF sensing systems in dynamic and personalized\nhealthcare scenarios."
                },
                "authors": [
                    {
                        "name": "Junshuo Liu"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Robert C. Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Qiu"
                },
                "author": "Robert C. Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17667v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17667v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11178v2",
                "updated": "2025-05-07T10:18:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    18,
                    31,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-16T15:58:54Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    58,
                    54,
                    6,
                    47,
                    0
                ],
                "title": "DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection"
                },
                "summary": "Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization."
                },
                "authors": [
                    {
                        "name": "A. Enes Doruk"
                    },
                    {
                        "name": "Hasan F. Ates"
                    }
                ],
                "author_detail": {
                    "name": "Hasan F. Ates"
                },
                "author": "Hasan F. Ates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03783v3",
                "updated": "2025-05-07T10:09:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    9,
                    56,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-03T16:12:03Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    12,
                    3,
                    3,
                    93,
                    0
                ],
                "title": "FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training"
                },
                "summary": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget."
                },
                "authors": [
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Mathias Funk"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07547v2",
                "updated": "2025-05-07T10:08:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    8,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2024-10-10T02:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    39,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment"
                },
                "summary": "Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption."
                },
                "authors": [
                    {
                        "name": "Zecheng Hao"
                    },
                    {
                        "name": "Yifan Huang"
                    },
                    {
                        "name": "Zijie Xu"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Yuanhong Tang"
                    },
                    {
                        "name": "Zhaofei Yu"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04294v1",
                "updated": "2025-05-07T10:05:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    5,
                    23,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T10:05:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    5,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "Massive MIMO: Instantaneous versus Statistical CSI-Based Power\n  Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive MIMO: Instantaneous versus Statistical CSI-Based Power\n  Allocation"
                },
                "summary": "The deployment of instantaneous CSI-based power control schemes necessitates\ncomputationally intensive signal processing operations, requiring substantial\nresources to handle real-time CSI updates and the associated overhead.\nConversely, statistical CSIbased schemes enable efficient implementation of\nadvanced power allocation algorithms within large-scale massive MIMO (mMIMO)\nsystems, where the algorithms are updated much less frequently. Nevertheless,\nthese schemes may deviate from optimal results in certain practical mMIMO\nconfigurations, necessitating the adoption of instantaneous CSI-based schemes.\nIn addition, they may be limited in practical implementation where\ninstantaneous CSI-based resource allocation and management schemes are widely\nadopted. This lecture provides a comprehensive comparison between the\nstatistical CSI-based power allocation and instantaneous CSI-based power\nallocation designs for mMIMO systems from performance, complexity, and\npractical implementation aspects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of instantaneous CSI-based power control schemes necessitates\ncomputationally intensive signal processing operations, requiring substantial\nresources to handle real-time CSI updates and the associated overhead.\nConversely, statistical CSIbased schemes enable efficient implementation of\nadvanced power allocation algorithms within large-scale massive MIMO (mMIMO)\nsystems, where the algorithms are updated much less frequently. Nevertheless,\nthese schemes may deviate from optimal results in certain practical mMIMO\nconfigurations, necessitating the adoption of instantaneous CSI-based schemes.\nIn addition, they may be limited in practical implementation where\ninstantaneous CSI-based resource allocation and management schemes are widely\nadopted. This lecture provides a comprehensive comparison between the\nstatistical CSI-based power allocation and instantaneous CSI-based power\nallocation designs for mMIMO systems from performance, complexity, and\npractical implementation aspects."
                },
                "authors": [
                    {
                        "name": "Zahra Mobini"
                    },
                    {
                        "name": "Hien Quoc Ngo"
                    }
                ],
                "author_detail": {
                    "name": "Hien Quoc Ngo"
                },
                "author": "Hien Quoc Ngo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13184v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13184v3",
                "updated": "2025-05-07T10:00:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    10,
                    0,
                    50,
                    2,
                    127,
                    0
                ],
                "published": "2024-08-23T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    16,
                    2,
                    54,
                    4,
                    236,
                    0
                ],
                "title": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating\n  the Hallucination for Path Planning"
                },
                "summary": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning in Large Language Models (LLMs) is the foundation for\nembodied intelligence. However, even in simple maze environments, LLMs still\nencounter challenges in long-term path-planning, primarily influenced by their\nspatial hallucination and context inconsistency hallucination by long-term\nreasoning. To address this challenge, this study proposes an innovative model,\nSpatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To\naddress the spatial hallucination of LLMs, we propose the Spatial-to-Relational\napproach, which transforms spatial prompts into entity relations and paths\nrepresenting entity relation chains. This approach fully taps the potential of\nLLMs in terms of sequential thinking. As a result, we design a path-planning\nalgorithm based on Q-learning to mitigate the context inconsistency\nhallucination, which enhances the reasoning ability of LLMs. Using the Q-value\nof state-action as auxiliary information for prompts, we correct the\nhallucinations of LLMs, thereby guiding LLMs to learn the optimal path.\nFinally, we propose a reverse curriculum learning technique based on LLMs to\nfurther mitigate the context inconsistency hallucination. LLMs can rapidly\naccumulate successful experiences by reducing task difficulty and leveraging\nthem to tackle more complex tasks. We performed comprehensive experiments based\non Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our\nS2RCQL achieved a 23%--40% improvement in both success and optimality rates\ncompared with advanced prompt engineering."
                },
                "authors": [
                    {
                        "name": "Hourui Deng"
                    },
                    {
                        "name": "Hongjie Zhang"
                    },
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Chaosheng Feng"
                    }
                ],
                "author_detail": {
                    "name": "Chaosheng Feng"
                },
                "author": "Chaosheng Feng",
                "arxiv_comment": "Accepted by ICIC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13184v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13184v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04291v1",
                "updated": "2025-05-07T09:59:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    59,
                    36,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:59:36Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    59,
                    36,
                    2,
                    127,
                    0
                ],
                "title": "From Incidents to Insights: Patterns of Responsibility following AI\n  Harms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Incidents to Insights: Patterns of Responsibility following AI\n  Harms"
                },
                "summary": "The AI Incident Database was inspired by aviation safety databases, which\nenable collective learning from failures to prevent future incidents. The\ndatabase documents hundreds of AI failures, collected from the news and media.\nHowever, criticism highlights that the AIID's reliance on media reporting\nlimits its utility for learning about implementation failures. In this paper,\nwe accept that the AIID falls short in its original mission, but argue that by\nlooking beyond technically-focused learning, the dataset can provide new,\nhighly valuable insights: specifically, opportunities to learn about patterns\nbetween developers, deployers, victims, wider society, and law-makers that\nemerge after AI failures. Through a three-tier mixed-methods analysis of 962\nincidents and 4,743 related reports from the AIID, we examine patterns across\nincidents, focusing on cases with public responses tagged in the database. We\nidentify 'typical' incidents found in the AIID, from Tesla crashes to deepfake\nscams.\n  Focusing on this interplay between relevant parties, we uncover patterns in\naccountability and social expectations of responsibility. We find that the\npresence of identifiable responsible parties does not necessarily lead to\nincreased accountability. The likelihood of a response and what it amounts to\ndepends highly on context, including who built the technology, who was harmed,\nand to what extent. Controversy-rich incidents provide valuable data about\nsocietal reactions, including insights into social expectations. Equally\ninformative are cases where controversy is notably absent. This work shows that\nthe AIID's value lies not just in preventing technical failures, but in\ndocumenting patterns of harms and of institutional response and social learning\naround AI incidents. These patterns offer crucial insights for understanding\nhow society adapts to and governs emerging AI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Incident Database was inspired by aviation safety databases, which\nenable collective learning from failures to prevent future incidents. The\ndatabase documents hundreds of AI failures, collected from the news and media.\nHowever, criticism highlights that the AIID's reliance on media reporting\nlimits its utility for learning about implementation failures. In this paper,\nwe accept that the AIID falls short in its original mission, but argue that by\nlooking beyond technically-focused learning, the dataset can provide new,\nhighly valuable insights: specifically, opportunities to learn about patterns\nbetween developers, deployers, victims, wider society, and law-makers that\nemerge after AI failures. Through a three-tier mixed-methods analysis of 962\nincidents and 4,743 related reports from the AIID, we examine patterns across\nincidents, focusing on cases with public responses tagged in the database. We\nidentify 'typical' incidents found in the AIID, from Tesla crashes to deepfake\nscams.\n  Focusing on this interplay between relevant parties, we uncover patterns in\naccountability and social expectations of responsibility. We find that the\npresence of identifiable responsible parties does not necessarily lead to\nincreased accountability. The likelihood of a response and what it amounts to\ndepends highly on context, including who built the technology, who was harmed,\nand to what extent. Controversy-rich incidents provide valuable data about\nsocietal reactions, including insights into social expectations. Equally\ninformative are cases where controversy is notably absent. This work shows that\nthe AIID's value lies not just in preventing technical failures, but in\ndocumenting patterns of harms and of institutional response and social learning\naround AI incidents. These patterns offer crucial insights for understanding\nhow society adapts to and governs emerging AI technologies."
                },
                "authors": [
                    {
                        "name": "Isabel Richards"
                    },
                    {
                        "name": "Claire Benn"
                    },
                    {
                        "name": "Miri Zilka"
                    }
                ],
                "author_detail": {
                    "name": "Miri Zilka"
                },
                "author": "Miri Zilka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04284v1",
                "updated": "2025-05-07T09:40:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    40,
                    18,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:40:18Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    40,
                    18,
                    2,
                    127,
                    0
                ],
                "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance"
                },
                "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sofia Jamil"
                    },
                    {
                        "name": "Aryan Dabad"
                    },
                    {
                        "name": "Bollampalli Areen Reddy"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Rajiv Misra"
                    },
                    {
                        "name": "Adil A. Shakur"
                    }
                ],
                "author_detail": {
                    "name": "Adil A. Shakur"
                },
                "author": "Adil A. Shakur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18504v3",
                "updated": "2025-05-07T09:39:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    39,
                    42,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-30T17:13:32Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    17,
                    13,
                    32,
                    3,
                    30,
                    0
                ],
                "title": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction"
                },
                "summary": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision."
                },
                "authors": [
                    {
                        "name": "Peter J. Bentley"
                    },
                    {
                        "name": "Soo Ling Lim"
                    },
                    {
                        "name": "Fuyuki Ishikawa"
                    }
                ],
                "author_detail": {
                    "name": "Fuyuki Ishikawa"
                },
                "author": "Fuyuki Ishikawa",
                "arxiv_doi": "10.1145/3712256.3726317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712256.3726317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.18504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages plus 2 pages of supplemental material",
                "arxiv_journal_ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  2025 (GECCO 25). ACM, Malaga, Spain",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68W50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.6; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04281v1",
                "updated": "2025-05-07T09:35:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    35,
                    5,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:35:05Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    35,
                    5,
                    2,
                    127,
                    0
                ],
                "title": "TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement"
                },
                "summary": "This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\nnoisy images by constructing multiple virtual cameras based on a noise space.\nCamera Feature Integration (CFI) modules are then designed to enable the model\nto learn generalizable features across diverse virtual cameras. During the\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\nfine-tuned using a small amount of real RAW data to adapt to the noise\ncharacteristics of specific cameras. A structural reparameterization technique\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\nduring the diffusion process, a color corrector is introduced to ensure color\nconsistency by dynamically adjusting global color distributions. Additionally,\na novel dataset, QID, is constructed, featuring quantifiable illumination\nlevels and a wide dynamic range, providing a comprehensive benchmark for\ntraining and evaluation under extreme low-light conditions. Experimental\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\ngeneralization, and color consistency across various cameras and illumination\nlevels. These findings highlight the robustness and versatility of TS-Diff,\nmaking it a practical solution for low-light imaging applications. Source codes\nand models are available at https://github.com/CircccleK/TS-Diff",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\nnoisy images by constructing multiple virtual cameras based on a noise space.\nCamera Feature Integration (CFI) modules are then designed to enable the model\nto learn generalizable features across diverse virtual cameras. During the\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\nfine-tuned using a small amount of real RAW data to adapt to the noise\ncharacteristics of specific cameras. A structural reparameterization technique\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\nduring the diffusion process, a color corrector is introduced to ensure color\nconsistency by dynamically adjusting global color distributions. Additionally,\na novel dataset, QID, is constructed, featuring quantifiable illumination\nlevels and a wide dynamic range, providing a comprehensive benchmark for\ntraining and evaluation under extreme low-light conditions. Experimental\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\ngeneralization, and color consistency across various cameras and illumination\nlevels. These findings highlight the robustness and versatility of TS-Diff,\nmaking it a practical solution for low-light imaging applications. Source codes\nand models are available at https://github.com/CircccleK/TS-Diff"
                },
                "authors": [
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Jiangnan Xia"
                    },
                    {
                        "name": "Jianghan Cheng"
                    },
                    {
                        "name": "Qilong Wu"
                    },
                    {
                        "name": "Junwei Li"
                    },
                    {
                        "name": "Yibin Tian"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "arxiv_comment": "International Joint Conference on Neural Networks (IJCNN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18827v2",
                "updated": "2025-05-07T09:29:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    29,
                    45,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T07:29:12Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    29,
                    12,
                    5,
                    116,
                    0
                ],
                "title": "Test It Before You Trust It: Applying Software Testing for Trustworthy\n  In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test It Before You Trust It: Applying Software Testing for Trustworthy\n  In-context Learning"
                },
                "summary": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs."
                },
                "authors": [
                    {
                        "name": "Teeradaj Racharak"
                    },
                    {
                        "name": "Chaiyong Ragkhitwetsagul"
                    },
                    {
                        "name": "Chommakorn Sontesadisai"
                    },
                    {
                        "name": "Thanwadee Sunetnanta"
                    }
                ],
                "author_detail": {
                    "name": "Thanwadee Sunetnanta"
                },
                "author": "Thanwadee Sunetnanta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04265v1",
                "updated": "2025-05-07T09:14:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    14,
                    55,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:14:55Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    14,
                    55,
                    2,
                    127,
                    0
                ],
                "title": "Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper"
                },
                "summary": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly."
                },
                "authors": [
                    {
                        "name": "Abdulrahman S Almuhaidib"
                    },
                    {
                        "name": "Azlan Mohd Zain"
                    },
                    {
                        "name": "Zalmiyah Zakaria"
                    },
                    {
                        "name": "Izyan Izzati Kamsani"
                    },
                    {
                        "name": "Abdulaziz S Almuhaidib"
                    }
                ],
                "author_detail": {
                    "name": "Abdulaziz S Almuhaidib"
                },
                "author": "Abdulaziz S Almuhaidib",
                "arxiv_comment": "Pre-print - Accepted for publication in the Proceedings of the\n  International Computer Sciences and Informatics Conference (ICSIC-2024),\n  published by AIP Publishing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04260v1",
                "updated": "2025-05-07T09:10:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    10,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T09:10:51Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    9,
                    10,
                    51,
                    2,
                    127,
                    0
                ],
                "title": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering"
                },
                "summary": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces."
                },
                "authors": [
                    {
                        "name": "Jessica Y. Bo"
                    },
                    {
                        "name": "Tianyu Xu"
                    },
                    {
                        "name": "Ishan Chatterjee"
                    },
                    {
                        "name": "Katrina Passarella-Ward"
                    },
                    {
                        "name": "Achin Kulshrestha"
                    },
                    {
                        "name": "D Shin"
                    }
                ],
                "author_detail": {
                    "name": "D Shin"
                },
                "author": "D Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04254v1",
                "updated": "2025-05-07T08:59:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    59,
                    14,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:59:14Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    59,
                    14,
                    2,
                    127,
                    0
                ],
                "title": "CompileAgent: Automated Real-World Repo-Level Compilation with\n  Tool-Integrated LLM-based Agent System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompileAgent: Automated Real-World Repo-Level Compilation with\n  Tool-Integrated LLM-based Agent System"
                },
                "summary": "With open-source projects growing in size and complexity, manual compilation\nbecomes tedious and error-prone, highlighting the need for automation to\nimprove efficiency and accuracy. However, the complexity of compilation\ninstruction search and error resolution makes automatic compilation\nchallenging. Inspired by the success of LLM-based agents in various fields, we\npropose CompileAgent, the first LLM-based agent framework dedicated to\nrepo-level compilation. CompileAgent integrates five tools and a flow-based\nagent strategy, enabling interaction with software artifacts for compilation\ninstruction search and error resolution. To measure the effectiveness of our\nmethod, we design a public repo-level benchmark CompileAgentBench, and we also\ndesign two baselines for comparison by combining two compilation-friendly\nschemes. The performance on this benchmark shows that our method significantly\nimproves the compilation success rate, ranging from 10% to 71%. Meanwhile, we\nevaluate the performance of CompileAgent under different agent strategies and\nverify the effectiveness of the flow-based strategy. Additionally, we emphasize\nthe scalability of CompileAgent, further expanding its application prospects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With open-source projects growing in size and complexity, manual compilation\nbecomes tedious and error-prone, highlighting the need for automation to\nimprove efficiency and accuracy. However, the complexity of compilation\ninstruction search and error resolution makes automatic compilation\nchallenging. Inspired by the success of LLM-based agents in various fields, we\npropose CompileAgent, the first LLM-based agent framework dedicated to\nrepo-level compilation. CompileAgent integrates five tools and a flow-based\nagent strategy, enabling interaction with software artifacts for compilation\ninstruction search and error resolution. To measure the effectiveness of our\nmethod, we design a public repo-level benchmark CompileAgentBench, and we also\ndesign two baselines for comparison by combining two compilation-friendly\nschemes. The performance on this benchmark shows that our method significantly\nimproves the compilation success rate, ranging from 10% to 71%. Meanwhile, we\nevaluate the performance of CompileAgent under different agent strategies and\nverify the effectiveness of the flow-based strategy. Additionally, we emphasize\nthe scalability of CompileAgent, further expanding its application prospects."
                },
                "authors": [
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Benlong Wu"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Xu Zhu"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04253v1",
                "updated": "2025-05-07T08:58:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    58,
                    52,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:58:52Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    58,
                    52,
                    2,
                    127,
                    0
                ],
                "title": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Independent Adaptive RAG: Let the Question Speak for Itself"
                },
                "summary": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models~(LLMs) are prone to hallucinations, and\nRetrieval-Augmented Generation (RAG) helps mitigate this, but at a high\ncomputational cost while risking misinformation. Adaptive retrieval aims to\nretrieve only when necessary, but existing approaches rely on LLM-based\nuncertainty estimation, which remain inefficient and impractical. In this\nstudy, we introduce lightweight LLM-independent adaptive retrieval methods\nbased on external information. We investigated 27 features, organized into 7\ngroups, and their hybrid combinations. We evaluated these methods on 6 QA\ndatasets, assessing the QA performance and efficiency. The results show that\nour approach matches the performance of complex LLM-based methods while\nachieving significant efficiency gains, demonstrating the potential of external\ninformation for adaptive retrieval."
                },
                "authors": [
                    {
                        "name": "Maria Marina"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Viktor Moskvoretskii"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Moskvoretskii"
                },
                "author": "Viktor Moskvoretskii",
                "arxiv_comment": "11 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04251v1",
                "updated": "2025-05-07T08:55:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    55,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:55:15Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    55,
                    15,
                    2,
                    127,
                    0
                ],
                "title": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering"
                },
                "summary": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented."
                },
                "authors": [
                    {
                        "name": "Krishna Ronanki"
                    }
                ],
                "author_detail": {
                    "name": "Krishna Ronanki"
                },
                "author": "Krishna Ronanki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04209v1",
                "updated": "2025-05-07T08:03:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    3,
                    25,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:03:25Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    3,
                    25,
                    2,
                    127,
                    0
                ],
                "title": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase\n  Relevance at eBay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase\n  Relevance at eBay"
                },
                "summary": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics."
                },
                "authors": [
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Binbin Li"
                },
                "author": "Binbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03885v3",
                "updated": "2025-05-07T08:02:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    2,
                    44,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-06T09:01:24Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    9,
                    1,
                    24,
                    3,
                    37,
                    0
                ],
                "title": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM\n  with Optical Circuit Switching Transceivers"
                },
                "summary": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model (LLM) training relies on multi-dimensional\nparallelism, where High-Bandwidth Domains (HBDs) are critical for\ncommunication-intensive parallelism like Tensor Parallelism (TP) and Expert\nParallelism (EP). However, existing HBD architectures face fundamental\nlimitations in scalability, cost, and fault resiliency: switch-centric HBDs\n(e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g.,\nTPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such\nas TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches,\nbut the fault explosion radius remains large at the cube level (e.g., 64 TPUs).\n  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that\nunifies connectivity and dynamic switching at the transceiver level using\nOptical Circuit Switching (OCS). By embedding OCS within each transceiver,\nInfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing\nthe topology to adapt into variable-size rings. This design provides: i)\ndatacenter-wide scalability without cost explosion; ii) fault resilience by\nisolating failures to a single node, and iii) full bandwidth utilization for\nfault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based\nlow-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology\nco-designed with intra-/inter-node communication, and an HBD-DCN orchestration\nalgorithm maximizing GPU utilization while minimizing cross-ToR datacenter\nnetwork traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of\nthe cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude\nlower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault\nratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to\nNVIDIA DGX (8 GPUs per Node)."
                },
                "authors": [
                    {
                        "name": "Chenchen Shou"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Huaiyu Meng"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Wenqing Lv"
                    },
                    {
                        "name": "Yelong Xu"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yichen Shen"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04196v1",
                "updated": "2025-05-07T07:50:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    50,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:50:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    50,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "A Large Language Model for Feasible and Diverse Population Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model for Feasible and Diverse Population Synthesis"
                },
                "summary": "Generating a synthetic population that is both feasible and diverse is\ncrucial for ensuring the validity of downstream activity schedule simulation in\nactivity-based models (ABMs). While deep generative models (DGMs), such as\nvariational autoencoders and generative adversarial networks, have been applied\nto this task, they often struggle to balance the inclusion of rare but\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\nones (i.e., structural zeros). To improve feasibility while maintaining\ndiversity, we propose a fine-tuning method for large language models (LLMs)\nthat explicitly controls the autoregressive generation process through\ntopological orderings derived from a Bayesian Network (BN). Experimental\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\nour approach achieves approximately 95% feasibility, significantly higher than\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\nwell-suited for practical applications. Importantly, the method is based on a\nlightweight open-source LLM, enabling fine-tuning and inference on standard\npersonal computing environments. This makes the approach cost-effective and\nscalable for large-scale applications, such as synthesizing populations in\nmegacities, without relying on expensive infrastructure. By initiating the ABM\npipeline with high-quality synthetic populations, our method improves overall\nsimulation reliability and reduces downstream error propagation. The source\ncode for these methods is available for research and practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating a synthetic population that is both feasible and diverse is\ncrucial for ensuring the validity of downstream activity schedule simulation in\nactivity-based models (ABMs). While deep generative models (DGMs), such as\nvariational autoencoders and generative adversarial networks, have been applied\nto this task, they often struggle to balance the inclusion of rare but\nplausible combinations (i.e., sampling zeros) with the exclusion of implausible\nones (i.e., structural zeros). To improve feasibility while maintaining\ndiversity, we propose a fine-tuning method for large language models (LLMs)\nthat explicitly controls the autoregressive generation process through\ntopological orderings derived from a Bayesian Network (BN). Experimental\nresults show that our hybrid LLM-BN approach outperforms both traditional DGMs\nand proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,\nour approach achieves approximately 95% feasibility, significantly higher than\nthe ~80% observed in DGMs, while maintaining comparable diversity, making it\nwell-suited for practical applications. Importantly, the method is based on a\nlightweight open-source LLM, enabling fine-tuning and inference on standard\npersonal computing environments. This makes the approach cost-effective and\nscalable for large-scale applications, such as synthesizing populations in\nmegacities, without relying on expensive infrastructure. By initiating the ABM\npipeline with high-quality synthetic populations, our method improves overall\nsimulation reliability and reduces downstream error propagation. The source\ncode for these methods is available for research and practical application."
                },
                "authors": [
                    {
                        "name": "Sung Yoo Lim"
                    },
                    {
                        "name": "Hyunsoo Yun"
                    },
                    {
                        "name": "Prateek Bansal"
                    },
                    {
                        "name": "Dong-Kyu Kim"
                    },
                    {
                        "name": "Eui-Jin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Jin Kim"
                },
                "author": "Eui-Jin Kim",
                "arxiv_comment": "28 pages, 7 figures, 6 tables. Submitted to Transportation Research\n  Part C: Emerging Technologies. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04195v1",
                "updated": "2025-05-07T07:49:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    49,
                    5,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:49:05Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    49,
                    5,
                    2,
                    127,
                    0
                ],
                "title": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE\n  Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE\n  Vulnerabilities"
                },
                "summary": "Large Language Models (LLMs) have emerged as promising tools in software\ndevelopment, enabling automated code generation and analysis. However, their\nknowledge is limited to a fixed cutoff date, making them prone to generating\ncode vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets\nis costly, and existing LLM-based approaches focus on oversimplified CWE\nexamples and require providing explicit bug locations to LLMs, limiting their\nability to patch complex real-world vulnerabilities. To address these\nlimitations, we propose AutoPatch, a multi-agent framework designed to patch\nvulnerable LLM-generated code, particularly those introduced after the LLMs'\nknowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)\nwith a structured database of recently disclosed vulnerabilities, comprising\n525 code snippets derived from 75 high-severity CVEs across real-world systems\nsuch as the Linux kernel and Chrome. AutoPatch combines semantic and taint\nanalysis to identify the most relevant CVE and leverages enhanced\nChain-of-Thought (CoT) reasoning to construct enriched prompts for verification\nand patching. Our unified similarity model, which selects the most relevant\nvulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch\nattains 89.5 percent F1-score for vulnerability verification and 95.0 percent\naccuracy in patching, while being over 50x more cost-efficient than traditional\nfine-tuning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as promising tools in software\ndevelopment, enabling automated code generation and analysis. However, their\nknowledge is limited to a fixed cutoff date, making them prone to generating\ncode vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets\nis costly, and existing LLM-based approaches focus on oversimplified CWE\nexamples and require providing explicit bug locations to LLMs, limiting their\nability to patch complex real-world vulnerabilities. To address these\nlimitations, we propose AutoPatch, a multi-agent framework designed to patch\nvulnerable LLM-generated code, particularly those introduced after the LLMs'\nknowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)\nwith a structured database of recently disclosed vulnerabilities, comprising\n525 code snippets derived from 75 high-severity CVEs across real-world systems\nsuch as the Linux kernel and Chrome. AutoPatch combines semantic and taint\nanalysis to identify the most relevant CVE and leverages enhanced\nChain-of-Thought (CoT) reasoning to construct enriched prompts for verification\nand patching. Our unified similarity model, which selects the most relevant\nvulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch\nattains 89.5 percent F1-score for vulnerability verification and 95.0 percent\naccuracy in patching, while being over 50x more cost-efficient than traditional\nfine-tuning approaches."
                },
                "authors": [
                    {
                        "name": "Minjae Seo"
                    },
                    {
                        "name": "Wonwoo Choi"
                    },
                    {
                        "name": "Myoungsung You"
                    },
                    {
                        "name": "Seungwon Shin"
                    }
                ],
                "author_detail": {
                    "name": "Seungwon Shin"
                },
                "author": "Seungwon Shin",
                "arxiv_comment": "16 pages, single column, 7 figures. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01496v2",
                "updated": "2025-05-07T07:42:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    42,
                    11,
                    2,
                    127,
                    0
                ],
                "published": "2025-03-03T13:08:00Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    13,
                    8,
                    0,
                    0,
                    62,
                    0
                ],
                "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liger: Linearizing Large Language Models to Gated Recurrent Structures"
                },
                "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization."
                },
                "authors": [
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Accepted by ICML 2025, 15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18837v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18837v3",
                "updated": "2025-05-07T07:37:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    37,
                    47,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-26T07:48:35Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    7,
                    48,
                    35,
                    5,
                    116,
                    0
                ],
                "title": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment and Social Signals in the Climate Crisis: A Survey on\n  Analyzing Social Media Responses to Extreme Weather Events"
                },
                "summary": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events driven by climate change, such as wildfires, floods,\nand heatwaves, prompt significant public reactions on social media platforms.\nAnalyzing the sentiment expressed in these online discussions can offer\nvaluable insights into public perception, inform policy decisions, and enhance\nemergency responses. Although sentiment analysis has been widely studied in\nvarious fields, its specific application to climate-induced events,\nparticularly in real-time, high-impact situations like the 2025 Los Angeles\nforest fires, remains underexplored. In this survey, we thoroughly examine the\nmethods, datasets, challenges, and ethical considerations related to sentiment\nanalysis of social media content concerning weather and climate change events.\nWe present a detailed taxonomy of approaches, ranging from lexicon-based and\nmachine learning models to the latest strategies driven by large language\nmodels (LLMs). Additionally, we discuss data collection and annotation\ntechniques, including weak supervision and real-time event tracking. Finally,\nwe highlight several open problems, such as misinformation detection,\nmultimodal sentiment extraction, and model alignment with human values. Our\ngoal is to guide researchers and practitioners in effectively understanding\nsentiment during the climate crisis era."
                },
                "authors": [
                    {
                        "name": "Pouya Shaeri"
                    },
                    {
                        "name": "Yasaman Mohammadpour"
                    },
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Ariane Middel"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "13 Pages, 1 figure, Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18837v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18837v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04180v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04180v2",
                "updated": "2025-05-08T07:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    51,
                    26,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-07T07:25:46Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    25,
                    46,
                    2,
                    127,
                    0
                ],
                "title": "Towards Large-scale Generative Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Large-scale Generative Ranking"
                },
                "summary": "Generative recommendation has recently emerged as a promising paradigm in\ninformation retrieval. However, generative ranking systems are still\nunderstudied, particularly with respect to their effectiveness and feasibility\nin large-scale industrial settings. This paper investigates this topic at the\nranking stage of Xiaohongshu's Explore Feed, a recommender system that serves\nhundreds of millions of users. Specifically, we first examine how generative\nranking outperforms current industrial recommenders. Through theoretical and\nempirical analyses, we find that the primary improvement in effectiveness stems\nfrom the generative architecture, rather than the training paradigm. To\nfacilitate efficient deployment of generative ranking, we introduce GenRank, a\nnovel generative architecture for ranking. We validate the effectiveness and\nefficiency of our solution through online A/B experiments. The results show\nthat GenRank achieves significant improvements in user satisfaction with nearly\nequivalent computational resources compared to the existing production system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative recommendation has recently emerged as a promising paradigm in\ninformation retrieval. However, generative ranking systems are still\nunderstudied, particularly with respect to their effectiveness and feasibility\nin large-scale industrial settings. This paper investigates this topic at the\nranking stage of Xiaohongshu's Explore Feed, a recommender system that serves\nhundreds of millions of users. Specifically, we first examine how generative\nranking outperforms current industrial recommenders. Through theoretical and\nempirical analyses, we find that the primary improvement in effectiveness stems\nfrom the generative architecture, rather than the training paradigm. To\nfacilitate efficient deployment of generative ranking, we introduce GenRank, a\nnovel generative architecture for ranking. We validate the effectiveness and\nefficiency of our solution through online A/B experiments. The results show\nthat GenRank achieves significant improvements in user satisfaction with nearly\nequivalent computational resources compared to the existing production system."
                },
                "authors": [
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Xiong Cao"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Mingliang Qi"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Qingchang Han"
                    },
                    {
                        "name": "Yaowei Liu"
                    },
                    {
                        "name": "Zhaoyu Liu"
                    },
                    {
                        "name": "Xuefeng Yao"
                    },
                    {
                        "name": "Yuting Jia"
                    },
                    {
                        "name": "Leilei Ma"
                    },
                    {
                        "name": "Yinqi Zhang"
                    },
                    {
                        "name": "Taoyu Zhu"
                    },
                    {
                        "name": "Liujie Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Weihang Chen"
                    },
                    {
                        "name": "Min Zhu"
                    },
                    {
                        "name": "Ruiwen Xu"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04180v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04180v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18681v3",
                "updated": "2025-05-07T07:22:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    22,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2023-11-30T16:28:40Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    28,
                    40,
                    3,
                    334,
                    0
                ],
                "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance"
                },
                "summary": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog."
                },
                "authors": [
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Özsoy"
                    },
                    {
                        "name": "Benjamin Busam"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Matthias Keicher"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Keicher"
                },
                "author": "Matthias Keicher",
                "arxiv_comment": "Accepted for publication at MIDL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04176v1",
                "updated": "2025-05-07T07:08:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    8,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:08:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    8,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Developing Assessment Methods for Evaluating Learning Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Assessment Methods for Evaluating Learning Experience"
                },
                "summary": "This research aims to investigate the gender-based learning experiences of\nengineering students enrolled in the Probability and Statistics course,\nfocusing on the four different assessment methods employed namely direct\nconceptual learning (DCL), symposium, applied deployment and collaborative\nlearning. The study encompasses 299 engineering students, comprising 90 females\nand 209 males. Multivariate Analysis of Variance (MANOVA), is used to gain\ndeeper insights into the complex interplay between assessment methods and their\ninfluence on student learning. The results of the statistical analysis reveal\nthat there are significant differences in the learning outcomes between female\nand male engineering students in the assessment methods of direct conceptual\nlearning, symposium, and applied deployment. The findings suggest that there is\nno significant difference in the learning outcomes between female and male\nengineering students in the collaborative learning assessment method. The\ngraphical representation visually confirms the significant differences in\ndirect conceptual learning, symposium, and applied deployment, while\nillustrating no significant difference in collaborative learning between female\nand male engineering students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research aims to investigate the gender-based learning experiences of\nengineering students enrolled in the Probability and Statistics course,\nfocusing on the four different assessment methods employed namely direct\nconceptual learning (DCL), symposium, applied deployment and collaborative\nlearning. The study encompasses 299 engineering students, comprising 90 females\nand 209 males. Multivariate Analysis of Variance (MANOVA), is used to gain\ndeeper insights into the complex interplay between assessment methods and their\ninfluence on student learning. The results of the statistical analysis reveal\nthat there are significant differences in the learning outcomes between female\nand male engineering students in the assessment methods of direct conceptual\nlearning, symposium, and applied deployment. The findings suggest that there is\nno significant difference in the learning outcomes between female and male\nengineering students in the collaborative learning assessment method. The\ngraphical representation visually confirms the significant differences in\ndirect conceptual learning, symposium, and applied deployment, while\nillustrating no significant difference in collaborative learning between female\nand male engineering students."
                },
                "authors": [
                    {
                        "name": "Maneesha"
                    }
                ],
                "author_detail": {
                    "name": "Maneesha"
                },
                "author": "Maneesha",
                "arxiv_comment": "9 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04174v1",
                "updated": "2025-05-07T07:04:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    4,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T07:04:49Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    4,
                    49,
                    2,
                    127,
                    0
                ],
                "title": "On-Device LLM for Context-Aware Wi-Fi Roaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-Device LLM for Context-Aware Wi-Fi Roaming"
                },
                "summary": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems."
                },
                "authors": [
                    {
                        "name": "Ju-Hyung Lee"
                    },
                    {
                        "name": "Yanqing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanqing Lu"
                },
                "author": "Yanqing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04171v1",
                "updated": "2025-05-07T06:53:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    53,
                    59,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T06:53:59Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    53,
                    59,
                    2,
                    127,
                    0
                ],
                "title": "Large Language Models are often politically extreme, usually\n  ideologically inconsistent, and persuasive even in informational contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are often politically extreme, usually\n  ideologically inconsistent, and persuasive even in informational contexts"
                },
                "summary": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are a transformational technology, fundamentally\nchanging how people obtain information and interact with the world. As people\nbecome increasingly reliant on them for an enormous variety of tasks, a body of\nacademic research has developed to examine these models for inherent biases,\nespecially political biases, often finding them small. We challenge this\nprevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a\nnationally representative sample of U.S. voters, we show that LLMs' apparently\nsmall overall partisan preference is the net result of offsetting extreme views\non specific topics, much like moderate voters. Second, in a randomized\nexperiment, we show that LLMs can promulgate their preferences into political\npersuasiveness even in information-seeking contexts: voters randomized to\ndiscuss political issues with an LLM chatbot are as much as 5 percentage points\nmore likely to express the same preferences as that chatbot. Contrary to\nexpectations, these persuasive effects are not moderated by familiarity with\nLLMs, news consumption, or interest in politics. LLMs, especially those\ncontrolled by private companies or governments, may become a powerful and\ntargeted vector for political influence."
                },
                "authors": [
                    {
                        "name": "Nouar Aldahoul"
                    },
                    {
                        "name": "Hazem Ibrahim"
                    },
                    {
                        "name": "Matteo Varvello"
                    },
                    {
                        "name": "Aaron Kaufman"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Yasir Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Zaki"
                },
                "author": "Yasir Zaki",
                "arxiv_comment": "61 pages, 29 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06799v2",
                "updated": "2025-05-07T06:38:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    38,
                    55,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-09T11:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    45,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "Compatibility of Missing Data Handling Methods across the Stages of\n  Producing Clinical Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of Missing Data Handling Methods across the Stages of\n  Producing Clinical Prediction Models"
                },
                "summary": "Missing data is a challenge when developing, validating and deploying\nclinical prediction models (CPMs). Traditionally, decisions concerning missing\ndata handling during CPM development and validation havent accounted for\nwhether missingness is allowed at deployment. We hypothesised that the missing\ndata approach used during model development should optimise model performance\nupon deployment, whilst the approach used during model validation should yield\nunbiased predictive performance estimates upon deployment; we term this\ncompatibility. We aimed to determine which combinations of missing data\nhandling methods across the CPM life cycle are compatible. We considered\nscenarios where CPMs are intended to be deployed with missing data allowed or\nnot, and we evaluated the impact of that choice on earlier modelling decisions.\nThrough a simulation study and an empirical analysis of thoracic surgery data,\nwe compared CPMs developed and validated using combinations of complete case\nanalysis, mean imputation, single regression imputation, multiple imputation,\nand pattern sub-modelling. If planning to deploy a CPM without allowing missing\ndata, then development and validation should use multiple imputation when\nrequired. Where missingness is allowed at deployment, the same imputation\nmethod must be used during development and validation. Commonly used\ncombinations of missing data handling methods result in biased predictive\nperformance estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data is a challenge when developing, validating and deploying\nclinical prediction models (CPMs). Traditionally, decisions concerning missing\ndata handling during CPM development and validation havent accounted for\nwhether missingness is allowed at deployment. We hypothesised that the missing\ndata approach used during model development should optimise model performance\nupon deployment, whilst the approach used during model validation should yield\nunbiased predictive performance estimates upon deployment; we term this\ncompatibility. We aimed to determine which combinations of missing data\nhandling methods across the CPM life cycle are compatible. We considered\nscenarios where CPMs are intended to be deployed with missing data allowed or\nnot, and we evaluated the impact of that choice on earlier modelling decisions.\nThrough a simulation study and an empirical analysis of thoracic surgery data,\nwe compared CPMs developed and validated using combinations of complete case\nanalysis, mean imputation, single regression imputation, multiple imputation,\nand pattern sub-modelling. If planning to deploy a CPM without allowing missing\ndata, then development and validation should use multiple imputation when\nrequired. Where missingness is allowed at deployment, the same imputation\nmethod must be used during development and validation. Commonly used\ncombinations of missing data handling methods result in biased predictive\nperformance estimates."
                },
                "authors": [
                    {
                        "name": "Antonia Tsvetanova"
                    },
                    {
                        "name": "Matthew Sperrin"
                    },
                    {
                        "name": "David A. Jenkins"
                    },
                    {
                        "name": "Niels Peek"
                    },
                    {
                        "name": "Iain Buchan"
                    },
                    {
                        "name": "Stephanie Hyland"
                    },
                    {
                        "name": "Marcus Taylor"
                    },
                    {
                        "name": "Angela Wood"
                    },
                    {
                        "name": "Richard D. Riley"
                    },
                    {
                        "name": "Glen P. Martin"
                    }
                ],
                "author_detail": {
                    "name": "Glen P. Martin"
                },
                "author": "Glen P. Martin",
                "arxiv_comment": "40 pages, 8 figures (10 supplementary figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04152v1",
                "updated": "2025-05-07T06:03:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    3,
                    37,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T06:03:37Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    3,
                    37,
                    2,
                    127,
                    0
                ],
                "title": "Can Language Models Understand Social Behavior in Clinical\n  Conversations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Understand Social Behavior in Clinical\n  Conversations?"
                },
                "summary": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective communication between providers and their patients influences\nhealth and care outcomes. The effectiveness of such conversations has been\nlinked not only to the exchange of clinical information, but also to a range of\ninterpersonal behaviors; commonly referred to as social signals, which are\noften conveyed through non-verbal cues and shape the quality of the\npatient-provider relationship. Recent advances in large language models (LLMs)\nhave demonstrated an increasing ability to infer emotional and social behaviors\neven when analyzing only textual information. As automation increases also in\nclinical settings, such as for transcription of patient-provider conversations,\nthere is growing potential for LLMs to automatically analyze and extract social\nbehaviors from these interactions. To explore the foundational capabilities of\nLLMs in tracking social signals in clinical dialogue, we designed task-specific\nprompts and evaluated model performance across multiple architectures and\nprompting styles using a highly imbalanced, annotated dataset spanning 20\ndistinct social signals such as provider dominance, patient warmth, etc. We\npresent the first system capable of tracking all these 20 coded signals, and\nuncover patterns in LLM behavior. Further analysis of model configurations and\nclinical context provides insights for enhancing LLM performance on social\nsignal processing tasks in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Manas Satish Bedmutha"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Andrea Hartzler"
                    },
                    {
                        "name": "Trevor Cohen"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; H.1.2; I.2.7; I.2.m; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04148v1",
                "updated": "2025-05-07T05:58:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    58,
                    31,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:58:31Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    58,
                    31,
                    2,
                    127,
                    0
                ],
                "title": "Energy Efficient RSMA-Based LEO Satellite Communications Assisted by\n  UAV-Mounted BD-Active RIS: A DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Efficient RSMA-Based LEO Satellite Communications Assisted by\n  UAV-Mounted BD-Active RIS: A DRL Approach"
                },
                "summary": "This paper proposes an advanced non-terrestrial communication architecture\nthat integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal\nActive Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the\ncoverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a\ngroup-connected structure to enhance signal amplification and adaptability,\nwhile RSMA enables efficient multi-user access by dividing messages into common\nand private components. The system jointly optimizes satellite beamforming, UAV\npositioning, power allocation, and rate-splitting ratios to maximize the\noverall energy efficiency (EE). To solve the resulting non-convex and\nhigh-dimensional problem, we employ three state-of-the-art deep reinforcement\nlearning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin\nDelayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage\nActor-Critic (A3C). Moreover, realistic models for the power consumption of\nboth the UAV and the BD-ARIS are considered. Simulation results reveal that\nTRPO consistently achieves the best performance in terms of EE and sum rate,\nespecially under high transmit powers and challenging deployment scenarios. TD3\nconverges faster and performs competitively in moderate settings, while A3C\nsuffers from instability due to its high variance. Additionally, the robustness\nof each algorithm under channel state information (CSI) uncertainty is\nevaluated, confirming TRPO resilience to imperfect observations. Overall, the\nproposed RSMA-BD-ARIS framework significantly outperforms conventional\nRIS-assisted designs and provides a scalable, energy-efficient solution for 6G\nand massive IoT applications in non-terrestrial networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an advanced non-terrestrial communication architecture\nthat integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal\nActive Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the\ncoverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a\ngroup-connected structure to enhance signal amplification and adaptability,\nwhile RSMA enables efficient multi-user access by dividing messages into common\nand private components. The system jointly optimizes satellite beamforming, UAV\npositioning, power allocation, and rate-splitting ratios to maximize the\noverall energy efficiency (EE). To solve the resulting non-convex and\nhigh-dimensional problem, we employ three state-of-the-art deep reinforcement\nlearning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin\nDelayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage\nActor-Critic (A3C). Moreover, realistic models for the power consumption of\nboth the UAV and the BD-ARIS are considered. Simulation results reveal that\nTRPO consistently achieves the best performance in terms of EE and sum rate,\nespecially under high transmit powers and challenging deployment scenarios. TD3\nconverges faster and performs competitively in moderate settings, while A3C\nsuffers from instability due to its high variance. Additionally, the robustness\nof each algorithm under channel state information (CSI) uncertainty is\nevaluated, confirming TRPO resilience to imperfect observations. Overall, the\nproposed RSMA-BD-ARIS framework significantly outperforms conventional\nRIS-assisted designs and provides a scalable, energy-efficient solution for 6G\nand massive IoT applications in non-terrestrial networks."
                },
                "authors": [
                    {
                        "name": "Rahman Saadat Yeganeh"
                    },
                    {
                        "name": "Hamid Behroozi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Behroozi"
                },
                "author": "Hamid Behroozi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04146v1",
                "updated": "2025-05-07T05:54:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    54,
                    4,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:54:04Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    54,
                    4,
                    2,
                    127,
                    0
                ],
                "title": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation\n  Jailbreaking and LLM Content Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking the Canvas: A Dynamic Benchmark for Image Generation\n  Jailbreaking and LLM Content Safety"
                },
                "summary": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure."
                },
                "authors": [
                    {
                        "name": "Variath Madhupal Gautham Nair"
                    },
                    {
                        "name": "Vishal Varma Dantuluri"
                    }
                ],
                "author_detail": {
                    "name": "Vishal Varma Dantuluri"
                },
                "author": "Vishal Varma Dantuluri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04141v1",
                "updated": "2025-05-07T05:45:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    45,
                    33,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:45:33Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    45,
                    33,
                    2,
                    127,
                    0
                ],
                "title": "NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance"
                },
                "summary": "Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality."
                },
                "authors": [
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Yiannis Kantaros"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Kantaros"
                },
                "author": "Yiannis Kantaros",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04135v1",
                "updated": "2025-05-07T05:13:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    13,
                    15,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:13:15Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    13,
                    15,
                    2,
                    127,
                    0
                ],
                "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought\n  Prompting in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Granular Sentiment Classification with Chain-of-Thought\n  Prompting in Large Language Models"
                },
                "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the use of Chain-of-Thought (CoT) prompting with large language\nmodels (LLMs) to improve the accuracy of granular sentiment categorization in\napp store reviews. Traditional numeric and polarity-based ratings often fail to\ncapture the nuanced sentiment embedded in user feedback. We evaluated the\neffectiveness of CoT prompting versus simple prompting on 2000 Amazon app\nreviews by comparing each method's predictions to human judgements. CoT\nprompting improved classification accuracy from 84% to 93% highlighting the\nbenefit of explicit reasoning in enhancing sentiment analysis performance."
                },
                "authors": [
                    {
                        "name": "Vihaan Miriyala"
                    },
                    {
                        "name": "Smrithi Bukkapatnam"
                    },
                    {
                        "name": "Lavanya Prahallad"
                    }
                ],
                "author_detail": {
                    "name": "Lavanya Prahallad"
                },
                "author": "Lavanya Prahallad",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02116v3",
                "updated": "2025-05-07T04:50:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    50,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-04T14:29:28Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    28,
                    0,
                    309,
                    0
                ],
                "title": "Advancements and limitations of LLMs in replicating human color-word\n  associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements and limitations of LLMs in replicating human color-word\n  associations"
                },
                "summary": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations."
                },
                "authors": [
                    {
                        "name": "Makoto Fukushima"
                    },
                    {
                        "name": "Shusuke Eshita"
                    },
                    {
                        "name": "Hiroshige Fukuhara"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshige Fukuhara"
                },
                "author": "Hiroshige Fukuhara",
                "arxiv_comment": "20 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05040v3",
                "updated": "2025-05-07T04:06:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    4,
                    6,
                    41,
                    2,
                    127,
                    0
                ],
                "published": "2025-01-09T07:54:24Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    7,
                    54,
                    24,
                    3,
                    9,
                    0
                ],
                "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application of LLMs is in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietary LLMs, which limits reproducibility,\naccessibility, and transparency. The critical components of LLMs for addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source framework designed to effectively and efficiently resolve\nGitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval\nmodule and a code editing module. The retrieval module employs BM25 along with\na lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the\ncode editing module utilizes the other model to generate patches for the\nidentified files. To mitigate the lack of publicly available datasets, we\ncompile an extensive dataset that includes 110K GitHub issues along with their\ncorresponding patches and train the two models of SWE-Fixer separately. We\nassess our approach on the SWE-Bench Lite and Verified benchmarks, achieving\ncompetitive performance among open-source models with scores of 22.0% and\n30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on\nLite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally,\nour approach requires only two model calls per instance, making it\nsignificantly more efficient than existing methods. These results highlight the\neffectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make\nour model, dataset, and code publicly available at\nhttps://github.com/InternLM/SWE-Fixer."
                },
                "authors": [
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Our code, data, and model will be released at\n  https://github.com/InternLM/SWE-Fixer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04110v1",
                "updated": "2025-05-07T03:56:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    56,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T03:56:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    56,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "Alpha Excel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alpha Excel Benchmark"
                },
                "summary": "This study presents a novel benchmark for evaluating Large Language Models\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\nchallenges into programmatically evaluable JSON formats and use this dataset to\ncompare the performance of several leading LLMs. Our findings demonstrate\nsignificant variations in performance across different challenge categories,\nwith models showing specific strengths in pattern recognition tasks but\nstruggling with complex numerical reasoning. The benchmark provides a\nstandardized framework for assessing LLM capabilities in realistic\nbusiness-oriented tasks rather than abstract academic problems. This research\ncontributes to the growing field of AI benchmarking by establishing proficiency\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\nevaluation metric that bridges the gap between academic AI benchmarks and\npractical business applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel benchmark for evaluating Large Language Models\n(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)\nExcel competitions. We introduce a methodology for converting 113 existing FMWC\nchallenges into programmatically evaluable JSON formats and use this dataset to\ncompare the performance of several leading LLMs. Our findings demonstrate\nsignificant variations in performance across different challenge categories,\nwith models showing specific strengths in pattern recognition tasks but\nstruggling with complex numerical reasoning. The benchmark provides a\nstandardized framework for assessing LLM capabilities in realistic\nbusiness-oriented tasks rather than abstract academic problems. This research\ncontributes to the growing field of AI benchmarking by establishing proficiency\namong the 1.5 billion people who daily use Microsoft Excel as a meaningful\nevaluation metric that bridges the gap between academic AI benchmarks and\npractical business applications."
                },
                "authors": [
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Forrest McKee"
                    }
                ],
                "author_detail": {
                    "name": "Forrest McKee"
                },
                "author": "Forrest McKee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21123v2",
                "updated": "2025-05-07T03:48:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    48,
                    31,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-30T17:52:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation"
                },
                "summary": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns arise over their potential to generate verbatim training content with\ncopyrighted or private information. However, current protections against web\ncrawling or sample-specific memorization are inherently limited, as they\nrequire compliance from crawlers (e.g., respecting robots.txt) or model\ntrainers (e.g., applying differential privacy). To empower data owners with\ndirect control, we propose ExpShiled, a proactive self-defense mechanism that\nmitigates sample-specific memorization via imperceptible text perturbations.\nThis approach requires no external collaboration while maintaining original\nreadability. To evaluate individual-level defense efficacy, we first propose\nthe metric of instance exploitation: a zero value indicates perfect defense,\nachieved when a protected text's log-perplexity ranking aligns with its\ncounterfactual untrained ranking. We then reveal and validate the memorization\ntrigger hypothesis, demonstrating that a model's memorization of a specific\ntext sample stems primarily from its outlier tokens. Leveraging this insight,\nwe design targeted perturbations that (1) prioritize inherent trigger tokens\nand (2) introduce artificial trigger tokens as pitfalls to disrupt memorization\non the protected sample. Experiments validate our defense across model scales,\nlanguages, vision-to-language tasks, and fine-tuning methods. Even with privacy\nbackdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55,\nand instance exploitation approaches zero. This suggests that compared to the\nideal no-misuse scenario, the risk of exposing a text instance remains nearly\nunchanged despite its inclusion in training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns arise over their potential to generate verbatim training content with\ncopyrighted or private information. However, current protections against web\ncrawling or sample-specific memorization are inherently limited, as they\nrequire compliance from crawlers (e.g., respecting robots.txt) or model\ntrainers (e.g., applying differential privacy). To empower data owners with\ndirect control, we propose ExpShiled, a proactive self-defense mechanism that\nmitigates sample-specific memorization via imperceptible text perturbations.\nThis approach requires no external collaboration while maintaining original\nreadability. To evaluate individual-level defense efficacy, we first propose\nthe metric of instance exploitation: a zero value indicates perfect defense,\nachieved when a protected text's log-perplexity ranking aligns with its\ncounterfactual untrained ranking. We then reveal and validate the memorization\ntrigger hypothesis, demonstrating that a model's memorization of a specific\ntext sample stems primarily from its outlier tokens. Leveraging this insight,\nwe design targeted perturbations that (1) prioritize inherent trigger tokens\nand (2) introduce artificial trigger tokens as pitfalls to disrupt memorization\non the protected sample. Experiments validate our defense across model scales,\nlanguages, vision-to-language tasks, and fine-tuning methods. Even with privacy\nbackdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55,\nand instance exploitation approaches zero. This suggests that compared to the\nideal no-misuse scenario, the risk of exposing a text instance remains nearly\nunchanged despite its inclusion in training data."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09604v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09604v2",
                "updated": "2025-05-07T03:39:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    39,
                    30,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-13T14:42:03Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    42,
                    3,
                    6,
                    103,
                    0
                ],
                "title": "Mitigating Many-Shot Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Many-Shot Jailbreaking"
                },
                "summary": "Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the\nlong context windows of modern LLMs to circumvent model safety training by\nincluding in the prompt many examples of a \"fake\" assistant responding\ninappropriately before the final request. With enough examples, the model's\nin-context learning abilities override its safety training, and it responds as\nif it were the \"fake\" assistant. In this work, we probe the effectiveness of\ndifferent fine-tuning and input sanitization approaches on mitigating MSJ\nattacks, alone and in combination. We find incremental mitigation effectiveness\nfor each, and show that the combined techniques significantly reduce the\neffectiveness of MSJ attacks, while retaining model performance in benign\nin-context learning and conversational tasks. We suggest that our approach\ncould meaningfully ameliorate this vulnerability if incorporated into model\nsafety post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the\nlong context windows of modern LLMs to circumvent model safety training by\nincluding in the prompt many examples of a \"fake\" assistant responding\ninappropriately before the final request. With enough examples, the model's\nin-context learning abilities override its safety training, and it responds as\nif it were the \"fake\" assistant. In this work, we probe the effectiveness of\ndifferent fine-tuning and input sanitization approaches on mitigating MSJ\nattacks, alone and in combination. We find incremental mitigation effectiveness\nfor each, and show that the combined techniques significantly reduce the\neffectiveness of MSJ attacks, while retaining model performance in benign\nin-context learning and conversational tasks. We suggest that our approach\ncould meaningfully ameliorate this vulnerability if incorporated into model\nsafety post-training."
                },
                "authors": [
                    {
                        "name": "Christopher M. Ackerman"
                    },
                    {
                        "name": "Nina Panickssery"
                    }
                ],
                "author_detail": {
                    "name": "Nina Panickssery"
                },
                "author": "Nina Panickssery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09604v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09604v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04101v1",
                "updated": "2025-05-07T03:37:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    37,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T03:37:49Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    37,
                    49,
                    2,
                    127,
                    0
                ],
                "title": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' Suitability for Network Security: A Case Study of STRIDE Threat\n  Modeling"
                },
                "summary": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases."
                },
                "authors": [
                    {
                        "name": "AbdulAziz AbdulGhaffar"
                    },
                    {
                        "name": "Ashraf Matrawy"
                    }
                ],
                "author_detail": {
                    "name": "Ashraf Matrawy"
                },
                "author": "Ashraf Matrawy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.15022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.15022v3",
                "updated": "2025-05-07T03:31:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    31,
                    8,
                    2,
                    127,
                    0
                ],
                "published": "2023-08-29T04:59:53Z",
                "published_parsed": [
                    2023,
                    8,
                    29,
                    4,
                    59,
                    53,
                    1,
                    241,
                    0
                ],
                "title": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models"
                },
                "summary": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later."
                },
                "authors": [
                    {
                        "name": "Qingyue Wang"
                    },
                    {
                        "name": "Yanhe Fu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.15022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.15022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11026v2",
                "updated": "2025-05-07T03:14:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    14,
                    53,
                    2,
                    127,
                    0
                ],
                "published": "2024-12-15T02:41:31Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    41,
                    31,
                    6,
                    350,
                    0
                ],
                "title": "SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph\n  Generation"
                },
                "summary": "Dynamic scenes contain intricate spatio-temporal information, crucial for\nmobile robots, UAVs, and autonomous driving systems to make informed decisions.\nParsing these scenes into semantic triplets <Subject-Predicate-Object> for\naccurate Scene Graph Generation (SGG) is highly challenging due to the\nfluctuating spatio-temporal complexity. Inspired by the reasoning capabilities\nof Large Language Models (LLMs), we propose SceneLLM, a novel framework that\nleverages LLMs as powerful scene analyzers for dynamic SGG. Our framework\nintroduces a Video-to-Language (V2L) mapping module that transforms video\nframes into linguistic signals (scene tokens), making the input more\ncomprehensible for LLMs. To better encode spatial information, we devise a\nSpatial Information Aggregation (SIA) scheme, inspired by the structure of\nChinese characters, which encodes spatial data into tokens. Using Optimal\nTransport (OT), we generate an implicit language signal from the frame-level\ntoken sequence that captures the video's spatio-temporal information. To\nfurther improve the LLM's ability to process this implicit linguistic input, we\napply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a\ntransformer-based SGG predictor to decode the LLM's reasoning and predict\nsemantic triplets. Our method achieves state-of-the-art results on the Action\nGenome (AG) benchmark, and extensive experiments show the effectiveness of\nSceneLLM in understanding and generating accurate dynamic scene graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic scenes contain intricate spatio-temporal information, crucial for\nmobile robots, UAVs, and autonomous driving systems to make informed decisions.\nParsing these scenes into semantic triplets <Subject-Predicate-Object> for\naccurate Scene Graph Generation (SGG) is highly challenging due to the\nfluctuating spatio-temporal complexity. Inspired by the reasoning capabilities\nof Large Language Models (LLMs), we propose SceneLLM, a novel framework that\nleverages LLMs as powerful scene analyzers for dynamic SGG. Our framework\nintroduces a Video-to-Language (V2L) mapping module that transforms video\nframes into linguistic signals (scene tokens), making the input more\ncomprehensible for LLMs. To better encode spatial information, we devise a\nSpatial Information Aggregation (SIA) scheme, inspired by the structure of\nChinese characters, which encodes spatial data into tokens. Using Optimal\nTransport (OT), we generate an implicit language signal from the frame-level\ntoken sequence that captures the video's spatio-temporal information. To\nfurther improve the LLM's ability to process this implicit linguistic input, we\napply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a\ntransformer-based SGG predictor to decode the LLM's reasoning and predict\nsemantic triplets. Our method achieves state-of-the-art results on the Action\nGenome (AG) benchmark, and extensive experiments show the effectiveness of\nSceneLLM in understanding and generating accurate dynamic scene graphs."
                },
                "authors": [
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Zhuoling Li"
                    },
                    {
                        "name": "Jun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Liu"
                },
                "author": "Jun Liu",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19346v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19346v4",
                "updated": "2025-05-07T03:14:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    14,
                    49,
                    2,
                    127,
                    0
                ],
                "published": "2024-03-28T12:04:28Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    12,
                    4,
                    28,
                    3,
                    88,
                    0
                ],
                "title": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Struggle to Cope with Unreasonability in Math\n  Problems"
                },
                "summary": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research have demonstrated LLMs' impressive performance in math and\nreasoning. However, the capacity of LLMs to address math problems under\nunconventional conditions, such as internal inconsistencies and flawed\nassumptions, remains largely unexplored. In this paper, we propose a novel\nbenchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to\nrecognize and respond to unreasonability in math problem. The benchmark\nconsists of a carefully curated collection of unreasonable math questions\nacross diverse types. Based on extensive experiments covering 19 LLMs, we\nobserve that even state-of-the-art models such as GPT-4o achieve only limited\nperformance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone\nto overthinking and unstable. We further explore strategies for improving the\nrecognition of unreasonable inputs, shedding light on both the possibility and\nlimitations of LLMs in this challenging setting."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Zihang Yuan"
                    },
                    {
                        "name": "Rui li"
                    },
                    {
                        "name": "Weilin Luo"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "32 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19346v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19346v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04084v1",
                "updated": "2025-05-07T02:51:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    51,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T02:51:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    51,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "An Empirical Study of OpenAI API Discussions on Stack Overflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of OpenAI API Discussions on Stack Overflow"
                },
                "summary": "The rapid advancement of large language models (LLMs), represented by\nOpenAI's GPT series, has significantly impacted various domains such as natural\nlanguage processing, software development, education, healthcare, finance, and\nscientific research. However, OpenAI APIs introduce unique challenges that\ndiffer from traditional APIs, such as the complexities of prompt engineering,\ntoken-based cost management, non-deterministic outputs, and operation as black\nboxes. To the best of our knowledge, the challenges developers encounter when\nusing OpenAI APIs have not been explored in previous empirical studies. To fill\nthis gap, we conduct the first comprehensive empirical study by analyzing 2,874\nOpenAI API-related discussions from the popular Q&A forum Stack Overflow. We\nfirst examine the popularity and difficulty of these posts. After manually\ncategorizing them into nine OpenAI API-related categories, we identify specific\nchallenges associated with each category through topic modeling analysis. Based\non our empirical findings, we finally propose actionable implications for\ndevelopers, LLM vendors, and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs), represented by\nOpenAI's GPT series, has significantly impacted various domains such as natural\nlanguage processing, software development, education, healthcare, finance, and\nscientific research. However, OpenAI APIs introduce unique challenges that\ndiffer from traditional APIs, such as the complexities of prompt engineering,\ntoken-based cost management, non-deterministic outputs, and operation as black\nboxes. To the best of our knowledge, the challenges developers encounter when\nusing OpenAI APIs have not been explored in previous empirical studies. To fill\nthis gap, we conduct the first comprehensive empirical study by analyzing 2,874\nOpenAI API-related discussions from the popular Q&A forum Stack Overflow. We\nfirst examine the popularity and difficulty of these posts. After manually\ncategorizing them into nine OpenAI API-related categories, we identify specific\nchallenges associated with each category through topic modeling analysis. Based\non our empirical findings, we finally propose actionable implications for\ndevelopers, LLM vendors, and researchers."
                },
                "authors": [
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jibin Wang"
                    },
                    {
                        "name": "Chaoyang Gao"
                    },
                    {
                        "name": "Xiaolin Ju"
                    },
                    {
                        "name": "Zhanqi Cui"
                    }
                ],
                "author_detail": {
                    "name": "Zhanqi Cui"
                },
                "author": "Zhanqi Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21043v2",
                "updated": "2025-05-07T02:31:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    31,
                    34,
                    2,
                    127,
                    0
                ],
                "published": "2025-04-28T14:14:16Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    14,
                    14,
                    16,
                    0,
                    118,
                    0
                ],
                "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code\n  Generation in Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeBC: A More Secure Large Language Model for Smart Contract Code\n  Generation in Blockchain"
                },
                "summary": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code."
                },
                "authors": [
                    {
                        "name": "Lingxiang Wang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Qinnan Zhang"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Hongwei Zheng"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04075v1",
                "updated": "2025-05-07T02:26:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    26,
                    17,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T02:26:17Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    26,
                    17,
                    2,
                    127,
                    0
                ],
                "title": "LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?"
                },
                "summary": "This paper examines whether large language model (LLM) capabilities can\ncontinue to advance without additional compute by analyzing the development and\nrole of algorithms used in state-of-the-art LLMs. Motivated by regulatory\nefforts that have largely focused on restricting access to high-performance\nhardware, we ask: Can LLMs progress in a compute-constrained environment, and\nhow do algorithmic innovations perform under such conditions?\n  To address these questions, we introduce a novel classification framework\nthat distinguishes between compute-dependent innovations -- which yield\ndisproportionate benefits at high compute levels (e.g., the Transformer\narchitecture and mixture-of-experts models) and compute-independent\ninnovations, which improve efficiency across all compute scales (e.g., rotary\npositional encoding, FlashAttention, or layer normalization). We quantify these\ncontributions using a metric called compute-equivalent gain (CEG), which\nestimates the additional compute that would be required to achieve similar\nimprovements without these algorithmic advancements.\n  To validate this framework, we conduct small-scale training experiments with\na scaled-down GPT-2 model. Our results confirm that compute-independent\nadvancements yield meaningful performance gains even in resource-constrained\nsettings, with a CEG of up to $3.5\\times$ over a baseline model. By contrast,\ncompute-dependent advancements provided little benefit or even degraded\nperformance at the small scale, reinforcing the importance of compute\navailability for certain algorithmic gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines whether large language model (LLM) capabilities can\ncontinue to advance without additional compute by analyzing the development and\nrole of algorithms used in state-of-the-art LLMs. Motivated by regulatory\nefforts that have largely focused on restricting access to high-performance\nhardware, we ask: Can LLMs progress in a compute-constrained environment, and\nhow do algorithmic innovations perform under such conditions?\n  To address these questions, we introduce a novel classification framework\nthat distinguishes between compute-dependent innovations -- which yield\ndisproportionate benefits at high compute levels (e.g., the Transformer\narchitecture and mixture-of-experts models) and compute-independent\ninnovations, which improve efficiency across all compute scales (e.g., rotary\npositional encoding, FlashAttention, or layer normalization). We quantify these\ncontributions using a metric called compute-equivalent gain (CEG), which\nestimates the additional compute that would be required to achieve similar\nimprovements without these algorithmic advancements.\n  To validate this framework, we conduct small-scale training experiments with\na scaled-down GPT-2 model. Our results confirm that compute-independent\nadvancements yield meaningful performance gains even in resource-constrained\nsettings, with a CEG of up to $3.5\\times$ over a baseline model. By contrast,\ncompute-dependent advancements provided little benefit or even degraded\nperformance at the small scale, reinforcing the importance of compute\navailability for certain algorithmic gains."
                },
                "authors": [
                    {
                        "name": "Teddy Foley"
                    },
                    {
                        "name": "Spencer Guo"
                    },
                    {
                        "name": "Henry Josephson"
                    },
                    {
                        "name": "Anqi Qu"
                    },
                    {
                        "name": "Jack Sanderson"
                    }
                ],
                "author_detail": {
                    "name": "Jack Sanderson"
                },
                "author": "Jack Sanderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04073v1",
                "updated": "2025-05-07T02:25:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    25,
                    29,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T02:25:29Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    25,
                    29,
                    2,
                    127,
                    0
                ],
                "title": "Natural Language Generation in Healthcare: A Review of Methods and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation in Healthcare: A Review of Methods and\n  Applications"
                },
                "summary": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language generation (NLG) is the key technology to achieve generative\nartificial intelligence (AI). With the breakthroughs in large language models\n(LLMs), NLG has been widely used in various medical applications, demonstrating\nthe potential to enhance clinical workflows, support clinical decision-making,\nand improve clinical documentation. Heterogeneous and diverse medical data\nmodalities, such as medical text, images, and knowledge bases, are utilized in\nNLG. Researchers have proposed many generative models and applied them in a\nnumber of healthcare applications. There is a need for a comprehensive review\nof NLG methods and applications in the medical domain. In this study, we\nsystematically reviewed 113 scientific publications from a total of 3,988\nNLG-related articles identified using a literature search, focusing on data\nmodality, model architecture, clinical applications, and evaluation methods.\nFollowing PRISMA (Preferred Reporting Items for Systematic reviews and\nMeta-Analyses) guidelines, we categorize key methods, identify clinical\napplications, and assess their capabilities, limitations, and emerging\nchallenges. This timely review covers the key NLG technologies and medical\napplications and provides valuable insights for future studies to leverage NLG\nto transform medical discovery and healthcare."
                },
                "authors": [
                    {
                        "name": "Mengxian Lyu"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Jinqian Pan"
                    },
                    {
                        "name": "Cheng Peng"
                    },
                    {
                        "name": "Sankalp Talankar"
                    },
                    {
                        "name": "Yonghui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yonghui Wu"
                },
                "author": "Yonghui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04072v1",
                "updated": "2025-05-07T02:25:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    25,
                    20,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T02:25:20Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    25,
                    20,
                    2,
                    127,
                    0
                ],
                "title": "Advancing and Benchmarking Personalized Tool Invocation for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing and Benchmarking Personalized Tool Invocation for LLMs"
                },
                "summary": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench."
                },
                "authors": [
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "14 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04068v1",
                "updated": "2025-05-07T02:11:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    11,
                    43,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T02:11:43Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    11,
                    43,
                    2,
                    127,
                    0
                ],
                "title": "Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in\n  Covert Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in\n  Covert Communications"
                },
                "summary": "Covert Communications (CC) can secure sensitive transmissions in industrial,\nmilitary, and mission-critical applications within 6G wireless networks.\nHowever, traditional optimization methods based on Artificial Noise (AN), power\ncontrol, and channel manipulation might not adapt to dynamic and adversarial\nenvironments due to the high dimensionality, nonlinearity, and stringent\nreal-time covertness requirements. To bridge this gap, we introduce Shadow\nWireless Intelligence (SWI), which integrates the reasoning capabilities of\nLarge Language Models (LLMs) with retrieval-augmented generation to enable\nintelligent decision-making in covert wireless systems. Specifically, we\nutilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning,\ncombined with real-time retrieval of domain-specific knowledge to improve\ncontext accuracy and mitigate hallucinations. Our approach develops a\nstructured CC knowledge base, supports context-aware retrieval, and performs\nsemantic optimization, allowing LLMs to generate and adapt CC strategies in\nreal time. In a case study on optimizing AN power in a full-duplex CC scenario,\nDeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in\nthe generation of simulation code, outperforming baseline models. These results\nvalidate SWI as a robust, interpretable, and adaptive foundation for LLM-driven\nintelligent covert wireless systems in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covert Communications (CC) can secure sensitive transmissions in industrial,\nmilitary, and mission-critical applications within 6G wireless networks.\nHowever, traditional optimization methods based on Artificial Noise (AN), power\ncontrol, and channel manipulation might not adapt to dynamic and adversarial\nenvironments due to the high dimensionality, nonlinearity, and stringent\nreal-time covertness requirements. To bridge this gap, we introduce Shadow\nWireless Intelligence (SWI), which integrates the reasoning capabilities of\nLarge Language Models (LLMs) with retrieval-augmented generation to enable\nintelligent decision-making in covert wireless systems. Specifically, we\nutilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning,\ncombined with real-time retrieval of domain-specific knowledge to improve\ncontext accuracy and mitigate hallucinations. Our approach develops a\nstructured CC knowledge base, supports context-aware retrieval, and performs\nsemantic optimization, allowing LLMs to generate and adapt CC strategies in\nreal time. In a case study on optimizing AN power in a full-duplex CC scenario,\nDeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in\nthe generation of simulation code, outperforming baseline models. These results\nvalidate SWI as a robust, interpretable, and adaptive foundation for LLM-driven\nintelligent covert wireless systems in 6G networks."
                },
                "authors": [
                    {
                        "name": "Yuanai Xie"
                    },
                    {
                        "name": "Zhaozhi Liu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Shihua Zhang"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00831v2",
                "updated": "2025-05-07T02:00:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    2,
                    0,
                    27,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-01T19:44:36Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    44,
                    36,
                    3,
                    121,
                    0
                ],
                "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation"
                },
                "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics."
                },
                "authors": [
                    {
                        "name": "Quang P. M. Pham"
                    },
                    {
                        "name": "Khoi T. N. Nguyen"
                    },
                    {
                        "name": "Nhi H. Doan"
                    },
                    {
                        "name": "Cuong A. Pham"
                    },
                    {
                        "name": "Kentaro Inui"
                    },
                    {
                        "name": "Dezhen Song"
                    }
                ],
                "author_detail": {
                    "name": "Dezhen Song"
                },
                "author": "Dezhen Song",
                "arxiv_comment": "Paper is under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04051v1",
                "updated": "2025-05-07T01:44:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    44,
                    2,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T01:44:02Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    44,
                    2,
                    2,
                    127,
                    0
                ],
                "title": "BuildingBlock: A Hybrid Approach for Structured Building Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BuildingBlock: A Hybrid Approach for Structured Building Generation"
                },
                "summary": "Three-dimensional building generation is vital for applications in gaming,\nvirtual reality, and digital twins, yet current methods face challenges in\nproducing diverse, structured, and hierarchically coherent buildings. We\npropose BuildingBlock, a hybrid approach that integrates generative models,\nprocedural content generation (PCG), and large language models (LLMs) to\naddress these limitations. Specifically, our method introduces a two-phase\npipeline: the Layout Generation Phase (LGP) and the Building Construction Phase\n(BCP).\n  LGP reframes box-based layout generation as a point-cloud generation task,\nutilizing a newly constructed architectural dataset and a Transformer-based\ndiffusion model to create globally consistent layouts. With LLMs, these layouts\nare extended into rule-based hierarchical designs, seamlessly incorporating\ncomponent styles and spatial structures.\n  The BCP leverages these layouts to guide PCG, enabling local-customizable,\nhigh-quality structured building generation. Experimental results demonstrate\nBuildingBlock's effectiveness in generating diverse and hierarchically\nstructured buildings, achieving state-of-the-art results on multiple\nbenchmarks, and paving the way for scalable and intuitive architectural\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-dimensional building generation is vital for applications in gaming,\nvirtual reality, and digital twins, yet current methods face challenges in\nproducing diverse, structured, and hierarchically coherent buildings. We\npropose BuildingBlock, a hybrid approach that integrates generative models,\nprocedural content generation (PCG), and large language models (LLMs) to\naddress these limitations. Specifically, our method introduces a two-phase\npipeline: the Layout Generation Phase (LGP) and the Building Construction Phase\n(BCP).\n  LGP reframes box-based layout generation as a point-cloud generation task,\nutilizing a newly constructed architectural dataset and a Transformer-based\ndiffusion model to create globally consistent layouts. With LLMs, these layouts\nare extended into rule-based hierarchical designs, seamlessly incorporating\ncomponent styles and spatial structures.\n  The BCP leverages these layouts to guide PCG, enabling local-customizable,\nhigh-quality structured building generation. Experimental results demonstrate\nBuildingBlock's effectiveness in generating diverse and hierarchically\nstructured buildings, achieving state-of-the-art results on multiple\nbenchmarks, and paving the way for scalable and intuitive architectural\nworkflows."
                },
                "authors": [
                    {
                        "name": "Junming Huang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Letian Li"
                    },
                    {
                        "name": "Changxin Huang"
                    },
                    {
                        "name": "Qiang Dai"
                    },
                    {
                        "name": "Weiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Xu"
                },
                "author": "Weiwei Xu",
                "arxiv_doi": "10.1145/3721238.3730705",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721238.3730705",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGGRAPH 2025 (Conference Track)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04040v1",
                "updated": "2025-05-07T00:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    0,
                    44,
                    32,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T00:44:32Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    0,
                    44,
                    32,
                    2,
                    127,
                    0
                ],
                "title": "Identification and Optimization of Redundant Code Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Optimization of Redundant Code Using Large Language\n  Models"
                },
                "summary": "Redundant code is a persistent challenge in software development that makes\nsystems harder to maintain, scale, and update. It adds unnecessary complexity,\nhinders bug fixes, and increases technical debt. Despite their impact, removing\nredundant code manually is risky and error-prone, often introducing new bugs or\nmissing dependencies. While studies highlight the prevalence and negative\nimpact of redundant code, little focus has been given to Artificial\nIntelligence (AI) system codebases and the common patterns that cause\nredundancy. Additionally, the reasons behind developers unintentionally\nintroducing redundant code remain largely unexplored. This research addresses\nthese gaps by leveraging large language models (LLMs) to automatically detect\nand optimize redundant code in AI projects. Our research aims to identify\nrecurring patterns of redundancy and analyze their underlying causes, such as\noutdated practices or insufficient awareness of best coding principles.\nAdditionally, we plan to propose an LLM agent that will facilitate the\ndetection and refactoring of redundancies on a large scale while preserving\noriginal functionality. This work advances the application of AI in identifying\nand optimizing redundant code, ultimately helping developers maintain cleaner,\nmore readable, and scalable codebases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redundant code is a persistent challenge in software development that makes\nsystems harder to maintain, scale, and update. It adds unnecessary complexity,\nhinders bug fixes, and increases technical debt. Despite their impact, removing\nredundant code manually is risky and error-prone, often introducing new bugs or\nmissing dependencies. While studies highlight the prevalence and negative\nimpact of redundant code, little focus has been given to Artificial\nIntelligence (AI) system codebases and the common patterns that cause\nredundancy. Additionally, the reasons behind developers unintentionally\nintroducing redundant code remain largely unexplored. This research addresses\nthese gaps by leveraging large language models (LLMs) to automatically detect\nand optimize redundant code in AI projects. Our research aims to identify\nrecurring patterns of redundancy and analyze their underlying causes, such as\noutdated practices or insufficient awareness of best coding principles.\nAdditionally, we plan to propose an LLM agent that will facilitate the\ndetection and refactoring of redundancies on a large scale while preserving\noriginal functionality. This work advances the application of AI in identifying\nand optimizing redundant code, ultimately helping developers maintain cleaner,\nmore readable, and scalable codebases."
                },
                "authors": [
                    {
                        "name": "Shamse Tasnim Cynthia"
                    }
                ],
                "author_detail": {
                    "name": "Shamse Tasnim Cynthia"
                },
                "author": "Shamse Tasnim Cynthia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11863v2",
                "updated": "2025-05-06T23:56:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    23,
                    56,
                    25,
                    1,
                    126,
                    0
                ],
                "published": "2024-03-18T15:17:15Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    15,
                    17,
                    15,
                    0,
                    78,
                    0
                ],
                "title": "Context-aware LLM-based Safe Control Against Latent Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware LLM-based Safe Control Against Latent Risks"
                },
                "summary": "Autonomous control systems face significant challenges in performing complex\ntasks in the presence of latent risks. To address this, we propose an\nintegrated framework that combines Large Language Models (LLMs), numerical\noptimization, and optimization-based control to facilitate efficient subtask\nlearning while ensuring safety against latent risks. The framework decomposes\ncomplex tasks into a sequence of context-aware subtasks that account for latent\nrisks. These subtasks and their parameters are then refined through a\nmulti-time-scale process: high-layer multi-turn in-context learning, mid-layer\nLLM Chain-of-Thought reasoning and numerical optimization, and low-layer model\npredictive control. The framework iteratively improves decisions by leveraging\nqualitative feedback and optimized trajectory data from lower-layer\noptimization processes and a physics simulator. We validate the proposed\nframework through simulated case studies involving robot arm and autonomous\nvehicle scenarios. The experiments demonstrate that the proposed framework can\nmediate actions based on the context and latent risks and learn complex\nbehaviors efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous control systems face significant challenges in performing complex\ntasks in the presence of latent risks. To address this, we propose an\nintegrated framework that combines Large Language Models (LLMs), numerical\noptimization, and optimization-based control to facilitate efficient subtask\nlearning while ensuring safety against latent risks. The framework decomposes\ncomplex tasks into a sequence of context-aware subtasks that account for latent\nrisks. These subtasks and their parameters are then refined through a\nmulti-time-scale process: high-layer multi-turn in-context learning, mid-layer\nLLM Chain-of-Thought reasoning and numerical optimization, and low-layer model\npredictive control. The framework iteratively improves decisions by leveraging\nqualitative feedback and optimized trajectory data from lower-layer\noptimization processes and a physics simulator. We validate the proposed\nframework through simulated case studies involving robot arm and autonomous\nvehicle scenarios. The experiments demonstrate that the proposed framework can\nmediate actions based on the context and latent risks and learn complex\nbehaviors efficiently."
                },
                "authors": [
                    {
                        "name": "Xiyu Deng"
                    },
                    {
                        "name": "Quan Khanh Luu"
                    },
                    {
                        "name": "Anh Van Ho"
                    },
                    {
                        "name": "Yorie Nakahira"
                    }
                ],
                "author_detail": {
                    "name": "Yorie Nakahira"
                },
                "author": "Yorie Nakahira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04021v1",
                "updated": "2025-05-06T23:38:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    23,
                    38,
                    33,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T23:38:33Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    23,
                    38,
                    33,
                    1,
                    126,
                    0
                ],
                "title": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving"
                },
                "summary": "Serving large language models (LLMs) is expensive, especially for providers\nhosting many models, making cost reduction essential. The unique workload\npatterns of serving multiple LLMs (i.e., multi-LLM serving) create new\nopportunities and challenges for this task. The long-tail popularity of models\nand their long idle periods present opportunities to improve utilization\nthrough GPU sharing. However, existing GPU sharing systems lack the ability to\nadjust their resource allocation and sharing policies at runtime, making them\nineffective at meeting latency service-level objectives (SLOs) under rapidly\nfluctuating workloads.\n  This paper presents Prism, a multi-LLM serving system that unleashes the full\npotential of GPU sharing to achieve both cost efficiency and SLO attainment. At\nits core, Prism tackles a key limitation of existing\nsystems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$,\nwhich is essential for flexibly sharing GPU memory across models under dynamic\nworkloads. Prism achieves this with two key designs. First, it supports\non-demand memory allocation by dynamically mapping physical to virtual memory\npages, allowing flexible memory redistribution among models that space- and\ntime-share a GPU. Second, it improves memory efficiency through a two-level\nscheduling policy that dynamically adjusts sharing strategies based on models'\nruntime demands. Evaluations on real-world traces show that Prism achieves more\nthan $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to\nstate-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is expensive, especially for providers\nhosting many models, making cost reduction essential. The unique workload\npatterns of serving multiple LLMs (i.e., multi-LLM serving) create new\nopportunities and challenges for this task. The long-tail popularity of models\nand their long idle periods present opportunities to improve utilization\nthrough GPU sharing. However, existing GPU sharing systems lack the ability to\nadjust their resource allocation and sharing policies at runtime, making them\nineffective at meeting latency service-level objectives (SLOs) under rapidly\nfluctuating workloads.\n  This paper presents Prism, a multi-LLM serving system that unleashes the full\npotential of GPU sharing to achieve both cost efficiency and SLO attainment. At\nits core, Prism tackles a key limitation of existing\nsystems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$,\nwhich is essential for flexibly sharing GPU memory across models under dynamic\nworkloads. Prism achieves this with two key designs. First, it supports\non-demand memory allocation by dynamically mapping physical to virtual memory\npages, allowing flexible memory redistribution among models that space- and\ntime-share a GPU. Second, it improves memory efficiency through a two-level\nscheduling policy that dynamically adjusts sharing strategies based on models'\nruntime demands. Evaluations on real-world traces show that Prism achieves more\nthan $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to\nstate-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Yangmin Li"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ke Bao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    },
                    {
                        "name": "Ying Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Ying Sheng"
                },
                "author": "Ying Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04016v1",
                "updated": "2025-05-06T23:29:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    23,
                    29,
                    43,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T23:29:43Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    23,
                    29,
                    43,
                    1,
                    126,
                    0
                ],
                "title": "SLOT: Structuring the Output of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Structuring the Output of Large Language Models"
                },
                "summary": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Darren Yow-Bang Wang"
                    },
                    {
                        "name": "Zhengyuan Shen"
                    },
                    {
                        "name": "Soumya Smruti Mishra"
                    },
                    {
                        "name": "Zhichao Xu"
                    },
                    {
                        "name": "Yifei Teng"
                    },
                    {
                        "name": "Haibo Ding"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Ding"
                },
                "author": "Haibo Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02044v3",
                "updated": "2025-05-06T22:24:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    22,
                    24,
                    50,
                    1,
                    126,
                    0
                ],
                "published": "2024-06-04T07:27:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    27,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "Towards Universal and Black-Box Query-Response Only Attack on LLMs with\n  QROA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Universal and Black-Box Query-Response Only Attack on LLMs with\n  QROA"
                },
                "summary": "The rapid adoption of Large Language Models (LLMs) has exposed critical\nsecurity and ethical vulnerabilities, particularly their susceptibility to\nadversarial manipulations. This paper introduces QROA, a novel black-box\njailbreak method designed to identify adversarial suffixes that can bypass LLM\nalignment safeguards when appended to a malicious instruction. Unlike existing\nsuffix-based jailbreak approaches, QROA does not require access to the model's\nlogit or any other internal information. It also eliminates reliance on\nhuman-crafted templates, operating solely through the standard query-response\ninterface of LLMs. By framing the attack as an optimization bandit problem,\nQROA employs a surrogate model and token level optimization to efficiently\nexplore suffix variations. Furthermore, we propose QROA-UNV, an extension that\nidentifies universal adversarial suffixes for individual models, enabling\none-query jailbreaks across a wide range of instructions. Testing on multiple\nmodels demonstrates Attack Success Rate (ASR) greater than 80\\%. These findings\nhighlight critical vulnerabilities, emphasize the need for advanced defenses,\nand contribute to the development of more robust safety evaluations for secure\nAI deployment. The code is made public on the following link:\nhttps://github.com/qroa/QROA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Models (LLMs) has exposed critical\nsecurity and ethical vulnerabilities, particularly their susceptibility to\nadversarial manipulations. This paper introduces QROA, a novel black-box\njailbreak method designed to identify adversarial suffixes that can bypass LLM\nalignment safeguards when appended to a malicious instruction. Unlike existing\nsuffix-based jailbreak approaches, QROA does not require access to the model's\nlogit or any other internal information. It also eliminates reliance on\nhuman-crafted templates, operating solely through the standard query-response\ninterface of LLMs. By framing the attack as an optimization bandit problem,\nQROA employs a surrogate model and token level optimization to efficiently\nexplore suffix variations. Furthermore, we propose QROA-UNV, an extension that\nidentifies universal adversarial suffixes for individual models, enabling\none-query jailbreaks across a wide range of instructions. Testing on multiple\nmodels demonstrates Attack Success Rate (ASR) greater than 80\\%. These findings\nhighlight critical vulnerabilities, emphasize the need for advanced defenses,\nand contribute to the development of more robust safety evaluations for secure\nAI deployment. The code is made public on the following link:\nhttps://github.com/qroa/QROA"
                },
                "authors": [
                    {
                        "name": "Hussein Jawad"
                    },
                    {
                        "name": "Yassine Chenik"
                    },
                    {
                        "name": "Nicolas J. -B. Brunel"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas J. -B. Brunel"
                },
                "author": "Nicolas J. -B. Brunel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03989v1",
                "updated": "2025-05-06T21:53:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    53,
                    44,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T21:53:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    53,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "An alignment safety case sketch based on debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An alignment safety case sketch based on debate"
                },
                "summary": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe."
                },
                "authors": [
                    {
                        "name": "Marie Davidsen Buhl"
                    },
                    {
                        "name": "Jacob Pfau"
                    },
                    {
                        "name": "Benjamin Hilton"
                    },
                    {
                        "name": "Geoffrey Irving"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Irving"
                },
                "author": "Geoffrey Irving",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18216v2",
                "updated": "2025-05-06T21:45:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    45,
                    53,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-23T21:38:19Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    21,
                    38,
                    19,
                    6,
                    82,
                    0
                ],
                "title": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA\n  Adapters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA\n  Adapters"
                },
                "summary": "Large Language Models (LLMs) are computationally intensive, particularly\nduring inference. Neuron-adaptive techniques, which selectively activate\nneurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer\nfrom limitations in modern Transformers. These include reliance on sparse\nactivations, incompatibility with attention layers, and the use of costly\nneuron masking techniques. To address these issues, we propose the Adaptive\nRank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)\nadapter. RaNA adapters leverage rank adapters, which operate on linear layers\nby applying both low-rank matrix decompositions and adaptive masking to\nefficiently allocate compute without depending on activation sparsity. This\nenables RaNA to be generally applied to MLPs and linear components of attention\nmodules, while eliminating the need for expensive maskers found in\nneuron-adaptive methods. Notably, when compared to neuron adapters, RaNA\nimproves perplexity by up to 7 points and increases accuracy by up to 8\npercentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art\nTransformer architectures. These results position RaNA as a robust solution for\nimproving inference efficiency in modern Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are computationally intensive, particularly\nduring inference. Neuron-adaptive techniques, which selectively activate\nneurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer\nfrom limitations in modern Transformers. These include reliance on sparse\nactivations, incompatibility with attention layers, and the use of costly\nneuron masking techniques. To address these issues, we propose the Adaptive\nRank Allocation framework and introduce the Rank and Neuron Allocator (RaNA)\nadapter. RaNA adapters leverage rank adapters, which operate on linear layers\nby applying both low-rank matrix decompositions and adaptive masking to\nefficiently allocate compute without depending on activation sparsity. This\nenables RaNA to be generally applied to MLPs and linear components of attention\nmodules, while eliminating the need for expensive maskers found in\nneuron-adaptive methods. Notably, when compared to neuron adapters, RaNA\nimproves perplexity by up to 7 points and increases accuracy by up to 8\npercentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art\nTransformer architectures. These results position RaNA as a robust solution for\nimproving inference efficiency in modern Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Roberto Garcia"
                    },
                    {
                        "name": "Jerry Liu"
                    },
                    {
                        "name": "Daniel Sorvisto"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    }
                ],
                "author_detail": {
                    "name": "Sabri Eyuboglu"
                },
                "author": "Sabri Eyuboglu",
                "arxiv_comment": "16 pages, 5 figures. ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17669v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17669v2",
                "updated": "2025-05-06T21:45:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    45,
                    48,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-24T15:38:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    38,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a HIPAA Compliant Agentic AI System in Healthcare"
                },
                "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification."
                },
                "authors": [
                    {
                        "name": "Subash Neupane"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Rahimi"
                },
                "author": "Shahram Rahimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17669v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03988v1",
                "updated": "2025-05-06T21:41:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    41,
                    20,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T21:41:20Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    21,
                    41,
                    20,
                    1,
                    126,
                    0
                ],
                "title": "Can Large Language Models Predict Parallel Code Performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict Parallel Code Performance?"
                },
                "summary": "Accurate determination of the performance of parallel GPU code typically\nrequires execution-time profiling on target hardware -- an increasingly\nprohibitive step due to limited access to high-end GPUs. This paper explores\nwhether Large Language Models (LLMs) can offer an alternative approach for GPU\nperformance prediction without relying on hardware. We frame the problem as a\nroofline classification task: given the source code of a GPU kernel and the\nhardware specifications of a target GPU, can an LLM predict whether the GPU\nkernel is compute-bound or bandwidth-bound?\n  For this study, we build a balanced dataset of 340 GPU kernels, obtained from\nHeCBench benchmark and written in CUDA and OpenMP, along with their\nground-truth labels obtained via empirical GPU profiling. We evaluate LLMs\nacross four scenarios: (1) with access to profiling data of the kernel source,\n(2) zero-shot with source code only, (3) few-shot with code and label pairs,\nand (4) fine-tuned on a small custom dataset.\n  Our results show that state-of-the-art LLMs have a strong understanding of\nthe Roofline model, achieving 100% classification accuracy when provided with\nexplicit profiling data. We also find that reasoning-capable LLMs significantly\noutperform standard LLMs in zero- and few-shot settings, achieving up to 64%\naccuracy on GPU source codes, without profiling information. Lastly, we find\nthat LLM fine-tuning will require much more data than what we currently have\navailable.\n  This work is among the first to use LLMs for source-level roofline\nperformance prediction via classification, and illustrates their potential to\nguide optimization efforts when runtime profiling is infeasible. Our findings\nsuggest that with better datasets and prompt strategies, LLMs could become\npractical tools for HPC performance analysis and performance portability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate determination of the performance of parallel GPU code typically\nrequires execution-time profiling on target hardware -- an increasingly\nprohibitive step due to limited access to high-end GPUs. This paper explores\nwhether Large Language Models (LLMs) can offer an alternative approach for GPU\nperformance prediction without relying on hardware. We frame the problem as a\nroofline classification task: given the source code of a GPU kernel and the\nhardware specifications of a target GPU, can an LLM predict whether the GPU\nkernel is compute-bound or bandwidth-bound?\n  For this study, we build a balanced dataset of 340 GPU kernels, obtained from\nHeCBench benchmark and written in CUDA and OpenMP, along with their\nground-truth labels obtained via empirical GPU profiling. We evaluate LLMs\nacross four scenarios: (1) with access to profiling data of the kernel source,\n(2) zero-shot with source code only, (3) few-shot with code and label pairs,\nand (4) fine-tuned on a small custom dataset.\n  Our results show that state-of-the-art LLMs have a strong understanding of\nthe Roofline model, achieving 100% classification accuracy when provided with\nexplicit profiling data. We also find that reasoning-capable LLMs significantly\noutperform standard LLMs in zero- and few-shot settings, achieving up to 64%\naccuracy on GPU source codes, without profiling information. Lastly, we find\nthat LLM fine-tuning will require much more data than what we currently have\navailable.\n  This work is among the first to use LLMs for source-level roofline\nperformance prediction via classification, and illustrates their potential to\nguide optimization efforts when runtime profiling is infeasible. Our findings\nsuggest that with better datasets and prompt strategies, LLMs could become\npractical tools for HPC performance analysis and performance portability."
                },
                "authors": [
                    {
                        "name": "Gregory Bolet"
                    },
                    {
                        "name": "Giorgis Georgakoudis"
                    },
                    {
                        "name": "Harshitha Menon"
                    },
                    {
                        "name": "Konstantinos Parasyris"
                    },
                    {
                        "name": "Niranjan Hasabnis"
                    },
                    {
                        "name": "Hayden Estes"
                    },
                    {
                        "name": "Kirk W. Cameron"
                    },
                    {
                        "name": "Gal Oren"
                    }
                ],
                "author_detail": {
                    "name": "Gal Oren"
                },
                "author": "Gal Oren",
                "arxiv_comment": "5 pages, 4 figures, accepted to AI4Sys Workshop at HPDC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]