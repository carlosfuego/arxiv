[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rúben Adão"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "João Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07533v1",
                "updated": "2025-06-09T08:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T08:16:24Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts"
                },
                "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Haocheng Lu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v2",
                "updated": "2025-06-09T07:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    58,
                    19,
                    0,
                    160,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "The paper needs major modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07334v1",
                "updated": "2025-06-09T00:30:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T00:30:08Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models"
                },
                "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07311v1",
                "updated": "2025-06-08T22:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T22:59:20Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference"
                },
                "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Joshi"
                    },
                    {
                        "name": "Herman Saini"
                    },
                    {
                        "name": "Neil Dhillon"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar El Maghraoui"
                },
                "author": "Kaoutar El Maghraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v3",
                "updated": "2025-06-08T21:23:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    21,
                    23,
                    22,
                    6,
                    159,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v3",
                "updated": "2025-06-08T20:04:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    20,
                    4,
                    17,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08508v2",
                "updated": "2025-06-08T16:07:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    7,
                    44,
                    6,
                    159,
                    0
                ],
                "published": "2022-10-16T11:21:26Z",
                "published_parsed": [
                    2022,
                    10,
                    16,
                    11,
                    21,
                    26,
                    6,
                    289,
                    0
                ],
                "title": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory"
                },
                "summary": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions."
                },
                "authors": [
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Geraldo F. Oliveira"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Rachata Ausavarungnirun"
                    },
                    {
                        "name": "Juan Gómez Luna"
                    },
                    {
                        "name": "João Ferreira"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Nandita Vijaykumar"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v5",
                "updated": "2025-06-08T16:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    4,
                    59,
                    6,
                    159,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07200v1",
                "updated": "2025-06-08T15:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T15:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions"
                },
                "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach."
                },
                "authors": [
                    {
                        "name": "Kanato Nakanishi"
                    },
                    {
                        "name": "Soramichi Akiyama"
                    }
                ],
                "author_detail": {
                    "name": "Soramichi Akiyama"
                },
                "author": "Soramichi Akiyama",
                "arxiv_comment": "Presented in Machine Learning for Computer Architecture and Systems\n  (MLArchSys), June 21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v2",
                "updated": "2025-06-08T09:30:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    9,
                    30,
                    12,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1"
                },
                "summary": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "arxiv_comment": "Accepted to appear at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v2",
                "updated": "2025-06-08T00:52:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    52,
                    33,
                    6,
                    159,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v2",
                "updated": "2025-06-07T19:22:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    19,
                    22,
                    5,
                    5,
                    158,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v2",
                "updated": "2025-06-07T14:03:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    14,
                    3,
                    6,
                    5,
                    158,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "arxiv_comment": "Published in the Proceedings of the 42nd International Conference on\n  Machine Learning (ICML), Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06773v1",
                "updated": "2025-06-07T11:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-07T11:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor"
                },
                "summary": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09."
                },
                "authors": [
                    {
                        "name": "Emet Behrendt"
                    },
                    {
                        "name": "Shing Wai Pun"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Paper accepted and presented at the 6th Championship Branch\n  Prediction (CBP) workshop, co-held with ISCA 2025, on June 21, 2025, Tokyo,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.2; B.2.1; C.4; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v2",
                "updated": "2025-06-07T01:36:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    36,
                    34,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v1",
                "updated": "2025-06-06T18:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v1",
                "updated": "2025-06-06T09:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05811v1",
                "updated": "2025-06-06T07:20:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T07:20:25Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "title": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul"
                },
                "summary": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander."
                },
                "authors": [
                    {
                        "name": "Kari Aaron Clark"
                    },
                    {
                        "name": "Zun Htay"
                    },
                    {
                        "name": "Zichuan Zhou"
                    },
                    {
                        "name": "Amany Kassem"
                    },
                    {
                        "name": "Andrea Pertoldi"
                    },
                    {
                        "name": "Benjamin Rudin"
                    },
                    {
                        "name": "Florian Emaury"
                    },
                    {
                        "name": "Izzat Darwazeh"
                    },
                    {
                        "name": "Zhixin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Liu"
                },
                "author": "Zhixin Liu",
                "arxiv_comment": "Conference manuscript submitted to the European Conference on Optical\n  Communication 2025 (ECOC 2025) on 2nd May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16800v2",
                "updated": "2025-06-06T06:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    6,
                    35,
                    52,
                    4,
                    157,
                    0
                ],
                "published": "2023-05-26T10:29:25Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    10,
                    29,
                    25,
                    4,
                    146,
                    0
                ],
                "title": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache"
                },
                "summary": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe."
                },
                "authors": [
                    {
                        "name": "Jiakai Sun"
                    },
                    {
                        "name": "Weijing Zhang"
                    },
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Tianyi Chu"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v3",
                "updated": "2025-06-06T02:29:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    29,
                    18,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17219v1",
                "updated": "2025-06-20T17:59:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
                },
                "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training."
                },
                "authors": [
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Haoxiang Guan"
                    },
                    {
                        "name": "Yilin Cheng"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuxin Zheng"
                    },
                    {
                        "name": "Jiyan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiyan He"
                },
                "author": "Jiyan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09404v2",
                "updated": "2025-06-20T17:57:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2024-02-14T18:59:33Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    18,
                    59,
                    33,
                    2,
                    45,
                    0
                ],
                "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential\n  Reasoning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential\n  Reasoning Ability"
                },
                "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench."
                },
                "authors": [
                    {
                        "name": "Siwei Yang"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17209v1",
                "updated": "2025-06-20T17:57:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:57:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency"
                },
                "summary": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future."
                },
                "authors": [
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Isar Nejadgholi"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear at LLMSEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17208v1",
                "updated": "2025-06-20T17:57:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    8,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:57:08Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    8,
                    4,
                    171,
                    0
                ],
                "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and\n  Architectures of LLM- and Agent-Based Repair Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and\n  Architectures of LLM- and Agent-Based Repair Systems"
                },
                "summary": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies."
                },
                "authors": [
                    {
                        "name": "Matias Martinez"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v4",
                "updated": "2025-06-20T17:55:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    55,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17203v1",
                "updated": "2025-06-20T17:54:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    54,
                    18,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:54:18Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    54,
                    18,
                    4,
                    171,
                    0
                ],
                "title": "Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction"
                },
                "summary": "Large Language Models (LLMs) have recently enabled natural language\ninterfaces that translate user queries into executable SQL, offering a powerful\nsolution for non-technical stakeholders to access structured data. However, one\nof the limitation that LLMs do not natively express uncertainty makes it\ndifficult to assess the reliability of their generated queries. This paper\npresents a case study that evaluates multiple approaches to estimate confidence\nscores for LLM-generated SQL in supply chain data retrieval. We investigated\nthree strategies: (1) translation-based consistency checks; (2) embedding-based\nsemantic similarity between user questions and generated SQL; and (3)\nself-reported confidence scores directly produced by the LLM. Our findings\nreveal that LLMs are often overconfident in their own outputs, which limits the\neffectiveness of self-reported confidence. In contrast, embedding-based\nsimilarity methods demonstrate strong discriminative power in identifying\ninaccurate SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently enabled natural language\ninterfaces that translate user queries into executable SQL, offering a powerful\nsolution for non-technical stakeholders to access structured data. However, one\nof the limitation that LLMs do not natively express uncertainty makes it\ndifficult to assess the reliability of their generated queries. This paper\npresents a case study that evaluates multiple approaches to estimate confidence\nscores for LLM-generated SQL in supply chain data retrieval. We investigated\nthree strategies: (1) translation-based consistency checks; (2) embedding-based\nsemantic similarity between user questions and generated SQL; and (3)\nself-reported confidence scores directly produced by the LLM. Our findings\nreveal that LLMs are often overconfident in their own outputs, which limits the\neffectiveness of self-reported confidence. In contrast, embedding-based\nsimilarity methods demonstrate strong discriminative power in identifying\ninaccurate SQL."
                },
                "authors": [
                    {
                        "name": "Jiekai Ma"
                    },
                    {
                        "name": "Yikai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yikai Zhao"
                },
                "author": "Yikai Zhao",
                "arxiv_doi": "10.1145/XXXXXX.XXXXXX",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/XXXXXX.XXXXXX",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD workshop AI for Supply Chain 2025",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17201v1",
                "updated": "2025-06-20T17:50:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:50:37Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    37,
                    4,
                    171,
                    0
                ],
                "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition"
                },
                "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Longhuang Wu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Tianbao Yu"
                    },
                    {
                        "name": "Zhiguo Cao"
                    },
                    {
                        "name": "Qinglin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qinglin Lu"
                },
                "author": "Qinglin Lu",
                "arxiv_comment": "Project page: https://hunyuan-gamecraft.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17196v1",
                "updated": "2025-06-20T17:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    47,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:47:36Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    47,
                    36,
                    4,
                    171,
                    0
                ],
                "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM-Generated Short Answers and Effects on Learner Performance"
                },
                "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases."
                },
                "authors": [
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Danielle R Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Isha Raghuvanshi"
                    },
                    {
                        "name": "Ralph Abboud"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Shivang Gupta"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Koedinger"
                },
                "author": "Kenneth Koedinger",
                "arxiv_comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17193v1",
                "updated": "2025-06-20T17:46:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    46,
                    15,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:46:15Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    46,
                    15,
                    4,
                    171,
                    0
                ],
                "title": "Any nonincreasing convergence curves are simultaneously possible for\n  GMRES and weighted GMRES, as well as for left and right preconditioned GMRES",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Any nonincreasing convergence curves are simultaneously possible for\n  GMRES and weighted GMRES, as well as for left and right preconditioned GMRES"
                },
                "summary": "The convergence of the GMRES linear solver is notoriously hard to predict. A\nparticularly enlightening result by [Greenbaum, Pt\\'ak, Strako\\v{s}, 1996] is\nthat, given any convergence curve, one can build a linear system for which\nGMRES realizes that convergence curve. What is even more extraordinary is that\nthe eigenvalues of the problem matrix can be chosen arbitrarily. We build upon\nthis idea to derive novel results about weighted GMRES. We prove that for any\nlinear system and any prescribed convergence curve, there exists a weight\nmatrix M for which weighted GMRES (i.e., GMRES in the inner product induced by\nM) realizes that convergence curve, and we characterize the form of M.\nAdditionally, we exhibit a necessary and sufficient condition on M for the\nsimultaneous prescription of two convergence curves, one realized by GMRES in\nthe Euclidean inner product, and the other in the inner product induced by M.\nThese results are then applied to infer some properties of preconditioned GMRES\nwhen the preconditioner is applied either on the left or on the right. For\ninstance, we show that any two convergence curves are simultaneously possible\nfor left and right preconditioned GMRES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of the GMRES linear solver is notoriously hard to predict. A\nparticularly enlightening result by [Greenbaum, Pt\\'ak, Strako\\v{s}, 1996] is\nthat, given any convergence curve, one can build a linear system for which\nGMRES realizes that convergence curve. What is even more extraordinary is that\nthe eigenvalues of the problem matrix can be chosen arbitrarily. We build upon\nthis idea to derive novel results about weighted GMRES. We prove that for any\nlinear system and any prescribed convergence curve, there exists a weight\nmatrix M for which weighted GMRES (i.e., GMRES in the inner product induced by\nM) realizes that convergence curve, and we characterize the form of M.\nAdditionally, we exhibit a necessary and sufficient condition on M for the\nsimultaneous prescription of two convergence curves, one realized by GMRES in\nthe Euclidean inner product, and the other in the inner product induced by M.\nThese results are then applied to infer some properties of preconditioned GMRES\nwhen the preconditioner is applied either on the left or on the right. For\ninstance, we show that any two convergence curves are simultaneously possible\nfor left and right preconditioned GMRES."
                },
                "authors": [
                    {
                        "name": "Pierre Matalon"
                    },
                    {
                        "name": "Nicole Spillane"
                    }
                ],
                "author_detail": {
                    "name": "Nicole Spillane"
                },
                "author": "Nicole Spillane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10, 65Y05, 68W40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03089v2",
                "updated": "2025-06-20T17:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    45,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-05T09:27:42Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    9,
                    27,
                    42,
                    2,
                    157,
                    0
                ],
                "title": "Particle Filter Optimization: A Bayesian Approach for Global Stochastic\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle Filter Optimization: A Bayesian Approach for Global Stochastic\n  Optimization"
                },
                "summary": "This paper proposes a novel global optimization algorithm, Particle\nFilter-Based Optimization (PFO), designed for a class of stochastic\noptimization problems in which the objective function lacks an analytical form\nand is subject to noisy evaluations. PFO utilizes the Bayesian inference\nframework of Particle Filters (PF) by reformulating the optimization task as a\nstate estimation problem. In this context, evaluations of the objective\nfunction are interpreted as measurements, and a customized transition model\nbased on covariance ellipsoids is introduced to guide particle propagation.\nThis model serves as a surrogate for classical acquisition functions, equipping\nthe PF framework with local search capabilities and supporting efficient\nexploration of the global optimum. To mitigate the adverse effects of\nmeasurement noise, the Unscented Transform (UT) is employed to approximate the\nunderlying mean of the objective function, enhancing the accuracy of particle\nupdates. The algorithm offers notable improvements over existing stochastic\noptimization algorithms for black-box multi-modal objective functions. First,\nPFO provides a fully probabilistic definition of particle weights, enhancing\nadaptability and robustness. Second, PFO integrates exploration and\nexploitation within a unified Bayesian framework, ensuring a non-zero\nprobability of sampling from unexplored regions throughout the optimization\nprocess. This approach contrasts with traditional particle filter methods that\nare primarily used for state estimation, and heuristic optimization algorithms\nthat lack theoretical guarantees. The novelty of PFO lies in its unique\nintegration of particle filtering with a dynamic search space prediction,\noffering a theoretically grounded alternative to acquisition functions in\nBayesian Optimization (BO).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel global optimization algorithm, Particle\nFilter-Based Optimization (PFO), designed for a class of stochastic\noptimization problems in which the objective function lacks an analytical form\nand is subject to noisy evaluations. PFO utilizes the Bayesian inference\nframework of Particle Filters (PF) by reformulating the optimization task as a\nstate estimation problem. In this context, evaluations of the objective\nfunction are interpreted as measurements, and a customized transition model\nbased on covariance ellipsoids is introduced to guide particle propagation.\nThis model serves as a surrogate for classical acquisition functions, equipping\nthe PF framework with local search capabilities and supporting efficient\nexploration of the global optimum. To mitigate the adverse effects of\nmeasurement noise, the Unscented Transform (UT) is employed to approximate the\nunderlying mean of the objective function, enhancing the accuracy of particle\nupdates. The algorithm offers notable improvements over existing stochastic\noptimization algorithms for black-box multi-modal objective functions. First,\nPFO provides a fully probabilistic definition of particle weights, enhancing\nadaptability and robustness. Second, PFO integrates exploration and\nexploitation within a unified Bayesian framework, ensuring a non-zero\nprobability of sampling from unexplored regions throughout the optimization\nprocess. This approach contrasts with traditional particle filter methods that\nare primarily used for state estimation, and heuristic optimization algorithms\nthat lack theoretical guarantees. The novelty of PFO lies in its unique\nintegration of particle filtering with a dynamic search space prediction,\noffering a theoretically grounded alternative to acquisition functions in\nBayesian Optimization (BO)."
                },
                "authors": [
                    {
                        "name": "Mostafa Eslami"
                    },
                    {
                        "name": "Maryam Babazadeh"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Babazadeh"
                },
                "author": "Maryam Babazadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17188v1",
                "updated": "2025-06-20T17:42:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    42,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:42:13Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    42,
                    13,
                    4,
                    171,
                    0
                ],
                "title": "Towards AI Search Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards AI Search Paradigm"
                },
                "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Rui Kong"
                    },
                    {
                        "name": "Xinran Chen"
                    },
                    {
                        "name": "Jiamin Chen"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Haojie Zhang"
                    },
                    {
                        "name": "Jiayi Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Yiqun Chen"
                    },
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Keyi Kong"
                    },
                    {
                        "name": "Wenwen Ye"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17180v1",
                "updated": "2025-06-20T17:35:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    35,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:35:36Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    35,
                    36,
                    4,
                    171,
                    0
                ],
                "title": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models"
                },
                "summary": "We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions\ndesigned to evaluate whether language models can determine if one statement\ncausally explains another. Each question present an assertion-reason pair and\nchallenge language models to distinguish between semantic relatedness and\ngenuine causal explanatory relationships. Through comprehensive evaluation of\n21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we\nidentify two fundamental findings. First, language models frequently confuse\nsemantic similarity with causality, relying on lexical and semantic overlap\ninstead of inferring actual causal explanatory relationships. Second, as\nparameter size increases, models tend to shift from being overly skeptical\nabout causal relationships to being excessively permissive in accepting them.\nDespite this shift, performance measured by the Matthews Correlation\nCoefficient plateaus at just 0.55, even for the best-performing models.Hence,\nCLEAR-3K provides a crucial benchmark for developing and evaluating genuine\ncausal reasoning in language models, which is an essential capability for\napplications that require accurate assessment of causal relationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions\ndesigned to evaluate whether language models can determine if one statement\ncausally explains another. Each question present an assertion-reason pair and\nchallenge language models to distinguish between semantic relatedness and\ngenuine causal explanatory relationships. Through comprehensive evaluation of\n21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we\nidentify two fundamental findings. First, language models frequently confuse\nsemantic similarity with causality, relying on lexical and semantic overlap\ninstead of inferring actual causal explanatory relationships. Second, as\nparameter size increases, models tend to shift from being overly skeptical\nabout causal relationships to being excessively permissive in accepting them.\nDespite this shift, performance measured by the Matthews Correlation\nCoefficient plateaus at just 0.55, even for the best-performing models.Hence,\nCLEAR-3K provides a crucial benchmark for developing and evaluating genuine\ncausal reasoning in language models, which is an essential capability for\napplications that require accurate assessment of causal relationships."
                },
                "authors": [
                    {
                        "name": "Naiming Liu"
                    },
                    {
                        "name": "Richard Baraniuk"
                    },
                    {
                        "name": "Shashank Sonkar"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Sonkar"
                },
                "author": "Shashank Sonkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12098v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12098v4",
                "updated": "2025-06-20T17:34:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    34,
                    15,
                    4,
                    171,
                    0
                ],
                "published": "2023-11-20T19:00:01Z",
                "published_parsed": [
                    2023,
                    11,
                    20,
                    19,
                    0,
                    1,
                    0,
                    324,
                    0
                ],
                "title": "Union Through UNITY: Cosmology with 2,000 SNe Using a Unified Bayesian\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Union Through UNITY: Cosmology with 2,000 SNe Using a Unified Bayesian\n  Framework"
                },
                "summary": "Type Ia supernovae (SNe Ia) were instrumental in establishing the\nacceleration of the universe's expansion. By virtue of their combination of\ndistance reach, precision, and prevalence, they continue to provide key\ncosmological constraints, complementing other cosmological probes. Individual\nSN surveys cover only over about a factor of two in redshift, so compilations\nof multiple SN datasets are strongly beneficial. We assemble an up-to-date\n\"Union\" compilation of 2087 cosmologically useful SNe Ia from 24 datasets\n(\"Union3\"). We take care to put all SNe on the same distance scale and update\nthe light-curve fitting with SALT3 to use the full rest-frame optical. Over the\nnext few years, the number of cosmologically useful SNe Ia will increase by\nmore than a factor of ten, and keeping systematic uncertainties subdominant\nwill be more challenging than ever. We discuss the importance of treating\noutliers, selection effects, light-curve shape and color populations and\nstandardization relations, unexplained dispersion, and heterogeneous\nobservations simultaneously. We present an updated Bayesian framework, called\nUNITY1.5 (Unified Nonlinear Inference for Type-Ia cosmologY), that incorporates\nsignificant improvements in our ability to model selection effects,\nstandardization, and systematic uncertainties compared to earlier analyses. As\nan analysis byproduct, we also recover the posterior of the SN-only\npeculiar-velocity field, although we do not interpret it in this work. We\ncompute updated cosmological constraints with Union3 and UNITY1.5, finding weak\n1.7--2.6 sigma tension with LambdaCDM and possible evidence for thawing dark\nenergy (w0 > -1, wa < 0). We release our SN distances, light-curve fits, and\nUNITY1.5 framework to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Type Ia supernovae (SNe Ia) were instrumental in establishing the\nacceleration of the universe's expansion. By virtue of their combination of\ndistance reach, precision, and prevalence, they continue to provide key\ncosmological constraints, complementing other cosmological probes. Individual\nSN surveys cover only over about a factor of two in redshift, so compilations\nof multiple SN datasets are strongly beneficial. We assemble an up-to-date\n\"Union\" compilation of 2087 cosmologically useful SNe Ia from 24 datasets\n(\"Union3\"). We take care to put all SNe on the same distance scale and update\nthe light-curve fitting with SALT3 to use the full rest-frame optical. Over the\nnext few years, the number of cosmologically useful SNe Ia will increase by\nmore than a factor of ten, and keeping systematic uncertainties subdominant\nwill be more challenging than ever. We discuss the importance of treating\noutliers, selection effects, light-curve shape and color populations and\nstandardization relations, unexplained dispersion, and heterogeneous\nobservations simultaneously. We present an updated Bayesian framework, called\nUNITY1.5 (Unified Nonlinear Inference for Type-Ia cosmologY), that incorporates\nsignificant improvements in our ability to model selection effects,\nstandardization, and systematic uncertainties compared to earlier analyses. As\nan analysis byproduct, we also recover the posterior of the SN-only\npeculiar-velocity field, although we do not interpret it in this work. We\ncompute updated cosmological constraints with Union3 and UNITY1.5, finding weak\n1.7--2.6 sigma tension with LambdaCDM and possible evidence for thawing dark\nenergy (w0 > -1, wa < 0). We release our SN distances, light-curve fits, and\nUNITY1.5 framework to the community."
                },
                "authors": [
                    {
                        "name": "David Rubin"
                    },
                    {
                        "name": "Greg Aldering"
                    },
                    {
                        "name": "Marc Betoule"
                    },
                    {
                        "name": "Andy Fruchter"
                    },
                    {
                        "name": "Xiaosheng Huang"
                    },
                    {
                        "name": "Alex G. Kim"
                    },
                    {
                        "name": "Chris Lidman"
                    },
                    {
                        "name": "Eric Linder"
                    },
                    {
                        "name": "Saul Perlmutter"
                    },
                    {
                        "name": "Pilar Ruiz-Lapuente"
                    },
                    {
                        "name": "Nao Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Nao Suzuki"
                },
                "author": "Nao Suzuki",
                "arxiv_comment": "ApJ accepted version, fixed distance-modulus figure (thanks to Qinxun\n  Li)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12098v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12098v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07385v2",
                "updated": "2025-06-20T17:31:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    31,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-10T02:08:41Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    2,
                    8,
                    41,
                    3,
                    100,
                    0
                ],
                "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large\n  Language Models"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Ali Emami"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17174v1",
                "updated": "2025-06-20T17:24:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    24,
                    58,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:24:58Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    24,
                    58,
                    4,
                    171,
                    0
                ],
                "title": "High-accuracy inference using HfO$_x$S$_y$/HfS$_2$ Memristors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-accuracy inference using HfO$_x$S$_y$/HfS$_2$ Memristors"
                },
                "summary": "We demonstrate high accuracy classification for handwritten digits from the\nMNIST dataset ($\\sim$98.00$\\%$) and RGB images from the CIFAR-10 dataset\n($\\sim$86.80$\\%$) by using resistive memories based on a 2D van-der-Waals\nsemiconductor: hafnium disulfide (HfS$_2$). These memories are fabricated via\ndry thermal oxidation, forming vertical crossbar HfO$_x$S$_y$/HfS$_2$ devices\nwith a highly-ordered oxide-semiconductor structure. Our devices operate\nwithout electroforming or current compliance and exhibit multi-state,\nnon-volatile resistive switching, allowing resistance to be tuned using voltage\npulse trains. Using low-energy potentiation and depression pulses (0.7V-0.995V,\n160ns-350ns), we achieve 31 ($\\sim$5 bits) stable conductance states with high\nlinearity, symmetry, and low variation over 100 cycles. Key performance\nmetrics-such as weight update, quantisation, and retention-are extracted from\nthese experimental devices. These characteristics are used to simulate neural\nnetworks with our resistive memories as weights. Neural networks are trained on\nstate-of-the-art (SOTA) digital hardware (CUDA cores) and a baseline inference\naccuracy is extracted. IBM's Analog Hardware Acceleration Kit (AIHWKIT) is used\nto modify and remap digital weights in the pretrained network, based on the\ncharacteristics of our devices. Simulations account for factors like\nconductance linearity, device variation, and converter resolution. In both\nimage recognition tasks, we demonstrate excellent performance, similar to SOTA,\nwith only $<$0.07$\\%$ and $<$1.00$\\%$ difference in inference accuracy for the\nMNIST and CIFAR-10 datasets respectively. The forming-free, compliance-free\noperation, fast switching, low energy consumption, and high accuracy\nclassification demonstrate the potential of HfO$_x$S$_y$/HfS$_2$-based\nresistive memories for energy-efficient neural network acceleration and\nneuromorphic computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate high accuracy classification for handwritten digits from the\nMNIST dataset ($\\sim$98.00$\\%$) and RGB images from the CIFAR-10 dataset\n($\\sim$86.80$\\%$) by using resistive memories based on a 2D van-der-Waals\nsemiconductor: hafnium disulfide (HfS$_2$). These memories are fabricated via\ndry thermal oxidation, forming vertical crossbar HfO$_x$S$_y$/HfS$_2$ devices\nwith a highly-ordered oxide-semiconductor structure. Our devices operate\nwithout electroforming or current compliance and exhibit multi-state,\nnon-volatile resistive switching, allowing resistance to be tuned using voltage\npulse trains. Using low-energy potentiation and depression pulses (0.7V-0.995V,\n160ns-350ns), we achieve 31 ($\\sim$5 bits) stable conductance states with high\nlinearity, symmetry, and low variation over 100 cycles. Key performance\nmetrics-such as weight update, quantisation, and retention-are extracted from\nthese experimental devices. These characteristics are used to simulate neural\nnetworks with our resistive memories as weights. Neural networks are trained on\nstate-of-the-art (SOTA) digital hardware (CUDA cores) and a baseline inference\naccuracy is extracted. IBM's Analog Hardware Acceleration Kit (AIHWKIT) is used\nto modify and remap digital weights in the pretrained network, based on the\ncharacteristics of our devices. Simulations account for factors like\nconductance linearity, device variation, and converter resolution. In both\nimage recognition tasks, we demonstrate excellent performance, similar to SOTA,\nwith only $<$0.07$\\%$ and $<$1.00$\\%$ difference in inference accuracy for the\nMNIST and CIFAR-10 datasets respectively. The forming-free, compliance-free\noperation, fast switching, low energy consumption, and high accuracy\nclassification demonstrate the potential of HfO$_x$S$_y$/HfS$_2$-based\nresistive memories for energy-efficient neural network acceleration and\nneuromorphic computing."
                },
                "authors": [
                    {
                        "name": "Aferdita Xhameni"
                    },
                    {
                        "name": "Antonio Lombardo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Lombardo"
                },
                "author": "Antonio Lombardo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02197v2",
                "updated": "2025-06-20T17:21:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    21,
                    42,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-04T17:35:07Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    17,
                    35,
                    7,
                    6,
                    124,
                    0
                ],
                "title": "Central limit theorems under non-stationarity via relative weak\n  convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Central limit theorems under non-stationarity via relative weak\n  convergence"
                },
                "summary": "Statistical inference for non-stationary data is hindered by the failure of\nclassical central limit theorems (CLTs), not least because there is no fixed\nGaussian limit to converge to. To resolve this, we introduce relative weak\nconvergence, an extension of weak convergence that compares a statistic or\nprocess to a sequence of evolving processes. Relative weak convergence retains\nthe essential consequences of classical weak convergence and coincides with it\nunder stationarity. Crucially, it applies in general non-stationary settings\nwhere classical weak convergence fails. We establish concrete relative CLTs for\nrandom vectors and empirical processes, along with sequential, weighted, and\nbootstrap variants, that parallel the state-of-the-art in stationary settings.\nOur framework and results offer simple, plug-in replacements for classical CLTs\nwhenever stationarity is untenable, as illustrated by applications in\nnonparametric trend estimation and hypothesis testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for non-stationary data is hindered by the failure of\nclassical central limit theorems (CLTs), not least because there is no fixed\nGaussian limit to converge to. To resolve this, we introduce relative weak\nconvergence, an extension of weak convergence that compares a statistic or\nprocess to a sequence of evolving processes. Relative weak convergence retains\nthe essential consequences of classical weak convergence and coincides with it\nunder stationarity. Crucially, it applies in general non-stationary settings\nwhere classical weak convergence fails. We establish concrete relative CLTs for\nrandom vectors and empirical processes, along with sequential, weighted, and\nbootstrap variants, that parallel the state-of-the-art in stationary settings.\nOur framework and results offer simple, plug-in replacements for classical CLTs\nwhenever stationarity is untenable, as illustrated by applications in\nnonparametric trend estimation and hypothesis testing."
                },
                "authors": [
                    {
                        "name": "Nicolai Palm"
                    },
                    {
                        "name": "Thomas Nagler"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Nagler"
                },
                "author": "Thomas Nagler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08801v2",
                "updated": "2025-06-20T17:11:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    11,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2024-02-13T21:15:33Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    21,
                    15,
                    33,
                    1,
                    44,
                    0
                ],
                "title": "LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges"
                },
                "summary": "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the\npremier platform for developers queries on programming and software\ndevelopment. Demonstrating an ability to generate instant, human-like responses\nto technical questions, ChatGPT has ignited debates within the developer\ncommunity about the evolving role of human-driven platforms in the age of\ngenerative AI. Two months after ChatGPT release, Meta released its answer with\nits own Large Language Model (LLM) called LLaMA: the race was on. We conducted\nan empirical study analyzing questions from Stack Overflow and using these LLMs\nto address them. This way, we aim to (i) quantify the reliability of LLMs\nanswers and their potential to replace Stack Overflow in the long term; (ii)\nidentify and understand why LLMs fail; (iii) measure users activity evolution\nwith Stack Overflow over time; and (iv) compare LLMs together. Our empirical\nresults are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do\nnot outperform it for some domains, while a significant decline in user posting\nactivity has been observed. Furthermore, we also discuss the impact of our\nfindings regarding the usage and development of new LLMs and provide guidelines\nfor future challenges faced by users and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the\npremier platform for developers queries on programming and software\ndevelopment. Demonstrating an ability to generate instant, human-like responses\nto technical questions, ChatGPT has ignited debates within the developer\ncommunity about the evolving role of human-driven platforms in the age of\ngenerative AI. Two months after ChatGPT release, Meta released its answer with\nits own Large Language Model (LLM) called LLaMA: the race was on. We conducted\nan empirical study analyzing questions from Stack Overflow and using these LLMs\nto address them. This way, we aim to (i) quantify the reliability of LLMs\nanswers and their potential to replace Stack Overflow in the long term; (ii)\nidentify and understand why LLMs fail; (iii) measure users activity evolution\nwith Stack Overflow over time; and (iv) compare LLMs together. Our empirical\nresults are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do\nnot outperform it for some domains, while a significant decline in user posting\nactivity has been observed. Furthermore, we also discuss the impact of our\nfindings regarding the usage and development of new LLMs and provide guidelines\nfor future challenges faced by users and researchers."
                },
                "authors": [
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Jordan Samhi"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "63 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17163v1",
                "updated": "2025-06-20T17:09:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    9,
                    27,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:09:27Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    9,
                    27,
                    4,
                    171,
                    0
                ],
                "title": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About\n  Human and Clinical LLM Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About\n  Human and Clinical LLM Decision Making"
                },
                "summary": "Clinical robustness is critical to the safe deployment of medical Large\nLanguage Models (LLMs), but key questions remain about how LLMs and humans may\ndiffer in response to the real-world variability typified by clinical settings.\nTo address this, we introduce MedPerturb, a dataset designed to systematically\nevaluate medical LLMs under controlled perturbations of clinical input.\nMedPerturb consists of clinical vignettes spanning a range of pathologies, each\ntransformed along three axes: (1) gender modifications (e.g., gender-swapping\nor gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial\ntone); and (3) format changes (e.g., LLM-generated multi-turn conversations or\nsummaries). With MedPerturb, we release a dataset of 800 clinical contexts\ngrounded in realistic input variability, outputs from four LLMs, and three\nhuman expert reads per clinical context. We use MedPerturb in two case studies\nto reveal how shifts in gender identity cues, language style, or format reflect\ndiverging treatment selections between humans and LLMs. We find that LLMs are\nmore sensitive to gender and style perturbations while human annotators are\nmore sensitive to LLM-generated format perturbations such as clinical\nsummaries. Our results highlight the need for evaluation frameworks that go\nbeyond static benchmarks to assess the similarity between human clinician and\nLLM decisions under the variability characteristic of clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical robustness is critical to the safe deployment of medical Large\nLanguage Models (LLMs), but key questions remain about how LLMs and humans may\ndiffer in response to the real-world variability typified by clinical settings.\nTo address this, we introduce MedPerturb, a dataset designed to systematically\nevaluate medical LLMs under controlled perturbations of clinical input.\nMedPerturb consists of clinical vignettes spanning a range of pathologies, each\ntransformed along three axes: (1) gender modifications (e.g., gender-swapping\nor gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial\ntone); and (3) format changes (e.g., LLM-generated multi-turn conversations or\nsummaries). With MedPerturb, we release a dataset of 800 clinical contexts\ngrounded in realistic input variability, outputs from four LLMs, and three\nhuman expert reads per clinical context. We use MedPerturb in two case studies\nto reveal how shifts in gender identity cues, language style, or format reflect\ndiverging treatment selections between humans and LLMs. We find that LLMs are\nmore sensitive to gender and style perturbations while human annotators are\nmore sensitive to LLM-generated format perturbations such as clinical\nsummaries. Our results highlight the need for evaluation frameworks that go\nbeyond static benchmarks to assess the similarity between human clinician and\nLLM decisions under the variability characteristic of clinical settings."
                },
                "authors": [
                    {
                        "name": "Abinitha Gourabathina"
                    },
                    {
                        "name": "Yuexing Hao"
                    },
                    {
                        "name": "Walter Gerych"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04684v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04684v4",
                "updated": "2025-06-20T16:50:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    50,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2023-12-07T20:36:10Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    20,
                    36,
                    10,
                    3,
                    341,
                    0
                ],
                "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks."
                },
                "authors": [
                    {
                        "name": "Zifan Xu"
                    },
                    {
                        "name": "Haozhu Wang"
                    },
                    {
                        "name": "Dmitriy Bespalov"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Yanjun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Qi"
                },
                "author": "Yanjun Qi",
                "arxiv_journal_ref": "Findings of Empirical Methods in Natural Language Processing 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04684v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04684v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17144v1",
                "updated": "2025-06-20T16:45:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    45,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:45:54Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    45,
                    54,
                    4,
                    171,
                    0
                ],
                "title": "Do We Need Large VLMs for Spotting Soccer Actions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need Large VLMs for Spotting Soccer Actions?"
                },
                "summary": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting."
                },
                "authors": [
                    {
                        "name": "Ritabrata Chakraborty"
                    },
                    {
                        "name": "Rajatsubhra Chakraborty"
                    },
                    {
                        "name": "Avijit Dasgupta"
                    },
                    {
                        "name": "Sandeep Chaurasia"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Chaurasia"
                },
                "author": "Sandeep Chaurasia",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17139v1",
                "updated": "2025-06-20T16:38:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    38,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:38:29Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    38,
                    29,
                    4,
                    171,
                    0
                ],
                "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based\n  Diffusion Models"
                },
                "summary": "Diffusion models have recently gained significant attention due to their\neffectiveness in various scientific domains, including biochemistry. When\ntrained on equilibrium molecular distributions, diffusion models provide both:\na generative procedure to sample equilibrium conformations and associated\nforces derived from the model's scores. However, using the forces for\ncoarse-grained molecular dynamics simulations uncovers inconsistencies in the\nsamples generated via classical diffusion inference and simulation, despite\nboth originating from the same model. Particularly at the small diffusion\ntimesteps required for simulations, diffusion models fail to satisfy the\nFokker-Planck equation, which governs how the score should evolve over time. We\ninterpret this deviation as an indication of the observed inconsistencies and\npropose an energy-based diffusion model with a Fokker-Planck-derived\nregularization term enforcing consistency. We demonstrate the effectiveness of\nour approach on toy systems, alanine dipeptide, and introduce a\nstate-of-the-art transferable Boltzmann emulator for dipeptides that supports\nsimulation and demonstrates enhanced consistency and efficient sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently gained significant attention due to their\neffectiveness in various scientific domains, including biochemistry. When\ntrained on equilibrium molecular distributions, diffusion models provide both:\na generative procedure to sample equilibrium conformations and associated\nforces derived from the model's scores. However, using the forces for\ncoarse-grained molecular dynamics simulations uncovers inconsistencies in the\nsamples generated via classical diffusion inference and simulation, despite\nboth originating from the same model. Particularly at the small diffusion\ntimesteps required for simulations, diffusion models fail to satisfy the\nFokker-Planck equation, which governs how the score should evolve over time. We\ninterpret this deviation as an indication of the observed inconsistencies and\npropose an energy-based diffusion model with a Fokker-Planck-derived\nregularization term enforcing consistency. We demonstrate the effectiveness of\nour approach on toy systems, alanine dipeptide, and introduce a\nstate-of-the-art transferable Boltzmann emulator for dipeptides that supports\nsimulation and demonstrates enhanced consistency and efficient sampling."
                },
                "authors": [
                    {
                        "name": "Michael Plainer"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Leon Klein"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Frank Noé"
                    }
                ],
                "author_detail": {
                    "name": "Frank Noé"
                },
                "author": "Frank Noé",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07836v3",
                "updated": "2025-06-20T16:30:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    30,
                    9,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-10T15:13:00Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    13,
                    0,
                    3,
                    100,
                    0
                ],
                "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations"
                },
                "summary": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Junli Liu"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Yiting Zhang"
                    },
                    {
                        "name": "Chi Yan"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18110v2",
                "updated": "2025-06-20T16:28:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    28,
                    3,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T17:04:27Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    4,
                    27,
                    4,
                    143,
                    0
                ],
                "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM"
                },
                "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released."
                },
                "authors": [
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Farid Boussaid"
                    },
                    {
                        "name": "Girish Dwivedi"
                    },
                    {
                        "name": "Luqi Gong"
                    },
                    {
                        "name": "Qiuhong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Qiuhong Ke"
                },
                "author": "Qiuhong Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17125v1",
                "updated": "2025-06-20T16:27:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    27,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:27:59Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    27,
                    59,
                    4,
                    171,
                    0
                ],
                "title": "Large Language Model Unlearning for Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Unlearning for Source Code"
                },
                "summary": "LLM4SE has demonstrated significant success, but LLMs' potential memorization\nof sensitive or outdated training data introduces critical risks to legal\ncompliance, software security, and code quality. LLM unlearning techniques,\nwhich can eliminate the influence of undesired data from LLMs in a\npost-training way, present a promising solution to address these concerns.\nWhile recent efforts in LLM unlearning show effectiveness in natural language,\ntheir applicability to source code remains underexplored. Our empirical study\nreveals that existing LLM unlearning approaches, when applied to source code,\ncause severe model utility degradation, rendering models practically unusable\nfor code generation. In this paper, we propose PROD, a novel unlearning\napproach that enables LLMs to forget undesired code content while effectively\npreserving their code generation capabilities. PROD suppresses the probability\nof forget data in LLMs' output distribution while promoting candidate\ndistributional components, enabling the model to jointly learn to forget\nspecific content and retain its general capabilities. To facilitate this study,\nwe establish a benchmark for code unlearning evaluation, which includes three\ncritical downstream tasks: copyrighted code unlearning, insecure code\nunlearning, and deprecated API unlearning. Our evaluation demonstrates that\nPROD achieves superior balance between forget quality and model utility\ncompared to existing unlearning approaches across three downstream tasks, while\nconsistently exhibiting improvements when applied to LLMs of varying series.\nPROD also exhibits superior robustness against adversarial attacks without\ngenerating or exposing the data to be forgotten. The results underscore that\nour approach not only extends the application boundary of unlearning techniques\nto source code, but also holds significant implications for advancing reliable\ncode generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SE has demonstrated significant success, but LLMs' potential memorization\nof sensitive or outdated training data introduces critical risks to legal\ncompliance, software security, and code quality. LLM unlearning techniques,\nwhich can eliminate the influence of undesired data from LLMs in a\npost-training way, present a promising solution to address these concerns.\nWhile recent efforts in LLM unlearning show effectiveness in natural language,\ntheir applicability to source code remains underexplored. Our empirical study\nreveals that existing LLM unlearning approaches, when applied to source code,\ncause severe model utility degradation, rendering models practically unusable\nfor code generation. In this paper, we propose PROD, a novel unlearning\napproach that enables LLMs to forget undesired code content while effectively\npreserving their code generation capabilities. PROD suppresses the probability\nof forget data in LLMs' output distribution while promoting candidate\ndistributional components, enabling the model to jointly learn to forget\nspecific content and retain its general capabilities. To facilitate this study,\nwe establish a benchmark for code unlearning evaluation, which includes three\ncritical downstream tasks: copyrighted code unlearning, insecure code\nunlearning, and deprecated API unlearning. Our evaluation demonstrates that\nPROD achieves superior balance between forget quality and model utility\ncompared to existing unlearning approaches across three downstream tasks, while\nconsistently exhibiting improvements when applied to LLMs of varying series.\nPROD also exhibits superior robustness against adversarial attacks without\ngenerating or exposing the data to be forgotten. The results underscore that\nour approach not only extends the application boundary of unlearning techniques\nto source code, but also holds significant implications for advancing reliable\ncode generation."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Tangxinyu Wang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05091v2",
                "updated": "2025-06-20T16:24:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    24,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-07T19:16:49Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    19,
                    16,
                    49,
                    3,
                    312,
                    0
                ],
                "title": "Watermarking Language Models through Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Language Models through Language Models"
                },
                "summary": "Watermarking the outputs of large language models (LLMs) is critical for\nprovenance tracing, content regulation, and model accountability. Existing\napproaches often rely on access to model internals or are constrained by static\nrules and token-level perturbations. Moreover, the idea of steering generative\nbehavior via prompt-based instruction control remains largely underexplored. We\nintroduce a prompt-guided watermarking framework that operates entirely at the\ninput level and requires no access to model parameters or decoding logits. The\nframework comprises three cooperating components: a Prompting LM that\nsynthesizes watermarking instructions from user prompts, a Marking LM that\ngenerates watermarked outputs conditioned on these instructions, and a\nDetecting LM trained to classify whether a response carries an embedded\nwatermark. This modular design enables dynamic watermarking that adapts to\nindividual prompts while remaining compatible with diverse LLM architectures,\nincluding both proprietary and open-weight models. We evaluate the framework\nover 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral,\nLLaMA3, and DeepSeek. Experimental results show that watermark signals\ngeneralize across architectures and remain robust under fine-tuning, model\ndistillation, and prompt-based adversarial attacks, demonstrating the\neffectiveness and robustness of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking the outputs of large language models (LLMs) is critical for\nprovenance tracing, content regulation, and model accountability. Existing\napproaches often rely on access to model internals or are constrained by static\nrules and token-level perturbations. Moreover, the idea of steering generative\nbehavior via prompt-based instruction control remains largely underexplored. We\nintroduce a prompt-guided watermarking framework that operates entirely at the\ninput level and requires no access to model parameters or decoding logits. The\nframework comprises three cooperating components: a Prompting LM that\nsynthesizes watermarking instructions from user prompts, a Marking LM that\ngenerates watermarked outputs conditioned on these instructions, and a\nDetecting LM trained to classify whether a response carries an embedded\nwatermark. This modular design enables dynamic watermarking that adapts to\nindividual prompts while remaining compatible with diverse LLM architectures,\nincluding both proprietary and open-weight models. We evaluate the framework\nover 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral,\nLLaMA3, and DeepSeek. Experimental results show that watermark signals\ngeneralize across architectures and remain robust under fine-tuning, model\ndistillation, and prompt-based adversarial attacks, demonstrating the\neffectiveness and robustness of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Agnibh Dasgupta"
                    },
                    {
                        "name": "Abdullah Tanvir"
                    },
                    {
                        "name": "Xin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhong"
                },
                "author": "Xin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19675v2",
                "updated": "2025-06-20T16:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    24,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-26T08:31:55Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    31,
                    55,
                    0,
                    146,
                    0
                ],
                "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy\n  Labels via Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy\n  Labels via Iterative Refinement"
                },
                "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."
                },
                "authors": [
                    {
                        "name": "Liqin Ye"
                    },
                    {
                        "name": "Agam Shah"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Sudheer Chava"
                    }
                ],
                "author_detail": {
                    "name": "Sudheer Chava"
                },
                "author": "Sudheer Chava",
                "arxiv_comment": "Accepted at KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17124v1",
                "updated": "2025-06-20T16:23:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    23,
                    46,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:23:46Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    23,
                    46,
                    4,
                    171,
                    0
                ],
                "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Can Model-Free Reinforcement Learning be Enough for Thinking?"
                },
                "summary": "Recent work on large language models has demonstrated the use of model-free\nreinforcement learning (RL) to train reasoning-like capabilities. The emergence\nof \"thinking\" through model-free RL is interesting as thinking actions neither\nproduce reward nor change the external world state to one where the agent is\nmore likely to get reward. This paper seeks to build a domain-independent\nunderstanding of when model-free RL will lead to \"thinking\" as a strategy for\nreward maximization. To build this understanding, we first introduce a\ntheoretical model which we call a \\textit{thought Markov decision process}\n(MDP). Thought MDPs minimally extend the classical MDP model to include an\nabstract notion of thought state and thought action. Using the thought MDP\nmodel, we prove the importance of policy initialization in determining whether\nor not thinking emerges and show formally that thought actions are equivalent\nto the agent choosing to perform a step of policy improvement before continuing\nto act. We then show that open-source LLMs satisfy the conditions that our\ntheory predicts are necessary for model-free RL to produce thinking-like\nbehavior. Finally, we hypothesize sufficient conditions that would enable\nthinking to be learned outside of language generation and introduce a toy\ndomain where a combination of multi-task pre-training and designated thought\nactions enable more data-efficient RL compared to non-thinking agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on large language models has demonstrated the use of model-free\nreinforcement learning (RL) to train reasoning-like capabilities. The emergence\nof \"thinking\" through model-free RL is interesting as thinking actions neither\nproduce reward nor change the external world state to one where the agent is\nmore likely to get reward. This paper seeks to build a domain-independent\nunderstanding of when model-free RL will lead to \"thinking\" as a strategy for\nreward maximization. To build this understanding, we first introduce a\ntheoretical model which we call a \\textit{thought Markov decision process}\n(MDP). Thought MDPs minimally extend the classical MDP model to include an\nabstract notion of thought state and thought action. Using the thought MDP\nmodel, we prove the importance of policy initialization in determining whether\nor not thinking emerges and show formally that thought actions are equivalent\nto the agent choosing to perform a step of policy improvement before continuing\nto act. We then show that open-source LLMs satisfy the conditions that our\ntheory predicts are necessary for model-free RL to produce thinking-like\nbehavior. Finally, we hypothesize sufficient conditions that would enable\nthinking to be learned outside of language generation and introduce a toy\ndomain where a combination of multi-task pre-training and designated thought\nactions enable more data-efficient RL compared to non-thinking agents."
                },
                "authors": [
                    {
                        "name": "Josiah P. Hanna"
                    },
                    {
                        "name": "Nicholas E. Corrado"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas E. Corrado"
                },
                "author": "Nicholas E. Corrado",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17119v1",
                "updated": "2025-06-20T16:19:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    19,
                    28,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:19:28Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    19,
                    28,
                    4,
                    171,
                    0
                ],
                "title": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking"
                },
                "summary": "We introduce a robust framework, RGBTrack, for real-time 6D pose estimation\nand tracking that operates solely on RGB data, thereby eliminating the need for\ndepth input for such dynamic and precise object pose tracking tasks. Building\non the FoundationPose architecture, we devise a novel binary search strategy\ncombined with a render-and-compare mechanism to efficiently infer depth and\ngenerate robust pose hypotheses from true-scale CAD models. To maintain stable\ntracking in dynamic scenarios, including rapid movements and occlusions,\nRGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman\nfilter and a state machine for proactive object pose recovery. In addition,\nRGBTrack's scale recovery module dynamically adapts CAD models of unknown scale\nusing an initial depth estimate, enabling seamless integration with modern\ngenerative reconstruction techniques. Extensive evaluations on benchmark\ndatasets demonstrate that RGBTrack's novel depth-free approach achieves\ncompetitive accuracy and real-time performance, making it a promising practical\nsolution candidate for application areas including robotics, augmented reality,\nand computer vision.\n  The source code for our implementation will be made publicly available at\nhttps://github.com/GreatenAnoymous/RGBTrack.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a robust framework, RGBTrack, for real-time 6D pose estimation\nand tracking that operates solely on RGB data, thereby eliminating the need for\ndepth input for such dynamic and precise object pose tracking tasks. Building\non the FoundationPose architecture, we devise a novel binary search strategy\ncombined with a render-and-compare mechanism to efficiently infer depth and\ngenerate robust pose hypotheses from true-scale CAD models. To maintain stable\ntracking in dynamic scenarios, including rapid movements and occlusions,\nRGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman\nfilter and a state machine for proactive object pose recovery. In addition,\nRGBTrack's scale recovery module dynamically adapts CAD models of unknown scale\nusing an initial depth estimate, enabling seamless integration with modern\ngenerative reconstruction techniques. Extensive evaluations on benchmark\ndatasets demonstrate that RGBTrack's novel depth-free approach achieves\ncompetitive accuracy and real-time performance, making it a promising practical\nsolution candidate for application areas including robotics, augmented reality,\nand computer vision.\n  The source code for our implementation will be made publicly available at\nhttps://github.com/GreatenAnoymous/RGBTrack.git."
                },
                "authors": [
                    {
                        "name": "Teng Guo"
                    },
                    {
                        "name": "Jingjin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jingjin Yu"
                },
                "author": "Jingjin Yu",
                "arxiv_comment": "Accepted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17104v1",
                "updated": "2025-06-20T16:09:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    9,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:09:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    9,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic\n  Theorem Proving"
                },
                "summary": "Large language models (LLMs) have shown promising first-order logic (FOL)\nreasoning capabilities with applications in various areas. However, their\neffectiveness in complex mathematical reasoning involving multi-step FOL\ndeductions is still under-researched. While LLMs perform competitively on\nestablished mathematical reasoning benchmarks, they struggle with multi-step\nFOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on\nour proposed theorem proving dataset. This issue arises from the limited\nexploration of diverse proof strategies and the potential for early reasoning\nmistakes to undermine entire proofs. To address these issues, we propose DREAM,\na self-adaptive solution that enhances the Diversity and REAsonability of LLMs'\ngeneration strategies. DREAM incorporates an Axiom-Driven Strategy\nDiversification mechanism to promote varied strategic outcomes and a\nSub-Proposition Error Feedback to help LLMs reflect on and correct their\nproofs. Our contributions include pioneering advancements in LLMs' mathematical\nreasoning through FOL theorem proving, introducing a novel inference stage\nsolution that improves performance by 0.6% to 6.4%, and providing a curated\ndataset of 447 mathematical theorems in Lean 4 format for evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising first-order logic (FOL)\nreasoning capabilities with applications in various areas. However, their\neffectiveness in complex mathematical reasoning involving multi-step FOL\ndeductions is still under-researched. While LLMs perform competitively on\nestablished mathematical reasoning benchmarks, they struggle with multi-step\nFOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on\nour proposed theorem proving dataset. This issue arises from the limited\nexploration of diverse proof strategies and the potential for early reasoning\nmistakes to undermine entire proofs. To address these issues, we propose DREAM,\na self-adaptive solution that enhances the Diversity and REAsonability of LLMs'\ngeneration strategies. DREAM incorporates an Axiom-Driven Strategy\nDiversification mechanism to promote varied strategic outcomes and a\nSub-Proposition Error Feedback to help LLMs reflect on and correct their\nproofs. Our contributions include pioneering advancements in LLMs' mathematical\nreasoning through FOL theorem proving, introducing a novel inference stage\nsolution that improves performance by 0.6% to 6.4%, and providing a curated\ndataset of 447 mathematical theorems in Lean 4 format for evaluation."
                },
                "authors": [
                    {
                        "name": "Chuxue Cao"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Jinluan Yang"
                    },
                    {
                        "name": "Zijian Zhao"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chengzhong Liu"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12958v2",
                "updated": "2025-06-20T15:52:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    52,
                    6,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-15T20:42:45Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    20,
                    42,
                    45,
                    6,
                    166,
                    0
                ],
                "title": "Domain Specific Benchmarks for Evaluating Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Specific Benchmarks for Evaluating Multimodal Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed across\ndisciplines due to their advanced reasoning and problem solving capabilities.\nTo measure their effectiveness, various benchmarks have been developed that\nmeasure aspects of LLM reasoning, comprehension, and problem-solving. While\nseveral surveys address LLM evaluation and benchmarks, a domain-specific\nanalysis remains underexplored in the literature. This paper introduces a\ntaxonomy of seven key disciplines, encompassing various domains and application\nareas where LLMs are extensively utilized. Additionally, we provide a\ncomprehensive review of LLM benchmarks and survey papers within each domain,\nhighlighting the unique capabilities of LLMs and the challenges faced in their\napplication. Finally, we compile and categorize these benchmarks by domain to\ncreate an accessible resource for researchers, aiming to pave the way for\nadvancements toward artificial general intelligence (AGI)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed across\ndisciplines due to their advanced reasoning and problem solving capabilities.\nTo measure their effectiveness, various benchmarks have been developed that\nmeasure aspects of LLM reasoning, comprehension, and problem-solving. While\nseveral surveys address LLM evaluation and benchmarks, a domain-specific\nanalysis remains underexplored in the literature. This paper introduces a\ntaxonomy of seven key disciplines, encompassing various domains and application\nareas where LLMs are extensively utilized. Additionally, we provide a\ncomprehensive review of LLM benchmarks and survey papers within each domain,\nhighlighting the unique capabilities of LLMs and the challenges faced in their\napplication. Finally, we compile and categorize these benchmarks by domain to\ncreate an accessible resource for researchers, aiming to pave the way for\nadvancements toward artificial general intelligence (AGI)"
                },
                "authors": [
                    {
                        "name": "Khizar Anjum"
                    },
                    {
                        "name": "Muhammad Arbab Arshad"
                    },
                    {
                        "name": "Kadhim Hayawi"
                    },
                    {
                        "name": "Efstathios Polyzos"
                    },
                    {
                        "name": "Asadullah Tariq"
                    },
                    {
                        "name": "Mohamed Adel Serhani"
                    },
                    {
                        "name": "Laiba Batool"
                    },
                    {
                        "name": "Brady Lund"
                    },
                    {
                        "name": "Nishith Reddy Mannuru"
                    },
                    {
                        "name": "Ravi Varma Kumar Bevara"
                    },
                    {
                        "name": "Taslim Mahbub"
                    },
                    {
                        "name": "Muhammad Zeeshan Akram"
                    },
                    {
                        "name": "Sakib Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Sakib Shahriar"
                },
                "author": "Sakib Shahriar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17088v1",
                "updated": "2025-06-20T15:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    49,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:49:37Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    49,
                    37,
                    4,
                    171,
                    0
                ],
                "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language\n  Models: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language\n  Models: An Empirical Evaluation"
                },
                "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect."
                },
                "authors": [
                    {
                        "name": "Jiahao Cheng"
                    },
                    {
                        "name": "Tiancheng Su"
                    },
                    {
                        "name": "Jia Yuan"
                    },
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xinqi Tao"
                    },
                    {
                        "name": "Jingwen Xie"
                    },
                    {
                        "name": "Huaxia Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaxia Li"
                },
                "author": "Huaxia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13784v2",
                "updated": "2025-06-20T15:48:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    48,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-11T02:05:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    5,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "ScholarSearch: Benchmarking Scholar Searching Ability of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScholarSearch: Benchmarking Scholar Searching Ability of LLMs"
                },
                "summary": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed ScholarSearch, the first dataset specifically designed to evaluate the\ncomplex information retrieval capabilities of Large Language Models (LLMs) in\nacademic research. ScholarSearch possesses the following key characteristics:\nAcademic Practicality, where question content closely mirrors real academic\nlearning and research environments, avoiding deliberately misleading models;\nHigh Difficulty, with answers that are challenging for single models (e.g.,\nGrok DeepSearch or Gemini Deep Research) to provide directly, often requiring\nat least three deep searches to derive; Concise Evaluation, where limiting\nconditions ensure answers are as unique as possible, accompanied by clear\nsources and brief solution explanations, greatly facilitating subsequent audit\nand verification, surpassing the current lack of analyzed search datasets both\ndomestically and internationally; and Broad Coverage, as the dataset spans at\nleast 15 different academic disciplines. Through ScholarSearch, we expect to\nmore precisely measure and promote the performance improvement of LLMs in\ncomplex academic information retrieval tasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed ScholarSearch, the first dataset specifically designed to evaluate the\ncomplex information retrieval capabilities of Large Language Models (LLMs) in\nacademic research. ScholarSearch possesses the following key characteristics:\nAcademic Practicality, where question content closely mirrors real academic\nlearning and research environments, avoiding deliberately misleading models;\nHigh Difficulty, with answers that are challenging for single models (e.g.,\nGrok DeepSearch or Gemini Deep Research) to provide directly, often requiring\nat least three deep searches to derive; Concise Evaluation, where limiting\nconditions ensure answers are as unique as possible, accompanied by clear\nsources and brief solution explanations, greatly facilitating subsequent audit\nand verification, surpassing the current lack of analyzed search datasets both\ndomestically and internationally; and Broad Coverage, as the dataset spans at\nleast 15 different academic disciplines. Through ScholarSearch, we expect to\nmore precisely measure and promote the performance improvement of LLMs in\ncomplex academic information retrieval tasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch"
                },
                "authors": [
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Nengyuan Zhang"
                    },
                    {
                        "name": "Tingjia Miao"
                    },
                    {
                        "name": "Zhihui Qi"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17080v1",
                "updated": "2025-06-20T15:30:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    30,
                    6,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:30:06Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    30,
                    6,
                    4,
                    171,
                    0
                ],
                "title": "Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs"
                },
                "summary": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization."
                },
                "authors": [
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "José Pombal"
                    },
                    {
                        "name": "João Alves"
                    },
                    {
                        "name": "Pedro Teixeirinha"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17077v1",
                "updated": "2025-06-20T15:27:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    27,
                    44,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:27:44Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    27,
                    44,
                    4,
                    171,
                    0
                ],
                "title": "Simultaneous Translation with Offline Speech and LLM Models in CUNI\n  Submission to IWSLT 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Translation with Offline Speech and LLM Models in CUNI\n  Submission to IWSLT 2025"
                },
                "summary": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency."
                },
                "authors": [
                    {
                        "name": "Dominik Macháček"
                    },
                    {
                        "name": "Peter Polák"
                    }
                ],
                "author_detail": {
                    "name": "Peter Polák"
                },
                "author": "Peter Polák",
                "arxiv_comment": "IWSLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17073v1",
                "updated": "2025-06-20T15:24:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    24,
                    31,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:24:31Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    24,
                    31,
                    4,
                    171,
                    0
                ],
                "title": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions,\n  Even When Transparently Disclosed as AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions,\n  Even When Transparently Disclosed as AI"
                },
                "summary": "A wide range of participation is essential for democracy, as it helps prevent\nthe dominance of extreme views, erosion of legitimacy, and political\npolarization. However, engagement in online political discussions often\nfeatures a limited spectrum of views due to high levels of self-selection and\nthe tendency of online platforms to facilitate exchanges primarily among\nlike-minded individuals. This study examines whether an LLM-based bot can widen\nthe scope of perspectives expressed by participants in online discussions\nthrough two pre-registered randomized experiments conducted in a chatroom. We\nevaluate the impact of a bot that actively monitors discussions, identifies\nmissing arguments, and introduces them into the conversation. The results\nindicate that our bot significantly expands the range of arguments, as measured\nby both objective and subjective metrics. Furthermore, disclosure of the bot as\nAI does not significantly alter these effects. These findings suggest that\nLLM-based moderation tools can positively influence online political discourse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of participation is essential for democracy, as it helps prevent\nthe dominance of extreme views, erosion of legitimacy, and political\npolarization. However, engagement in online political discussions often\nfeatures a limited spectrum of views due to high levels of self-selection and\nthe tendency of online platforms to facilitate exchanges primarily among\nlike-minded individuals. This study examines whether an LLM-based bot can widen\nthe scope of perspectives expressed by participants in online discussions\nthrough two pre-registered randomized experiments conducted in a chatroom. We\nevaluate the impact of a bot that actively monitors discussions, identifies\nmissing arguments, and introduces them into the conversation. The results\nindicate that our bot significantly expands the range of arguments, as measured\nby both objective and subjective metrics. Furthermore, disclosure of the bot as\nAI does not significantly alter these effects. These findings suggest that\nLLM-based moderation tools can positively influence online political discourse."
                },
                "authors": [
                    {
                        "name": "Valeria Vuk"
                    },
                    {
                        "name": "Cristina Sarasua"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Gilardi"
                },
                "author": "Fabrizio Gilardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01702v2",
                "updated": "2025-06-20T15:22:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    22,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-03T09:54:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    54,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models"
                },
                "summary": "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative."
                },
                "authors": [
                    {
                        "name": "Christopher E. Mower"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou-Ammar"
                },
                "author": "Haitham Bou-Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17067v1",
                "updated": "2025-06-20T15:14:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    14,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:14:29Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    14,
                    29,
                    4,
                    171,
                    0
                ],
                "title": "Empowering Near-Field Communications in Low-Altitude Economy with LLM:\n  Fundamentals, Potentials, Solutions, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Near-Field Communications in Low-Altitude Economy with LLM:\n  Fundamentals, Potentials, Solutions, and Future Directions"
                },
                "summary": "The low-altitude economy (LAE) is gaining significant attention from academia\nand industry. Fortunately, LAE naturally aligns with near-field communications\nin extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field\nbeamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,\nwhile the additional distance dimension boosts overall spectrum efficiency.\nHowever, near-field communications in LAE still face several challenges, such\nas the increase in signal processing complexity and the necessity of\ndistinguishing between far and near-field users. Inspired by the large language\nmodels (LLM) with powerful ability to handle complex problems, we apply LLM to\nsolve challenges of near-field communications in LAE. The objective of this\narticle is to provide a comprehensive analysis and discussion on LLM-empowered\nnear-field communications in LAE. Specifically, we first introduce fundamentals\nof LLM and near-field communications, including the key advantages of LLM and\nkey characteristics of near-field communications. Then, we reveal the\nopportunities and challenges of near-field communications in LAE. To address\nthese challenges, we present a LLM-based scheme for near-field communications\nin LAE, and provide a case study which jointly distinguishes far and near-field\nusers and designs multi-user precoding matrix. Finally, we outline and\nhighlight several future research directions and open issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-altitude economy (LAE) is gaining significant attention from academia\nand industry. Fortunately, LAE naturally aligns with near-field communications\nin extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field\nbeamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,\nwhile the additional distance dimension boosts overall spectrum efficiency.\nHowever, near-field communications in LAE still face several challenges, such\nas the increase in signal processing complexity and the necessity of\ndistinguishing between far and near-field users. Inspired by the large language\nmodels (LLM) with powerful ability to handle complex problems, we apply LLM to\nsolve challenges of near-field communications in LAE. The objective of this\narticle is to provide a comprehensive analysis and discussion on LLM-empowered\nnear-field communications in LAE. Specifically, we first introduce fundamentals\nof LLM and near-field communications, including the key advantages of LLM and\nkey characteristics of near-field communications. Then, we reveal the\nopportunities and challenges of near-field communications in LAE. To address\nthese challenges, we present a LLM-based scheme for near-field communications\nin LAE, and provide a case study which jointly distinguishes far and near-field\nusers and designs multi-user precoding matrix. Finally, we outline and\nhighlight several future research directions and open issues."
                },
                "authors": [
                    {
                        "name": "Zhuo Xu"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Linglong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Linglong Dai"
                },
                "author": "Linglong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17065v1",
                "updated": "2025-06-20T15:12:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    12,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:12:43Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    12,
                    43,
                    4,
                    171,
                    0
                ],
                "title": "Flow-Based Non-stationary Temporal Regime Causal Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-Based Non-stationary Temporal Regime Causal Structure Learning"
                },
                "summary": "Understanding causal relationships in multivariate time series is crucial in\nmany scenarios, such as those dealing with financial or neurological data. Many\nsuch time series exhibit multiple regimes, i.e., consecutive temporal segments\nwith a priori unknown boundaries, with each regime having its own causal\nstructure. Inferring causal dependencies and regime shifts is critical for\nanalyzing the underlying processes. However, causal structure learning in this\nsetting is challenging due to (1) non stationarity, i.e., each regime can have\nits own causal graph and mixing function, and (2) complex noise distributions,\nwhich may be non Gaussian or heteroscedastic. Existing causal discovery\napproaches cannot address these challenges, since generally assume stationarity\nor Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified\nframework for causal discovery that handles non stationary processes along with\nnon Gaussian and heteroscedastic noises. FANTOM simultaneously infers the\nnumber of regimes and their corresponding indices and learns each regime's\nDirected Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm\nthat maximizes the evidence lower bound of the data log likelihood. On the\ntheoretical side, we prove, under mild assumptions, that temporal\nheteroscedastic causal models, introduced in FANTOM's formulation, are\nidentifiable in both stationary and non stationary settings. In addition,\nextensive experiments on synthetic and real data show that FANTOM outperforms\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relationships in multivariate time series is crucial in\nmany scenarios, such as those dealing with financial or neurological data. Many\nsuch time series exhibit multiple regimes, i.e., consecutive temporal segments\nwith a priori unknown boundaries, with each regime having its own causal\nstructure. Inferring causal dependencies and regime shifts is critical for\nanalyzing the underlying processes. However, causal structure learning in this\nsetting is challenging due to (1) non stationarity, i.e., each regime can have\nits own causal graph and mixing function, and (2) complex noise distributions,\nwhich may be non Gaussian or heteroscedastic. Existing causal discovery\napproaches cannot address these challenges, since generally assume stationarity\nor Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified\nframework for causal discovery that handles non stationary processes along with\nnon Gaussian and heteroscedastic noises. FANTOM simultaneously infers the\nnumber of regimes and their corresponding indices and learns each regime's\nDirected Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm\nthat maximizes the evidence lower bound of the data log likelihood. On the\ntheoretical side, we prove, under mild assumptions, that temporal\nheteroscedastic causal models, introduced in FANTOM's formulation, are\nidentifiable in both stationary and non stationary settings. In addition,\nextensive experiments on synthetic and real data show that FANTOM outperforms\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Abdellah Rahmani"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15591v2",
                "updated": "2025-06-20T15:05:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    5,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-18T16:06:30Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    6,
                    30,
                    2,
                    169,
                    0
                ],
                "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video\n  Super-Resolution"
                },
                "summary": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL."
                },
                "authors": [
                    {
                        "name": "Yujing Sun"
                    },
                    {
                        "name": "Lingchen Sun"
                    },
                    {
                        "name": "Shuaizheng Liu"
                    },
                    {
                        "name": "Rongyuan Wu"
                    },
                    {
                        "name": "Zhengqiang Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17052v1",
                "updated": "2025-06-20T15:04:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    4,
                    11,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:04:11Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    4,
                    11,
                    4,
                    171,
                    0
                ],
                "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Concepts to Components: Concept-Agnostic Attention Module Discovery\n  in Transformers"
                },
                "summary": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet."
                },
                "authors": [
                    {
                        "name": "Jingtong Su"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Karen Ullrich"
                    }
                ],
                "author_detail": {
                    "name": "Karen Ullrich"
                },
                "author": "Karen Ullrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06751v2",
                "updated": "2025-06-20T15:03:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    3,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-07T10:45:17Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    10,
                    45,
                    17,
                    5,
                    158,
                    0
                ],
                "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models"
                },
                "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research."
                },
                "authors": [
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Dmitrii Korzh"
                    },
                    {
                        "name": "Ivan Lazichny"
                    },
                    {
                        "name": "Elvir Karimov"
                    },
                    {
                        "name": "Artyom Iudin"
                    },
                    {
                        "name": "Ivan Oseledets"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    },
                    {
                        "name": "Natalia Loukachevitch"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00824v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00824v7",
                "updated": "2025-06-20T14:59:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    59,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-01-01T13:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference"
                },
                "summary": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability."
                },
                "authors": [
                    {
                        "name": "Rongke Liu"
                    },
                    {
                        "name": "Youwen Zhu"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Gaoning Pan"
                    },
                    {
                        "name": "Xingyu He"
                    },
                    {
                        "name": "Weizhi Meng"
                    }
                ],
                "author_detail": {
                    "name": "Weizhi Meng"
                },
                "author": "Weizhi Meng",
                "arxiv_comment": "15 pages, 5 figures, 6 tables. The experimental data have been\n  corrected, and some explanations have been supplemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00824v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00824v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08316v3",
                "updated": "2025-06-20T14:52:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    52,
                    2,
                    4,
                    171,
                    0
                ],
                "published": "2024-10-10T19:06:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    19,
                    6,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework"
                },
                "summary": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose a Conditioned One-Shot fine-tuning framework\n(COS-DPO) that extends the Direct Preference Optimization technique, originally\ndeveloped for efficient LLM alignment with preference data, to accommodate the\nMOFT settings. By direct conditioning on the weight across auxiliary\nobjectives, our Weight-COS-DPO method enjoys an efficient one-shot training\nprocess for profiling the Pareto front and is capable of achieving\ncomprehensive trade-off solutions even in the post-training stage. Based on our\ntheoretical findings on the linear transformation properties of the loss\nfunction, we further propose the Temperature-COS-DPO method that augments the\ntemperature parameter to the model input, enhancing the flexibility of\npost-training control over the trade-offs between the main and auxiliary\nobjectives. We demonstrate the effectiveness and efficiency of the COS-DPO\nframework through its applications to various tasks, including the\nLearning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for\nlarge-scale ML deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose a Conditioned One-Shot fine-tuning framework\n(COS-DPO) that extends the Direct Preference Optimization technique, originally\ndeveloped for efficient LLM alignment with preference data, to accommodate the\nMOFT settings. By direct conditioning on the weight across auxiliary\nobjectives, our Weight-COS-DPO method enjoys an efficient one-shot training\nprocess for profiling the Pareto front and is capable of achieving\ncomprehensive trade-off solutions even in the post-training stage. Based on our\ntheoretical findings on the linear transformation properties of the loss\nfunction, we further propose the Temperature-COS-DPO method that augments the\ntemperature parameter to the model input, enhancing the flexibility of\npost-training control over the trade-offs between the main and auxiliary\nobjectives. We demonstrate the effectiveness and efficiency of the COS-DPO\nframework through its applications to various tasks, including the\nLearning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for\nlarge-scale ML deployments."
                },
                "authors": [
                    {
                        "name": "Yinuo Ren"
                    },
                    {
                        "name": "Tesi Xiao"
                    },
                    {
                        "name": "Michael Shavlovsky"
                    },
                    {
                        "name": "Lexing Ying"
                    },
                    {
                        "name": "Holakou Rahmanian"
                    }
                ],
                "author_detail": {
                    "name": "Holakou Rahmanian"
                },
                "author": "Holakou Rahmanian",
                "arxiv_comment": "Published at UAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17040v1",
                "updated": "2025-06-20T14:49:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    49,
                    35,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T14:49:35Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    49,
                    35,
                    4,
                    171,
                    0
                ],
                "title": "Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the\n  Hidden Landscape of Visual Invariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the\n  Hidden Landscape of Visual Invariance"
                },
                "summary": "Uncovering which features' combinations high-level visual units encode is\ncritical to understand how images are transformed into representations that\nsupport recognition. While existing feature visualization approaches typically\ninfer a unit's most exciting images, this is insufficient to reveal the\nmanifold of transformations under which responses remain invariant, which is\nkey to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),\nan unbiased, model-agnostic, and gradient-free framework to systematically\ncharacterize a unit's invariance landscape and its vulnerability to adversarial\nperturbations in both biological and artificial visual systems. SnS frames\nthese transformations as bi-objective optimization problems. To probe\ninvariance, SnS seeks image perturbations that maximally alter the\nrepresentation of a reference stimulus in a given processing stage while\npreserving unit activation. To probe adversarial sensitivity, SnS seeks\nperturbations that minimally alter the stimulus while suppressing unit\nactivation. Applied to convolutional neural networks (CNNs), SnS revealed image\nvariations that were further from a reference image in pixel-space than those\nproduced by affine transformations, while more strongly preserving the target\nunit's response. The discovered invariant images differed dramatically\ndepending on the choice of image representation used for optimization:\npixel-level changes primarily affected luminance and contrast, while stretching\nmid- and late-layer CNN representations altered texture and pose respectively.\nNotably, the invariant images from robust networks were more recognizable by\nhuman subjects than those from standard networks, supporting the higher\nfidelity of robust CNNs as models of the visual system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering which features' combinations high-level visual units encode is\ncritical to understand how images are transformed into representations that\nsupport recognition. While existing feature visualization approaches typically\ninfer a unit's most exciting images, this is insufficient to reveal the\nmanifold of transformations under which responses remain invariant, which is\nkey to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),\nan unbiased, model-agnostic, and gradient-free framework to systematically\ncharacterize a unit's invariance landscape and its vulnerability to adversarial\nperturbations in both biological and artificial visual systems. SnS frames\nthese transformations as bi-objective optimization problems. To probe\ninvariance, SnS seeks image perturbations that maximally alter the\nrepresentation of a reference stimulus in a given processing stage while\npreserving unit activation. To probe adversarial sensitivity, SnS seeks\nperturbations that minimally alter the stimulus while suppressing unit\nactivation. Applied to convolutional neural networks (CNNs), SnS revealed image\nvariations that were further from a reference image in pixel-space than those\nproduced by affine transformations, while more strongly preserving the target\nunit's response. The discovered invariant images differed dramatically\ndepending on the choice of image representation used for optimization:\npixel-level changes primarily affected luminance and contrast, while stretching\nmid- and late-layer CNN representations altered texture and pose respectively.\nNotably, the invariant images from robust networks were more recognizable by\nhuman subjects than those from standard networks, supporting the higher\nfidelity of robust CNNs as models of the visual system."
                },
                "authors": [
                    {
                        "name": "Lorenzo Tausani"
                    },
                    {
                        "name": "Paolo Muratore"
                    },
                    {
                        "name": "Morgan B. Talbot"
                    },
                    {
                        "name": "Giacomo Amerio"
                    },
                    {
                        "name": "Gabriel Kreiman"
                    },
                    {
                        "name": "Davide Zoccolan"
                    }
                ],
                "author_detail": {
                    "name": "Davide Zoccolan"
                },
                "author": "Davide Zoccolan",
                "arxiv_comment": "21 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11985v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11985v3",
                "updated": "2025-06-20T14:38:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    38,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-16T11:27:23Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    11,
                    27,
                    23,
                    2,
                    106,
                    0
                ],
                "title": "A Heavy-Metal Scenario of Ultra-High-Energy Cosmic Rays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Heavy-Metal Scenario of Ultra-High-Energy Cosmic Rays"
                },
                "summary": "The mass composition of ultra-high-energy cosmic rays is an open problem in\nastroparticle physics. It is usually inferred from the depth of the shower\nmaximum (Xmax) of cosmic-ray showers, which is only ambiguously determined by\nmodern hadronic interaction models. We examine a data-driven scenario, in which\nwe consider the expectation value of Xmax as a free parameter. We test the\nnovel hypothesis whether the cosmic-ray data from the Pierre Auger Observatory\ncan be interpreted in a consistent picture, under the assumption that the mass\ncomposition of cosmic rays at the highest energies is dominated by high\nmetallicity, resulting in pure iron nuclei at energies above ~40 EeV. We\ninvestigate the implications on astrophysical observations and hadronic\ninteractions, and we discuss the global consistency of the data assuming this\nheavy-metal scenario. We conclude that the data from the Pierre Auger\nObservatory can be interpreted consistently if the expectation values for Xmax\nfrom modern hadronic interaction models are shifted to larger values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass composition of ultra-high-energy cosmic rays is an open problem in\nastroparticle physics. It is usually inferred from the depth of the shower\nmaximum (Xmax) of cosmic-ray showers, which is only ambiguously determined by\nmodern hadronic interaction models. We examine a data-driven scenario, in which\nwe consider the expectation value of Xmax as a free parameter. We test the\nnovel hypothesis whether the cosmic-ray data from the Pierre Auger Observatory\ncan be interpreted in a consistent picture, under the assumption that the mass\ncomposition of cosmic rays at the highest energies is dominated by high\nmetallicity, resulting in pure iron nuclei at energies above ~40 EeV. We\ninvestigate the implications on astrophysical observations and hadronic\ninteractions, and we discuss the global consistency of the data assuming this\nheavy-metal scenario. We conclude that the data from the Pierre Auger\nObservatory can be interpreted consistently if the expectation values for Xmax\nfrom modern hadronic interaction models are shifted to larger values."
                },
                "authors": [
                    {
                        "name": "Jakub Vícha"
                    },
                    {
                        "name": "Alena Bakalová"
                    },
                    {
                        "name": "Ana L. Müller"
                    },
                    {
                        "name": "Olena Tkachenko"
                    },
                    {
                        "name": "Maximilian K. Stadelmaier"
                    }
                ],
                "author_detail": {
                    "name": "Maximilian K. Stadelmaier"
                },
                "author": "Maximilian K. Stadelmaier",
                "arxiv_doi": "10.3847/2041-8213/add536",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/add536",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.11985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11985v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted version",
                "arxiv_journal_ref": "ApJL 986 (2025) L34",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16813v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16813v3",
                "updated": "2025-06-20T14:35:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    35,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-25T15:28:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    28,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political\n  Argumentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political\n  Argumentation"
                },
                "summary": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support."
                },
                "authors": [
                    {
                        "name": "Svetlana Churina"
                    },
                    {
                        "name": "Kokil Jaidka"
                    }
                ],
                "author_detail": {
                    "name": "Kokil Jaidka"
                },
                "author": "Kokil Jaidka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16813v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16813v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05677v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05677v3",
                "updated": "2025-06-20T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    29,
                    2,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-08T22:27:38Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    22,
                    27,
                    38,
                    3,
                    128,
                    0
                ],
                "title": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment\n  Effect Estimation Under Non-adherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment\n  Effect Estimation Under Non-adherence"
                },
                "summary": "Estimates of heterogeneous treatment assignment effects can inform treatment\ndecisions. Under the presence of non-adherence (e.g., patients do not adhere to\ntheir assigned treatment), both the standard backdoor adjustment (SBD) and the\nconditional front-door adjustment (CFD) can recover unbiased estimates of the\ntreatment assignment effects. However, the estimation variance of these\napproaches may vary widely across settings, which remains underexplored in the\nliterature. In this work, we demonstrate theoretically and empirically that CFD\nyields lower-variance estimates than SBD when the true effect of treatment\nassignment is small (i.e., assigning an intervention leads to small changes in\npatients' future outcome). Additionally, since CFD requires estimating multiple\nnuisance parameters, we introduce LobsterNet, a multi-task neural network that\nimplements CFD with joint modeling of the nuisance parameters. Empirically,\nLobsterNet reduces estimation error across several semi-synthetic and\nreal-world datasets compared to baselines. Our findings suggest CFD with shared\nnuisance parameter modeling can improve treatment assignment effect estimation\nunder non-adherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimates of heterogeneous treatment assignment effects can inform treatment\ndecisions. Under the presence of non-adherence (e.g., patients do not adhere to\ntheir assigned treatment), both the standard backdoor adjustment (SBD) and the\nconditional front-door adjustment (CFD) can recover unbiased estimates of the\ntreatment assignment effects. However, the estimation variance of these\napproaches may vary widely across settings, which remains underexplored in the\nliterature. In this work, we demonstrate theoretically and empirically that CFD\nyields lower-variance estimates than SBD when the true effect of treatment\nassignment is small (i.e., assigning an intervention leads to small changes in\npatients' future outcome). Additionally, since CFD requires estimating multiple\nnuisance parameters, we introduce LobsterNet, a multi-task neural network that\nimplements CFD with joint modeling of the nuisance parameters. Empirically,\nLobsterNet reduces estimation error across several semi-synthetic and\nreal-world datasets compared to baselines. Our findings suggest CFD with shared\nnuisance parameter modeling can improve treatment assignment effect estimation\nunder non-adherence."
                },
                "authors": [
                    {
                        "name": "Winston Chen"
                    },
                    {
                        "name": "Trenton Chang"
                    },
                    {
                        "name": "Jenna Wiens"
                    }
                ],
                "author_detail": {
                    "name": "Jenna Wiens"
                },
                "author": "Jenna Wiens",
                "arxiv_comment": "Conference on Health, Inference, and Learning (CHIL) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05677v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02819v3",
                "updated": "2025-06-20T14:27:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    27,
                    20,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-05T17:47:42Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer\n  Block Linearization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReplaceMe: Network Simplification via Depth Pruning and Transformer\n  Block Linearization"
                },
                "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Sergey Zagoruyko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Zagoruyko"
                },
                "author": "Sergey Zagoruyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01437v2",
                "updated": "2025-06-20T14:24:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    24,
                    49,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-03T11:39:03Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    11,
                    39,
                    3,
                    0,
                    62,
                    0
                ],
                "title": "Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eau De $Q$-Network: Adaptive Distillation of Neural Networks in Deep\n  Reinforcement Learning"
                },
                "summary": "Recent works have successfully demonstrated that sparse deep reinforcement\nlearning agents can be competitive against their dense counterparts. This opens\nup opportunities for reinforcement learning applications in fields where\ninference time and memory requirements are cost-sensitive or limited by\nhardware. Until now, dense-to-sparse methods have relied on hand-designed\nsparsity schedules that are not synchronized with the agent's learning pace.\nCrucially, the final sparsity level is chosen as a hyperparameter, which\nrequires careful tuning as setting it too high might lead to poor performances.\nIn this work, we address these shortcomings by crafting a dense-to-sparse\nalgorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at\nthe agent's learning pace, we consider multiple online networks with different\nsparsity levels, where each online network is trained from a shared target\nnetwork. At each target update, the online network with the smallest loss is\nchosen as the next target network, while the other networks are replaced by a\npruned version of the chosen network. We evaluate the proposed approach on the\nAtari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN\nreaches high sparsity levels while keeping performances high.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have successfully demonstrated that sparse deep reinforcement\nlearning agents can be competitive against their dense counterparts. This opens\nup opportunities for reinforcement learning applications in fields where\ninference time and memory requirements are cost-sensitive or limited by\nhardware. Until now, dense-to-sparse methods have relied on hand-designed\nsparsity schedules that are not synchronized with the agent's learning pace.\nCrucially, the final sparsity level is chosen as a hyperparameter, which\nrequires careful tuning as setting it too high might lead to poor performances.\nIn this work, we address these shortcomings by crafting a dense-to-sparse\nalgorithm that we name Eau De $Q$-Network (EauDeQN). To increase sparsity at\nthe agent's learning pace, we consider multiple online networks with different\nsparsity levels, where each online network is trained from a shared target\nnetwork. At each target update, the online network with the smallest loss is\nchosen as the next target network, while the other networks are replaced by a\npruned version of the chosen network. We evaluate the proposed approach on the\nAtari $2600$ benchmark and the MuJoCo physics simulator, showing that EauDeQN\nreaches high sparsity levels while keeping performances high."
                },
                "authors": [
                    {
                        "name": "Théo Vincent"
                    },
                    {
                        "name": "Tim Faust"
                    },
                    {
                        "name": "Yogesh Tripathi"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Carlo D'Eramo"
                    }
                ],
                "author_detail": {
                    "name": "Carlo D'Eramo"
                },
                "author": "Carlo D'Eramo",
                "arxiv_comment": "Published at RLC 2025:\n  https://openreview.net/forum?id=Bb84iBj4wU#discussion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00128v3",
                "updated": "2025-06-20T14:13:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    13,
                    17,
                    4,
                    171,
                    0
                ],
                "published": "2024-08-29T05:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    18,
                    50,
                    3,
                    242,
                    0
                ],
                "title": "Can Large Language Models Replace Human Subjects? A Large-Scale\n  Replication of Scenario-Based Experiments in Psychology and Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Replace Human Subjects? A Large-Scale\n  Replication of Scenario-Based Experiments in Psychology and Management"
                },
                "summary": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions."
                },
                "authors": [
                    {
                        "name": "Ziyan Cui"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Huaikang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huaikang Zhou"
                },
                "author": "Huaikang Zhou",
                "arxiv_comment": "5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17006v1",
                "updated": "2025-06-20T13:59:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    59,
                    14,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    59,
                    14,
                    4,
                    171,
                    0
                ],
                "title": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It"
                },
                "summary": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility."
                },
                "authors": [
                    {
                        "name": "Danielle R. Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Shivang Gupta"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_comment": "Full research paper accepted at EC-TEL '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17001v1",
                "updated": "2025-06-20T13:52:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:52:15Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "title": "PersonalAI: Towards digital twins in the graph form",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalAI: Towards digital twins in the graph form"
                },
                "summary": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Ruslan Kostoev"
                    },
                    {
                        "name": "Ilya Perepechkin"
                    },
                    {
                        "name": "Ilnaz Salimov"
                    },
                    {
                        "name": "Victoria Dochkina"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Nikita Semenov"
                    }
                ],
                "author_detail": {
                    "name": "Nikita Semenov"
                },
                "author": "Nikita Semenov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15397v2",
                "updated": "2025-06-20T13:50:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    50,
                    0,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-23T00:47:13Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    0,
                    47,
                    13,
                    5,
                    328,
                    0
                ],
                "title": "Efficient Online Inference of Vision Transformers by Training-Free\n  Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Online Inference of Vision Transformers by Training-Free\n  Tokenization"
                },
                "summary": "The cost of deploying vision transformers increasingly represents a barrier\nto wider industrial adoption. Existing compression techniques require\nadditional end-to-end fine-tuning or incur a significant drawback to runtime,\nmaking them ill-suited for online (real-time) inference, where a prediction is\nmade on any new input as it comes in. We introduce the $\\textbf{Visual Word\nTokenizer}$ (VWT), a training-free method for reducing energy costs while\nretaining performance and runtime. The VWT groups visual subwords (image\npatches) that are frequently used into visual words while infrequent ones\nremain intact. To do so, $\\textit{intra}$-image or $\\textit{inter}$-image\nstatistics are leveraged to identify similar visual concepts for sequence\ncompression. Experimentally, we demonstrate a reduction in wattage of up to 25%\nwith only a 20% increase in runtime at most. Comparative approaches of 8-bit\nquantization and token merging achieve a lower or similar energy efficiency but\nexact a higher toll on runtime (up to 100% or more). Our results indicate that\nVWTs are well-suited for efficient online inference with a marginal compromise\non performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cost of deploying vision transformers increasingly represents a barrier\nto wider industrial adoption. Existing compression techniques require\nadditional end-to-end fine-tuning or incur a significant drawback to runtime,\nmaking them ill-suited for online (real-time) inference, where a prediction is\nmade on any new input as it comes in. We introduce the $\\textbf{Visual Word\nTokenizer}$ (VWT), a training-free method for reducing energy costs while\nretaining performance and runtime. The VWT groups visual subwords (image\npatches) that are frequently used into visual words while infrequent ones\nremain intact. To do so, $\\textit{intra}$-image or $\\textit{inter}$-image\nstatistics are leveraged to identify similar visual concepts for sequence\ncompression. Experimentally, we demonstrate a reduction in wattage of up to 25%\nwith only a 20% increase in runtime at most. Comparative approaches of 8-bit\nquantization and token merging achieve a lower or similar energy efficiency but\nexact a higher toll on runtime (up to 100% or more). Our results indicate that\nVWTs are well-suited for efficient online inference with a marginal compromise\non performance."
                },
                "authors": [
                    {
                        "name": "Leonidas Gee"
                    },
                    {
                        "name": "Wing Yan Li"
                    },
                    {
                        "name": "Viktoriia Sharmanska"
                    },
                    {
                        "name": "Novi Quadrianto"
                    }
                ],
                "author_detail": {
                    "name": "Novi Quadrianto"
                },
                "author": "Novi Quadrianto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14860v2",
                "updated": "2025-06-20T13:47:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    47,
                    25,
                    4,
                    171,
                    0
                ],
                "published": "2024-12-19T13:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling"
                },
                "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16994v1",
                "updated": "2025-06-20T13:43:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    43,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:43:54Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    43,
                    54,
                    4,
                    171,
                    0
                ],
                "title": "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for\n  Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for\n  Resource-Constrained Environments"
                },
                "summary": "Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world\nvision systems, especially in resource-constrained environments like drones,\nwhere memory and computation are limited. Existing prompt-driven UDA methods\ntypically rely on large vision-language models and require full access to\nsource-domain data during adaptation, limiting their applicability. In this\nwork, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain\nadaptation framework built around a teacher-student paradigm guided by\nprompt-based feature alignment. At the core of our method is a distilled and\nfine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A\nsmall set of low-level source features is aligned to the target domain\nsemantics-specified only through a natural language prompt-via Prompt-driven\nInstance Normalization (PIN). These semantically steered features are used to\nbriefly fine-tune the detection head of the teacher model. The adapted teacher\nthen generates high-quality pseudo-labels, which guide the on-the-fly\nadaptation of a compact student model. Experiments on the MDS-A dataset\ndemonstrate that Prmpt2Adpt achieves competitive detection performance compared\nto state-of-the-art methods, while delivering up to 7x faster adaptation and 5x\nfaster inference speed using few source images-making it a practical and\nscalable solution for real-time adaptation in low-resource domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world\nvision systems, especially in resource-constrained environments like drones,\nwhere memory and computation are limited. Existing prompt-driven UDA methods\ntypically rely on large vision-language models and require full access to\nsource-domain data during adaptation, limiting their applicability. In this\nwork, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain\nadaptation framework built around a teacher-student paradigm guided by\nprompt-based feature alignment. At the core of our method is a distilled and\nfine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A\nsmall set of low-level source features is aligned to the target domain\nsemantics-specified only through a natural language prompt-via Prompt-driven\nInstance Normalization (PIN). These semantically steered features are used to\nbriefly fine-tune the detection head of the teacher model. The adapted teacher\nthen generates high-quality pseudo-labels, which guide the on-the-fly\nadaptation of a compact student model. Experiments on the MDS-A dataset\ndemonstrate that Prmpt2Adpt achieves competitive detection performance compared\nto state-of-the-art methods, while delivering up to 7x faster adaptation and 5x\nfaster inference speed using few source images-making it a practical and\nscalable solution for real-time adaptation in low-resource domains."
                },
                "authors": [
                    {
                        "name": "Yasir Ali Farrukh"
                    },
                    {
                        "name": "Syed Wali"
                    },
                    {
                        "name": "Irfan Khan"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    }
                ],
                "author_detail": {
                    "name": "Nathaniel D. Bastian"
                },
                "author": "Nathaniel D. Bastian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16991v1",
                "updated": "2025-06-20T13:39:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    27,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:39:27Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    27,
                    4,
                    171,
                    0
                ],
                "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of\n  Forest LiDAR 3D Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of\n  Forest LiDAR 3D Point Clouds"
                },
                "summary": "The segmentation of forest LiDAR 3D point clouds, including both individual\ntree and semantic segmentation, is fundamental for advancing forest management\nand ecological research. However, current approaches often struggle with the\ncomplexity and variability of natural forest environments. We present\nForestFormer3D, a new unified and end-to-end framework designed for precise\nindividual tree and semantic segmentation. ForestFormer3D incorporates\nISA-guided query point selection, a score-based block merging strategy during\ninference, and a one-to-many association mechanism for effective training. By\ncombining these new components, our model achieves state-of-the-art performance\nfor individual tree segmentation on the newly introduced FOR-instanceV2\ndataset, which spans diverse forest types and regions. Additionally,\nForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),\nshowcasing its robustness across different forest conditions and sensor\nmodalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The segmentation of forest LiDAR 3D point clouds, including both individual\ntree and semantic segmentation, is fundamental for advancing forest management\nand ecological research. However, current approaches often struggle with the\ncomplexity and variability of natural forest environments. We present\nForestFormer3D, a new unified and end-to-end framework designed for precise\nindividual tree and semantic segmentation. ForestFormer3D incorporates\nISA-guided query point selection, a score-based block merging strategy during\ninference, and a one-to-many association mechanism for effective training. By\ncombining these new components, our model achieves state-of-the-art performance\nfor individual tree segmentation on the newly introduced FOR-instanceV2\ndataset, which spans diverse forest types and regions. Additionally,\nForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),\nshowcasing its robustness across different forest conditions and sensor\nmodalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Binbin Xiang"
                    },
                    {
                        "name": "Maciej Wielgosz"
                    },
                    {
                        "name": "Stefano Puliti"
                    },
                    {
                        "name": "Kamil Král"
                    },
                    {
                        "name": "Martin Krůček"
                    },
                    {
                        "name": "Azim Missarov"
                    },
                    {
                        "name": "Rasmus Astrup"
                    }
                ],
                "author_detail": {
                    "name": "Rasmus Astrup"
                },
                "author": "Rasmus Astrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16990v1",
                "updated": "2025-06-20T13:39:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    16,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:39:16Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    16,
                    4,
                    171,
                    0
                ],
                "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by\n  LLMs"
                },
                "summary": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert."
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Vijaykant Nadadur"
                    }
                ],
                "author_detail": {
                    "name": "Vijaykant Nadadur"
                },
                "author": "Vijaykant Nadadur",
                "arxiv_comment": "Accepted to the SDProc Workshop @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v3",
                "updated": "2025-06-20T13:34:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    34,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12911v2",
                "updated": "2025-06-20T13:24:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    24,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-18T14:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation"
                },
                "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes."
                },
                "authors": [
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16982v1",
                "updated": "2025-06-20T13:21:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    21,
                    14,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:21:14Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    21,
                    14,
                    4,
                    171,
                    0
                ],
                "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge\n  Tracing and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Bottleneck Models: A Framework for Interpretable Knowledge\n  Tracing and Beyond"
                },
                "summary": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality."
                },
                "authors": [
                    {
                        "name": "Antonin Berthon"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16975v1",
                "updated": "2025-06-20T13:08:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    8,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:08:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    8,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Latent Concept Disentanglement in Transformer-based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Concept Disentanglement in Transformer-based Language Models"
                },
                "summary": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Bhavya Vasudeva"
                    },
                    {
                        "name": "Vatsal Sharan"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16966v1",
                "updated": "2025-06-20T12:56:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    56,
                    11,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T12:56:11Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    56,
                    11,
                    4,
                    171,
                    0
                ],
                "title": "Autoregressive Hypergraph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Hypergraph"
                },
                "summary": "Traditional graph representations are insufficient for modelling real-world\nphenomena involving multi-entity interactions, such as collaborative projects\nor protein complexes, necessitating the use of hypergraphs. While hypergraphs\npreserve the intrinsic nature of such complex relationships, existing models\noften overlook temporal evolution in relational data. To address this, we\nintroduce a first-order autoregressive (i.e. AR(1)) model for dynamic\nnon-uniform hypergraphs. This is the first dynamic hypergraph model with\nprovable theoretical guarantees, explicitly defining the temporal evolution of\nhyperedge presence through transition probabilities that govern persistence and\nchange dynamics. This framework provides closed-form expressions for key\nprobabilistic properties and facilitates straightforward maximum-likelihood\ninference with uniform error bounds and asymptotic normality, along with a\npermutation-based diagnostic test. We also consider an AR(1) hypergraph\nstochastic block model (HSBM), where a novel Laplacian enables exact and\nefficient latent community recovery via a spectral clustering algorithm.\nFurthermore, we develop a likelihood-based change-point estimator for the HSBM\nto detect structural breaks within the time series. The efficacy and practical\nvalue of our methods are comprehensively demonstrated through extensive\nsimulation studies and compelling applications to a primary school interaction\ndata set and the Enron email corpus, revealing insightful community structures\nand significant temporal changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional graph representations are insufficient for modelling real-world\nphenomena involving multi-entity interactions, such as collaborative projects\nor protein complexes, necessitating the use of hypergraphs. While hypergraphs\npreserve the intrinsic nature of such complex relationships, existing models\noften overlook temporal evolution in relational data. To address this, we\nintroduce a first-order autoregressive (i.e. AR(1)) model for dynamic\nnon-uniform hypergraphs. This is the first dynamic hypergraph model with\nprovable theoretical guarantees, explicitly defining the temporal evolution of\nhyperedge presence through transition probabilities that govern persistence and\nchange dynamics. This framework provides closed-form expressions for key\nprobabilistic properties and facilitates straightforward maximum-likelihood\ninference with uniform error bounds and asymptotic normality, along with a\npermutation-based diagnostic test. We also consider an AR(1) hypergraph\nstochastic block model (HSBM), where a novel Laplacian enables exact and\nefficient latent community recovery via a spectral clustering algorithm.\nFurthermore, we develop a likelihood-based change-point estimator for the HSBM\nto detect structural breaks within the time series. The efficacy and practical\nvalue of our methods are comprehensively demonstrated through extensive\nsimulation studies and compelling applications to a primary school interaction\ndata set and the Enron email corpus, revealing insightful community structures\nand significant temporal changes."
                },
                "authors": [
                    {
                        "name": "Xianghe Zhu"
                    },
                    {
                        "name": "Qiwei Yao"
                    }
                ],
                "author_detail": {
                    "name": "Qiwei Yao"
                },
                "author": "Qiwei Yao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2010.04492",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v3",
                "updated": "2025-06-20T12:52:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    52,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "LogProber: Disentangling confidence from contamination in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogProber: Disentangling confidence from contamination in LLM responses"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical. In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical. In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10599v3",
                "updated": "2025-06-20T12:43:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    43,
                    30,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-12T11:43:40Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    43,
                    40,
                    3,
                    163,
                    0
                ],
                "title": "Enhancing Taiji's Parameter Estimation under Non-Stationarity: a\n  Time-Frequency Domain Framework for Galactic Binaries and Instrumental Noises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Taiji's Parameter Estimation under Non-Stationarity: a\n  Time-Frequency Domain Framework for Galactic Binaries and Instrumental Noises"
                },
                "summary": "The data analysis of space-based gravitational wave detectors like Taiji\nfaces significant challenges from non-stationary noise, which compromises the\nefficacy of traditional frequency-domain analysis. This work proposes a unified\nframework based on short-time Fourier transform (STFT) to enhance parameter\nestimation of Galactic binary and characterization of instrumental noise under\nnon-stationarity. Segmenting data into locally stationary intervals, we derive\nSTFT-based models for signals and noises, and implement Bayesian inference via\nthe extended Whittle likelihood. Validated through the analysis of verification\nGalactic binaries and instrumental noises, our STFT approach outperforms\nfrequency-domain methods by reducing the uncertainty and bias of estimation,\nsuccessfully recovering low signal-to-noise ratio signals missed by\nfrequency-domain analysis, and mitigating the degeneracy among noise\nparameters. The framework's robustness against noise drifts and computational\nefficiency highlight its potential for integration into future global analysis\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data analysis of space-based gravitational wave detectors like Taiji\nfaces significant challenges from non-stationary noise, which compromises the\nefficacy of traditional frequency-domain analysis. This work proposes a unified\nframework based on short-time Fourier transform (STFT) to enhance parameter\nestimation of Galactic binary and characterization of instrumental noise under\nnon-stationarity. Segmenting data into locally stationary intervals, we derive\nSTFT-based models for signals and noises, and implement Bayesian inference via\nthe extended Whittle likelihood. Validated through the analysis of verification\nGalactic binaries and instrumental noises, our STFT approach outperforms\nfrequency-domain methods by reducing the uncertainty and bias of estimation,\nsuccessfully recovering low signal-to-noise ratio signals missed by\nfrequency-domain analysis, and mitigating the degeneracy among noise\nparameters. The framework's robustness against noise drifts and computational\nefficiency highlight its potential for integration into future global analysis\npipelines."
                },
                "authors": [
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "Ziren Luo"
                    },
                    {
                        "name": "Peng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Xu"
                },
                "author": "Peng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05692v3",
                "updated": "2025-06-20T12:42:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    42,
                    57,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-06T02:48:02Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    48,
                    2,
                    4,
                    157,
                    0
                ],
                "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection\n  in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection\n  in LLM-Generated Code"
                },
                "summary": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce SafeGenBench, a benchmark specifically\ndesigned to assess the security of LLM-generated code. The dataset encompasses\na wide range of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce SafeGenBench, a benchmark specifically\ndesigned to assess the security of LLM-generated code. The dataset encompasses\na wide range of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon."
                },
                "authors": [
                    {
                        "name": "Xinghang Li"
                    },
                    {
                        "name": "Jingzhe Ding"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Bing Zhao"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hongwan Gao"
                    },
                    {
                        "name": "Xinchen Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xinchen Gu"
                },
                "author": "Xinchen Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16952v1",
                "updated": "2025-06-20T12:37:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    37,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T12:37:43Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    37,
                    43,
                    4,
                    171,
                    0
                ],
                "title": "Modeling and Visualization Reasoning for Stakeholders in Education and\n  Industry Integration Systems: Research on Structured Synthetic Dialogue Data\n  Generation Based on NIST Standards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Visualization Reasoning for Stakeholders in Education and\n  Industry Integration Systems: Research on Structured Synthetic Dialogue Data\n  Generation Based on NIST Standards"
                },
                "summary": "This study addresses the structural complexity and semantic ambiguity in\nstakeholder interactions within the Education-Industry Integration (EII)\nsystem. The scarcity of real interview data, absence of structured variable\nmodeling, and lack of interpretability in inference mechanisms have limited the\nanalytical accuracy and policy responsiveness of EII research. To resolve these\nchallenges, we propose a structural modeling paradigm based on the National\nInstitute of Standards and Technology (NIST) synthetic data quality framework,\nfocusing on consistency, authenticity, and traceability. We design a five-layer\narchitecture that includes prompt-driven synthetic dialogue generation, a\nstructured variable system covering skills, institutional, and emotional\ndimensions, dependency and causal path modeling, graph-based structure design,\nand an interactive inference engine. Empirical results demonstrate the\neffectiveness of the approach using a 15-segment synthetic corpus, with 41,597\ntokens, 127 annotated variables, and 820 semantic relationship triples. The\nmodel exhibits strong structural consistency (Krippendorff alpha = 0.83),\nconstruct validity (RMSEA = 0.048, CFI = 0.93), and semantic alignment (mean\ncosine similarity > 0.78 via BERT). A key causal loop is identified: system\nmismatch leads to emotional frustration, reduced participation, skill gaps, and\nrecurrence of mismatch, revealing a structural degradation cycle. This research\nintroduces the first NIST-compliant AI modeling framework for stakeholder\nsystems and provides a foundation for policy simulation, curriculum design, and\ncollaborative strategy modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the structural complexity and semantic ambiguity in\nstakeholder interactions within the Education-Industry Integration (EII)\nsystem. The scarcity of real interview data, absence of structured variable\nmodeling, and lack of interpretability in inference mechanisms have limited the\nanalytical accuracy and policy responsiveness of EII research. To resolve these\nchallenges, we propose a structural modeling paradigm based on the National\nInstitute of Standards and Technology (NIST) synthetic data quality framework,\nfocusing on consistency, authenticity, and traceability. We design a five-layer\narchitecture that includes prompt-driven synthetic dialogue generation, a\nstructured variable system covering skills, institutional, and emotional\ndimensions, dependency and causal path modeling, graph-based structure design,\nand an interactive inference engine. Empirical results demonstrate the\neffectiveness of the approach using a 15-segment synthetic corpus, with 41,597\ntokens, 127 annotated variables, and 820 semantic relationship triples. The\nmodel exhibits strong structural consistency (Krippendorff alpha = 0.83),\nconstruct validity (RMSEA = 0.048, CFI = 0.93), and semantic alignment (mean\ncosine similarity > 0.78 via BERT). A key causal loop is identified: system\nmismatch leads to emotional frustration, reduced participation, skill gaps, and\nrecurrence of mismatch, revealing a structural degradation cycle. This research\nintroduces the first NIST-compliant AI modeling framework for stakeholder\nsystems and provides a foundation for policy simulation, curriculum design, and\ncollaborative strategy modeling."
                },
                "authors": [
                    {
                        "name": "Wei Meng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Meng"
                },
                "author": "Wei Meng",
                "arxiv_comment": "This paper presents an innovative and rigorous framework for\n  stakeholder modelling in education-industry integration, combining\n  NIST-compliant synthetic data generation with interpretable visual reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30, 91B06, 05C82, 62H30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; H.2.8; K.3.1; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13008v2",
                "updated": "2025-06-20T12:30:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    30,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-19T11:51:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    51,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Adversarial Reasoning for Repair Based on Inferred Program Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Reasoning for Repair Based on Inferred Program Intent"
                },
                "summary": "Automated program repair (APR) has shown promising results, particularly with\nthe use of neural networks. Currently, most APR tools focus on code\ntransformations specified by test suites, rather than reasoning about the\nprogram intent and the high-level bug specification. Without a proper\nunderstanding of program intent, these tools tend to generate patches that\noverfit incomplete test suites and fail to reflect the developers intentions.\nHowever, reasoning about program intent is challenging. In our work, we propose\nan approach called AdverIntent-Agent, based on critique and adversarial\nreasoning. Our approach is novel to shift the focus from generating multiple\nAPR patches to inferring multiple potential program intents. Ideally, we aim to\ninfer intents that are, to some extent, adversarial to each other, maximizing\nthe probability that at least one aligns closely with the developers original\nintent. AdverIntent-Agent is a multi-agent approach consisting of three agents:\na reasoning agent, a test agent, and a repair agent. First, the reasoning agent\ngenerates adversarial program intents along with the corresponding faulty\nstatements. Next, the test agent produces adversarial test cases that align\nwith each inferred intent, constructing oracles that use the same inputs but\nhave different expected outputs. Finally, the repair agent uses dynamic and\nprecise LLM prompts to generate patches that satisfy both the inferred program\nintent and the generated tests. AdverIntent-Agent was evaluated on two\nbenchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly\nrepaired 77 and 105 bugs in both benchmarks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) has shown promising results, particularly with\nthe use of neural networks. Currently, most APR tools focus on code\ntransformations specified by test suites, rather than reasoning about the\nprogram intent and the high-level bug specification. Without a proper\nunderstanding of program intent, these tools tend to generate patches that\noverfit incomplete test suites and fail to reflect the developers intentions.\nHowever, reasoning about program intent is challenging. In our work, we propose\nan approach called AdverIntent-Agent, based on critique and adversarial\nreasoning. Our approach is novel to shift the focus from generating multiple\nAPR patches to inferring multiple potential program intents. Ideally, we aim to\ninfer intents that are, to some extent, adversarial to each other, maximizing\nthe probability that at least one aligns closely with the developers original\nintent. AdverIntent-Agent is a multi-agent approach consisting of three agents:\na reasoning agent, a test agent, and a repair agent. First, the reasoning agent\ngenerates adversarial program intents along with the corresponding faulty\nstatements. Next, the test agent produces adversarial test cases that align\nwith each inferred intent, constructing oracles that use the same inputs but\nhave different expected outputs. Finally, the repair agent uses dynamic and\nprecise LLM prompts to generate patches that satisfy both the inferred program\nintent and the generated tests. AdverIntent-Agent was evaluated on two\nbenchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly\nrepaired 77 and 105 bugs in both benchmarks, respectively."
                },
                "authors": [
                    {
                        "name": "He Ye"
                    },
                    {
                        "name": "Aidan Z. H. Yang"
                    },
                    {
                        "name": "Chang Hu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13593v2",
                "updated": "2025-06-20T12:12:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    12,
                    17,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-16T15:21:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs"
                },
                "summary": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models."
                },
                "authors": [
                    {
                        "name": "Hen Davidov"
                    },
                    {
                        "name": "Gilad Freidkin"
                    },
                    {
                        "name": "Shai Feldman"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v2",
                "updated": "2025-06-20T12:02:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    2,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "Penetration-testing, while critical for validating defenses and uncovering\nvulnerabilities, is often limited by high operational costs and the scarcity of\nhuman expertise. This paper investigates the feasibility and effectiveness of\nusing Large Language Model (LLM)-driven autonomous systems to address these\nchallenges in real-world Microsoft Active Directory (AD) enterprise networks.\nOur novel prototype, cochise, represents the first demonstration of a fully\nautonomous, LLM-driven framework capable of compromising accounts within a\nreal-life Microsoft AD testbed (GOAD). The evaluation deliberately utilizes\nGOAD to capture the intricate interactions and sometimes nondeterministic\noutcomes of live network pen-testing, moving beyond the limitations of\nsynthetic benchmarks. We perform our empirical evaluation using five LLMs,\ncomparing reasoning to non-reasoning models as well as including open-weight\nmodels. Through comprehensive quantitative and qualitative analysis,\nincorporating insights from cybersecurity experts, we demonstrate that\nautonomous LLMs can effectively conduct Assumed Breach simulations. Key\nfindings highlight their ability to dynamically adapt attack strategies,\nperform inter-context attacks, and generate scenario-specific attack\nparameters. Cochise also exhibits robust self-correction mechanisms,\nautomatically installing missing tools and rectifying invalid command\ngenerations. Critically, we find that the associated costs are competitive with\nthose incurred by professional pen-testers, suggesting a path toward\ndemocratizing access to essential security testing for organizations with\nbudgetary constraints. However, our research also illuminates existing\nlimitations, including instances of LLM ``going down rabbit holes'', challenges\nin comprehensive information transfer between planning and execution modules,\nand critical safety concerns that necessitate human oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration-testing, while critical for validating defenses and uncovering\nvulnerabilities, is often limited by high operational costs and the scarcity of\nhuman expertise. This paper investigates the feasibility and effectiveness of\nusing Large Language Model (LLM)-driven autonomous systems to address these\nchallenges in real-world Microsoft Active Directory (AD) enterprise networks.\nOur novel prototype, cochise, represents the first demonstration of a fully\nautonomous, LLM-driven framework capable of compromising accounts within a\nreal-life Microsoft AD testbed (GOAD). The evaluation deliberately utilizes\nGOAD to capture the intricate interactions and sometimes nondeterministic\noutcomes of live network pen-testing, moving beyond the limitations of\nsynthetic benchmarks. We perform our empirical evaluation using five LLMs,\ncomparing reasoning to non-reasoning models as well as including open-weight\nmodels. Through comprehensive quantitative and qualitative analysis,\nincorporating insights from cybersecurity experts, we demonstrate that\nautonomous LLMs can effectively conduct Assumed Breach simulations. Key\nfindings highlight their ability to dynamically adapt attack strategies,\nperform inter-context attacks, and generate scenario-specific attack\nparameters. Cochise also exhibits robust self-correction mechanisms,\nautomatically installing missing tools and rectifying invalid command\ngenerations. Critically, we find that the associated costs are competitive with\nthose incurred by professional pen-testers, suggesting a path toward\ndemocratizing access to essential security testing for organizations with\nbudgetary constraints. However, our research also illuminates existing\nlimitations, including instances of LLM ``going down rabbit holes'', challenges\nin comprehensive information transfer between planning and execution modules,\nand critical safety concerns that necessitate human oversight."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09562v2",
                "updated": "2025-06-20T11:49:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    11,
                    49,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-13T13:26:01Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    13,
                    26,
                    1,
                    6,
                    103,
                    0
                ],
                "title": "From Geometry to Observation: Gravitational Waves and the Raychaudhuri\n  Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Geometry to Observation: Gravitational Waves and the Raychaudhuri\n  Equation"
                },
                "summary": "Gravitational waves (GWs) are independent of any particular theory of\ngravity. The universality of this notion is highlighted by the Raychaudhuri\nequation (RE), which is independent of any theory of gravity and contains the\nRicci tensor $R_{\\mu\\nu}$ as a key ingredient, thereby connecting spacetime\ngeometry with matter-energy content. Under small metric perturbations,\n$R_{\\mu\\nu} \\propto \\Box h_{\\mu\\nu}$, where $h_{\\mu\\nu}$ is the perturbation,\nindicating that various gravity theories, via their corresponding $R_{\\mu\\nu}$,\nproduce different gravitational wave equations. In the framework of Einstein's\ngravity, this leads to the standard wave equation. This study analyzes a\nmodified form, {\\it GW-inspired RE}, within the homogeneous and isotropic FLRW\nbackground to investigate late-time cosmic acceleration and structure\nformation. We employ {\\it Pantheon+ SNe Ia, Hubble, and BAO} datasets to\nconstrain model parameters through Bayesian inference utilizing NUTS in {\\it\nNumPyro}. A nuisance parameter $\\mu_0$ is introduced to address residual\nsystematics. This facilitates a robust estimation of $H_0$, $\\Omega_{DE,0}$,\nand $r_d$, which addresses the resolution of the Hubble tension. We analyze the\nredshift evolution of the deceleration parameter, $q(z)$, both with and without\n$\\mu_0$, emphasizing its influence on cosmic dynamics. The GW-inspired RE is\nreformulated as a harmonic oscillator, providing insight into expansion and\ngeodesic focusing. A graphical comparison demonstrates the relationship\n$d^{GW}_L(z) = d^{EM}_L(z)$ utilizing GWOSC data. Thus, the RE in the context\nof small perturbation of the metric opens up whole new vistas of {\\it\nobservational astronomy.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) are independent of any particular theory of\ngravity. The universality of this notion is highlighted by the Raychaudhuri\nequation (RE), which is independent of any theory of gravity and contains the\nRicci tensor $R_{\\mu\\nu}$ as a key ingredient, thereby connecting spacetime\ngeometry with matter-energy content. Under small metric perturbations,\n$R_{\\mu\\nu} \\propto \\Box h_{\\mu\\nu}$, where $h_{\\mu\\nu}$ is the perturbation,\nindicating that various gravity theories, via their corresponding $R_{\\mu\\nu}$,\nproduce different gravitational wave equations. In the framework of Einstein's\ngravity, this leads to the standard wave equation. This study analyzes a\nmodified form, {\\it GW-inspired RE}, within the homogeneous and isotropic FLRW\nbackground to investigate late-time cosmic acceleration and structure\nformation. We employ {\\it Pantheon+ SNe Ia, Hubble, and BAO} datasets to\nconstrain model parameters through Bayesian inference utilizing NUTS in {\\it\nNumPyro}. A nuisance parameter $\\mu_0$ is introduced to address residual\nsystematics. This facilitates a robust estimation of $H_0$, $\\Omega_{DE,0}$,\nand $r_d$, which addresses the resolution of the Hubble tension. We analyze the\nredshift evolution of the deceleration parameter, $q(z)$, both with and without\n$\\mu_0$, emphasizing its influence on cosmic dynamics. The GW-inspired RE is\nreformulated as a harmonic oscillator, providing insight into expansion and\ngeodesic focusing. A graphical comparison demonstrates the relationship\n$d^{GW}_L(z) = d^{EM}_L(z)$ utilizing GWOSC data. Thus, the RE in the context\nof small perturbation of the metric opens up whole new vistas of {\\it\nobservational astronomy.}"
                },
                "authors": [
                    {
                        "name": "Sougata Bhunia"
                    },
                    {
                        "name": "Anubhab Dutta"
                    },
                    {
                        "name": "Debashis Gangopadhyay"
                    },
                    {
                        "name": "Goutam Manna"
                    }
                ],
                "author_detail": {
                    "name": "Goutam Manna"
                },
                "author": "Goutam Manna",
                "arxiv_comment": "17 pages, 10 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03819v2",
                "updated": "2025-06-20T11:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    11,
                    37,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-05T19:00:02Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    19,
                    0,
                    2,
                    2,
                    64,
                    0
                ],
                "title": "Exploring the evolution of gravitational-wave emitters with efficient\n  emulation: Constraining the origins of binary black holes using normalising\n  flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the evolution of gravitational-wave emitters with efficient\n  emulation: Constraining the origins of binary black holes using normalising\n  flows"
                },
                "summary": "Binary population synthesis simulations allow detailed modelling of\ngravitational-wave sources from a variety of formation channels. These\npopulation models can be compared to the observed catalogue of merging binaries\nto infer the uncertain astrophysical input parameters describing binary\nformation and evolution, as well as relative rates between various formation\npathways. However, it is computationally infeasible to run population synthesis\nsimulations for all variations of uncertain input physics. We demonstrate the\nuse of normalising flows to emulate population synthesis results and\ninterpolate between astrophysical input parameters. Using current\ngravitational-wave observations of binary black holes, we use our trained\nnormalising flows to infer branching ratios between multiple formation\nchannels, and simultaneously infer common-envelope efficiency and natal spins\nacross a continuous parameter range. Given our set of formation channel models,\nwe infer the natal spin to be $0.04^{+0.04}_{-0.01}$, and the common-envelope\nefficiency to be $>3.7$ at 90% credibility, with the majority of underlying\nmergers coming from the common-envelope channel. Our framework allows us to\nmeasure population synthesis inputs where we do not have simulations, and\nbetter constrain the astrophysics underlying current gravitational-wave\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary population synthesis simulations allow detailed modelling of\ngravitational-wave sources from a variety of formation channels. These\npopulation models can be compared to the observed catalogue of merging binaries\nto infer the uncertain astrophysical input parameters describing binary\nformation and evolution, as well as relative rates between various formation\npathways. However, it is computationally infeasible to run population synthesis\nsimulations for all variations of uncertain input physics. We demonstrate the\nuse of normalising flows to emulate population synthesis results and\ninterpolate between astrophysical input parameters. Using current\ngravitational-wave observations of binary black holes, we use our trained\nnormalising flows to infer branching ratios between multiple formation\nchannels, and simultaneously infer common-envelope efficiency and natal spins\nacross a continuous parameter range. Given our set of formation channel models,\nwe infer the natal spin to be $0.04^{+0.04}_{-0.01}$, and the common-envelope\nefficiency to be $>3.7$ at 90% credibility, with the majority of underlying\nmergers coming from the common-envelope channel. Our framework allows us to\nmeasure population synthesis inputs where we do not have simulations, and\nbetter constrain the astrophysics underlying current gravitational-wave\npopulations."
                },
                "authors": [
                    {
                        "name": "Storm Colloms"
                    },
                    {
                        "name": "Christopher P L Berry"
                    },
                    {
                        "name": "John Veitch"
                    },
                    {
                        "name": "Michael Zevin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zevin"
                },
                "author": "Michael Zevin",
                "arxiv_doi": "10.3847/1538-4357/ade546",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ade546",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.03819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 6 figures, 2 tables, 1 appendix. Accepted to Astrophysical\n  Journal. Data release at https://doi.org/10.5281/zenodo.14967687",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16916v1",
                "updated": "2025-06-20T11:18:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    11,
                    18,
                    32,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T11:18:32Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    11,
                    18,
                    32,
                    4,
                    171,
                    0
                ],
                "title": "Inference for SDEs driven by Hermite processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for SDEs driven by Hermite processes"
                },
                "summary": "In the paper, we address parametric and non-parametric estimation for\nnonlinear stochastic differential equations with additive Hermite noise with\npossibly nonlinear scaling. We assume that a single trajectory of the solution\nis observed discretely and we propose estimators of the Hurst parameter and the\nHermite order of the driving process as well as of the average noise intensity\nand noise intensity function. The estimators are based on the weighted\nquadratic variation whose properties are used, in particular, to prove weak\nconsistency of the proposed estimators under in-fill asymptotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the paper, we address parametric and non-parametric estimation for\nnonlinear stochastic differential equations with additive Hermite noise with\npossibly nonlinear scaling. We assume that a single trajectory of the solution\nis observed discretely and we propose estimators of the Hurst parameter and the\nHermite order of the driving process as well as of the average noise intensity\nand noise intensity function. The estimators are based on the weighted\nquadratic variation whose properties are used, in particular, to prove weak\nconsistency of the proposed estimators under in-fill asymptotics."
                },
                "authors": [
                    {
                        "name": "Petr Coupek"
                    },
                    {
                        "name": "Pavel Kriz"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Kriz"
                },
                "author": "Pavel Kriz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G22, 62M09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01208v3",
                "updated": "2025-06-20T10:54:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    54,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-03T09:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "On Almost Surely Safe Alignment of Large Language Models at\n  Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Almost Surely Safe Alignment of Large Language Models at\n  Inference-Time"
                },
                "summary": "We introduce a novel inference-time alignment approach for LLMs that aims to\ngenerate safe responses almost surely, i.e., with probability approaching one.\nOur approach models the generation of safe responses as a constrained Markov\nDecision Process (MDP) within the LLM's latent space. We augment a safety state\nthat tracks the evolution of safety constraints and dynamically penalize unsafe\ngenerations to ensure the generation of safe responses. Consequently, we\ndemonstrate formal safety guarantees w.r.t. the given cost model upon solving\nthe MDP in the latent space with sufficiently large penalties. Building on this\nfoundation, we propose InferenceGuard, a practical implementation that safely\naligns LLMs without modifying the model weights. Empirically, we demonstrate\nthat InferenceGuard effectively balances safety and task performance,\noutperforming existing inference-time alignment methods in generating safe and\naligned responses. Our findings contribute to the advancement of safer LLM\ndeployment through alignment at inference-time, thus presenting a promising\nalternative to resource-intensive, overfitting-prone alignment techniques like\nRLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel inference-time alignment approach for LLMs that aims to\ngenerate safe responses almost surely, i.e., with probability approaching one.\nOur approach models the generation of safe responses as a constrained Markov\nDecision Process (MDP) within the LLM's latent space. We augment a safety state\nthat tracks the evolution of safety constraints and dynamically penalize unsafe\ngenerations to ensure the generation of safe responses. Consequently, we\ndemonstrate formal safety guarantees w.r.t. the given cost model upon solving\nthe MDP in the latent space with sufficiently large penalties. Building on this\nfoundation, we propose InferenceGuard, a practical implementation that safely\naligns LLMs without modifying the model weights. Empirically, we demonstrate\nthat InferenceGuard effectively balances safety and task performance,\noutperforming existing inference-time alignment methods in generating safe and\naligned responses. Our findings contribute to the advancement of safer LLM\ndeployment through alignment at inference-time, thus presenting a promising\nalternative to resource-intensive, overfitting-prone alignment techniques like\nRLHF."
                },
                "authors": [
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Shyam Sundhar Ramesh"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16899v1",
                "updated": "2025-06-20T10:46:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    46,
                    35,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T10:46:35Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    46,
                    35,
                    4,
                    171,
                    0
                ],
                "title": "Towards Effective Complementary Security Analysis using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective Complementary Security Analysis using Large Language\n  Models"
                },
                "summary": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms."
                },
                "authors": [
                    {
                        "name": "Jonas Wagner"
                    },
                    {
                        "name": "Simon Müller"
                    },
                    {
                        "name": "Christian Näther"
                    },
                    {
                        "name": "Jan-Philipp Steghöfer"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11480v2",
                "updated": "2025-06-20T10:31:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    31,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-13T06:05:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    5,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large\n  Language Models Based on Improved Gradient Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large\n  Language Models Based on Improved Gradient Alignment"
                },
                "summary": "Reinforcement learning (RL) has become a key technique for enhancing LLMs'\nreasoning abilities, yet its data inefficiency remains a major bottleneck. To\naddress this critical yet challenging issue, we present a novel\ngradient-alignment-based method, named LearnAlign, which intelligently selects\nthe learnable and representative training reasoning data for RL post-training.\nTo overcome the issue of response-length bias in gradient norms, we introduce\nthe data learnability based on the success rate, which can indicate the\nlearning potential of each data point. Experiments across three mathematical\nreasoning benchmarks demonstrate that our method significantly reduces training\ndata requirements while achieving minor performance degradation or even\nimproving performance compared to full-data training. For example, it reduces\ndata requirements by up to 1,000 data points with better performance (77.53%)\nthan that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show\nits effectiveness in the staged RL setting. This work provides valuable\ninsights into data-efficient RL post-training and establishes a foundation for\nfuture research in optimizing reasoning data selection. To facilitate future\nwork, we will release code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a key technique for enhancing LLMs'\nreasoning abilities, yet its data inefficiency remains a major bottleneck. To\naddress this critical yet challenging issue, we present a novel\ngradient-alignment-based method, named LearnAlign, which intelligently selects\nthe learnable and representative training reasoning data for RL post-training.\nTo overcome the issue of response-length bias in gradient norms, we introduce\nthe data learnability based on the success rate, which can indicate the\nlearning potential of each data point. Experiments across three mathematical\nreasoning benchmarks demonstrate that our method significantly reduces training\ndata requirements while achieving minor performance degradation or even\nimproving performance compared to full-data training. For example, it reduces\ndata requirements by up to 1,000 data points with better performance (77.53%)\nthan that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show\nits effectiveness in the staged RL setting. This work provides valuable\ninsights into data-efficient RL post-training and establishes a foundation for\nfuture research in optimizing reasoning data selection. To facilitate future\nwork, we will release code."
                },
                "authors": [
                    {
                        "name": "Shikun Li"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Zhiqin Yang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Gaode Chen"
                    },
                    {
                        "name": "Xiaobo Xia"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Zhe Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Peng"
                },
                "author": "Zhe Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05328v2",
                "updated": "2025-06-20T10:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    27,
                    28,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-07T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models"
                },
                "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems."
                },
                "authors": [
                    {
                        "name": "Anar Yeginbergen"
                    },
                    {
                        "name": "Maite Oronoz"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07717v3",
                "updated": "2025-06-20T10:23:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    23,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-10T13:09:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16889v1",
                "updated": "2025-06-20T10:21:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    21,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T10:21:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    21,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of\n  Music Mastering Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITO-Master: Inference-Time Optimization for Audio Effects Modeling of\n  Music Mastering Processors"
                },
                "summary": "Music mastering style transfer aims to model and apply the mastering\ncharacteristics of a reference track to a target track, simulating the\nprofessional mastering process. However, existing methods apply fixed\nprocessing based on a reference track, limiting users' ability to fine-tune the\nresults to match their artistic intent. In this paper, we introduce the\nITO-Master framework, a reference-based mastering style transfer system that\nintegrates Inference-Time Optimization (ITO) to enable finer user control over\nthe mastering process. By optimizing the reference embedding during inference,\nour approach allows users to refine the output dynamically, making micro-level\nadjustments to achieve more precise mastering results. We explore both\nblack-box and white-box methods for modeling mastering processors and\ndemonstrate that ITO improves mastering performance across different styles.\nThrough objective evaluation, subjective listening tests, and qualitative\nanalysis using text-based conditioning with CLAP embeddings, we validate that\nITO enhances mastering style similarity while offering increased adaptability.\nOur framework provides an effective and user-controllable solution for\nmastering style transfer, allowing users to refine their results beyond the\ninitial style transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Music mastering style transfer aims to model and apply the mastering\ncharacteristics of a reference track to a target track, simulating the\nprofessional mastering process. However, existing methods apply fixed\nprocessing based on a reference track, limiting users' ability to fine-tune the\nresults to match their artistic intent. In this paper, we introduce the\nITO-Master framework, a reference-based mastering style transfer system that\nintegrates Inference-Time Optimization (ITO) to enable finer user control over\nthe mastering process. By optimizing the reference embedding during inference,\nour approach allows users to refine the output dynamically, making micro-level\nadjustments to achieve more precise mastering results. We explore both\nblack-box and white-box methods for modeling mastering processors and\ndemonstrate that ITO improves mastering performance across different styles.\nThrough objective evaluation, subjective listening tests, and qualitative\nanalysis using text-based conditioning with CLAP embeddings, we validate that\nITO enhances mastering style similarity while offering increased adaptability.\nOur framework provides an effective and user-controllable solution for\nmastering style transfer, allowing users to refine their results beyond the\ninitial style transfer."
                },
                "authors": [
                    {
                        "name": "Junghyun Koo"
                    },
                    {
                        "name": "Marco A. Martinez-Ramirez"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Giorgio Fabbro"
                    },
                    {
                        "name": "Michele Mancusi"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "ISMIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00119v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00119v3",
                "updated": "2025-06-20T10:02:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    2,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-28T09:12:04Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    9,
                    12,
                    4,
                    3,
                    333,
                    0
                ],
                "title": "Training Multi-Layer Binary Neural Networks With Local Binary Error\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Multi-Layer Binary Neural Networks With Local Binary Error\n  Signals"
                },
                "summary": "Binary Neural Networks (BNNs) significantly reduce computational complexity\nand memory usage in machine and deep learning by representing weights and\nactivations with just one bit. However, most existing training algorithms for\nBNNs rely on quantization-aware floating-point Stochastic Gradient Descent\n(SGD), limiting the full exploitation of binary operations to the inference\nphase only. In this work, we propose, for the first time, a fully binary and\ngradient-free training algorithm for multi-layer BNNs, eliminating the need for\nback-propagated floating-point gradients. Specifically, the proposed algorithm\nrelies on local binary error signals and binary weight updates, employing\ninteger-valued hidden weights that serve as a synaptic metaplasticity\nmechanism, thereby enhancing its neurobiological plausibility. Our proposed\nsolution enables the training of binary multi-layer perceptrons by using\nexclusively XNOR, Popcount, and increment/decrement operations. Experimental\nresults on multi-class classification benchmarks show test accuracy\nimprovements of up to +35.47% over the only existing fully binary single-layer\nstate-of-the-art solution. Compared to full-precision SGD, our solution\nimproves test accuracy by up to +35.30% under the same total memory demand,\nwhile also reducing computational cost by two to three orders of magnitude in\nterms of the total number of Boolean gates. The proposed algorithm is made\navailable to the scientific community as a public repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary Neural Networks (BNNs) significantly reduce computational complexity\nand memory usage in machine and deep learning by representing weights and\nactivations with just one bit. However, most existing training algorithms for\nBNNs rely on quantization-aware floating-point Stochastic Gradient Descent\n(SGD), limiting the full exploitation of binary operations to the inference\nphase only. In this work, we propose, for the first time, a fully binary and\ngradient-free training algorithm for multi-layer BNNs, eliminating the need for\nback-propagated floating-point gradients. Specifically, the proposed algorithm\nrelies on local binary error signals and binary weight updates, employing\ninteger-valued hidden weights that serve as a synaptic metaplasticity\nmechanism, thereby enhancing its neurobiological plausibility. Our proposed\nsolution enables the training of binary multi-layer perceptrons by using\nexclusively XNOR, Popcount, and increment/decrement operations. Experimental\nresults on multi-class classification benchmarks show test accuracy\nimprovements of up to +35.47% over the only existing fully binary single-layer\nstate-of-the-art solution. Compared to full-precision SGD, our solution\nimproves test accuracy by up to +35.30% under the same total memory demand,\nwhile also reducing computational cost by two to three orders of magnitude in\nterms of the total number of Boolean gates. The proposed algorithm is made\navailable to the scientific community as a public repository."
                },
                "authors": [
                    {
                        "name": "Luca Colombo"
                    },
                    {
                        "name": "Fabrizio Pittorino"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00119v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00119v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16876v1",
                "updated": "2025-06-20T09:55:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    55,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T09:55:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    55,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "Revolutionizing Validation and Verification: Explainable Testing\n  Methodologies for Intelligent Automotive Decision-Making Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Validation and Verification: Explainable Testing\n  Methodologies for Intelligent Automotive Decision-Making Systems"
                },
                "summary": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Halit Eris"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner",
                "arxiv_comment": "Preprint to be published at SE4ADS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16853v1",
                "updated": "2025-06-20T09:02:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    2,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T09:02:05Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    2,
                    5,
                    4,
                    171,
                    0
                ],
                "title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models"
                },
                "summary": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, using\nup to 3.5 times less inference budget, and, given sufficient inference budget,\nachieves performance comparable to learning-based baselines that require\nreward-specific fine-tuning. The code is available at\nhttps://github.com/seminkim/RATTPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, using\nup to 3.5 times less inference budget, and, given sufficient inference budget,\nachieves performance comparable to learning-based baselines that require\nreward-specific fine-tuning. The code is available at\nhttps://github.com/seminkim/RATTPO."
                },
                "authors": [
                    {
                        "name": "Semin Kim"
                    },
                    {
                        "name": "Yeonwoo Cha"
                    },
                    {
                        "name": "Jaehoon Yoo"
                    },
                    {
                        "name": "Seunghoon Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghoon Hong"
                },
                "author": "Seunghoon Hong",
                "arxiv_comment": "28 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00412v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00412v4",
                "updated": "2025-06-20T08:54:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    54,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-01T07:18:31Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    18,
                    31,
                    4,
                    306,
                    0
                ],
                "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation"
                },
                "summary": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Yadi Cao"
                    },
                    {
                        "name": "Duncan Watson-Parris"
                    },
                    {
                        "name": "Leon Bergen"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "37 pages, 16 figures",
                "arxiv_journal_ref": "In Proceedings of the Forty-second International Conference on\n  Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00412v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00412v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15230v2",
                "updated": "2025-06-20T08:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    48,
                    16,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-18T08:13:58Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    8,
                    13,
                    58,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary models for the Very Massive Stars in the R136 cluster of 30\n  Doradus in the Large Magellanic Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary models for the Very Massive Stars in the R136 cluster of 30\n  Doradus in the Large Magellanic Cloud"
                },
                "summary": "The cluster R136 in the LMC contains a population of stars in excess of 100\nM$_\\odot$, including R136a1, the most massive star known. Very Massive Stars\n(VMSs) play an influential role in feedback processes and may potentially\nproduce exotic supernova types and black holes of tens of solar masses. The\nevolutionary history and final fate of the three most luminous stars, R136a1,\nR136a2, and R136a3, has been a puzzling issue. We aim to resolve this by\nrotating single-star MESA models. We produce interpolated model grids and apply\na Markov-Chain Monte Carlo analysis to compare our models with observations.\nThe nature of supernova progenitors strongly depends on mass loss and the AM\ncoupling schemes. We predict no pair-instability and no GRB progenitors from\nour fiducial model grid at LMC metallicity. The onset of Wolf-Rayet-type\nmass-loss rates on the main sequence leads to a rapid decrease in stellar mass\nand luminosity. The mass turnover implies that the evolutionary history can\nonly be inferred if additional constraints are available. We utilise the\nsurface helium abundance, which poses a conundrum: R136a1, the most luminous\nstar, is less enriched in helium than R136a2 and R136a3. We propose that this\ncan be explained if both R136a2 and R136a3 were initially more massive than\nR136a1. From a rigorous confrontation of our models to\nspectroscopically-derived observables, we estimate an initial mass of\n346$\\pm41$ M$_\\odot$ for R136a1, and $\\gtrsim$500 M$_\\odot$ for R136a2 and\nR136a3. Even though VMSs are only present in the youngest clusters below 2 Myr\nof age, our study strengthens their role in local and galaxy evolution. At LMC\nmetallicity, they will be observable as helium-enriched massive stars after\ntheir drastic mass loss, produced via single-star evolution. If the core\ncollapse leads to a supernova, it will be of Type Ib/c. [abridged]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cluster R136 in the LMC contains a population of stars in excess of 100\nM$_\\odot$, including R136a1, the most massive star known. Very Massive Stars\n(VMSs) play an influential role in feedback processes and may potentially\nproduce exotic supernova types and black holes of tens of solar masses. The\nevolutionary history and final fate of the three most luminous stars, R136a1,\nR136a2, and R136a3, has been a puzzling issue. We aim to resolve this by\nrotating single-star MESA models. We produce interpolated model grids and apply\na Markov-Chain Monte Carlo analysis to compare our models with observations.\nThe nature of supernova progenitors strongly depends on mass loss and the AM\ncoupling schemes. We predict no pair-instability and no GRB progenitors from\nour fiducial model grid at LMC metallicity. The onset of Wolf-Rayet-type\nmass-loss rates on the main sequence leads to a rapid decrease in stellar mass\nand luminosity. The mass turnover implies that the evolutionary history can\nonly be inferred if additional constraints are available. We utilise the\nsurface helium abundance, which poses a conundrum: R136a1, the most luminous\nstar, is less enriched in helium than R136a2 and R136a3. We propose that this\ncan be explained if both R136a2 and R136a3 were initially more massive than\nR136a1. From a rigorous confrontation of our models to\nspectroscopically-derived observables, we estimate an initial mass of\n346$\\pm41$ M$_\\odot$ for R136a1, and $\\gtrsim$500 M$_\\odot$ for R136a2 and\nR136a3. Even though VMSs are only present in the youngest clusters below 2 Myr\nof age, our study strengthens their role in local and galaxy evolution. At LMC\nmetallicity, they will be observable as helium-enriched massive stars after\ntheir drastic mass loss, produced via single-star evolution. If the core\ncollapse leads to a supernova, it will be of Type Ib/c. [abridged]"
                },
                "authors": [
                    {
                        "name": "Z. Keszthelyi"
                    },
                    {
                        "name": "S. A. Brands"
                    },
                    {
                        "name": "A. de Koter"
                    },
                    {
                        "name": "N. Langer"
                    },
                    {
                        "name": "J. Puls"
                    }
                ],
                "author_detail": {
                    "name": "J. Puls"
                },
                "author": "J. Puls",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16824v1",
                "updated": "2025-06-20T08:26:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:26:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Predicting New Research Directions in Materials Science using Large\n  Language Models and Concept Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting New Research Directions in Materials Science using Large\n  Language Models and Concept Graphs"
                },
                "summary": "Due to an exponential increase in published research articles, it is\nimpossible for individual scientists to read all publications, even within\ntheir own research field. In this work, we investigate the use of large\nlanguage models (LLMs) for the purpose of extracting the main concepts and\nsemantic information from scientific abstracts in the domain of materials\nscience to find links that were not noticed by humans and thus to suggest\ninspiring near/mid-term future research directions. We show that LLMs can\nextract concepts more efficiently than automated keyword extraction methods to\nbuild a concept graph as an abstraction of the scientific literature. A machine\nlearning model is trained to predict emerging combinations of concepts, i.e.\nnew research ideas, based on historical data. We demonstrate that integrating\nsemantic concept information leads to an increased prediction performance. The\napplicability of our model is demonstrated in qualitative interviews with\ndomain experts based on individualized model suggestions. We show that the\nmodel can inspire materials scientists in their creative thinking process by\npredicting innovative combinations of topics that have not yet been\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to an exponential increase in published research articles, it is\nimpossible for individual scientists to read all publications, even within\ntheir own research field. In this work, we investigate the use of large\nlanguage models (LLMs) for the purpose of extracting the main concepts and\nsemantic information from scientific abstracts in the domain of materials\nscience to find links that were not noticed by humans and thus to suggest\ninspiring near/mid-term future research directions. We show that LLMs can\nextract concepts more efficiently than automated keyword extraction methods to\nbuild a concept graph as an abstraction of the scientific literature. A machine\nlearning model is trained to predict emerging combinations of concepts, i.e.\nnew research ideas, based on historical data. We demonstrate that integrating\nsemantic concept information leads to an increased prediction performance. The\napplicability of our model is demonstrated in qualitative interviews with\ndomain experts based on individualized model suggestions. We show that the\nmodel can inspire materials scientists in their creative thinking process by\npredicting innovative combinations of topics that have not yet been\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Thomas Marwitz"
                    },
                    {
                        "name": "Alexander Colsmann"
                    },
                    {
                        "name": "Ben Breitung"
                    },
                    {
                        "name": "Christoph Brabec"
                    },
                    {
                        "name": "Christoph Kirchlechner"
                    },
                    {
                        "name": "Eva Blasco"
                    },
                    {
                        "name": "Gabriel Cadilha Marques"
                    },
                    {
                        "name": "Horst Hahn"
                    },
                    {
                        "name": "Michael Hirtz"
                    },
                    {
                        "name": "Pavel A. Levkin"
                    },
                    {
                        "name": "Yolita M. Eggeler"
                    },
                    {
                        "name": "Tobias Schlöder"
                    },
                    {
                        "name": "Pascal Friederich"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Friederich"
                },
                "author": "Pascal Friederich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07666v2",
                "updated": "2025-06-20T08:08:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    8,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2024-12-10T16:57:07Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    57,
                    7,
                    1,
                    345,
                    0
                ],
                "title": "Hamiltonian-learning quantum magnets with non-local impurity tomography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian-learning quantum magnets with non-local impurity tomography"
                },
                "summary": "Impurities in quantum materials have provided successful strategies for\nlearning properties of complex states, ranging from unconventional\nsuperconductors to topological insulators. In quantum magnetism, inferring the\nHamiltonian of an engineered system becomes a challenging open problem in the\npresence of complex interactions. Here we show how a supervised\nmachine-learning technique can be used to infer Hamiltonian parameters from\natomically engineered quantum magnets by inferring fluctuations of the ground\nstates due to the presence of impurities. We demonstrate our methodology both\nwith a fermionic model with spin-orbit coupling, as well as with many-body spin\nmodels with long-range exchange and anisotropic exchange interactions. We show\nthat our approach enables performing Hamiltonian extraction in the presence of\nsignificant noise, providing a strategy to perform Hamiltonian learning with\nexperimental observables in atomic-scale quantum magnets. Our results establish\na strategy to perform Hamiltonian learning by exploiting the impact of\nimpurities in complex quantum many-body states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impurities in quantum materials have provided successful strategies for\nlearning properties of complex states, ranging from unconventional\nsuperconductors to topological insulators. In quantum magnetism, inferring the\nHamiltonian of an engineered system becomes a challenging open problem in the\npresence of complex interactions. Here we show how a supervised\nmachine-learning technique can be used to infer Hamiltonian parameters from\natomically engineered quantum magnets by inferring fluctuations of the ground\nstates due to the presence of impurities. We demonstrate our methodology both\nwith a fermionic model with spin-orbit coupling, as well as with many-body spin\nmodels with long-range exchange and anisotropic exchange interactions. We show\nthat our approach enables performing Hamiltonian extraction in the presence of\nsignificant noise, providing a strategy to perform Hamiltonian learning with\nexperimental observables in atomic-scale quantum magnets. Our results establish\na strategy to perform Hamiltonian learning by exploiting the impact of\nimpurities in complex quantum many-body states."
                },
                "authors": [
                    {
                        "name": "Greta Lupi"
                    },
                    {
                        "name": "Jose L. Lado"
                    }
                ],
                "author_detail": {
                    "name": "Jose L. Lado"
                },
                "author": "Jose L. Lado",
                "arxiv_doi": "10.1103/PhysRevApplied.23.054077",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevApplied.23.054077",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.07666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures",
                "arxiv_journal_ref": "Phys. Rev. Applied 23, 054077 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16813v1",
                "updated": "2025-06-20T08:03:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    3,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:03:36Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    3,
                    36,
                    4,
                    171,
                    0
                ],
                "title": "Integrating Traditional Technical Analysis with AI: A Multi-Agent\n  LLM-Based Approach to Stock Market Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Traditional Technical Analysis with AI: A Multi-Agent\n  LLM-Based Approach to Stock Market Forecasting"
                },
                "summary": "Traditional technical analysis methods face limitations in accurately\npredicting trends in today's complex financial markets. This paper introduces\nElliottAgents, an multi-agent system that integrates the Elliott Wave Principle\nwith AI for stock market forecasting. The inherent complexity of financial\nmarkets, characterized by non-linear dynamics, noise, and susceptibility to\nunpredictable external factors, poses significant challenges for accurate\nprediction. To address these challenges, the system employs LLMs to enhance\nnatural language understanding and decision-making capabilities within a\nmulti-agent framework. By leveraging technologies such as Retrieval-Augmented\nGeneration (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs\ncontinuous, multi-faceted analysis of market data to identify wave patterns and\npredict future price movements. The research explores the system's ability to\nprocess historical stock data, recognize Elliott wave patterns, and generate\nactionable insights for traders. Experimental results, conducted on historical\ndata from major U.S. companies, validate the system's effectiveness in pattern\nrecognition and trend forecasting across various time frames. This paper\ncontributes to the field of AI-driven financial analysis by demonstrating how\ntraditional technical analysis methods can be effectively combined with modern\nAI approaches to create more reliable and interpretable market prediction\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional technical analysis methods face limitations in accurately\npredicting trends in today's complex financial markets. This paper introduces\nElliottAgents, an multi-agent system that integrates the Elliott Wave Principle\nwith AI for stock market forecasting. The inherent complexity of financial\nmarkets, characterized by non-linear dynamics, noise, and susceptibility to\nunpredictable external factors, poses significant challenges for accurate\nprediction. To address these challenges, the system employs LLMs to enhance\nnatural language understanding and decision-making capabilities within a\nmulti-agent framework. By leveraging technologies such as Retrieval-Augmented\nGeneration (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs\ncontinuous, multi-faceted analysis of market data to identify wave patterns and\npredict future price movements. The research explores the system's ability to\nprocess historical stock data, recognize Elliott wave patterns, and generate\nactionable insights for traders. Experimental results, conducted on historical\ndata from major U.S. companies, validate the system's effectiveness in pattern\nrecognition and trend forecasting across various time frames. This paper\ncontributes to the field of AI-driven financial analysis by demonstrating how\ntraditional technical analysis methods can be effectively combined with modern\nAI approaches to create more reliable and interpretable market prediction\nsystems."
                },
                "authors": [
                    {
                        "name": "Michał Wawer"
                    },
                    {
                        "name": "Jarosław A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosław A. Chudziak"
                },
                "author": "Jarosław A. Chudziak",
                "arxiv_doi": "10.5220/0013191200003890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013191200003890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures, 1 table. This is the accepted version of the\n  paper presented at the 17th International Conference on Agents and Artificial\n  Intelligence (ICAART 2025), Porto, Portugal",
                "arxiv_journal_ref": "Proceedings of the 17th International Conference on Agents and\n  Artificial Intelligence - Volume 1 (ICAART 2025), pages 100-111. SciTePress,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91G68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16810v1",
                "updated": "2025-06-20T07:57:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    57,
                    55,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T07:57:55Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    57,
                    55,
                    4,
                    171,
                    0
                ],
                "title": "Transformers for Stratified Spectropolarimetric Inversion: Proof of\n  Concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers for Stratified Spectropolarimetric Inversion: Proof of\n  Concept"
                },
                "summary": "Solar spectropolarimetric inversion -- inferring atmospheric conditions from\nthe Stokes vector -- is a key diagnostic tool for understanding solar\nmagnetism, but traditional inversion methods are computationally expensive and\nsensitive to local minima. Advances in artificial intelligence (AI) offer\nfaster solutions, but are often restricted to shallow models or a few spectral\nlines. We present a proof-of-concept study using a transformer machine learning\n(ML) model for multi-line, full-Stokes inversion, to infer stratified\nparameters from synthetic spectra produced from 3D magnetohydrodynamic\nsimulations. We synthesise a large set of Stokes vectors using forward\nmodelling across 15 spectral lines spanning the deep photosphere towards the\nchromosphere. The model maps full-Stokes input to temperature, magnetic field\nstrength, inclination, azimuth (encoded as $\\sin2\\phi$, $\\cos2\\phi$), and\nline-of-sight velocity as a function of optical depth. The transformer\nincorporates an attention mechanism that allows the model to focus on the most\ninformative regions of the spectrum for each inferred parameter, and uses\npositional embedding to encode wavelength and depth order. We benchmark it\nagainst a multilayer perceptron (MLP), test robustness to noise, and assess\ngeneralisation. The transformer outperforms the MLP, especially in the higher\nlayers and for magnetic parameters, yielding higher correlations and more\nregularised stratifications. The model retains strong performance across a\nrange of noise levels typical for real observations, with magnetic parameter\ninference degrading predictably while temperature and velocity remain stable.\nWe establish transformer architectures as a powerful tool for\nspectropolarimetric regression. This approach paves the way for analysis of\nlarge datasets from large solar telescopes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar spectropolarimetric inversion -- inferring atmospheric conditions from\nthe Stokes vector -- is a key diagnostic tool for understanding solar\nmagnetism, but traditional inversion methods are computationally expensive and\nsensitive to local minima. Advances in artificial intelligence (AI) offer\nfaster solutions, but are often restricted to shallow models or a few spectral\nlines. We present a proof-of-concept study using a transformer machine learning\n(ML) model for multi-line, full-Stokes inversion, to infer stratified\nparameters from synthetic spectra produced from 3D magnetohydrodynamic\nsimulations. We synthesise a large set of Stokes vectors using forward\nmodelling across 15 spectral lines spanning the deep photosphere towards the\nchromosphere. The model maps full-Stokes input to temperature, magnetic field\nstrength, inclination, azimuth (encoded as $\\sin2\\phi$, $\\cos2\\phi$), and\nline-of-sight velocity as a function of optical depth. The transformer\nincorporates an attention mechanism that allows the model to focus on the most\ninformative regions of the spectrum for each inferred parameter, and uses\npositional embedding to encode wavelength and depth order. We benchmark it\nagainst a multilayer perceptron (MLP), test robustness to noise, and assess\ngeneralisation. The transformer outperforms the MLP, especially in the higher\nlayers and for magnetic parameters, yielding higher correlations and more\nregularised stratifications. The model retains strong performance across a\nrange of noise levels typical for real observations, with magnetic parameter\ninference degrading predictably while temperature and velocity remain stable.\nWe establish transformer architectures as a powerful tool for\nspectropolarimetric regression. This approach paves the way for analysis of\nlarge datasets from large solar telescopes."
                },
                "authors": [
                    {
                        "name": "Ryan James Campbell"
                    },
                    {
                        "name": "Mihalis Mathioudakis"
                    },
                    {
                        "name": "Carlos Quintero Noda"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Quintero Noda"
                },
                "author": "Carlos Quintero Noda",
                "arxiv_comment": "Preprint, submitted to the Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06295v2",
                "updated": "2025-06-20T07:42:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    42,
                    34,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-10T09:34:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    34,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DVFS-Aware DNN Inference on GPUs: Latency Modeling and Performance\n  Analysis"
                },
                "summary": "The rapid development of deep neural networks (DNNs) is inherently\naccompanied by the problem of high computational costs. To tackle this\nchallenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising\ntechnology for balancing the latency and energy consumption of DNN inference by\nadjusting the computing frequency of processors. However, most existing models\nof DNN inference time are based on the CPU-DVFS technique, and directly\napplying the CPU-DVFS model to DNN inference on GPUs will lead to significant\nerrors in optimizing latency and energy consumption. In this paper, we propose\na DVFS-aware latency model to precisely characterize DNN inference time on\nGPUs. We first formulate the DNN inference time based on extensive experiment\nresults for different devices and analyze the impact of fitting parameters.\nThen by dividing DNNs into multiple blocks and obtaining the actual inference\ntime, the proposed model is further verified. Finally, we compare our proposed\nmodel with the CPU-DVFS model in two specific cases. Evaluation results\ndemonstrate that local inference optimization with our proposed model achieves\na reduction of no less than 66% and 69% in inference time and energy\nconsumption respectively. In addition, cooperative inference with our proposed\nmodel can improve the partition policy and reduce the energy consumption\ncompared to the CPU-DVFS model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of deep neural networks (DNNs) is inherently\naccompanied by the problem of high computational costs. To tackle this\nchallenge, dynamic voltage frequency scaling (DVFS) is emerging as a promising\ntechnology for balancing the latency and energy consumption of DNN inference by\nadjusting the computing frequency of processors. However, most existing models\nof DNN inference time are based on the CPU-DVFS technique, and directly\napplying the CPU-DVFS model to DNN inference on GPUs will lead to significant\nerrors in optimizing latency and energy consumption. In this paper, we propose\na DVFS-aware latency model to precisely characterize DNN inference time on\nGPUs. We first formulate the DNN inference time based on extensive experiment\nresults for different devices and analyze the impact of fitting parameters.\nThen by dividing DNNs into multiple blocks and obtaining the actual inference\ntime, the proposed model is further verified. Finally, we compare our proposed\nmodel with the CPU-DVFS model in two specific cases. Evaluation results\ndemonstrate that local inference optimization with our proposed model achieves\na reduction of no less than 66% and 69% in inference time and energy\nconsumption respectively. In addition, cooperative inference with our proposed\nmodel can improve the partition policy and reduce the energy consumption\ncompared to the CPU-DVFS model."
                },
                "authors": [
                    {
                        "name": "Yunchu Han"
                    },
                    {
                        "name": "Zhaojun Nan"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Zhisheng Niu"
                    }
                ],
                "author_detail": {
                    "name": "Zhisheng Niu"
                },
                "author": "Zhisheng Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15551v2",
                "updated": "2025-06-20T07:32:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    32,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-18T15:16:10Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    10,
                    1,
                    77,
                    0
                ],
                "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting\n  Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting\n  Attack"
                },
                "summary": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
                },
                "authors": [
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "arxiv_comment": "Accepted to ACL Findings, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v4",
                "updated": "2025-06-20T07:25:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    25,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-Following Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-Following Ability"
                },
                "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions\nthrough an iterative feedback framework, which enables models to self-correct\nbased on specific requirement failures in each turn, better reflecting\nreal-world user-end usage patterns. Meanwhile, the benchmark implements a\ncomprehensive evaluation system with 38 capability tags organized across three\ndimensions: Intent Recognition, Granular Content Validation, and Output\nStructure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions\nthrough an iterative feedback framework, which enables models to self-correct\nbased on specific requirement failures in each turn, better reflecting\nreal-world user-end usage patterns. Meanwhile, the benchmark implements a\ncomprehensive evaluation system with 38 capability tags organized across three\ndimensions: Intent Recognition, Granular Content Validation, and Output\nStructure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaming Wang"
                    },
                    {
                        "name": "Yunke Zhao"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Jun Kuang"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16796v1",
                "updated": "2025-06-20T07:21:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    21,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T07:21:21Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    21,
                    21,
                    4,
                    171,
                    0
                ],
                "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought"
                },
                "summary": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation."
                },
                "authors": [
                    {
                        "name": "Junbo Qiao"
                    },
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xudong Huang"
                    },
                    {
                        "name": "Gaoqi He"
                    },
                    {
                        "name": "Jiao Xie"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16792v1",
                "updated": "2025-06-20T07:16:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T07:16:47Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning"
                },
                "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST."
                },
                "authors": [
                    {
                        "name": "Muyang Zheng"
                    },
                    {
                        "name": "Yuanzhi Yao"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11422v3",
                "updated": "2025-06-20T07:14:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    14,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-17T04:35:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    4,
                    35,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization"
                },
                "summary": "Heuristics have achieved great success in solving combinatorial optimization\nproblems~(COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Since Large Language Models~(LLMs) possess strong\ncapabilities to understand and generate content with a knowledge base that\ncovers various domains, they offer potential ways to automatically optimize\nheuristics. To this end, we propose Planning of Heuristics~(PoH), an\noptimization method that integrates LLM self-reflection with Monte Carlo Tree\nSearch, a well-known planning algorithm. PoH iteratively refines generated\nheuristics by evaluating their performance and providing improvement\nsuggestions. Our method enables to iteratively evaluate the generated\nheuristics~(states) and improve them based on the improvement\nsuggestions~(actions) and evaluation results~(rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop\nScheduling Problem. The experimental results show that PoH outperforms\nhand-crafted heuristics and other Automatic Heuristic Design methods based on\nLLMs, and achieves the state-of-the-art performance in automating heuristic\noptimization with LLMs to solve tested COPs, especially with large sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristics have achieved great success in solving combinatorial optimization\nproblems~(COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Since Large Language Models~(LLMs) possess strong\ncapabilities to understand and generate content with a knowledge base that\ncovers various domains, they offer potential ways to automatically optimize\nheuristics. To this end, we propose Planning of Heuristics~(PoH), an\noptimization method that integrates LLM self-reflection with Monte Carlo Tree\nSearch, a well-known planning algorithm. PoH iteratively refines generated\nheuristics by evaluating their performance and providing improvement\nsuggestions. Our method enables to iteratively evaluate the generated\nheuristics~(states) and improve them based on the improvement\nsuggestions~(actions) and evaluation results~(rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop\nScheduling Problem. The experimental results show that PoH outperforms\nhand-crafted heuristics and other Automatic Heuristic Design methods based on\nLLMs, and achieves the state-of-the-art performance in automating heuristic\noptimization with LLMs to solve tested COPs, especially with large sizes."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xufeng Zhang"
                    },
                    {
                        "name": "Chaoxu Mu"
                    }
                ],
                "author_detail": {
                    "name": "Chaoxu Mu"
                },
                "author": "Chaoxu Mu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12345v3",
                "updated": "2025-06-20T07:09:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    9,
                    39,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-15T16:58:11Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    11,
                    1,
                    105,
                    0
                ],
                "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models"
                },
                "summary": "Urban causal research is essential for understanding the complex, dynamic\nprocesses that shape cities and for informing evidence-based policies. However,\ncurrent practices are often constrained by inefficient and biased hypothesis\nformulation, challenges in integrating multimodal data, and fragile\nexperimental methodologies. Imagine a system that automatically estimates the\ncausal impact of congestion pricing on commute times by income group or\nmeasures how new green spaces affect asthma rates across neighborhoods using\nsatellite imagery and health reports, and then generates comprehensive,\npolicy-ready outputs, including causal estimates, subgroup analyses, and\nactionable recommendations. In this Perspective, we propose UrbanCIA, an\nLLM-driven conceptual framework composed of four distinct modular agents\nresponsible for hypothesis generation, data engineering, experiment design and\nexecution, and results interpretation with policy insights. We begin by\nexamining the current landscape of urban causal research through a structured\ntaxonomy of research topics, data sources, and methodological approaches,\nrevealing systemic limitations across the workflow. Next, we introduce the\ndesign principles and technological roadmap for the four modules in the\nproposed framework. We also propose evaluation criteria to assess the rigor and\ntransparency of these AI-augmented processes. Finally, we reflect on the\nbroader implications for human-AI collaboration, equity, and accountability. We\ncall for a new research agenda that embraces LLM-driven tools as catalysts for\nmore scalable, reproducible, and inclusive urban research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban causal research is essential for understanding the complex, dynamic\nprocesses that shape cities and for informing evidence-based policies. However,\ncurrent practices are often constrained by inefficient and biased hypothesis\nformulation, challenges in integrating multimodal data, and fragile\nexperimental methodologies. Imagine a system that automatically estimates the\ncausal impact of congestion pricing on commute times by income group or\nmeasures how new green spaces affect asthma rates across neighborhoods using\nsatellite imagery and health reports, and then generates comprehensive,\npolicy-ready outputs, including causal estimates, subgroup analyses, and\nactionable recommendations. In this Perspective, we propose UrbanCIA, an\nLLM-driven conceptual framework composed of four distinct modular agents\nresponsible for hypothesis generation, data engineering, experiment design and\nexecution, and results interpretation with policy insights. We begin by\nexamining the current landscape of urban causal research through a structured\ntaxonomy of research topics, data sources, and methodological approaches,\nrevealing systemic limitations across the workflow. Next, we introduce the\ndesign principles and technological roadmap for the four modules in the\nproposed framework. We also propose evaluation criteria to assess the rigor and\ntransparency of these AI-augmented processes. Finally, we reflect on the\nbroader implications for human-AI collaboration, equity, and accountability. We\ncall for a new research agenda that embraces LLM-driven tools as catalysts for\nmore scalable, reproducible, and inclusive urban research."
                },
                "authors": [
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yunhan Zheng"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Dingyi Zhuang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Shenhao Wang"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Jinhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Zhao"
                },
                "author": "Jinhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24183v2",
                "updated": "2025-06-20T07:05:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    5,
                    18,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-30T03:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    3,
                    51,
                    6,
                    4,
                    150,
                    0
                ],
                "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeV-R1: Reasoning-Enhanced Verilog Generation"
                },
                "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities."
                },
                "authors": [
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Hanqi Lyu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jinghua Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04311v2",
                "updated": "2025-06-20T07:03:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    3,
                    22,
                    4,
                    171,
                    0
                ],
                "published": "2024-03-07T08:30:26Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    8,
                    30,
                    26,
                    3,
                    67,
                    0
                ],
                "title": "Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry"
                },
                "summary": "Compound AI applications chain together subcomponents such as generative\nlanguage models, document retrievers, and embedding models. Applying\ntraditional systems optimizations such as parallelism and pipelining in\ncompound AI systems is difficult because each component has different\nconstraints in terms of the granularity and type of data that it ingests. New\ndata is often generated during intermediate computations, and text streams may\nbe split into smaller, independent fragments (such as documents to sentences)\nwhich may then be re-aggregated at later parts of the computation. Due to this\ncomplexity, existing systems to serve compound AI queries do not fully take\nadvantage of parallelism and pipelining opportunities.\n  We present Alto, a framework that automatically optimizes execution of\ncompound AI queries through streaming and parallelism. Bento introduces a new\nabstraction called nested ancestry, a metadata hierarchy that allows the system\nto correctly track partial outputs and aggregate data across the heterogeneous\nconstraints of the components of compound AI applications. This metadata is\nautomatically inferred from the programming model, allowing developers to\nexpress complex dataflow patterns without needing to reason manually about the\ndetails of routing and aggregation. Implementations of four applications in\nAlto outperform or match implementations in LangGraph, a popular existing AI\nprogramming framework. Alto implementations match or improve latency by between\n10-30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI applications chain together subcomponents such as generative\nlanguage models, document retrievers, and embedding models. Applying\ntraditional systems optimizations such as parallelism and pipelining in\ncompound AI systems is difficult because each component has different\nconstraints in terms of the granularity and type of data that it ingests. New\ndata is often generated during intermediate computations, and text streams may\nbe split into smaller, independent fragments (such as documents to sentences)\nwhich may then be re-aggregated at later parts of the computation. Due to this\ncomplexity, existing systems to serve compound AI queries do not fully take\nadvantage of parallelism and pipelining opportunities.\n  We present Alto, a framework that automatically optimizes execution of\ncompound AI queries through streaming and parallelism. Bento introduces a new\nabstraction called nested ancestry, a metadata hierarchy that allows the system\nto correctly track partial outputs and aggregate data across the heterogeneous\nconstraints of the components of compound AI applications. This metadata is\nautomatically inferred from the programming model, allowing developers to\nexpress complex dataflow patterns without needing to reason manually about the\ndetails of routing and aggregation. Implementations of four applications in\nAlto outperform or match implementations in LangGraph, a popular existing AI\nprogramming framework. Alto implementations match or improve latency by between\n10-30%."
                },
                "authors": [
                    {
                        "name": "Deepti Raghavan"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Muhammad Shahir Rahman"
                    },
                    {
                        "name": "Nayani Modugula"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Maximilien Cura"
                    },
                    {
                        "name": "Houjun Liu"
                    },
                    {
                        "name": "Pratiksha Thaker"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16777v1",
                "updated": "2025-06-20T06:45:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    45,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T06:45:40Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    45,
                    40,
                    4,
                    171,
                    0
                ],
                "title": "DistillNote: LLM-based clinical note summaries improve heart failure\n  diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistillNote: LLM-based clinical note summaries improve heart failure\n  diagnosis"
                },
                "summary": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research."
                },
                "authors": [
                    {
                        "name": "Heloisa Oss Boll"
                    },
                    {
                        "name": "Antonio Oss Boll"
                    },
                    {
                        "name": "Leticia Puttlitz Boll"
                    },
                    {
                        "name": "Ameen Abu Hanna"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17219v1",
                "updated": "2025-06-20T17:59:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    52,
                    4,
                    171,
                    0
                ],
                "title": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Free Lunch: Rethinking Internal Feedback for LLM Reasoning"
                },
                "summary": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning has emerged as a powerful paradigm for post-training\nlarge language models (LLMs) to improve reasoning. Approaches like\nReinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) have shown strong results, but they require\nextensive external supervision. We investigate an alternative class of methods,\nReinforcement Learning from Internal Feedback (RLIF), which relies solely on\nintrinsic model-derived signals instead of external rewards. In particular, we\nleverage unsupervised reward proxies such as token-level entropy,\ntrajectory-level entropy, and self-certainty. Our theoretical analysis shows\nthese internal objectives are partially equivalent, and we empirically evaluate\nvarious RLIF strategies on challenging math reasoning benchmarks. Experimental\nresults demonstrate that RLIF can boost the reasoning performance of base LLMs\nat the beginning phase of the training, matching or surpassing RLVR techniques\non these tasks. However, when training progresses, performance degrades even\nbelow the model before training. Moreover, we find that RLIF yields little\nimprovement for instruction-tuned models, indicating diminishing returns of\nintrinsic feedback once an LLM is already instruction-tuned. We further analyze\nthis limitation by mixing model weights and explain the reason of RLIF's\ntraining behaviors, providing practical guidelines for integrating internal\nfeedback signals into LLM training. We hope our analysis of internal feedback\nwill inform more principled and effective strategies for LLM post-training."
                },
                "authors": [
                    {
                        "name": "Yanzhi Zhang"
                    },
                    {
                        "name": "Zhaoxi Zhang"
                    },
                    {
                        "name": "Haoxiang Guan"
                    },
                    {
                        "name": "Yilin Cheng"
                    },
                    {
                        "name": "Yitong Duan"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Shuxin Zheng"
                    },
                    {
                        "name": "Jiyan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiyan He"
                },
                "author": "Jiyan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17213v1",
                "updated": "2025-06-20T17:59:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:59:21Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    59,
                    21,
                    4,
                    171,
                    0
                ],
                "title": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term Traffic Simulation with Interleaved Autoregressive Motion and\n  Scenario Generation"
                },
                "summary": "An ideal traffic simulator replicates the realistic long-term point-to-point\ntrip that a self-driving system experiences during deployment. Prior models and\nbenchmarks focus on closed-loop motion simulation for initial agents in a\nscene. This is problematic for long-term simulation. Agents enter and exit the\nscene as the ego vehicle enters new regions. We propose InfGen, a unified\nnext-token prediction model that performs interleaved closed-loop motion\nsimulation and scene generation. InfGen automatically switches between\nclosed-loop motion simulation and scene generation mode. It enables stable\nlong-term rollout simulation. InfGen performs at the state-of-the-art in\nshort-term (9s) traffic simulation, and significantly outperforms all other\nmethods in long-term (30s) simulation. The code and model of InfGen will be\nreleased at https://orangesodahub.github.io/InfGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ideal traffic simulator replicates the realistic long-term point-to-point\ntrip that a self-driving system experiences during deployment. Prior models and\nbenchmarks focus on closed-loop motion simulation for initial agents in a\nscene. This is problematic for long-term simulation. Agents enter and exit the\nscene as the ego vehicle enters new regions. We propose InfGen, a unified\nnext-token prediction model that performs interleaved closed-loop motion\nsimulation and scene generation. InfGen automatically switches between\nclosed-loop motion simulation and scene generation mode. It enables stable\nlong-term rollout simulation. InfGen performs at the state-of-the-art in\nshort-term (9s) traffic simulation, and significantly outperforms all other\nmethods in long-term (30s) simulation. The code and model of InfGen will be\nreleased at https://orangesodahub.github.io/InfGen"
                },
                "authors": [
                    {
                        "name": "Xiuyu Yang"
                    },
                    {
                        "name": "Shuhan Tan"
                    },
                    {
                        "name": "Philipp Krähenbühl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Krähenbühl"
                },
                "author": "Philipp Krähenbühl",
                "arxiv_comment": "Preprint. Project page: https://orangesodahub.github.io/InfGen Code:\n  https://github.com/OrangeSodahub/infgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09404v2",
                "updated": "2025-06-20T17:57:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    43,
                    4,
                    171,
                    0
                ],
                "published": "2024-02-14T18:59:33Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    18,
                    59,
                    33,
                    2,
                    45,
                    0
                ],
                "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential\n  Reasoning Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential\n  Reasoning Ability"
                },
                "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol - for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves considering the possible\nenvironmental feedback in the future steps. We comprehensively build AQA-Bench\nwith three different algorithms, namely binary search, depth-first search, and\nbreadth-first search, and to evaluate the sequential reasoning ability of 14\ndifferent LLMs. Our investigations reveal several interesting findings: (1)\nClosed-source models like GPT-4 and Gemini generally show much stronger\nsequential reasoning ability, significantly outperforming open-source LLMs. (2)\nNaively providing in-context examples may inadvertently hurt few-shot\nperformance in an interactive environment due to over-fitting to examples. (3)\nInstead of using optimal steps from another test case as the in-context\nexample, a very limited number of predecessor steps in the current test case\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The performance gap between weak models and strong models is greatly due to\nthe incapability of weak models to start well. (5) The scaling correlation\nbetween performance and model size is not always significant, sometimes even\nshowcasing an inverse trend. We hope our study can catalyze future work on\nadvancing the understanding and enhancement of LLMs' capabilities in sequential\nreasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench."
                },
                "authors": [
                    {
                        "name": "Siwei Yang"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17209v1",
                "updated": "2025-06-20T17:57:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:57:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency"
                },
                "summary": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future."
                },
                "authors": [
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Isar Nejadgholi"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_comment": "to appear at LLMSEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17208v1",
                "updated": "2025-06-20T17:57:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    8,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:57:08Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    57,
                    8,
                    4,
                    171,
                    0
                ],
                "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and\n  Architectures of LLM- and Agent-Based Repair Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and\n  Architectures of LLM- and Agent-Based Repair Systems"
                },
                "summary": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies."
                },
                "authors": [
                    {
                        "name": "Matias Martinez"
                    },
                    {
                        "name": "Xavier Franch"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Franch"
                },
                "author": "Xavier Franch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11280v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11280v4",
                "updated": "2025-06-20T17:55:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    55,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-14T10:39:27Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    10,
                    39,
                    27,
                    4,
                    73,
                    0
                ],
                "title": "High-Dimensional Interlingual Representations of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Dimensional Interlingual Representations of Large Language Models"
                },
                "summary": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning."
                },
                "authors": [
                    {
                        "name": "Bryan Wilie"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11280v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11280v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17203v1",
                "updated": "2025-06-20T17:54:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    54,
                    18,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:54:18Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    54,
                    18,
                    4,
                    171,
                    0
                ],
                "title": "Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction"
                },
                "summary": "Large Language Models (LLMs) have recently enabled natural language\ninterfaces that translate user queries into executable SQL, offering a powerful\nsolution for non-technical stakeholders to access structured data. However, one\nof the limitation that LLMs do not natively express uncertainty makes it\ndifficult to assess the reliability of their generated queries. This paper\npresents a case study that evaluates multiple approaches to estimate confidence\nscores for LLM-generated SQL in supply chain data retrieval. We investigated\nthree strategies: (1) translation-based consistency checks; (2) embedding-based\nsemantic similarity between user questions and generated SQL; and (3)\nself-reported confidence scores directly produced by the LLM. Our findings\nreveal that LLMs are often overconfident in their own outputs, which limits the\neffectiveness of self-reported confidence. In contrast, embedding-based\nsimilarity methods demonstrate strong discriminative power in identifying\ninaccurate SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently enabled natural language\ninterfaces that translate user queries into executable SQL, offering a powerful\nsolution for non-technical stakeholders to access structured data. However, one\nof the limitation that LLMs do not natively express uncertainty makes it\ndifficult to assess the reliability of their generated queries. This paper\npresents a case study that evaluates multiple approaches to estimate confidence\nscores for LLM-generated SQL in supply chain data retrieval. We investigated\nthree strategies: (1) translation-based consistency checks; (2) embedding-based\nsemantic similarity between user questions and generated SQL; and (3)\nself-reported confidence scores directly produced by the LLM. Our findings\nreveal that LLMs are often overconfident in their own outputs, which limits the\neffectiveness of self-reported confidence. In contrast, embedding-based\nsimilarity methods demonstrate strong discriminative power in identifying\ninaccurate SQL."
                },
                "authors": [
                    {
                        "name": "Jiekai Ma"
                    },
                    {
                        "name": "Yikai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yikai Zhao"
                },
                "author": "Yikai Zhao",
                "arxiv_doi": "10.1145/XXXXXX.XXXXXX",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/XXXXXX.XXXXXX",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD workshop AI for Supply Chain 2025",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17201v1",
                "updated": "2025-06-20T17:50:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:50:37Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    37,
                    4,
                    171,
                    0
                ],
                "title": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with\n  Hybrid History Condition"
                },
                "summary": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based and controllable video generation have\nenabled high-quality and temporally coherent video synthesis, laying the\ngroundwork for immersive interactive gaming experiences. However, current\nmethods face limitations in dynamics, generality, long-term consistency, and\nefficiency, which limit the ability to create various gameplay videos. To\naddress these gaps, we introduce Hunyuan-GameCraft, a novel framework for\nhigh-dynamic interactive video generation in game environments. To achieve\nfine-grained action control, we unify standard keyboard and mouse inputs into a\nshared camera representation space, facilitating smooth interpolation between\nvarious camera and movement operations. Then we propose a hybrid\nhistory-conditioned training strategy that extends video sequences\nautoregressively while preserving game scene information. Additionally, to\nenhance inference efficiency and playability, we achieve model distillation to\nreduce computational overhead while maintaining consistency across long\ntemporal sequences, making it suitable for real-time deployment in complex\ninteractive environments. The model is trained on a large-scale dataset\ncomprising over one million gameplay recordings across over 100 AAA games,\nensuring broad coverage and diversity, then fine-tuned on a carefully annotated\nsynthetic dataset to enhance precision and control. The curated game scene data\nsignificantly improves the visual fidelity, realism and action controllability.\nExtensive experiments demonstrate that Hunyuan-GameCraft significantly\noutperforms existing models, advancing the realism and playability of\ninteractive game video generation."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Longhuang Wu"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Tianbao Yu"
                    },
                    {
                        "name": "Zhiguo Cao"
                    },
                    {
                        "name": "Qinglin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qinglin Lu"
                },
                "author": "Qinglin Lu",
                "arxiv_comment": "Project page: https://hunyuan-gamecraft.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17200v1",
                "updated": "2025-06-20T17:50:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:50:07Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    50,
                    7,
                    4,
                    171,
                    0
                ],
                "title": "Intelligent Reflecting Surfaces for THz Communications: Fundamentals,\n  Key Solutions, and System Prototyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Reflecting Surfaces for THz Communications: Fundamentals,\n  Key Solutions, and System Prototyping"
                },
                "summary": "Intelligent reflecting surfaces (IRSs) have emerged as a cost-effective\ntechnology for terahertz (THz) communications by enabling programmable control\nof the wireless environment. This paper provides a comprehensive overview of\nIRSs-aided THz communications, covering hardware designs, advanced signal\nprocessing techniques, and practical deployment strategies. It first examines\nkey THz reconfigurable metasurface architectures, including electronic,\noptical, phase-change material, and micro-electromechanical systems\n(MEMS)-based implementations, highlighting their reconfiguration mechanisms and\nchallenges. Then, fundamental effects including near field and beam squint in\nwideband THz systems are analyzed, along with their impacts on system\nperformance. The paper further explores conventional and beam-squint-assisted\nchannel estimation methods, innovative beam management strategies, and\ndeployment considerations across large- and small-scale scenarios. Practical\nexperiments at 220 gigahertz (GHz) validate the effectiveness of IRS in\nimproving signal strength and communication reliability for both single-user\nand multi-user setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent reflecting surfaces (IRSs) have emerged as a cost-effective\ntechnology for terahertz (THz) communications by enabling programmable control\nof the wireless environment. This paper provides a comprehensive overview of\nIRSs-aided THz communications, covering hardware designs, advanced signal\nprocessing techniques, and practical deployment strategies. It first examines\nkey THz reconfigurable metasurface architectures, including electronic,\noptical, phase-change material, and micro-electromechanical systems\n(MEMS)-based implementations, highlighting their reconfiguration mechanisms and\nchallenges. Then, fundamental effects including near field and beam squint in\nwideband THz systems are analyzed, along with their impacts on system\nperformance. The paper further explores conventional and beam-squint-assisted\nchannel estimation methods, innovative beam management strategies, and\ndeployment considerations across large- and small-scale scenarios. Practical\nexperiments at 220 gigahertz (GHz) validate the effectiveness of IRS in\nimproving signal strength and communication reliability for both single-user\nand multi-user setups."
                },
                "authors": [
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Yanze Zhu"
                    },
                    {
                        "name": "Qiaoyan Peng"
                    },
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Yanzhao Hou"
                    },
                    {
                        "name": "Fengyuan Yang"
                    },
                    {
                        "name": "Wencai Yan"
                    },
                    {
                        "name": "Guoning Wang"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Chi Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Chi Qiu"
                },
                "author": "Chi Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17196v1",
                "updated": "2025-06-20T17:47:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    47,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:47:36Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    47,
                    36,
                    4,
                    171,
                    0
                ],
                "title": "Detecting LLM-Generated Short Answers and Effects on Learner Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM-Generated Short Answers and Effects on Learner Performance"
                },
                "summary": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of large language models (LLMs) has raised\nconcerns about their potential misuse in online learning. While tools for\ndetecting LLM-generated text exist and are widely used by researchers and\neducators, their reliability varies. Few studies have compared the accuracy of\ndetection methods, defined criteria to identify content generated by LLM, or\nevaluated the effect on learner performance from LLM misuse within learning. In\nthis study, we define LLM-generated text within open responses as those\nproduced by any LLM without paraphrasing or refinement, as evaluated by human\ncoders. We then fine-tune GPT-4o to detect LLM-generated responses and assess\nthe impact on learning from LLM misuse. We find that our fine-tuned LLM\noutperforms the existing AI detection tool GPTZero, achieving an accuracy of\n80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1\nscore of 0.50, demonstrating superior performance in detecting LLM-generated\nresponses. We also find that learners suspected of LLM misuse in the open\nresponse question were more than twice as likely to correctly answer the\ncorresponding posttest MCQ, suggesting potential misuse across both question\ntypes and indicating a bypass of the learning process. We pave the way for\nfuture work by demonstrating a structured, code-based approach to improve\nLLM-generated response detection and propose using auxiliary statistical\nindicators such as unusually high assessment scores on related tasks,\nreadability scores, and response duration. In support of open science, we\ncontribute data and code to support the fine-tuning of similar models for\nsimilar use cases."
                },
                "authors": [
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Danielle R Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Isha Raghuvanshi"
                    },
                    {
                        "name": "Ralph Abboud"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Shivang Gupta"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Koedinger"
                },
                "author": "Kenneth Koedinger",
                "arxiv_comment": "Accepted for publication at the 19th European Conference on\n  Technology Enhanced Learning (ECTEL 2025). This is the author's accepted\n  manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17188v1",
                "updated": "2025-06-20T17:42:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    42,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:42:13Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    42,
                    13,
                    4,
                    171,
                    0
                ],
                "title": "Towards AI Search Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards AI Search Paradigm"
                },
                "summary": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems."
                },
                "authors": [
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Rui Kong"
                    },
                    {
                        "name": "Xinran Chen"
                    },
                    {
                        "name": "Jiamin Chen"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Haojie Zhang"
                    },
                    {
                        "name": "Jiayi Li"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Yiqun Chen"
                    },
                    {
                        "name": "Changle Qu"
                    },
                    {
                        "name": "Keyi Kong"
                    },
                    {
                        "name": "Wenwen Ye"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Haoyi Xiong"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07385v2",
                "updated": "2025-06-20T17:31:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    31,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-10T02:08:41Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    2,
                    8,
                    41,
                    3,
                    100,
                    0
                ],
                "title": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large\n  Language Models"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world, autonomous applications, relying on static, pre-annotated\nreferences for evaluation poses significant challenges in cost, scalability,\nand completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework\nto assess LLM outputs without predetermined ground-truth answers. Unlike\nconventional metrics that compare to fixed references or depend solely on\nLLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities\nthat actively retrieves and synthesizes external evidence. It iteratively\ngenerates web queries, collects information, summarizes findings, and refines\nsubsequent searches through reflection. By shifting away from static\nreferences, TALE aligns with free-form question-answering tasks common in\nreal-world scenarios. Experimental results on multiple free-form QA benchmarks\nshow that TALE not only outperforms standard reference-based metrics for\nmeasuring response accuracy but also achieves substantial to near-perfect\nagreement with human evaluations. TALE enhances the reliability of LLM\nevaluations in real-world, dynamic scenarios without relying on static\nreferences."
                },
                "authors": [
                    {
                        "name": "Sher Badshah"
                    },
                    {
                        "name": "Ali Emami"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08801v2",
                "updated": "2025-06-20T17:11:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    11,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2024-02-13T21:15:33Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    21,
                    15,
                    33,
                    1,
                    44,
                    0
                ],
                "title": "LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs and Stack Overflow Discussions: Reliability, Impact, and Challenges"
                },
                "summary": "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the\npremier platform for developers queries on programming and software\ndevelopment. Demonstrating an ability to generate instant, human-like responses\nto technical questions, ChatGPT has ignited debates within the developer\ncommunity about the evolving role of human-driven platforms in the age of\ngenerative AI. Two months after ChatGPT release, Meta released its answer with\nits own Large Language Model (LLM) called LLaMA: the race was on. We conducted\nan empirical study analyzing questions from Stack Overflow and using these LLMs\nto address them. This way, we aim to (i) quantify the reliability of LLMs\nanswers and their potential to replace Stack Overflow in the long term; (ii)\nidentify and understand why LLMs fail; (iii) measure users activity evolution\nwith Stack Overflow over time; and (iv) compare LLMs together. Our empirical\nresults are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do\nnot outperform it for some domains, while a significant decline in user posting\nactivity has been observed. Furthermore, we also discuss the impact of our\nfindings regarding the usage and development of new LLMs and provide guidelines\nfor future challenges faced by users and researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the\npremier platform for developers queries on programming and software\ndevelopment. Demonstrating an ability to generate instant, human-like responses\nto technical questions, ChatGPT has ignited debates within the developer\ncommunity about the evolving role of human-driven platforms in the age of\ngenerative AI. Two months after ChatGPT release, Meta released its answer with\nits own Large Language Model (LLM) called LLaMA: the race was on. We conducted\nan empirical study analyzing questions from Stack Overflow and using these LLMs\nto address them. This way, we aim to (i) quantify the reliability of LLMs\nanswers and their potential to replace Stack Overflow in the long term; (ii)\nidentify and understand why LLMs fail; (iii) measure users activity evolution\nwith Stack Overflow over time; and (iv) compare LLMs together. Our empirical\nresults are unequivocal: ChatGPT and LLaMA challenge human expertise, yet do\nnot outperform it for some domains, while a significant decline in user posting\nactivity has been observed. Furthermore, we also discuss the impact of our\nfindings regarding the usage and development of new LLMs and provide guidelines\nfor future challenges faced by users and researchers."
                },
                "authors": [
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Jordan Samhi"
                    },
                    {
                        "name": "Foutse Khomh"
                    }
                ],
                "author_detail": {
                    "name": "Foutse Khomh"
                },
                "author": "Foutse Khomh",
                "arxiv_comment": "63 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17163v1",
                "updated": "2025-06-20T17:09:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    9,
                    27,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T17:09:27Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    17,
                    9,
                    27,
                    4,
                    171,
                    0
                ],
                "title": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About\n  Human and Clinical LLM Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The MedPerturb Dataset: What Non-Content Perturbations Reveal About\n  Human and Clinical LLM Decision Making"
                },
                "summary": "Clinical robustness is critical to the safe deployment of medical Large\nLanguage Models (LLMs), but key questions remain about how LLMs and humans may\ndiffer in response to the real-world variability typified by clinical settings.\nTo address this, we introduce MedPerturb, a dataset designed to systematically\nevaluate medical LLMs under controlled perturbations of clinical input.\nMedPerturb consists of clinical vignettes spanning a range of pathologies, each\ntransformed along three axes: (1) gender modifications (e.g., gender-swapping\nor gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial\ntone); and (3) format changes (e.g., LLM-generated multi-turn conversations or\nsummaries). With MedPerturb, we release a dataset of 800 clinical contexts\ngrounded in realistic input variability, outputs from four LLMs, and three\nhuman expert reads per clinical context. We use MedPerturb in two case studies\nto reveal how shifts in gender identity cues, language style, or format reflect\ndiverging treatment selections between humans and LLMs. We find that LLMs are\nmore sensitive to gender and style perturbations while human annotators are\nmore sensitive to LLM-generated format perturbations such as clinical\nsummaries. Our results highlight the need for evaluation frameworks that go\nbeyond static benchmarks to assess the similarity between human clinician and\nLLM decisions under the variability characteristic of clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical robustness is critical to the safe deployment of medical Large\nLanguage Models (LLMs), but key questions remain about how LLMs and humans may\ndiffer in response to the real-world variability typified by clinical settings.\nTo address this, we introduce MedPerturb, a dataset designed to systematically\nevaluate medical LLMs under controlled perturbations of clinical input.\nMedPerturb consists of clinical vignettes spanning a range of pathologies, each\ntransformed along three axes: (1) gender modifications (e.g., gender-swapping\nor gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial\ntone); and (3) format changes (e.g., LLM-generated multi-turn conversations or\nsummaries). With MedPerturb, we release a dataset of 800 clinical contexts\ngrounded in realistic input variability, outputs from four LLMs, and three\nhuman expert reads per clinical context. We use MedPerturb in two case studies\nto reveal how shifts in gender identity cues, language style, or format reflect\ndiverging treatment selections between humans and LLMs. We find that LLMs are\nmore sensitive to gender and style perturbations while human annotators are\nmore sensitive to LLM-generated format perturbations such as clinical\nsummaries. Our results highlight the need for evaluation frameworks that go\nbeyond static benchmarks to assess the similarity between human clinician and\nLLM decisions under the variability characteristic of clinical settings."
                },
                "authors": [
                    {
                        "name": "Abinitha Gourabathina"
                    },
                    {
                        "name": "Yuexing Hao"
                    },
                    {
                        "name": "Walter Gerych"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04684v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04684v4",
                "updated": "2025-06-20T16:50:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    50,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2023-12-07T20:36:10Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    20,
                    36,
                    10,
                    3,
                    341,
                    0
                ],
                "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning"
                },
                "summary": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) prompting is a popular in-context learning (ICL)\napproach for large language models (LLMs), especially when tackling complex\nreasoning tasks. Traditional ICL approaches construct prompts using examples\nthat contain questions similar to the input question. However, CoT prompting,\nwhich includes crucial intermediate reasoning steps (rationales) within its\nexamples, necessitates selecting examples based on these rationales rather than\nthe questions themselves. Existing methods require human experts or pre-trained\nLLMs to describe the skill, a high-level abstraction of rationales, to guide\nthe selection. These methods, however, are often costly and difficult to scale.\nInstead, this paper introduces a new approach named Latent Reasoning Skills\n(LaRS) that employs unsupervised learning to create a latent space\nrepresentation of rationales, with a latent variable called a reasoning skill.\nConcurrently, LaRS learns a reasoning policy to determine the required\nreasoning skill for a given question. Then the ICL examples are selected by\naligning the reasoning skills between past examples and the question. This\napproach is theoretically grounded and compute-efficient, eliminating the need\nfor auxiliary LLM inference or manual prompt design. Empirical results\ndemonstrate that LaRS consistently outperforms SOTA skill-based selection\nmethods, processing example banks four times faster, reducing LLM inferences\nduring the selection stage by half, and showing greater robustness to\nsub-optimal example banks."
                },
                "authors": [
                    {
                        "name": "Zifan Xu"
                    },
                    {
                        "name": "Haozhu Wang"
                    },
                    {
                        "name": "Dmitriy Bespalov"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Yanjun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Qi"
                },
                "author": "Yanjun Qi",
                "arxiv_journal_ref": "Findings of Empirical Methods in Natural Language Processing 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04684v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04684v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17144v1",
                "updated": "2025-06-20T16:45:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    45,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:45:54Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    45,
                    54,
                    4,
                    171,
                    0
                ],
                "title": "Do We Need Large VLMs for Spotting Soccer Actions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do We Need Large VLMs for Spotting Soccer Actions?"
                },
                "summary": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video-based tasks like soccer action spotting rely heavily on\nvisual inputs, often requiring complex and computationally expensive models to\nprocess dense video data. In this work, we propose a shift from this\nvideo-centric approach to a text-based task, making it lightweight and scalable\nby utilizing Large Language Models (LLMs) instead of Vision-Language Models\n(VLMs). We posit that expert commentary, which provides rich, fine-grained\ndescriptions and contextual cues such as excitement and tactical insights,\ncontains enough information to reliably spot key actions in a match. To\ndemonstrate this, we use the SoccerNet Echoes dataset, which provides\ntimestamped commentary, and employ a system of three LLMs acting as judges\nspecializing in outcome, excitement, and tactics. Each LLM evaluates sliding\nwindows of commentary to identify actions like goals, cards, and substitutions,\ngenerating accurate timestamps for these events. Our experiments show that this\nlanguage-centric approach performs effectively in detecting critical match\nevents, providing a lightweight and training-free alternative to traditional\nvideo-based methods for action spotting."
                },
                "authors": [
                    {
                        "name": "Ritabrata Chakraborty"
                    },
                    {
                        "name": "Rajatsubhra Chakraborty"
                    },
                    {
                        "name": "Avijit Dasgupta"
                    },
                    {
                        "name": "Sandeep Chaurasia"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Chaurasia"
                },
                "author": "Sandeep Chaurasia",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18110v2",
                "updated": "2025-06-20T16:28:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    28,
                    3,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T17:04:27Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    4,
                    27,
                    4,
                    143,
                    0
                ],
                "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM"
                },
                "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released."
                },
                "authors": [
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yongxin Guo"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Farid Boussaid"
                    },
                    {
                        "name": "Girish Dwivedi"
                    },
                    {
                        "name": "Luqi Gong"
                    },
                    {
                        "name": "Qiuhong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Qiuhong Ke"
                },
                "author": "Qiuhong Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17125v1",
                "updated": "2025-06-20T16:27:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    27,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:27:59Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    27,
                    59,
                    4,
                    171,
                    0
                ],
                "title": "Large Language Model Unlearning for Source Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Unlearning for Source Code"
                },
                "summary": "LLM4SE has demonstrated significant success, but LLMs' potential memorization\nof sensitive or outdated training data introduces critical risks to legal\ncompliance, software security, and code quality. LLM unlearning techniques,\nwhich can eliminate the influence of undesired data from LLMs in a\npost-training way, present a promising solution to address these concerns.\nWhile recent efforts in LLM unlearning show effectiveness in natural language,\ntheir applicability to source code remains underexplored. Our empirical study\nreveals that existing LLM unlearning approaches, when applied to source code,\ncause severe model utility degradation, rendering models practically unusable\nfor code generation. In this paper, we propose PROD, a novel unlearning\napproach that enables LLMs to forget undesired code content while effectively\npreserving their code generation capabilities. PROD suppresses the probability\nof forget data in LLMs' output distribution while promoting candidate\ndistributional components, enabling the model to jointly learn to forget\nspecific content and retain its general capabilities. To facilitate this study,\nwe establish a benchmark for code unlearning evaluation, which includes three\ncritical downstream tasks: copyrighted code unlearning, insecure code\nunlearning, and deprecated API unlearning. Our evaluation demonstrates that\nPROD achieves superior balance between forget quality and model utility\ncompared to existing unlearning approaches across three downstream tasks, while\nconsistently exhibiting improvements when applied to LLMs of varying series.\nPROD also exhibits superior robustness against adversarial attacks without\ngenerating or exposing the data to be forgotten. The results underscore that\nour approach not only extends the application boundary of unlearning techniques\nto source code, but also holds significant implications for advancing reliable\ncode generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SE has demonstrated significant success, but LLMs' potential memorization\nof sensitive or outdated training data introduces critical risks to legal\ncompliance, software security, and code quality. LLM unlearning techniques,\nwhich can eliminate the influence of undesired data from LLMs in a\npost-training way, present a promising solution to address these concerns.\nWhile recent efforts in LLM unlearning show effectiveness in natural language,\ntheir applicability to source code remains underexplored. Our empirical study\nreveals that existing LLM unlearning approaches, when applied to source code,\ncause severe model utility degradation, rendering models practically unusable\nfor code generation. In this paper, we propose PROD, a novel unlearning\napproach that enables LLMs to forget undesired code content while effectively\npreserving their code generation capabilities. PROD suppresses the probability\nof forget data in LLMs' output distribution while promoting candidate\ndistributional components, enabling the model to jointly learn to forget\nspecific content and retain its general capabilities. To facilitate this study,\nwe establish a benchmark for code unlearning evaluation, which includes three\ncritical downstream tasks: copyrighted code unlearning, insecure code\nunlearning, and deprecated API unlearning. Our evaluation demonstrates that\nPROD achieves superior balance between forget quality and model utility\ncompared to existing unlearning approaches across three downstream tasks, while\nconsistently exhibiting improvements when applied to LLMs of varying series.\nPROD also exhibits superior robustness against adversarial attacks without\ngenerating or exposing the data to be forgotten. The results underscore that\nour approach not only extends the application boundary of unlearning techniques\nto source code, but also holds significant implications for advancing reliable\ncode generation."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Tangxinyu Wang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05091v2",
                "updated": "2025-06-20T16:24:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    24,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-07T19:16:49Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    19,
                    16,
                    49,
                    3,
                    312,
                    0
                ],
                "title": "Watermarking Language Models through Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking Language Models through Language Models"
                },
                "summary": "Watermarking the outputs of large language models (LLMs) is critical for\nprovenance tracing, content regulation, and model accountability. Existing\napproaches often rely on access to model internals or are constrained by static\nrules and token-level perturbations. Moreover, the idea of steering generative\nbehavior via prompt-based instruction control remains largely underexplored. We\nintroduce a prompt-guided watermarking framework that operates entirely at the\ninput level and requires no access to model parameters or decoding logits. The\nframework comprises three cooperating components: a Prompting LM that\nsynthesizes watermarking instructions from user prompts, a Marking LM that\ngenerates watermarked outputs conditioned on these instructions, and a\nDetecting LM trained to classify whether a response carries an embedded\nwatermark. This modular design enables dynamic watermarking that adapts to\nindividual prompts while remaining compatible with diverse LLM architectures,\nincluding both proprietary and open-weight models. We evaluate the framework\nover 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral,\nLLaMA3, and DeepSeek. Experimental results show that watermark signals\ngeneralize across architectures and remain robust under fine-tuning, model\ndistillation, and prompt-based adversarial attacks, demonstrating the\neffectiveness and robustness of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking the outputs of large language models (LLMs) is critical for\nprovenance tracing, content regulation, and model accountability. Existing\napproaches often rely on access to model internals or are constrained by static\nrules and token-level perturbations. Moreover, the idea of steering generative\nbehavior via prompt-based instruction control remains largely underexplored. We\nintroduce a prompt-guided watermarking framework that operates entirely at the\ninput level and requires no access to model parameters or decoding logits. The\nframework comprises three cooperating components: a Prompting LM that\nsynthesizes watermarking instructions from user prompts, a Marking LM that\ngenerates watermarked outputs conditioned on these instructions, and a\nDetecting LM trained to classify whether a response carries an embedded\nwatermark. This modular design enables dynamic watermarking that adapts to\nindividual prompts while remaining compatible with diverse LLM architectures,\nincluding both proprietary and open-weight models. We evaluate the framework\nover 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral,\nLLaMA3, and DeepSeek. Experimental results show that watermark signals\ngeneralize across architectures and remain robust under fine-tuning, model\ndistillation, and prompt-based adversarial attacks, demonstrating the\neffectiveness and robustness of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Agnibh Dasgupta"
                    },
                    {
                        "name": "Abdullah Tanvir"
                    },
                    {
                        "name": "Xin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhong"
                },
                "author": "Xin Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19675v2",
                "updated": "2025-06-20T16:24:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    24,
                    7,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-26T08:31:55Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    31,
                    55,
                    0,
                    146,
                    0
                ],
                "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy\n  Labels via Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy\n  Labels via Iterative Refinement"
                },
                "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."
                },
                "authors": [
                    {
                        "name": "Liqin Ye"
                    },
                    {
                        "name": "Agam Shah"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Sudheer Chava"
                    }
                ],
                "author_detail": {
                    "name": "Sudheer Chava"
                },
                "author": "Sudheer Chava",
                "arxiv_comment": "Accepted at KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17124v1",
                "updated": "2025-06-20T16:23:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    23,
                    46,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:23:46Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    23,
                    46,
                    4,
                    171,
                    0
                ],
                "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Can Model-Free Reinforcement Learning be Enough for Thinking?"
                },
                "summary": "Recent work on large language models has demonstrated the use of model-free\nreinforcement learning (RL) to train reasoning-like capabilities. The emergence\nof \"thinking\" through model-free RL is interesting as thinking actions neither\nproduce reward nor change the external world state to one where the agent is\nmore likely to get reward. This paper seeks to build a domain-independent\nunderstanding of when model-free RL will lead to \"thinking\" as a strategy for\nreward maximization. To build this understanding, we first introduce a\ntheoretical model which we call a \\textit{thought Markov decision process}\n(MDP). Thought MDPs minimally extend the classical MDP model to include an\nabstract notion of thought state and thought action. Using the thought MDP\nmodel, we prove the importance of policy initialization in determining whether\nor not thinking emerges and show formally that thought actions are equivalent\nto the agent choosing to perform a step of policy improvement before continuing\nto act. We then show that open-source LLMs satisfy the conditions that our\ntheory predicts are necessary for model-free RL to produce thinking-like\nbehavior. Finally, we hypothesize sufficient conditions that would enable\nthinking to be learned outside of language generation and introduce a toy\ndomain where a combination of multi-task pre-training and designated thought\nactions enable more data-efficient RL compared to non-thinking agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work on large language models has demonstrated the use of model-free\nreinforcement learning (RL) to train reasoning-like capabilities. The emergence\nof \"thinking\" through model-free RL is interesting as thinking actions neither\nproduce reward nor change the external world state to one where the agent is\nmore likely to get reward. This paper seeks to build a domain-independent\nunderstanding of when model-free RL will lead to \"thinking\" as a strategy for\nreward maximization. To build this understanding, we first introduce a\ntheoretical model which we call a \\textit{thought Markov decision process}\n(MDP). Thought MDPs minimally extend the classical MDP model to include an\nabstract notion of thought state and thought action. Using the thought MDP\nmodel, we prove the importance of policy initialization in determining whether\nor not thinking emerges and show formally that thought actions are equivalent\nto the agent choosing to perform a step of policy improvement before continuing\nto act. We then show that open-source LLMs satisfy the conditions that our\ntheory predicts are necessary for model-free RL to produce thinking-like\nbehavior. Finally, we hypothesize sufficient conditions that would enable\nthinking to be learned outside of language generation and introduce a toy\ndomain where a combination of multi-task pre-training and designated thought\nactions enable more data-efficient RL compared to non-thinking agents."
                },
                "authors": [
                    {
                        "name": "Josiah P. Hanna"
                    },
                    {
                        "name": "Nicholas E. Corrado"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas E. Corrado"
                },
                "author": "Nicholas E. Corrado",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17104v1",
                "updated": "2025-06-20T16:09:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    9,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:09:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    9,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic\n  Theorem Proving"
                },
                "summary": "Large language models (LLMs) have shown promising first-order logic (FOL)\nreasoning capabilities with applications in various areas. However, their\neffectiveness in complex mathematical reasoning involving multi-step FOL\ndeductions is still under-researched. While LLMs perform competitively on\nestablished mathematical reasoning benchmarks, they struggle with multi-step\nFOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on\nour proposed theorem proving dataset. This issue arises from the limited\nexploration of diverse proof strategies and the potential for early reasoning\nmistakes to undermine entire proofs. To address these issues, we propose DREAM,\na self-adaptive solution that enhances the Diversity and REAsonability of LLMs'\ngeneration strategies. DREAM incorporates an Axiom-Driven Strategy\nDiversification mechanism to promote varied strategic outcomes and a\nSub-Proposition Error Feedback to help LLMs reflect on and correct their\nproofs. Our contributions include pioneering advancements in LLMs' mathematical\nreasoning through FOL theorem proving, introducing a novel inference stage\nsolution that improves performance by 0.6% to 6.4%, and providing a curated\ndataset of 447 mathematical theorems in Lean 4 format for evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising first-order logic (FOL)\nreasoning capabilities with applications in various areas. However, their\neffectiveness in complex mathematical reasoning involving multi-step FOL\ndeductions is still under-researched. While LLMs perform competitively on\nestablished mathematical reasoning benchmarks, they struggle with multi-step\nFOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on\nour proposed theorem proving dataset. This issue arises from the limited\nexploration of diverse proof strategies and the potential for early reasoning\nmistakes to undermine entire proofs. To address these issues, we propose DREAM,\na self-adaptive solution that enhances the Diversity and REAsonability of LLMs'\ngeneration strategies. DREAM incorporates an Axiom-Driven Strategy\nDiversification mechanism to promote varied strategic outcomes and a\nSub-Proposition Error Feedback to help LLMs reflect on and correct their\nproofs. Our contributions include pioneering advancements in LLMs' mathematical\nreasoning through FOL theorem proving, introducing a novel inference stage\nsolution that improves performance by 0.6% to 6.4%, and providing a curated\ndataset of 447 mathematical theorems in Lean 4 format for evaluation."
                },
                "authors": [
                    {
                        "name": "Chuxue Cao"
                    },
                    {
                        "name": "Mengze Li"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Jinluan Yang"
                    },
                    {
                        "name": "Zijian Zhao"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Chengzhong Liu"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12997v2",
                "updated": "2025-06-20T16:05:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    5,
                    46,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-15T23:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    23,
                    46,
                    49,
                    6,
                    166,
                    0
                ],
                "title": "MORIC: CSI Delay-Doppler Decomposition for Robust Wi-Fi-based Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MORIC: CSI Delay-Doppler Decomposition for Robust Wi-Fi-based Human\n  Activity Recognition"
                },
                "summary": "The newly established IEEE 802.11bf Task Group aims to amend the WLAN\nstandard to support advanced sensing applications such as human activity\nrecognition (HAR). Although studies have demonstrated the potential of sub-7\nGHz Wi-Fi Channel State Information (CSI) for HAR, no method currently performs\nreliably in real-world scenarios. This work tackles the poor generalization of\nWi-Fi-based HAR by introducing an innovative approach to extracting and\nutilizing movement-related representations, which makes it robust to noise and\nstatic environmental properties. This is achieved by transforming CSI signals\ninto the delay profile space and decomposing them into various Doppler\nvelocities, which serve as informative projections of a mobile point's velocity\nfrom different unknown random angles. To mitigate the impact of this\nrandomness, MORIC is introduced as a novel time series classification model\nbased on random convolutional kernels, designed to be invariant to the random\norder and repetition of input representations, thereby enabling robust Wi-Fi\nCSI-based activity classification. Experimental results on the collected\ndataset demonstrate that the proposed method outperforms state-of-the-art\napproaches in terms of generalization accuracy for hand motion recognition,\nparticularly for challenging gestures. Furthermore, incorporating a small\nnumber of calibration samples leads to a significant improvement in accuracy,\nenhancing the practicality of the method for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The newly established IEEE 802.11bf Task Group aims to amend the WLAN\nstandard to support advanced sensing applications such as human activity\nrecognition (HAR). Although studies have demonstrated the potential of sub-7\nGHz Wi-Fi Channel State Information (CSI) for HAR, no method currently performs\nreliably in real-world scenarios. This work tackles the poor generalization of\nWi-Fi-based HAR by introducing an innovative approach to extracting and\nutilizing movement-related representations, which makes it robust to noise and\nstatic environmental properties. This is achieved by transforming CSI signals\ninto the delay profile space and decomposing them into various Doppler\nvelocities, which serve as informative projections of a mobile point's velocity\nfrom different unknown random angles. To mitigate the impact of this\nrandomness, MORIC is introduced as a novel time series classification model\nbased on random convolutional kernels, designed to be invariant to the random\norder and repetition of input representations, thereby enabling robust Wi-Fi\nCSI-based activity classification. Experimental results on the collected\ndataset demonstrate that the proposed method outperforms state-of-the-art\napproaches in terms of generalization accuracy for hand motion recognition,\nparticularly for challenging gestures. Furthermore, incorporating a small\nnumber of calibration samples leads to a significant improvement in accuracy,\nenhancing the practicality of the method for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Navid Hasanzadeh"
                    },
                    {
                        "name": "Shahrokh Valaee"
                    }
                ],
                "author_detail": {
                    "name": "Shahrokh Valaee"
                },
                "author": "Shahrokh Valaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17090v1",
                "updated": "2025-06-20T15:53:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    53,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:53:51Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    53,
                    51,
                    4,
                    171,
                    0
                ],
                "title": "Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions"
                },
                "summary": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known."
                },
                "authors": [
                    {
                        "name": "Murtaza Nazir"
                    },
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12958v2",
                "updated": "2025-06-20T15:52:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    52,
                    6,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-15T20:42:45Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    20,
                    42,
                    45,
                    6,
                    166,
                    0
                ],
                "title": "Domain Specific Benchmarks for Evaluating Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Specific Benchmarks for Evaluating Multimodal Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed across\ndisciplines due to their advanced reasoning and problem solving capabilities.\nTo measure their effectiveness, various benchmarks have been developed that\nmeasure aspects of LLM reasoning, comprehension, and problem-solving. While\nseveral surveys address LLM evaluation and benchmarks, a domain-specific\nanalysis remains underexplored in the literature. This paper introduces a\ntaxonomy of seven key disciplines, encompassing various domains and application\nareas where LLMs are extensively utilized. Additionally, we provide a\ncomprehensive review of LLM benchmarks and survey papers within each domain,\nhighlighting the unique capabilities of LLMs and the challenges faced in their\napplication. Finally, we compile and categorize these benchmarks by domain to\ncreate an accessible resource for researchers, aiming to pave the way for\nadvancements toward artificial general intelligence (AGI)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed across\ndisciplines due to their advanced reasoning and problem solving capabilities.\nTo measure their effectiveness, various benchmarks have been developed that\nmeasure aspects of LLM reasoning, comprehension, and problem-solving. While\nseveral surveys address LLM evaluation and benchmarks, a domain-specific\nanalysis remains underexplored in the literature. This paper introduces a\ntaxonomy of seven key disciplines, encompassing various domains and application\nareas where LLMs are extensively utilized. Additionally, we provide a\ncomprehensive review of LLM benchmarks and survey papers within each domain,\nhighlighting the unique capabilities of LLMs and the challenges faced in their\napplication. Finally, we compile and categorize these benchmarks by domain to\ncreate an accessible resource for researchers, aiming to pave the way for\nadvancements toward artificial general intelligence (AGI)"
                },
                "authors": [
                    {
                        "name": "Khizar Anjum"
                    },
                    {
                        "name": "Muhammad Arbab Arshad"
                    },
                    {
                        "name": "Kadhim Hayawi"
                    },
                    {
                        "name": "Efstathios Polyzos"
                    },
                    {
                        "name": "Asadullah Tariq"
                    },
                    {
                        "name": "Mohamed Adel Serhani"
                    },
                    {
                        "name": "Laiba Batool"
                    },
                    {
                        "name": "Brady Lund"
                    },
                    {
                        "name": "Nishith Reddy Mannuru"
                    },
                    {
                        "name": "Ravi Varma Kumar Bevara"
                    },
                    {
                        "name": "Taslim Mahbub"
                    },
                    {
                        "name": "Muhammad Zeeshan Akram"
                    },
                    {
                        "name": "Sakib Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Sakib Shahriar"
                },
                "author": "Sakib Shahriar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17088v1",
                "updated": "2025-06-20T15:49:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    49,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:49:37Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    49,
                    37,
                    4,
                    171,
                    0
                ],
                "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language\n  Models: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language\n  Models: An Empirical Evaluation"
                },
                "summary": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect."
                },
                "authors": [
                    {
                        "name": "Jiahao Cheng"
                    },
                    {
                        "name": "Tiancheng Su"
                    },
                    {
                        "name": "Jia Yuan"
                    },
                    {
                        "name": "Guoxiu He"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Xinqi Tao"
                    },
                    {
                        "name": "Jingwen Xie"
                    },
                    {
                        "name": "Huaxia Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaxia Li"
                },
                "author": "Huaxia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13784v2",
                "updated": "2025-06-20T15:48:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    48,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-11T02:05:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    5,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "ScholarSearch: Benchmarking Scholar Searching Ability of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScholarSearch: Benchmarking Scholar Searching Ability of LLMs"
                },
                "summary": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed ScholarSearch, the first dataset specifically designed to evaluate the\ncomplex information retrieval capabilities of Large Language Models (LLMs) in\nacademic research. ScholarSearch possesses the following key characteristics:\nAcademic Practicality, where question content closely mirrors real academic\nlearning and research environments, avoiding deliberately misleading models;\nHigh Difficulty, with answers that are challenging for single models (e.g.,\nGrok DeepSearch or Gemini Deep Research) to provide directly, often requiring\nat least three deep searches to derive; Concise Evaluation, where limiting\nconditions ensure answers are as unique as possible, accompanied by clear\nsources and brief solution explanations, greatly facilitating subsequent audit\nand verification, surpassing the current lack of analyzed search datasets both\ndomestically and internationally; and Broad Coverage, as the dataset spans at\nleast 15 different academic disciplines. Through ScholarSearch, we expect to\nmore precisely measure and promote the performance improvement of LLMs in\ncomplex academic information retrieval tasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs)' search capabilities have garnered significant\nattention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on\ngeneral search scenarios and fail to adequately address the specific demands of\nacademic search. These demands include deeper literature tracing and\norganization, professional support for academic databases, the ability to\nnavigate long-tail academic knowledge, and ensuring academic rigor. Here, we\nproposed ScholarSearch, the first dataset specifically designed to evaluate the\ncomplex information retrieval capabilities of Large Language Models (LLMs) in\nacademic research. ScholarSearch possesses the following key characteristics:\nAcademic Practicality, where question content closely mirrors real academic\nlearning and research environments, avoiding deliberately misleading models;\nHigh Difficulty, with answers that are challenging for single models (e.g.,\nGrok DeepSearch or Gemini Deep Research) to provide directly, often requiring\nat least three deep searches to derive; Concise Evaluation, where limiting\nconditions ensure answers are as unique as possible, accompanied by clear\nsources and brief solution explanations, greatly facilitating subsequent audit\nand verification, surpassing the current lack of analyzed search datasets both\ndomestically and internationally; and Broad Coverage, as the dataset spans at\nleast 15 different academic disciplines. Through ScholarSearch, we expect to\nmore precisely measure and promote the performance improvement of LLMs in\ncomplex academic information retrieval tasks. The data is available at:\nhttps://huggingface.co/datasets/PKU-DS-LAB/ScholarSearch"
                },
                "authors": [
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Yiyan Liao"
                    },
                    {
                        "name": "Nengyuan Zhang"
                    },
                    {
                        "name": "Tingjia Miao"
                    },
                    {
                        "name": "Zhihui Qi"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01547v2",
                "updated": "2025-06-20T15:42:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    42,
                    53,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-02T19:19:40Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    19,
                    19,
                    40,
                    4,
                    122,
                    0
                ],
                "title": "ASAP-MO:Advanced Situational Awareness and Perception for\n  Mission-critical Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASAP-MO:Advanced Situational Awareness and Perception for\n  Mission-critical Operations"
                },
                "summary": "Deploying robotic missions can be challenging due to the complexity of\ncontrolling robots with multiple degrees of freedom, fusing diverse sensory\ninputs, and managing communication delays and interferences. In nuclear\ninspection, robots can be crucial in assessing environments where human\npresence is limited, requiring precise teleoperation and coordination.\nTeleoperation requires extensive training, as operators must process multiple\noutputs while ensuring safe interaction with critical assets. These challenges\nare amplified when operating a fleet of heterogeneous robots across multiple\nenvironments, as each robot may have distinct control interfaces, sensory\nsystems, and operational constraints. Efficient coordination in such settings\nremains an open problem. This paper presents a field report on how we\nintegrated robot fleet capabilities - including mapping, localization, and\ntelecommunication - toward a joint mission. We simulated a nuclear inspection\nscenario for exposed areas, using lights to represent a radiation source. We\ndeployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and\noutdoor environments while remotely controlled from a single base station.\nDespite having distinct operational goals, the robots produced a unified map\noutput, demonstrating the feasibility of coordinated multi-robot missions. Our\nresults highlight key operational challenges and provide insights into\nimproving adaptability and situational awareness in remote robotic deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying robotic missions can be challenging due to the complexity of\ncontrolling robots with multiple degrees of freedom, fusing diverse sensory\ninputs, and managing communication delays and interferences. In nuclear\ninspection, robots can be crucial in assessing environments where human\npresence is limited, requiring precise teleoperation and coordination.\nTeleoperation requires extensive training, as operators must process multiple\noutputs while ensuring safe interaction with critical assets. These challenges\nare amplified when operating a fleet of heterogeneous robots across multiple\nenvironments, as each robot may have distinct control interfaces, sensory\nsystems, and operational constraints. Efficient coordination in such settings\nremains an open problem. This paper presents a field report on how we\nintegrated robot fleet capabilities - including mapping, localization, and\ntelecommunication - toward a joint mission. We simulated a nuclear inspection\nscenario for exposed areas, using lights to represent a radiation source. We\ndeployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and\noutdoor environments while remotely controlled from a single base station.\nDespite having distinct operational goals, the robots produced a unified map\noutput, demonstrating the feasibility of coordinated multi-robot missions. Our\nresults highlight key operational challenges and provide insights into\nimproving adaptability and situational awareness in remote robotic deployments."
                },
                "authors": [
                    {
                        "name": "Veronica Vannini"
                    },
                    {
                        "name": "William Dubois"
                    },
                    {
                        "name": "Olivier Gamache"
                    },
                    {
                        "name": "Jean-Michel Fortin"
                    },
                    {
                        "name": "Nicolas Samson"
                    },
                    {
                        "name": "Effie Daum"
                    },
                    {
                        "name": "François Pomerleau"
                    },
                    {
                        "name": "Edith Brotherton"
                    }
                ],
                "author_detail": {
                    "name": "Edith Brotherton"
                },
                "author": "Edith Brotherton",
                "arxiv_comment": "6 pages + references, 7 figures, Presented at the 2025 IEEE ICRA\n  Workshop on Field Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17080v1",
                "updated": "2025-06-20T15:30:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    30,
                    6,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:30:06Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    30,
                    6,
                    4,
                    171,
                    0
                ],
                "title": "Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tower+: Bridging Generality and Translation Specialization in\n  Multilingual LLMs"
                },
                "summary": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization."
                },
                "authors": [
                    {
                        "name": "Ricardo Rei"
                    },
                    {
                        "name": "Nuno M. Guerreiro"
                    },
                    {
                        "name": "José Pombal"
                    },
                    {
                        "name": "João Alves"
                    },
                    {
                        "name": "Pedro Teixeirinha"
                    },
                    {
                        "name": "Amin Farajian"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17077v1",
                "updated": "2025-06-20T15:27:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    27,
                    44,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:27:44Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    27,
                    44,
                    4,
                    171,
                    0
                ],
                "title": "Simultaneous Translation with Offline Speech and LLM Models in CUNI\n  Submission to IWSLT 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Translation with Offline Speech and LLM Models in CUNI\n  Submission to IWSLT 2025"
                },
                "summary": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency."
                },
                "authors": [
                    {
                        "name": "Dominik Macháček"
                    },
                    {
                        "name": "Peter Polák"
                    }
                ],
                "author_detail": {
                    "name": "Peter Polák"
                },
                "author": "Peter Polák",
                "arxiv_comment": "IWSLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17073v1",
                "updated": "2025-06-20T15:24:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    24,
                    31,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:24:31Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    24,
                    31,
                    4,
                    171,
                    0
                ],
                "title": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions,\n  Even When Transparently Disclosed as AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Bot Broadens the Range of Arguments in Online Discussions,\n  Even When Transparently Disclosed as AI"
                },
                "summary": "A wide range of participation is essential for democracy, as it helps prevent\nthe dominance of extreme views, erosion of legitimacy, and political\npolarization. However, engagement in online political discussions often\nfeatures a limited spectrum of views due to high levels of self-selection and\nthe tendency of online platforms to facilitate exchanges primarily among\nlike-minded individuals. This study examines whether an LLM-based bot can widen\nthe scope of perspectives expressed by participants in online discussions\nthrough two pre-registered randomized experiments conducted in a chatroom. We\nevaluate the impact of a bot that actively monitors discussions, identifies\nmissing arguments, and introduces them into the conversation. The results\nindicate that our bot significantly expands the range of arguments, as measured\nby both objective and subjective metrics. Furthermore, disclosure of the bot as\nAI does not significantly alter these effects. These findings suggest that\nLLM-based moderation tools can positively influence online political discourse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wide range of participation is essential for democracy, as it helps prevent\nthe dominance of extreme views, erosion of legitimacy, and political\npolarization. However, engagement in online political discussions often\nfeatures a limited spectrum of views due to high levels of self-selection and\nthe tendency of online platforms to facilitate exchanges primarily among\nlike-minded individuals. This study examines whether an LLM-based bot can widen\nthe scope of perspectives expressed by participants in online discussions\nthrough two pre-registered randomized experiments conducted in a chatroom. We\nevaluate the impact of a bot that actively monitors discussions, identifies\nmissing arguments, and introduces them into the conversation. The results\nindicate that our bot significantly expands the range of arguments, as measured\nby both objective and subjective metrics. Furthermore, disclosure of the bot as\nAI does not significantly alter these effects. These findings suggest that\nLLM-based moderation tools can positively influence online political discourse."
                },
                "authors": [
                    {
                        "name": "Valeria Vuk"
                    },
                    {
                        "name": "Cristina Sarasua"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Gilardi"
                },
                "author": "Fabrizio Gilardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01702v2",
                "updated": "2025-06-20T15:22:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    22,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-03T09:54:00Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    54,
                    0,
                    0,
                    34,
                    0
                ],
                "title": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Al-Khwarizmi: Discovering Physical Laws with Foundation Models"
                },
                "summary": "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative."
                },
                "authors": [
                    {
                        "name": "Christopher E. Mower"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou-Ammar"
                },
                "author": "Haitham Bou-Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17067v1",
                "updated": "2025-06-20T15:14:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    14,
                    29,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:14:29Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    14,
                    29,
                    4,
                    171,
                    0
                ],
                "title": "Empowering Near-Field Communications in Low-Altitude Economy with LLM:\n  Fundamentals, Potentials, Solutions, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Near-Field Communications in Low-Altitude Economy with LLM:\n  Fundamentals, Potentials, Solutions, and Future Directions"
                },
                "summary": "The low-altitude economy (LAE) is gaining significant attention from academia\nand industry. Fortunately, LAE naturally aligns with near-field communications\nin extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field\nbeamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,\nwhile the additional distance dimension boosts overall spectrum efficiency.\nHowever, near-field communications in LAE still face several challenges, such\nas the increase in signal processing complexity and the necessity of\ndistinguishing between far and near-field users. Inspired by the large language\nmodels (LLM) with powerful ability to handle complex problems, we apply LLM to\nsolve challenges of near-field communications in LAE. The objective of this\narticle is to provide a comprehensive analysis and discussion on LLM-empowered\nnear-field communications in LAE. Specifically, we first introduce fundamentals\nof LLM and near-field communications, including the key advantages of LLM and\nkey characteristics of near-field communications. Then, we reveal the\nopportunities and challenges of near-field communications in LAE. To address\nthese challenges, we present a LLM-based scheme for near-field communications\nin LAE, and provide a case study which jointly distinguishes far and near-field\nusers and designs multi-user precoding matrix. Finally, we outline and\nhighlight several future research directions and open issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The low-altitude economy (LAE) is gaining significant attention from academia\nand industry. Fortunately, LAE naturally aligns with near-field communications\nin extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field\nbeamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,\nwhile the additional distance dimension boosts overall spectrum efficiency.\nHowever, near-field communications in LAE still face several challenges, such\nas the increase in signal processing complexity and the necessity of\ndistinguishing between far and near-field users. Inspired by the large language\nmodels (LLM) with powerful ability to handle complex problems, we apply LLM to\nsolve challenges of near-field communications in LAE. The objective of this\narticle is to provide a comprehensive analysis and discussion on LLM-empowered\nnear-field communications in LAE. Specifically, we first introduce fundamentals\nof LLM and near-field communications, including the key advantages of LLM and\nkey characteristics of near-field communications. Then, we reveal the\nopportunities and challenges of near-field communications in LAE. To address\nthese challenges, we present a LLM-based scheme for near-field communications\nin LAE, and provide a case study which jointly distinguishes far and near-field\nusers and designs multi-user precoding matrix. Finally, we outline and\nhighlight several future research directions and open issues."
                },
                "authors": [
                    {
                        "name": "Zhuo Xu"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Linglong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Linglong Dai"
                },
                "author": "Linglong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17063v1",
                "updated": "2025-06-20T15:11:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    11,
                    20,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:11:20Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    11,
                    20,
                    4,
                    171,
                    0
                ],
                "title": "Client Selection Strategies for Federated Semantic Communications in\n  Heterogeneous IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client Selection Strategies for Federated Semantic Communications in\n  Heterogeneous IoT Networks"
                },
                "summary": "The exponential growth of IoT devices presents critical challenges in\nbandwidth-constrained wireless networks, particularly regarding efficient data\ntransmission and privacy preservation. This paper presents a novel federated\nsemantic communication (SC) framework that enables collaborative training of\nbandwidth-efficient models for image reconstruction across heterogeneous IoT\ndevices. By leveraging SC principles to transmit only semantic features, our\napproach dramatically reduces communication overhead while preserving\nreconstruction quality. We address the fundamental challenge of client\nselection in federated learning environments where devices exhibit significant\ndisparities in dataset sizes and data distributions. Our framework implements\nthree distinct client selection strategies that explore different trade-offs\nbetween system performance and fairness in resource allocation. The system\nemploys an end-to-end SC architecture with semantic bottlenecks, coupled with a\nloss-based aggregation mechanism that naturally adapts to client heterogeneity.\nExperimental evaluation on image data demonstrates that while Utilitarian\nselection achieves the highest reconstruction quality, Proportional Fairness\nmaintains competitive performance while significantly reducing participation\ninequality and improving computational efficiency. These results establish that\nfederated SC can successfully balance reconstruction quality, resource\nefficiency, and fairness in heterogeneous IoT deployments, paving the way for\nsustainable and privacy-preserving edge intelligence applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of IoT devices presents critical challenges in\nbandwidth-constrained wireless networks, particularly regarding efficient data\ntransmission and privacy preservation. This paper presents a novel federated\nsemantic communication (SC) framework that enables collaborative training of\nbandwidth-efficient models for image reconstruction across heterogeneous IoT\ndevices. By leveraging SC principles to transmit only semantic features, our\napproach dramatically reduces communication overhead while preserving\nreconstruction quality. We address the fundamental challenge of client\nselection in federated learning environments where devices exhibit significant\ndisparities in dataset sizes and data distributions. Our framework implements\nthree distinct client selection strategies that explore different trade-offs\nbetween system performance and fairness in resource allocation. The system\nemploys an end-to-end SC architecture with semantic bottlenecks, coupled with a\nloss-based aggregation mechanism that naturally adapts to client heterogeneity.\nExperimental evaluation on image data demonstrates that while Utilitarian\nselection achieves the highest reconstruction quality, Proportional Fairness\nmaintains competitive performance while significantly reducing participation\ninequality and improving computational efficiency. These results establish that\nfederated SC can successfully balance reconstruction quality, resource\nefficiency, and fairness in heterogeneous IoT deployments, paving the way for\nsustainable and privacy-preserving edge intelligence applications."
                },
                "authors": [
                    {
                        "name": "Samer Lahoud"
                    },
                    {
                        "name": "Kinda Khawam"
                    }
                ],
                "author_detail": {
                    "name": "Kinda Khawam"
                },
                "author": "Kinda Khawam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17052v1",
                "updated": "2025-06-20T15:04:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    4,
                    11,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T15:04:11Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    4,
                    11,
                    4,
                    171,
                    0
                ],
                "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Concepts to Components: Concept-Agnostic Attention Module Discovery\n  in Transformers"
                },
                "summary": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet."
                },
                "authors": [
                    {
                        "name": "Jingtong Su"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Karen Ullrich"
                    }
                ],
                "author_detail": {
                    "name": "Karen Ullrich"
                },
                "author": "Karen Ullrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06751v2",
                "updated": "2025-06-20T15:03:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    3,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-07T10:45:17Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    10,
                    45,
                    17,
                    5,
                    158,
                    0
                ],
                "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models"
                },
                "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research."
                },
                "authors": [
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Dmitrii Korzh"
                    },
                    {
                        "name": "Ivan Lazichny"
                    },
                    {
                        "name": "Elvir Karimov"
                    },
                    {
                        "name": "Artyom Iudin"
                    },
                    {
                        "name": "Ivan Oseledets"
                    },
                    {
                        "name": "Oleg Y. Rogov"
                    },
                    {
                        "name": "Natalia Loukachevitch"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00824v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00824v7",
                "updated": "2025-06-20T14:59:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    59,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-01-01T13:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Breakable Is Privacy: Probing and Resisting Model Inversion Attacks\n  in Collaborative Inference"
                },
                "summary": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative inference (CI) improves computational efficiency for edge\ndevices by transmitting intermediate features to cloud models. However, this\nprocess inevitably exposes feature representations to model inversion attacks\n(MIAs), enabling unauthorized data reconstruction. Despite extensive research,\nthere is no established criterion for assessing the difficulty of MIA\nimplementation, leaving a fundamental question unanswered: \\textit{What factors\ntruly and verifiably determine the attack's success in CI?} Moreover, existing\ndefenses lack the theoretical foundation described above, making it challenging\nto regulate feature information effectively while ensuring privacy and\nminimizing computational overhead. These shortcomings introduce three key\nchallenges: theoretical gap, methodological limitation, and practical\nconstraint.\n  To overcome these challenges, we propose the first theoretical criterion to\nassess MIA difficulty in CI, identifying mutual information, entropy, and\neffective information volume as key influencing factors. The validity of this\ncriterion is demonstrated by using the mutual information neural estimator.\nBuilding on this insight, we propose SiftFunnel, a privacy-preserving framework\nto resist MIA while maintaining usability. Specifically, we incorporate linear\nand non-linear correlation constraints alongside label smoothing to suppress\nredundant information transmission, effectively balancing privacy and\nusability. To enhance deployability, the edge model adopts a funnel-shaped\nstructure with attention mechanisms, strengthening privacy while reducing\ncomputational and storage burdens. Experiments show that, compared to\nstate-of-the-art defense, SiftFunnel increases reconstruction error by\n$\\sim$30\\%, lowers mutual and effective information metrics by $\\geq$50\\%, and\nreduces edge burdens by almost $20\\times$, while maintaining comparable\nusability."
                },
                "authors": [
                    {
                        "name": "Rongke Liu"
                    },
                    {
                        "name": "Youwen Zhu"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Gaoning Pan"
                    },
                    {
                        "name": "Xingyu He"
                    },
                    {
                        "name": "Weizhi Meng"
                    }
                ],
                "author_detail": {
                    "name": "Weizhi Meng"
                },
                "author": "Weizhi Meng",
                "arxiv_comment": "15 pages, 5 figures, 6 tables. The experimental data have been\n  corrected, and some explanations have been supplemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00824v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00824v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08316v3",
                "updated": "2025-06-20T14:52:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    52,
                    2,
                    4,
                    171,
                    0
                ],
                "published": "2024-10-10T19:06:39Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    19,
                    6,
                    39,
                    3,
                    284,
                    0
                ],
                "title": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework"
                },
                "summary": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose a Conditioned One-Shot fine-tuning framework\n(COS-DPO) that extends the Direct Preference Optimization technique, originally\ndeveloped for efficient LLM alignment with preference data, to accommodate the\nMOFT settings. By direct conditioning on the weight across auxiliary\nobjectives, our Weight-COS-DPO method enjoys an efficient one-shot training\nprocess for profiling the Pareto front and is capable of achieving\ncomprehensive trade-off solutions even in the post-training stage. Based on our\ntheoretical findings on the linear transformation properties of the loss\nfunction, we further propose the Temperature-COS-DPO method that augments the\ntemperature parameter to the model input, enhancing the flexibility of\npost-training control over the trade-offs between the main and auxiliary\nobjectives. We demonstrate the effectiveness and efficiency of the COS-DPO\nframework through its applications to various tasks, including the\nLearning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for\nlarge-scale ML deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In LLM alignment and many other ML applications, one often faces the\nMulti-Objective Fine-Tuning (MOFT) problem, i.e., fine-tuning an existing model\nwith datasets labeled w.r.t. different objectives simultaneously. To address\nthe challenge, we propose a Conditioned One-Shot fine-tuning framework\n(COS-DPO) that extends the Direct Preference Optimization technique, originally\ndeveloped for efficient LLM alignment with preference data, to accommodate the\nMOFT settings. By direct conditioning on the weight across auxiliary\nobjectives, our Weight-COS-DPO method enjoys an efficient one-shot training\nprocess for profiling the Pareto front and is capable of achieving\ncomprehensive trade-off solutions even in the post-training stage. Based on our\ntheoretical findings on the linear transformation properties of the loss\nfunction, we further propose the Temperature-COS-DPO method that augments the\ntemperature parameter to the model input, enhancing the flexibility of\npost-training control over the trade-offs between the main and auxiliary\nobjectives. We demonstrate the effectiveness and efficiency of the COS-DPO\nframework through its applications to various tasks, including the\nLearning-to-Rank (LTR) and LLM alignment tasks, highlighting its viability for\nlarge-scale ML deployments."
                },
                "authors": [
                    {
                        "name": "Yinuo Ren"
                    },
                    {
                        "name": "Tesi Xiao"
                    },
                    {
                        "name": "Michael Shavlovsky"
                    },
                    {
                        "name": "Lexing Ying"
                    },
                    {
                        "name": "Holakou Rahmanian"
                    }
                ],
                "author_detail": {
                    "name": "Holakou Rahmanian"
                },
                "author": "Holakou Rahmanian",
                "arxiv_comment": "Published at UAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16813v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16813v3",
                "updated": "2025-06-20T14:35:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    35,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-25T15:28:11Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    15,
                    28,
                    11,
                    0,
                    330,
                    0
                ],
                "title": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political\n  Argumentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political\n  Argumentation"
                },
                "summary": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The incivility prevalent on platforms like Twitter (now X) and Reddit poses a\nchallenge for developing AI systems that can support productive and\nrhetorically sound political argumentation. In this study, we report\nexperiments with GPT-3.5 Turbo, fine-tuned on two contrasting datasets of\npolitical discussions: high-variance, high-incivility Twitter replies to U.S.\nCongress, and low-variance, low-incivility posts from Reddit's r/ChangeMyView.\nWe systematically evaluate how these data sources and prompting strategies\nshape the rhetorical framing and deliberative quality of model-generated\narguments. Our results show that Reddit-finetuned models produce safer but\nrhetorically rigid arguments, while cross-platform fine-tuning amplifies\ntoxicity. Prompting reduces specific toxic behaviors, such as personal attacks,\nbut fails to fully mitigate the influence of high-incivility training data. We\nintroduce and validate a rhetorical evaluation rubric and provide practical\nguidelines for deploying LLMs in content authoring, moderation, and\ndeliberation support."
                },
                "authors": [
                    {
                        "name": "Svetlana Churina"
                    },
                    {
                        "name": "Kokil Jaidka"
                    }
                ],
                "author_detail": {
                    "name": "Kokil Jaidka"
                },
                "author": "Kokil Jaidka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16813v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16813v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02819v3",
                "updated": "2025-06-20T14:27:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    27,
                    20,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-05T17:47:42Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    47,
                    42,
                    0,
                    125,
                    0
                ],
                "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer\n  Block Linearization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReplaceMe: Network Simplification via Depth Pruning and Transformer\n  Block Linearization"
                },
                "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation, which approximates the pruned blocks.\nThe estimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at https://github.com/mts-ai/ReplaceMe."
                },
                "authors": [
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Sergey Zagoruyko"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Zagoruyko"
                },
                "author": "Sergey Zagoruyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17029v1",
                "updated": "2025-06-20T14:25:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    25,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T14:25:23Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    25,
                    23,
                    4,
                    171,
                    0
                ],
                "title": "Scalable and Reliable Multi-agent Reinforcement Learning for Traffic\n  Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Reliable Multi-agent Reinforcement Learning for Traffic\n  Assignment"
                },
                "summary": "The evolution of metropolitan cities and the increase in travel demands\nimpose stringent requirements on traffic assignment methods. Multi-agent\nreinforcement learning (MARL) approaches outperform traditional methods in\nmodeling adaptive routing behavior without requiring explicit system dynamics,\nwhich is beneficial for real-world deployment. However, MARL frameworks face\nchallenges in scalability and reliability when managing extensive networks with\nsubstantial travel demand, which limiting their practical applicability in\nsolving large-scale traffic assignment problems. To address these challenges,\nthis study introduces MARL-OD-DA, a new MARL framework for the traffic\nassignment problem, which redefines agents as origin-destination (OD) pair\nrouters rather than individual travelers, significantly enhancing scalability.\nAdditionally, a Dirichlet-based action space with action pruning and a reward\nfunction based on the local relative gap are designed to enhance solution\nreliability and improve convergence efficiency. Experiments demonstrate that\nthe proposed MARL framework effectively handles medium-sized networks with\nextensive and varied city-level OD demand, surpassing existing MARL methods.\nWhen implemented in the SiouxFalls network, MARL-OD-DA achieves better\nassignment solutions in 10 steps, with a relative gap that is 94.99% lower than\nthat of conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of metropolitan cities and the increase in travel demands\nimpose stringent requirements on traffic assignment methods. Multi-agent\nreinforcement learning (MARL) approaches outperform traditional methods in\nmodeling adaptive routing behavior without requiring explicit system dynamics,\nwhich is beneficial for real-world deployment. However, MARL frameworks face\nchallenges in scalability and reliability when managing extensive networks with\nsubstantial travel demand, which limiting their practical applicability in\nsolving large-scale traffic assignment problems. To address these challenges,\nthis study introduces MARL-OD-DA, a new MARL framework for the traffic\nassignment problem, which redefines agents as origin-destination (OD) pair\nrouters rather than individual travelers, significantly enhancing scalability.\nAdditionally, a Dirichlet-based action space with action pruning and a reward\nfunction based on the local relative gap are designed to enhance solution\nreliability and improve convergence efficiency. Experiments demonstrate that\nthe proposed MARL framework effectively handles medium-sized networks with\nextensive and varied city-level OD demand, surpassing existing MARL methods.\nWhen implemented in the SiouxFalls network, MARL-OD-DA achieves better\nassignment solutions in 10 steps, with a relative gap that is 94.99% lower than\nthat of conventional methods."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Cheng Lyu"
                    },
                    {
                        "name": "Zewen Wang"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00128v3",
                "updated": "2025-06-20T14:13:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    14,
                    13,
                    17,
                    4,
                    171,
                    0
                ],
                "published": "2024-08-29T05:18:50Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    5,
                    18,
                    50,
                    3,
                    242,
                    0
                ],
                "title": "Can Large Language Models Replace Human Subjects? A Large-Scale\n  Replication of Scenario-Based Experiments in Psychology and Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Replace Human Subjects? A Large-Scale\n  Replication of Scenario-Based Experiments in Psychology and Management"
                },
                "summary": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is increasingly being integrated into scientific\nresearch, particularly in the social sciences, where understanding human\nbehavior is critical. Large Language Models (LLMs) have shown promise in\nreplicating human-like responses in various psychological experiments. We\nconducted a large-scale study replicating 156 psychological experiments from\ntop social science journals using three state-of-the-art LLMs (GPT-4, Claude\n3.5 Sonnet, and DeepSeek v3). Our results reveal that while LLMs demonstrate\nhigh replication rates for main effects (73-81%) and moderate to strong success\nwith interaction effects (46-63%), They consistently produce larger effect\nsizes than human studies, with Fisher Z values approximately 2-3 times higher\nthan human studies. Notably, LLMs show significantly lower replication rates\nfor studies involving socially sensitive topics such as race, gender and\nethics. When original studies reported null findings, LLMs produced significant\nresults at remarkably high rates (68-83%) - while this could reflect cleaner\ndata with less noise, as evidenced by narrower confidence intervals, it also\nsuggests potential risks of effect size overestimation. Our results demonstrate\nboth the promise and challenges of LLMs in psychological research, offering\nefficient tools for pilot testing and rapid hypothesis validation while\nenriching rather than replacing traditional human subject studies, yet\nrequiring more nuanced interpretation and human validation for complex social\nphenomena and culturally sensitive research questions."
                },
                "authors": [
                    {
                        "name": "Ziyan Cui"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Huaikang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huaikang Zhou"
                },
                "author": "Huaikang Zhou",
                "arxiv_comment": "5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17006v1",
                "updated": "2025-06-20T13:59:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    59,
                    14,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    59,
                    14,
                    4,
                    171,
                    0
                ],
                "title": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It"
                },
                "summary": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility."
                },
                "authors": [
                    {
                        "name": "Danielle R. Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Shivang Gupta"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_comment": "Full research paper accepted at EC-TEL '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17001v1",
                "updated": "2025-06-20T13:52:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:52:15Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    52,
                    15,
                    4,
                    171,
                    0
                ],
                "title": "PersonalAI: Towards digital twins in the graph form",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonalAI: Towards digital twins in the graph form"
                },
                "summary": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies."
                },
                "authors": [
                    {
                        "name": "Mikhail Menschikov"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Ruslan Kostoev"
                    },
                    {
                        "name": "Ilya Perepechkin"
                    },
                    {
                        "name": "Ilnaz Salimov"
                    },
                    {
                        "name": "Victoria Dochkina"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "name": "Nikita Semenov"
                    }
                ],
                "author_detail": {
                    "name": "Nikita Semenov"
                },
                "author": "Nikita Semenov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14860v2",
                "updated": "2025-06-20T13:47:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    47,
                    25,
                    4,
                    171,
                    0
                ],
                "published": "2024-12-19T13:55:48Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    55,
                    48,
                    3,
                    354,
                    0
                ],
                "title": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think&Cite: Improving Attributed Text Generation with Self-Guided Tree\n  Search and Progress Reward Modeling"
                },
                "summary": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their outstanding capabilities, large language models (LLMs) are\nprone to hallucination and producing factually incorrect information. This\nchallenge has spurred efforts in attributed text generation, which prompts LLMs\nto generate content with supporting evidence. In this paper, we propose a novel\nframework, called Think&Cite, and formulate attributed text generation as a\nmulti-step reasoning problem integrated with search. Specifically, we propose\nSelf-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the\nself-reflection capability of LLMs to reason about the intermediate states of\nMCTS for guiding the tree expansion process. To provide reliable and\ncomprehensive feedback, we introduce Progress Reward Modeling to measure the\nprogress of tree search from the root to the current state from two aspects,\ni.e., generation and attribution progress. We conduct extensive experiments on\nthree datasets and the results show that our approach significantly outperforms\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Hwee Tou Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hwee Tou Ng"
                },
                "author": "Hwee Tou Ng",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16990v1",
                "updated": "2025-06-20T13:39:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    16,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:39:16Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    39,
                    16,
                    4,
                    171,
                    0
                ],
                "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by\n  LLMs"
                },
                "summary": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert."
                },
                "authors": [
                    {
                        "name": "Sahil Kale"
                    },
                    {
                        "name": "Vijaykant Nadadur"
                    }
                ],
                "author_detail": {
                    "name": "Vijaykant Nadadur"
                },
                "author": "Vijaykant Nadadur",
                "arxiv_comment": "Accepted to the SDProc Workshop @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v3",
                "updated": "2025-06-20T13:34:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    34,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12911v2",
                "updated": "2025-06-20T13:24:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    24,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-18T14:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    53,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL\n  Generation"
                },
                "summary": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating SQLs from user queries is a long-standing challenge, where the\naccuracy of initial schema linking significantly impacts subsequent SQL\ngeneration performance. However, current schema linking models still struggle\nwith missing relevant schema elements or an excess of redundant ones. A crucial\nreason for this is that commonly used metrics, recall and precision, fail to\ncapture relevant element missing and thus cannot reflect actual schema linking\nperformance. Motivated by this, we propose enhanced schema linking metrics by\nintroducing a restricted missing indicator. Accordingly, we introduce Knapsack\noptimization-based Schema Linking Approach (KaSLA), a plug-in schema linking\nmethod designed to prevent the missing of relevant schema elements while\nminimizing the inclusion of redundant ones. KaSLA employs a hierarchical\nlinking strategy that first identifies the optimal table linking and\nsubsequently links columns within the selected table to reduce linking\ncandidate space. In each linking process, it utilizes a knapsack optimization\napproach to link potentially relevant elements while accounting for a limited\ntolerance of potentially redundant ones. With this optimization, KaSLA-1.6B\nachieves superior schema linking results compared to large-scale LLMs,\nincluding deepseek-v3 with the state-of-the-art (SOTA) schema linking method.\nExtensive experiments on Spider and BIRD benchmarks verify that KaSLA can\nsignificantly improve the SQL generation performance of SOTA Text2SQL models by\nsubstituting their schema linking processes."
                },
                "authors": [
                    {
                        "name": "Zheng Yuan"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16982v1",
                "updated": "2025-06-20T13:21:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    21,
                    14,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:21:14Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    21,
                    14,
                    4,
                    171,
                    0
                ],
                "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge\n  Tracing and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Bottleneck Models: A Framework for Interpretable Knowledge\n  Tracing and Beyond"
                },
                "summary": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality."
                },
                "authors": [
                    {
                        "name": "Antonin Berthon"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16975v1",
                "updated": "2025-06-20T13:08:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    8,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:08:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    8,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Latent Concept Disentanglement in Transformer-based Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Concept Disentanglement in Transformer-based Language Models"
                },
                "summary": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Bhavya Vasudeva"
                    },
                    {
                        "name": "Vatsal Sharan"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14352v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14352v3",
                "updated": "2025-06-20T12:52:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    52,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2024-08-26T15:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    15,
                    29,
                    34,
                    0,
                    239,
                    0
                ],
                "title": "LogProber: Disentangling confidence from contamination in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogProber: Disentangling confidence from contamination in LLM responses"
                },
                "summary": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical. In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning, contamination refers to situations where testing data\nleak into the training set. The issue is particularly relevant for the\nevaluation of the performance of Large Language Models (LLMs), which are\ngenerally trained on gargantuan, and generally opaque, corpora of text scraped\nfrom the world wide web. Developing tools to detect contamination is therefore\ncrucial to be able to fairly and properly track the evolution of the\nperformance of LLMs. To date, only a few recent studies have attempted to\naddress the issue of quantifying and detecting contamination in short text\nsequences, such as those commonly found in benchmarks. However, these methods\nhave limitations that can sometimes render them impractical. In the present\npaper, we introduce LogProber, a novel, efficient algorithm that we show to be\nable to detect contamination in a black box setting that tries to tackle some\nof these drawbacks by focusing on the familiarity with the question rather than\nthe answer. Here, we explore the properties of the proposed method in\ncomparison with concurrent approaches, identify its advantages and limitations,\nand illustrate how different forms of contamination can go undetected depending\non the design of the detection algorithm."
                },
                "authors": [
                    {
                        "name": "Nicolas Yax"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Stefano Palminteri"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Palminteri"
                },
                "author": "Stefano Palminteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14352v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14352v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05692v3",
                "updated": "2025-06-20T12:42:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    42,
                    57,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-06T02:48:02Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    48,
                    2,
                    4,
                    157,
                    0
                ],
                "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection\n  in LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection\n  in LLM-Generated Code"
                },
                "summary": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce SafeGenBench, a benchmark specifically\ndesigned to assess the security of LLM-generated code. The dataset encompasses\na wide range of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce SafeGenBench, a benchmark specifically\ndesigned to assess the security of LLM-generated code. The dataset encompasses\na wide range of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on SafeGenBench, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon."
                },
                "authors": [
                    {
                        "name": "Xinghang Li"
                    },
                    {
                        "name": "Jingzhe Ding"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Bing Zhao"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Hongwan Gao"
                    },
                    {
                        "name": "Xinchen Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xinchen Gu"
                },
                "author": "Xinchen Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13008v2",
                "updated": "2025-06-20T12:30:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    30,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-19T11:51:56Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    51,
                    56,
                    0,
                    139,
                    0
                ],
                "title": "Adversarial Reasoning for Repair Based on Inferred Program Intent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Reasoning for Repair Based on Inferred Program Intent"
                },
                "summary": "Automated program repair (APR) has shown promising results, particularly with\nthe use of neural networks. Currently, most APR tools focus on code\ntransformations specified by test suites, rather than reasoning about the\nprogram intent and the high-level bug specification. Without a proper\nunderstanding of program intent, these tools tend to generate patches that\noverfit incomplete test suites and fail to reflect the developers intentions.\nHowever, reasoning about program intent is challenging. In our work, we propose\nan approach called AdverIntent-Agent, based on critique and adversarial\nreasoning. Our approach is novel to shift the focus from generating multiple\nAPR patches to inferring multiple potential program intents. Ideally, we aim to\ninfer intents that are, to some extent, adversarial to each other, maximizing\nthe probability that at least one aligns closely with the developers original\nintent. AdverIntent-Agent is a multi-agent approach consisting of three agents:\na reasoning agent, a test agent, and a repair agent. First, the reasoning agent\ngenerates adversarial program intents along with the corresponding faulty\nstatements. Next, the test agent produces adversarial test cases that align\nwith each inferred intent, constructing oracles that use the same inputs but\nhave different expected outputs. Finally, the repair agent uses dynamic and\nprecise LLM prompts to generate patches that satisfy both the inferred program\nintent and the generated tests. AdverIntent-Agent was evaluated on two\nbenchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly\nrepaired 77 and 105 bugs in both benchmarks, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) has shown promising results, particularly with\nthe use of neural networks. Currently, most APR tools focus on code\ntransformations specified by test suites, rather than reasoning about the\nprogram intent and the high-level bug specification. Without a proper\nunderstanding of program intent, these tools tend to generate patches that\noverfit incomplete test suites and fail to reflect the developers intentions.\nHowever, reasoning about program intent is challenging. In our work, we propose\nan approach called AdverIntent-Agent, based on critique and adversarial\nreasoning. Our approach is novel to shift the focus from generating multiple\nAPR patches to inferring multiple potential program intents. Ideally, we aim to\ninfer intents that are, to some extent, adversarial to each other, maximizing\nthe probability that at least one aligns closely with the developers original\nintent. AdverIntent-Agent is a multi-agent approach consisting of three agents:\na reasoning agent, a test agent, and a repair agent. First, the reasoning agent\ngenerates adversarial program intents along with the corresponding faulty\nstatements. Next, the test agent produces adversarial test cases that align\nwith each inferred intent, constructing oracles that use the same inputs but\nhave different expected outputs. Finally, the repair agent uses dynamic and\nprecise LLM prompts to generate patches that satisfy both the inferred program\nintent and the generated tests. AdverIntent-Agent was evaluated on two\nbenchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly\nrepaired 77 and 105 bugs in both benchmarks, respectively."
                },
                "authors": [
                    {
                        "name": "He Ye"
                    },
                    {
                        "name": "Aidan Z. H. Yang"
                    },
                    {
                        "name": "Chang Hu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02161v2",
                "updated": "2025-06-20T12:19:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    19,
                    48,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-04T09:37:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    37,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space"
                },
                "summary": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space."
                },
                "authors": [
                    {
                        "name": "Jingxian Ji"
                    },
                    {
                        "name": "Shambo Mukherjee"
                    },
                    {
                        "name": "Alexander Kuhl"
                    },
                    {
                        "name": "Sebastian Koke"
                    },
                    {
                        "name": "Markus Leipe"
                    },
                    {
                        "name": "Markus Rothe"
                    },
                    {
                        "name": "Fabian Steinlechner"
                    },
                    {
                        "name": "Jochen Kronjäger"
                    }
                ],
                "author_detail": {
                    "name": "Jochen Kronjäger"
                },
                "author": "Jochen Kronjäger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13593v2",
                "updated": "2025-06-20T12:12:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    12,
                    17,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-16T15:21:25Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    15,
                    21,
                    25,
                    0,
                    167,
                    0
                ],
                "title": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs"
                },
                "summary": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework to quantify the time-to-unsafe-sampling - the number\nof large language model (LLM) generations required to trigger an unsafe (e.g.,\ntoxic) response. Estimating this quantity is challenging, since unsafe\nresponses are exceedingly rare in well-aligned LLMs, potentially occurring only\nonce in thousands of generations. As a result, directly estimating\ntime-to-unsafe-sampling would require collecting training data with a\nprohibitively large number of generations per prompt. However, with realistic\nsampling budgets, we often cannot generate enough responses to observe an\nunsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved\nin many cases, making the estimation and evaluation tasks particularly\nchallenging. To address this, we frame this estimation problem as one of\nsurvival analysis and develop a provably calibrated lower predictive bound\n(LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent\nadvances in conformal prediction. Our key innovation is designing an adaptive,\nper-prompt sampling strategy, formulated as a convex optimization problem. The\nobjective function guiding this optimized sampling allocation is designed to\nreduce the variance of the estimators used to construct the LPB, leading to\nimproved statistical efficiency over naive methods that use a fixed sampling\nbudget per prompt. Experiments on both synthetic and real data support our\ntheoretical results and demonstrate the practical utility of our method for\nsafety risk assessment in generative AI models."
                },
                "authors": [
                    {
                        "name": "Hen Davidov"
                    },
                    {
                        "name": "Gilad Freidkin"
                    },
                    {
                        "name": "Shai Feldman"
                    },
                    {
                        "name": "Yaniv Romano"
                    }
                ],
                "author_detail": {
                    "name": "Yaniv Romano"
                },
                "author": "Yaniv Romano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16940v1",
                "updated": "2025-06-20T12:06:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    6,
                    47,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T12:06:47Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    6,
                    47,
                    4,
                    171,
                    0
                ],
                "title": "LunarLoc: Segment-Based Global Localization on the Moon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LunarLoc: Segment-Based Global Localization on the Moon"
                },
                "summary": "Global localization is necessary for autonomous operations on the lunar\nsurface where traditional Earth-based navigation infrastructure, such as GPS,\nis unavailable. As NASA advances toward sustained lunar presence under the\nArtemis program, autonomous operations will be an essential component of tasks\nsuch as robotic exploration and infrastructure deployment. Tasks such as\nexcavation and transport of regolith require precise pose estimation, but\nproposed approaches such as visual-inertial odometry (VIO) accumulate odometry\ndrift over long traverses. Precise pose estimation is particularly important\nfor upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on\nautonomous agents to operate over extended timescales and varied terrain. To\nhelp overcome odometry drift over long traverses, we propose LunarLoc, an\napproach to global localization that leverages instance segmentation for\nzero-shot extraction of boulder landmarks from onboard stereo imagery. Segment\ndetections are used to construct a graph-based representation of the terrain,\nwhich is then aligned with a reference map of the environment captured during a\nprevious session using graph-theoretic data association. This method enables\naccurate and drift-free global localization in visually ambiguous settings.\nLunarLoc achieves sub-cm level accuracy in multi-session global localization\nexperiments, significantly outperforming the state of the art in lunar global\nlocalization. To encourage the development of further methods for global\nlocalization on the Moon, we release our datasets publicly with a playback\nmodule: https://github.com/mit-acl/lunarloc-data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global localization is necessary for autonomous operations on the lunar\nsurface where traditional Earth-based navigation infrastructure, such as GPS,\nis unavailable. As NASA advances toward sustained lunar presence under the\nArtemis program, autonomous operations will be an essential component of tasks\nsuch as robotic exploration and infrastructure deployment. Tasks such as\nexcavation and transport of regolith require precise pose estimation, but\nproposed approaches such as visual-inertial odometry (VIO) accumulate odometry\ndrift over long traverses. Precise pose estimation is particularly important\nfor upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on\nautonomous agents to operate over extended timescales and varied terrain. To\nhelp overcome odometry drift over long traverses, we propose LunarLoc, an\napproach to global localization that leverages instance segmentation for\nzero-shot extraction of boulder landmarks from onboard stereo imagery. Segment\ndetections are used to construct a graph-based representation of the terrain,\nwhich is then aligned with a reference map of the environment captured during a\nprevious session using graph-theoretic data association. This method enables\naccurate and drift-free global localization in visually ambiguous settings.\nLunarLoc achieves sub-cm level accuracy in multi-session global localization\nexperiments, significantly outperforming the state of the art in lunar global\nlocalization. To encourage the development of further methods for global\nlocalization on the Moon, we release our datasets publicly with a playback\nmodule: https://github.com/mit-acl/lunarloc-data."
                },
                "authors": [
                    {
                        "name": "Annika Thomas"
                    },
                    {
                        "name": "Robaire Galliath"
                    },
                    {
                        "name": "Aleksander Garbuz"
                    },
                    {
                        "name": "Luke Anger"
                    },
                    {
                        "name": "Cormac O'Neill"
                    },
                    {
                        "name": "Trevor Johst"
                    },
                    {
                        "name": "Dami Thomas"
                    },
                    {
                        "name": "George Lordos"
                    },
                    {
                        "name": "Jonathan P. How"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. How"
                },
                "author": "Jonathan P. How",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04227v2",
                "updated": "2025-06-20T12:02:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    2,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-06T17:12:43Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    17,
                    12,
                    43,
                    3,
                    37,
                    0
                ],
                "title": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Hack Enterprise Networks? Autonomous Assumed Breach\n  Penetration-Testing Active Directory Networks"
                },
                "summary": "Penetration-testing, while critical for validating defenses and uncovering\nvulnerabilities, is often limited by high operational costs and the scarcity of\nhuman expertise. This paper investigates the feasibility and effectiveness of\nusing Large Language Model (LLM)-driven autonomous systems to address these\nchallenges in real-world Microsoft Active Directory (AD) enterprise networks.\nOur novel prototype, cochise, represents the first demonstration of a fully\nautonomous, LLM-driven framework capable of compromising accounts within a\nreal-life Microsoft AD testbed (GOAD). The evaluation deliberately utilizes\nGOAD to capture the intricate interactions and sometimes nondeterministic\noutcomes of live network pen-testing, moving beyond the limitations of\nsynthetic benchmarks. We perform our empirical evaluation using five LLMs,\ncomparing reasoning to non-reasoning models as well as including open-weight\nmodels. Through comprehensive quantitative and qualitative analysis,\nincorporating insights from cybersecurity experts, we demonstrate that\nautonomous LLMs can effectively conduct Assumed Breach simulations. Key\nfindings highlight their ability to dynamically adapt attack strategies,\nperform inter-context attacks, and generate scenario-specific attack\nparameters. Cochise also exhibits robust self-correction mechanisms,\nautomatically installing missing tools and rectifying invalid command\ngenerations. Critically, we find that the associated costs are competitive with\nthose incurred by professional pen-testers, suggesting a path toward\ndemocratizing access to essential security testing for organizations with\nbudgetary constraints. However, our research also illuminates existing\nlimitations, including instances of LLM ``going down rabbit holes'', challenges\nin comprehensive information transfer between planning and execution modules,\nand critical safety concerns that necessitate human oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration-testing, while critical for validating defenses and uncovering\nvulnerabilities, is often limited by high operational costs and the scarcity of\nhuman expertise. This paper investigates the feasibility and effectiveness of\nusing Large Language Model (LLM)-driven autonomous systems to address these\nchallenges in real-world Microsoft Active Directory (AD) enterprise networks.\nOur novel prototype, cochise, represents the first demonstration of a fully\nautonomous, LLM-driven framework capable of compromising accounts within a\nreal-life Microsoft AD testbed (GOAD). The evaluation deliberately utilizes\nGOAD to capture the intricate interactions and sometimes nondeterministic\noutcomes of live network pen-testing, moving beyond the limitations of\nsynthetic benchmarks. We perform our empirical evaluation using five LLMs,\ncomparing reasoning to non-reasoning models as well as including open-weight\nmodels. Through comprehensive quantitative and qualitative analysis,\nincorporating insights from cybersecurity experts, we demonstrate that\nautonomous LLMs can effectively conduct Assumed Breach simulations. Key\nfindings highlight their ability to dynamically adapt attack strategies,\nperform inter-context attacks, and generate scenario-specific attack\nparameters. Cochise also exhibits robust self-correction mechanisms,\nautomatically installing missing tools and rectifying invalid command\ngenerations. Critically, we find that the associated costs are competitive with\nthose incurred by professional pen-testers, suggesting a path toward\ndemocratizing access to essential security testing for organizations with\nbudgetary constraints. However, our research also illuminates existing\nlimitations, including instances of LLM ``going down rabbit holes'', challenges\nin comprehensive information transfer between planning and execution modules,\nand critical safety concerns that necessitate human oversight."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01208v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01208v3",
                "updated": "2025-06-20T10:54:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    54,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-03T09:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "On Almost Surely Safe Alignment of Large Language Models at\n  Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Almost Surely Safe Alignment of Large Language Models at\n  Inference-Time"
                },
                "summary": "We introduce a novel inference-time alignment approach for LLMs that aims to\ngenerate safe responses almost surely, i.e., with probability approaching one.\nOur approach models the generation of safe responses as a constrained Markov\nDecision Process (MDP) within the LLM's latent space. We augment a safety state\nthat tracks the evolution of safety constraints and dynamically penalize unsafe\ngenerations to ensure the generation of safe responses. Consequently, we\ndemonstrate formal safety guarantees w.r.t. the given cost model upon solving\nthe MDP in the latent space with sufficiently large penalties. Building on this\nfoundation, we propose InferenceGuard, a practical implementation that safely\naligns LLMs without modifying the model weights. Empirically, we demonstrate\nthat InferenceGuard effectively balances safety and task performance,\noutperforming existing inference-time alignment methods in generating safe and\naligned responses. Our findings contribute to the advancement of safer LLM\ndeployment through alignment at inference-time, thus presenting a promising\nalternative to resource-intensive, overfitting-prone alignment techniques like\nRLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel inference-time alignment approach for LLMs that aims to\ngenerate safe responses almost surely, i.e., with probability approaching one.\nOur approach models the generation of safe responses as a constrained Markov\nDecision Process (MDP) within the LLM's latent space. We augment a safety state\nthat tracks the evolution of safety constraints and dynamically penalize unsafe\ngenerations to ensure the generation of safe responses. Consequently, we\ndemonstrate formal safety guarantees w.r.t. the given cost model upon solving\nthe MDP in the latent space with sufficiently large penalties. Building on this\nfoundation, we propose InferenceGuard, a practical implementation that safely\naligns LLMs without modifying the model weights. Empirically, we demonstrate\nthat InferenceGuard effectively balances safety and task performance,\noutperforming existing inference-time alignment methods in generating safe and\naligned responses. Our findings contribute to the advancement of safer LLM\ndeployment through alignment at inference-time, thus presenting a promising\nalternative to resource-intensive, overfitting-prone alignment techniques like\nRLHF."
                },
                "authors": [
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Shyam Sundhar Ramesh"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01208v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01208v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16899v1",
                "updated": "2025-06-20T10:46:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    46,
                    35,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T10:46:35Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    46,
                    35,
                    4,
                    171,
                    0
                ],
                "title": "Towards Effective Complementary Security Analysis using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective Complementary Security Analysis using Large Language\n  Models"
                },
                "summary": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in security analysis is the manual evaluation of potential\nsecurity weaknesses generated by static application security testing (SAST)\ntools. Numerous false positives (FPs) in these reports reduce the effectiveness\nof security analysis. We propose using Large Language Models (LLMs) to improve\nthe assessment of SAST findings. We investigate the ability of LLMs to reduce\nFPs while trying to maintain a perfect true positive rate, using datasets\nextracted from the OWASP Benchmark (v1.2) and a real-world software project.\nOur results indicate that advanced prompting techniques, such as\nChain-of-Thought and Self-Consistency, substantially improve FP detection.\nNotably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark\ndataset without missing genuine weaknesses. Combining detections from different\nLLMs would increase this FP detection to approximately 78.9%. Additionally, we\ndemonstrate our approach's generalizability using a real-world dataset covering\nfive SAST tools, three programming languages, and infrastructure files. The\nbest LLM detected 33.85% of all FPs without missing genuine weaknesses, while\ncombining detections from different LLMs would increase this detection to\n38.46%. Our findings highlight the potential of LLMs to complement traditional\nSAST tools, enhancing automation and reducing resources spent addressing false\nalarms."
                },
                "authors": [
                    {
                        "name": "Jonas Wagner"
                    },
                    {
                        "name": "Simon Müller"
                    },
                    {
                        "name": "Christian Näther"
                    },
                    {
                        "name": "Jan-Philipp Steghöfer"
                    },
                    {
                        "name": "Andreas Both"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Both"
                },
                "author": "Andreas Both",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11480v2",
                "updated": "2025-06-20T10:31:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    31,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-13T06:05:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    5,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large\n  Language Models Based on Improved Gradient Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large\n  Language Models Based on Improved Gradient Alignment"
                },
                "summary": "Reinforcement learning (RL) has become a key technique for enhancing LLMs'\nreasoning abilities, yet its data inefficiency remains a major bottleneck. To\naddress this critical yet challenging issue, we present a novel\ngradient-alignment-based method, named LearnAlign, which intelligently selects\nthe learnable and representative training reasoning data for RL post-training.\nTo overcome the issue of response-length bias in gradient norms, we introduce\nthe data learnability based on the success rate, which can indicate the\nlearning potential of each data point. Experiments across three mathematical\nreasoning benchmarks demonstrate that our method significantly reduces training\ndata requirements while achieving minor performance degradation or even\nimproving performance compared to full-data training. For example, it reduces\ndata requirements by up to 1,000 data points with better performance (77.53%)\nthan that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show\nits effectiveness in the staged RL setting. This work provides valuable\ninsights into data-efficient RL post-training and establishes a foundation for\nfuture research in optimizing reasoning data selection. To facilitate future\nwork, we will release code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a key technique for enhancing LLMs'\nreasoning abilities, yet its data inefficiency remains a major bottleneck. To\naddress this critical yet challenging issue, we present a novel\ngradient-alignment-based method, named LearnAlign, which intelligently selects\nthe learnable and representative training reasoning data for RL post-training.\nTo overcome the issue of response-length bias in gradient norms, we introduce\nthe data learnability based on the success rate, which can indicate the\nlearning potential of each data point. Experiments across three mathematical\nreasoning benchmarks demonstrate that our method significantly reduces training\ndata requirements while achieving minor performance degradation or even\nimproving performance compared to full-data training. For example, it reduces\ndata requirements by up to 1,000 data points with better performance (77.53%)\nthan that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show\nits effectiveness in the staged RL setting. This work provides valuable\ninsights into data-efficient RL post-training and establishes a foundation for\nfuture research in optimizing reasoning data selection. To facilitate future\nwork, we will release code."
                },
                "authors": [
                    {
                        "name": "Shikun Li"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Zhiqin Yang"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Gaode Chen"
                    },
                    {
                        "name": "Xiaobo Xia"
                    },
                    {
                        "name": "Hengyu Liu"
                    },
                    {
                        "name": "Zhe Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Peng"
                },
                "author": "Zhe Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05328v2",
                "updated": "2025-06-20T10:27:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    27,
                    28,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-07T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    13,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument\n  Generation with Large Language Models"
                },
                "summary": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems."
                },
                "authors": [
                    {
                        "name": "Anar Yeginbergen"
                    },
                    {
                        "name": "Maite Oronoz"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07717v3",
                "updated": "2025-06-20T10:23:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    23,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-10T13:09:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    13,
                    9,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented\n  Generation in Large Language Models via Bilevel Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of applications, e.g., medical question-answering, mathematical\nsciences, and code generation. However, they also exhibit inherent limitations,\nsuch as outdated knowledge and susceptibility to hallucinations.\nRetrieval-Augmented Generation (RAG) has emerged as a promising paradigm to\naddress these issues, but it also introduces new vulnerabilities. Recent\nefforts have focused on the security of RAG-based LLMs, yet existing attack\nmethods face three critical challenges: (1) their effectiveness declines\nsharply when only a limited number of poisoned texts can be injected into the\nknowledge database, (2) they lack sufficient stealth, as the attacks are often\ndetectable by anomaly detection systems, which compromises their effectiveness,\nand (3) they rely on heuristic approaches to generate poisoned texts, lacking\nformal optimization frameworks and theoretic guarantees, which limits their\neffectiveness and applicability. To address these issues, we propose\ncoordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack\nthat introduces a small number of poisoned texts into the knowledge database\nwhile embedding a backdoor trigger within the prompt. When activated, the\ntrigger causes the LLM to generate pre-designed responses to targeted queries,\nwhile maintaining normal behavior in other contexts. This ensures both high\neffectiveness and stealth. We formulate the attack generation process as a\nbilevel optimization problem leveraging a principled optimization framework to\ndevelop optimal poisoned texts and triggers. Extensive experiments across\ndiverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving\na high attack success rate even with a limited number of poisoned texts and\nsignificantly improved stealth compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Xiaodong Wang"
                    },
                    {
                        "name": "Kai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yang"
                },
                "author": "Kai Yang",
                "arxiv_comment": "Accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01287v2",
                "updated": "2025-06-20T10:23:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    23,
                    37,
                    4,
                    171,
                    0
                ],
                "published": "2024-08-02T14:19:34Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    14,
                    19,
                    34,
                    4,
                    215,
                    0
                ],
                "title": "Deep Learning based Visually Rich Document Content Understanding: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning based Visually Rich Document Content Understanding: A\n  Survey"
                },
                "summary": "Visually Rich Documents (VRDs) play a vital role in domains such as academia,\nfinance, healthcare, and marketing, as they convey information through a\ncombination of text, layout, and visual elements. Traditional approaches to\nextracting information from VRDs rely heavily on expert knowledge and manual\nannotation, making them labor-intensive and inefficient. Recent advances in\ndeep learning have transformed this landscape by enabling multimodal models\nthat integrate vision, language, and layout features through pretraining,\nsignificantly improving information extraction performance. This survey\npresents a comprehensive overview of deep learning-based frameworks for VRD\nContent Understanding (VRD-CU). We categorize existing methods based on their\nmodeling strategies and downstream tasks, and provide a comparative analysis of\nkey components, including feature representation, fusion techniques, model\narchitectures, and pretraining objectives. Additionally, we highlight the\nstrengths and limitations of each approach and discuss their suitability for\ndifferent applications. The paper concludes with a discussion of current\nchallenges and emerging trends, offering guidance for future research and\npractical deployment in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visually Rich Documents (VRDs) play a vital role in domains such as academia,\nfinance, healthcare, and marketing, as they convey information through a\ncombination of text, layout, and visual elements. Traditional approaches to\nextracting information from VRDs rely heavily on expert knowledge and manual\nannotation, making them labor-intensive and inefficient. Recent advances in\ndeep learning have transformed this landscape by enabling multimodal models\nthat integrate vision, language, and layout features through pretraining,\nsignificantly improving information extraction performance. This survey\npresents a comprehensive overview of deep learning-based frameworks for VRD\nContent Understanding (VRD-CU). We categorize existing methods based on their\nmodeling strategies and downstream tasks, and provide a comparative analysis of\nkey components, including feature representation, fusion techniques, model\narchitectures, and pretraining objectives. Additionally, we highlight the\nstrengths and limitations of each approach and discuss their suitability for\ndifferent applications. The paper concludes with a discussion of current\nchallenges and emerging trends, offering guidance for future research and\npractical deployment in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yihao Ding"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    },
                    {
                        "name": "Jean Lee"
                    },
                    {
                        "name": "Eduard Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Hovy"
                },
                "author": "Eduard Hovy",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16688v3",
                "updated": "2025-06-20T10:06:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    10,
                    6,
                    39,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-23T13:19:35Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    13,
                    19,
                    35,
                    2,
                    113,
                    0
                ],
                "title": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation\n  for 6G: MLR, ANOVA, and Residual Distribution Analysis"
                },
                "summary": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models (GMMs) with 2 to 5 components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling path loss in indoor LoRaWAN technology deployments is inherently\nchallenging due to structural obstructions, occupant density and activities,\nand fluctuating environmental conditions. This study proposes a two-stage\napproach to capture and analyze these complexities using an extensive dataset\nof 1,328,334 field measurements collected over six months in a single-floor\noffice at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,\nwe implement a multiple linear regression model that includes traditional\npropagation metrics (distance, structural walls) and an extension with proposed\nenvironmental variables (relative humidity, temperature, carbon dioxide,\nparticulate matter, and barometric pressure). Using analysis of variance, we\ndemonstrate that adding these environmental factors can reduce unexplained\nvariance by 42.32 percent. Secondly, we examine residual distributions by\nfitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,\nStudent's t, and Gaussian Mixture Models (GMMs) with 2 to 5 components. Our\nresults show that a four-component Gaussian Mixture Model captures the residual\nheterogeneity of indoor signal propagation most accurately, significantly\noutperforming single-distribution approaches. Given the push toward\nultra-reliable, context-aware communications in 6G networks, our analysis shows\nthat environment-aware modeling can substantially improve LoRaWAN network\ndesign in dynamic indoor IoT deployments."
                },
                "authors": [
                    {
                        "name": "Nahshon Mokua Obiri"
                    },
                    {
                        "name": "Kristof Van Laerhoven"
                    }
                ],
                "author_detail": {
                    "name": "Kristof Van Laerhoven"
                },
                "author": "Kristof Van Laerhoven",
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media. This is the accepted version of the article: To appear in the\n  2025 Joint European Conference on Networks and Communications & 6G Summit\n  (EuCNC/6G Summit)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16876v1",
                "updated": "2025-06-20T09:55:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    55,
                    56,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T09:55:56Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    55,
                    56,
                    4,
                    171,
                    0
                ],
                "title": "Revolutionizing Validation and Verification: Explainable Testing\n  Methodologies for Intelligent Automotive Decision-Making Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Validation and Verification: Explainable Testing\n  Methodologies for Intelligent Automotive Decision-Making Systems"
                },
                "summary": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADS) use complex decision-making (DM) models with\nmultimodal sensory inputs, making rigorous validation and verification (V&V)\nessential for safety and reliability. These models pose challenges in\ndiagnosing failures, tracing anomalies, and maintaining transparency, with\ncurrent manual testing methods being inefficient and labor-intensive. This\nvision paper presents a methodology that integrates explainability,\ntransparency, and interpretability into V&V processes. We propose refining V&V\nrequirements through literature reviews and stakeholder input, generating\nexplainable test scenarios via large language models (LLMs), and enabling\nreal-time validation in simulation environments. Our framework includes test\noracle, explanation generation, and a test chatbot, with empirical studies\nplanned to evaluate improvements in diagnostic efficiency and transparency. Our\ngoal is to streamline V&V, reduce resources, and build user trust in autonomous\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Halit Eris"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner",
                "arxiv_comment": "Preprint to be published at SE4ADS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16853v1",
                "updated": "2025-06-20T09:02:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    2,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T09:02:05Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    9,
                    2,
                    5,
                    4,
                    171,
                    0
                ],
                "title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models"
                },
                "summary": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, using\nup to 3.5 times less inference budget, and, given sufficient inference budget,\nachieves performance comparable to learning-based baselines that require\nreward-specific fine-tuning. The code is available at\nhttps://github.com/seminkim/RATTPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a general approach for improving user prompts in text-to-image\n(T2I) diffusion models by finding prompts that maximize a reward function\nspecified at test-time. Although diverse reward models are used for evaluating\nimage generation, existing automated prompt engineering methods typically\ntarget specific reward configurations. Consequently, these specialized designs\nexhibit suboptimal performance when applied to new prompt engineering scenarios\ninvolving different reward models. To address this limitation, we introduce\nRATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time\noptimization method applicable across various reward scenarios without\nmodification. RATTPO iteratively searches for optimized prompts by querying\nlarge language models (LLMs) \\textit{without} requiring reward-specific task\ndescriptions. Instead, it uses the optimization trajectory and a novel\nreward-aware feedback signal (termed a \"hint\") as context. Empirical results\ndemonstrate the versatility of RATTPO, effectively enhancing user prompts\nacross diverse reward setups that assess various generation aspects, such as\naesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, using\nup to 3.5 times less inference budget, and, given sufficient inference budget,\nachieves performance comparable to learning-based baselines that require\nreward-specific fine-tuning. The code is available at\nhttps://github.com/seminkim/RATTPO."
                },
                "authors": [
                    {
                        "name": "Semin Kim"
                    },
                    {
                        "name": "Yeonwoo Cha"
                    },
                    {
                        "name": "Jaehoon Yoo"
                    },
                    {
                        "name": "Seunghoon Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seunghoon Hong"
                },
                "author": "Seunghoon Hong",
                "arxiv_comment": "28 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00412v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00412v4",
                "updated": "2025-06-20T08:54:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    54,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2024-11-01T07:18:31Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    7,
                    18,
                    31,
                    4,
                    306,
                    0
                ],
                "title": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting While Learning: Grounding LLMs for Scientific Problems with\n  Intelligent Tool Usage Adaptation"
                },
                "summary": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate promising capabilities in solving\nscientific problems but often suffer from the issue of hallucination. While\nintegrating LLMs with tools can mitigate this issue, models fine-tuned on tool\nusage become overreliant on them and incur unnecessary costs. Inspired by how\nhuman experts assess problem complexity before selecting solutions, we propose\na novel two-component fine-tuning method, Adapting While Learning (AWL). In the\nfirst component, World Knowledge Learning (WKL), LLMs internalize scientific\nknowledge by learning from tool-generated solutions. In the second component,\nTool Usage Adaptation (TUA), we categorize problems as easy or hard based on\nthe model's accuracy, and train it to maintain direct reasoning for easy\nproblems while switching to tools for hard ones. We validate our method on six\nscientific benchmark datasets across climate science, epidemiology, physics,\nand other domains. Compared to the original instruct model (8B), models\npost-trained with AWL achieve 29.11% higher answer accuracy and 12.72% better\ntool usage accuracy, even surpassing state-of-the-art models including GPT-4o\nand Claude-3.5 on four custom-created datasets. Our code is open-source at\nhttps://github.com/Rose-STL-Lab/Adapting-While-Learning."
                },
                "authors": [
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Yadi Cao"
                    },
                    {
                        "name": "Duncan Watson-Parris"
                    },
                    {
                        "name": "Leon Bergen"
                    },
                    {
                        "name": "Taylor Berg-Kirkpatrick"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "37 pages, 16 figures",
                "arxiv_journal_ref": "In Proceedings of the Forty-second International Conference on\n  Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00412v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00412v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16826v1",
                "updated": "2025-06-20T08:31:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    31,
                    13,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:31:13Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    31,
                    13,
                    4,
                    171,
                    0
                ],
                "title": "AnyTraverse: An off-road traversability framework with VLM and human\n  operator in the loop",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyTraverse: An off-road traversability framework with VLM and human\n  operator in the loop"
                },
                "summary": "Off-road traversability segmentation enables autonomous navigation with\napplications in search-and-rescue, military operations, wildlife exploration,\nand agriculture. Current frameworks struggle due to significant variations in\nunstructured environments and uncertain scene changes, and are not adaptive to\nbe used for different robot types. We present AnyTraverse, a framework\ncombining natural language-based prompts with human-operator assistance to\ndetermine navigable regions for diverse robotic vehicles. The system segments\nscenes for a given set of prompts and calls the operator only when encountering\npreviously unexplored scenery or unknown class not part of the prompt in its\nregion-of-interest, thus reducing active supervision load while adapting to\nvarying outdoor scenes. Our zero-shot learning approach eliminates the need for\nextensive data collection or retraining. Our experimental validation includes\ntesting on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate\nreal-world deployment on multiple robot platforms. The results show that\nAnyTraverse performs better than GA-NAV and Off-seg while offering a\nvehicle-agnostic approach to off-road traversability that balances automation\nwith targeted human supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-road traversability segmentation enables autonomous navigation with\napplications in search-and-rescue, military operations, wildlife exploration,\nand agriculture. Current frameworks struggle due to significant variations in\nunstructured environments and uncertain scene changes, and are not adaptive to\nbe used for different robot types. We present AnyTraverse, a framework\ncombining natural language-based prompts with human-operator assistance to\ndetermine navigable regions for diverse robotic vehicles. The system segments\nscenes for a given set of prompts and calls the operator only when encountering\npreviously unexplored scenery or unknown class not part of the prompt in its\nregion-of-interest, thus reducing active supervision load while adapting to\nvarying outdoor scenes. Our zero-shot learning approach eliminates the need for\nextensive data collection or retraining. Our experimental validation includes\ntesting on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate\nreal-world deployment on multiple robot platforms. The results show that\nAnyTraverse performs better than GA-NAV and Off-seg while offering a\nvehicle-agnostic approach to off-road traversability that balances automation\nwith targeted human supervision."
                },
                "authors": [
                    {
                        "name": "Sattwik Sahu"
                    },
                    {
                        "name": "Agamdeep Singh"
                    },
                    {
                        "name": "Karthik Nambiar"
                    },
                    {
                        "name": "Srikanth Saripalli"
                    },
                    {
                        "name": "P. B. Sujit"
                    }
                ],
                "author_detail": {
                    "name": "P. B. Sujit"
                },
                "author": "P. B. Sujit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16824v1",
                "updated": "2025-06-20T08:26:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:26:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    26,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Predicting New Research Directions in Materials Science using Large\n  Language Models and Concept Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting New Research Directions in Materials Science using Large\n  Language Models and Concept Graphs"
                },
                "summary": "Due to an exponential increase in published research articles, it is\nimpossible for individual scientists to read all publications, even within\ntheir own research field. In this work, we investigate the use of large\nlanguage models (LLMs) for the purpose of extracting the main concepts and\nsemantic information from scientific abstracts in the domain of materials\nscience to find links that were not noticed by humans and thus to suggest\ninspiring near/mid-term future research directions. We show that LLMs can\nextract concepts more efficiently than automated keyword extraction methods to\nbuild a concept graph as an abstraction of the scientific literature. A machine\nlearning model is trained to predict emerging combinations of concepts, i.e.\nnew research ideas, based on historical data. We demonstrate that integrating\nsemantic concept information leads to an increased prediction performance. The\napplicability of our model is demonstrated in qualitative interviews with\ndomain experts based on individualized model suggestions. We show that the\nmodel can inspire materials scientists in their creative thinking process by\npredicting innovative combinations of topics that have not yet been\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to an exponential increase in published research articles, it is\nimpossible for individual scientists to read all publications, even within\ntheir own research field. In this work, we investigate the use of large\nlanguage models (LLMs) for the purpose of extracting the main concepts and\nsemantic information from scientific abstracts in the domain of materials\nscience to find links that were not noticed by humans and thus to suggest\ninspiring near/mid-term future research directions. We show that LLMs can\nextract concepts more efficiently than automated keyword extraction methods to\nbuild a concept graph as an abstraction of the scientific literature. A machine\nlearning model is trained to predict emerging combinations of concepts, i.e.\nnew research ideas, based on historical data. We demonstrate that integrating\nsemantic concept information leads to an increased prediction performance. The\napplicability of our model is demonstrated in qualitative interviews with\ndomain experts based on individualized model suggestions. We show that the\nmodel can inspire materials scientists in their creative thinking process by\npredicting innovative combinations of topics that have not yet been\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Thomas Marwitz"
                    },
                    {
                        "name": "Alexander Colsmann"
                    },
                    {
                        "name": "Ben Breitung"
                    },
                    {
                        "name": "Christoph Brabec"
                    },
                    {
                        "name": "Christoph Kirchlechner"
                    },
                    {
                        "name": "Eva Blasco"
                    },
                    {
                        "name": "Gabriel Cadilha Marques"
                    },
                    {
                        "name": "Horst Hahn"
                    },
                    {
                        "name": "Michael Hirtz"
                    },
                    {
                        "name": "Pavel A. Levkin"
                    },
                    {
                        "name": "Yolita M. Eggeler"
                    },
                    {
                        "name": "Tobias Schlöder"
                    },
                    {
                        "name": "Pascal Friederich"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Friederich"
                },
                "author": "Pascal Friederich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16821v1",
                "updated": "2025-06-20T08:21:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    21,
                    34,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:21:34Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    21,
                    34,
                    4,
                    171,
                    0
                ],
                "title": "Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer\n  Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer\n  Robots"
                },
                "summary": "Robust and accurate ball detection is a critical component for autonomous\nhumanoid soccer robots, particularly in dynamic and challenging environments\nsuch as RoboCup outdoor fields. However, traditional supervised approaches\nrequire extensive manual annotation, which is costly and time-intensive. To\novercome this problem, we present a self-supervised learning framework for\ndomain-adaptive feature extraction to enhance ball detection performance. The\nproposed approach leverages a general-purpose pretrained model to generate\npseudo-labels, which are then used in a suite of self-supervised pretext tasks\n-- including colorization, edge detection, and triplet loss -- to learn robust\nvisual features without relying on manual annotations. Additionally, a\nmodel-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid\nadaptation to new deployment scenarios with minimal supervision. A new dataset\ncomprising 10,000 labeled images from outdoor RoboCup SPL matches is\nintroduced, used to validate the method, and made available to the community.\nExperimental results demonstrate that the proposed pipeline outperforms\nbaseline models in terms of accuracy, F1 score, and IoU, while also exhibiting\nfaster convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and accurate ball detection is a critical component for autonomous\nhumanoid soccer robots, particularly in dynamic and challenging environments\nsuch as RoboCup outdoor fields. However, traditional supervised approaches\nrequire extensive manual annotation, which is costly and time-intensive. To\novercome this problem, we present a self-supervised learning framework for\ndomain-adaptive feature extraction to enhance ball detection performance. The\nproposed approach leverages a general-purpose pretrained model to generate\npseudo-labels, which are then used in a suite of self-supervised pretext tasks\n-- including colorization, edge detection, and triplet loss -- to learn robust\nvisual features without relying on manual annotations. Additionally, a\nmodel-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid\nadaptation to new deployment scenarios with minimal supervision. A new dataset\ncomprising 10,000 labeled images from outdoor RoboCup SPL matches is\nintroduced, used to validate the method, and made available to the community.\nExperimental results demonstrate that the proposed pipeline outperforms\nbaseline models in terms of accuracy, F1 score, and IoU, while also exhibiting\nfaster convergence."
                },
                "authors": [
                    {
                        "name": "Can Lin"
                    },
                    {
                        "name": "Daniele Affinita"
                    },
                    {
                        "name": "Marco E. P. Zimmatore"
                    },
                    {
                        "name": "Daniele Nardi"
                    },
                    {
                        "name": "Domenico D. Bloisi"
                    },
                    {
                        "name": "Vincenzo Suriani"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Suriani"
                },
                "author": "Vincenzo Suriani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10036v2",
                "updated": "2025-06-20T08:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    7,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-14T09:26:59Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    26,
                    59,
                    4,
                    45,
                    0
                ],
                "title": "Automation Bias in the AI Act: On the Legal Implications of Attempting\n  to De-Bias Human Oversight of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation Bias in the AI Act: On the Legal Implications of Attempting\n  to De-Bias Human Oversight of AI"
                },
                "summary": "This paper examines the legal implications of the explicit mentioning of\nautomation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates\nhuman oversight for high-risk AI systems and requires providers to enable\nawareness of AB, i.e., the human tendency to over-rely on AI outputs. The paper\nanalyses the embedding of this extra-juridical concept in the AIA, the\nasymmetric division of responsibility between AI providers and deployers for\nmitigating AB, and the challenges of legally enforcing this novel awareness\nrequirement. The analysis shows that the AIA's focus on providers does not\nadequately address design and context as causes of AB, and questions whether\nthe AIA should directly regulate the risk of AB rather than just mandating\nawareness. As the AIA's approach requires a balance between legal mandates and\nbehavioural science, the paper proposes that harmonised standards should\nreference the state of research on AB and human-AI interaction, holding both\nproviders and deployers accountable. Ultimately, further empirical research on\nhuman-AI interaction will be essential for effective safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the legal implications of the explicit mentioning of\nautomation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates\nhuman oversight for high-risk AI systems and requires providers to enable\nawareness of AB, i.e., the human tendency to over-rely on AI outputs. The paper\nanalyses the embedding of this extra-juridical concept in the AIA, the\nasymmetric division of responsibility between AI providers and deployers for\nmitigating AB, and the challenges of legally enforcing this novel awareness\nrequirement. The analysis shows that the AIA's focus on providers does not\nadequately address design and context as causes of AB, and questions whether\nthe AIA should directly regulate the risk of AB rather than just mandating\nawareness. As the AIA's approach requires a balance between legal mandates and\nbehavioural science, the paper proposes that harmonised standards should\nreference the state of research on AB and human-AI interaction, holding both\nproviders and deployers accountable. Ultimately, further empirical research on\nhuman-AI interaction will be essential for effective safeguards."
                },
                "authors": [
                    {
                        "name": "Johann Laux"
                    },
                    {
                        "name": "Hannah Ruschemeier"
                    }
                ],
                "author_detail": {
                    "name": "Hannah Ruschemeier"
                },
                "author": "Hannah Ruschemeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16813v1",
                "updated": "2025-06-20T08:03:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    3,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T08:03:36Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    8,
                    3,
                    36,
                    4,
                    171,
                    0
                ],
                "title": "Integrating Traditional Technical Analysis with AI: A Multi-Agent\n  LLM-Based Approach to Stock Market Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Traditional Technical Analysis with AI: A Multi-Agent\n  LLM-Based Approach to Stock Market Forecasting"
                },
                "summary": "Traditional technical analysis methods face limitations in accurately\npredicting trends in today's complex financial markets. This paper introduces\nElliottAgents, an multi-agent system that integrates the Elliott Wave Principle\nwith AI for stock market forecasting. The inherent complexity of financial\nmarkets, characterized by non-linear dynamics, noise, and susceptibility to\nunpredictable external factors, poses significant challenges for accurate\nprediction. To address these challenges, the system employs LLMs to enhance\nnatural language understanding and decision-making capabilities within a\nmulti-agent framework. By leveraging technologies such as Retrieval-Augmented\nGeneration (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs\ncontinuous, multi-faceted analysis of market data to identify wave patterns and\npredict future price movements. The research explores the system's ability to\nprocess historical stock data, recognize Elliott wave patterns, and generate\nactionable insights for traders. Experimental results, conducted on historical\ndata from major U.S. companies, validate the system's effectiveness in pattern\nrecognition and trend forecasting across various time frames. This paper\ncontributes to the field of AI-driven financial analysis by demonstrating how\ntraditional technical analysis methods can be effectively combined with modern\nAI approaches to create more reliable and interpretable market prediction\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional technical analysis methods face limitations in accurately\npredicting trends in today's complex financial markets. This paper introduces\nElliottAgents, an multi-agent system that integrates the Elliott Wave Principle\nwith AI for stock market forecasting. The inherent complexity of financial\nmarkets, characterized by non-linear dynamics, noise, and susceptibility to\nunpredictable external factors, poses significant challenges for accurate\nprediction. To address these challenges, the system employs LLMs to enhance\nnatural language understanding and decision-making capabilities within a\nmulti-agent framework. By leveraging technologies such as Retrieval-Augmented\nGeneration (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs\ncontinuous, multi-faceted analysis of market data to identify wave patterns and\npredict future price movements. The research explores the system's ability to\nprocess historical stock data, recognize Elliott wave patterns, and generate\nactionable insights for traders. Experimental results, conducted on historical\ndata from major U.S. companies, validate the system's effectiveness in pattern\nrecognition and trend forecasting across various time frames. This paper\ncontributes to the field of AI-driven financial analysis by demonstrating how\ntraditional technical analysis methods can be effectively combined with modern\nAI approaches to create more reliable and interpretable market prediction\nsystems."
                },
                "authors": [
                    {
                        "name": "Michał Wawer"
                    },
                    {
                        "name": "Jarosław A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosław A. Chudziak"
                },
                "author": "Jarosław A. Chudziak",
                "arxiv_doi": "10.5220/0013191200003890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013191200003890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures, 1 table. This is the accepted version of the\n  paper presented at the 17th International Conference on Agents and Artificial\n  Intelligence (ICAART 2025), Porto, Portugal",
                "arxiv_journal_ref": "Proceedings of the 17th International Conference on Agents and\n  Artificial Intelligence - Volume 1 (ICAART 2025), pages 100-111. SciTePress,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91G68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15551v2",
                "updated": "2025-06-20T07:32:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    32,
                    36,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-18T15:16:10Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    16,
                    10,
                    1,
                    77,
                    0
                ],
                "title": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting\n  Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient but Vulnerable: Benchmarking and Defending LLM Batch Prompting\n  Attack"
                },
                "summary": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch prompting, which combines a batch of multiple queries sharing the same\ncontext in one inference, has emerged as a promising solution to reduce\ninference costs. However, our study reveals a significant security\nvulnerability in batch prompting: malicious users can inject attack\ninstructions into a batch, leading to unwanted interference across all queries,\nwhich can result in the inclusion of harmful content, such as phishing links,\nor the disruption of logical reasoning. In this paper, we construct\nBATCHSAFEBENCH, a comprehensive benchmark comprising 150 attack instructions of\ntwo types and 8k batch instances, to study the batch prompting vulnerability\nsystematically. Our evaluation of both closed-source and open-weight LLMs\ndemonstrates that all LLMs are susceptible to batch-prompting attacks. We then\nexplore multiple defending approaches. While the prompting-based defense shows\nlimited effectiveness for smaller LLMs, the probing-based approach achieves\nabout 95% accuracy in detecting attacks. Additionally, we perform a mechanistic\nanalysis to understand the attack and identify attention heads that are\nresponsible for it."
                },
                "authors": [
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "arxiv_comment": "Accepted to ACL Findings, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21625v4",
                "updated": "2025-06-20T07:25:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    25,
                    51,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-30T13:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    13,
                    28,
                    19,
                    2,
                    120,
                    0
                ],
                "title": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-Following Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs'\n  Multi-turn Instruction-Following Ability"
                },
                "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions\nthrough an iterative feedback framework, which enables models to self-correct\nbased on specific requirement failures in each turn, better reflecting\nreal-world user-end usage patterns. Meanwhile, the benchmark implements a\ncomprehensive evaluation system with 38 capability tags organized across three\ndimensions: Intent Recognition, Granular Content Validation, and Output\nStructure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nFor complex instructions, LLMs often struggle to fulfill all requirements in a\nsingle attempt. In practice, users typically provide iterative feedback until\nthe LLM generates a response that meets all requirements. However, existing\ninstruction-following benchmarks are either single-turn or introduce new\nrequirements in each turn without allowing self-correction. To address this\ngap, we propose Meeseeks. Meeseeks simulates realistic human-LLM interactions\nthrough an iterative feedback framework, which enables models to self-correct\nbased on specific requirement failures in each turn, better reflecting\nreal-world user-end usage patterns. Meanwhile, the benchmark implements a\ncomprehensive evaluation system with 38 capability tags organized across three\ndimensions: Intent Recognition, Granular Content Validation, and Output\nStructure Validation. Through rigorous evaluation across LLMs, Meeseeks\nprovides valuable insights into LLMs' instruction-following capabilities in\nmulti-turn scenarios."
                },
                "authors": [
                    {
                        "name": "Jiaming Wang"
                    },
                    {
                        "name": "Yunke Zhao"
                    },
                    {
                        "name": "Peng Ding"
                    },
                    {
                        "name": "Jun Kuang"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16796v1",
                "updated": "2025-06-20T07:21:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    21,
                    21,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T07:21:21Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    21,
                    21,
                    4,
                    171,
                    0
                ],
                "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought"
                },
                "summary": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation."
                },
                "authors": [
                    {
                        "name": "Junbo Qiao"
                    },
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xudong Huang"
                    },
                    {
                        "name": "Gaoqi He"
                    },
                    {
                        "name": "Jiao Xie"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16792v1",
                "updated": "2025-06-20T07:16:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T07:16:47Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning"
                },
                "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST."
                },
                "authors": [
                    {
                        "name": "Muyang Zheng"
                    },
                    {
                        "name": "Yuanzhi Yao"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11422v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11422v3",
                "updated": "2025-06-20T07:14:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    14,
                    59,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-17T04:35:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    4,
                    35,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization"
                },
                "summary": "Heuristics have achieved great success in solving combinatorial optimization\nproblems~(COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Since Large Language Models~(LLMs) possess strong\ncapabilities to understand and generate content with a knowledge base that\ncovers various domains, they offer potential ways to automatically optimize\nheuristics. To this end, we propose Planning of Heuristics~(PoH), an\noptimization method that integrates LLM self-reflection with Monte Carlo Tree\nSearch, a well-known planning algorithm. PoH iteratively refines generated\nheuristics by evaluating their performance and providing improvement\nsuggestions. Our method enables to iteratively evaluate the generated\nheuristics~(states) and improve them based on the improvement\nsuggestions~(actions) and evaluation results~(rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop\nScheduling Problem. The experimental results show that PoH outperforms\nhand-crafted heuristics and other Automatic Heuristic Design methods based on\nLLMs, and achieves the state-of-the-art performance in automating heuristic\noptimization with LLMs to solve tested COPs, especially with large sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristics have achieved great success in solving combinatorial optimization\nproblems~(COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Since Large Language Models~(LLMs) possess strong\ncapabilities to understand and generate content with a knowledge base that\ncovers various domains, they offer potential ways to automatically optimize\nheuristics. To this end, we propose Planning of Heuristics~(PoH), an\noptimization method that integrates LLM self-reflection with Monte Carlo Tree\nSearch, a well-known planning algorithm. PoH iteratively refines generated\nheuristics by evaluating their performance and providing improvement\nsuggestions. Our method enables to iteratively evaluate the generated\nheuristics~(states) and improve them based on the improvement\nsuggestions~(actions) and evaluation results~(rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem and the Flow Shop\nScheduling Problem. The experimental results show that PoH outperforms\nhand-crafted heuristics and other Automatic Heuristic Design methods based on\nLLMs, and achieves the state-of-the-art performance in automating heuristic\noptimization with LLMs to solve tested COPs, especially with large sizes."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Xufeng Zhang"
                    },
                    {
                        "name": "Chaoxu Mu"
                    }
                ],
                "author_detail": {
                    "name": "Chaoxu Mu"
                },
                "author": "Chaoxu Mu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11422v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11422v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12345v3",
                "updated": "2025-06-20T07:09:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    9,
                    39,
                    4,
                    171,
                    0
                ],
                "published": "2025-04-15T16:58:11Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    58,
                    11,
                    1,
                    105,
                    0
                ],
                "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Urban Science: Scaling Causal Inference with Large Language\n  Models"
                },
                "summary": "Urban causal research is essential for understanding the complex, dynamic\nprocesses that shape cities and for informing evidence-based policies. However,\ncurrent practices are often constrained by inefficient and biased hypothesis\nformulation, challenges in integrating multimodal data, and fragile\nexperimental methodologies. Imagine a system that automatically estimates the\ncausal impact of congestion pricing on commute times by income group or\nmeasures how new green spaces affect asthma rates across neighborhoods using\nsatellite imagery and health reports, and then generates comprehensive,\npolicy-ready outputs, including causal estimates, subgroup analyses, and\nactionable recommendations. In this Perspective, we propose UrbanCIA, an\nLLM-driven conceptual framework composed of four distinct modular agents\nresponsible for hypothesis generation, data engineering, experiment design and\nexecution, and results interpretation with policy insights. We begin by\nexamining the current landscape of urban causal research through a structured\ntaxonomy of research topics, data sources, and methodological approaches,\nrevealing systemic limitations across the workflow. Next, we introduce the\ndesign principles and technological roadmap for the four modules in the\nproposed framework. We also propose evaluation criteria to assess the rigor and\ntransparency of these AI-augmented processes. Finally, we reflect on the\nbroader implications for human-AI collaboration, equity, and accountability. We\ncall for a new research agenda that embraces LLM-driven tools as catalysts for\nmore scalable, reproducible, and inclusive urban research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban causal research is essential for understanding the complex, dynamic\nprocesses that shape cities and for informing evidence-based policies. However,\ncurrent practices are often constrained by inefficient and biased hypothesis\nformulation, challenges in integrating multimodal data, and fragile\nexperimental methodologies. Imagine a system that automatically estimates the\ncausal impact of congestion pricing on commute times by income group or\nmeasures how new green spaces affect asthma rates across neighborhoods using\nsatellite imagery and health reports, and then generates comprehensive,\npolicy-ready outputs, including causal estimates, subgroup analyses, and\nactionable recommendations. In this Perspective, we propose UrbanCIA, an\nLLM-driven conceptual framework composed of four distinct modular agents\nresponsible for hypothesis generation, data engineering, experiment design and\nexecution, and results interpretation with policy insights. We begin by\nexamining the current landscape of urban causal research through a structured\ntaxonomy of research topics, data sources, and methodological approaches,\nrevealing systemic limitations across the workflow. Next, we introduce the\ndesign principles and technological roadmap for the four modules in the\nproposed framework. We also propose evaluation criteria to assess the rigor and\ntransparency of these AI-augmented processes. Finally, we reflect on the\nbroader implications for human-AI collaboration, equity, and accountability. We\ncall for a new research agenda that embraces LLM-driven tools as catalysts for\nmore scalable, reproducible, and inclusive urban research."
                },
                "authors": [
                    {
                        "name": "Yutong Xia"
                    },
                    {
                        "name": "Ao Qu"
                    },
                    {
                        "name": "Yunhan Zheng"
                    },
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Dingyi Zhuang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    },
                    {
                        "name": "Shenhao Wang"
                    },
                    {
                        "name": "Cathy Wu"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Jinhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Zhao"
                },
                "author": "Jinhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24183v2",
                "updated": "2025-06-20T07:05:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    5,
                    18,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-30T03:51:06Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    3,
                    51,
                    6,
                    4,
                    150,
                    0
                ],
                "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeV-R1: Reasoning-Enhanced Verilog Generation"
                },
                "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities."
                },
                "authors": [
                    {
                        "name": "Yaoyu Zhu"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Hanqi Lyu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Jianan Mu"
                    },
                    {
                        "name": "Jinghua Wang"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16777v1",
                "updated": "2025-06-20T06:45:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    45,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T06:45:40Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    45,
                    40,
                    4,
                    171,
                    0
                ],
                "title": "DistillNote: LLM-based clinical note summaries improve heart failure\n  diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistillNote: LLM-based clinical note summaries improve heart failure\n  diagnosis"
                },
                "summary": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research."
                },
                "authors": [
                    {
                        "name": "Heloisa Oss Boll"
                    },
                    {
                        "name": "Antonio Oss Boll"
                    },
                    {
                        "name": "Leticia Puttlitz Boll"
                    },
                    {
                        "name": "Ameen Abu Hanna"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16637v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16637v3",
                "updated": "2025-06-20T06:38:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    38,
                    44,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-22T13:08:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    8,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine\n  Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine\n  Translation"
                },
                "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."
                },
                "authors": [
                    {
                        "name": "Wenjie Yang"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Sitong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sitong Wang"
                },
                "author": "Sitong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16637v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16637v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16768v1",
                "updated": "2025-06-20T06:07:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    7,
                    20,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T06:07:20Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    7,
                    20,
                    4,
                    171,
                    0
                ],
                "title": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eSapiens: A Real-World NLP Framework for Multimodal Document\n  Understanding and Enterprise Knowledge Processing"
                },
                "summary": "We introduce eSapiens, a unified question-answering system designed for\nenterprise settings, which bridges structured databases and unstructured\ntextual corpora via a dual-module architecture. The system combines a\nText-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)\npipeline, enabling natural language access to both relational data and\nfree-form documents. To enhance answer faithfulness, the RAG module integrates\ndense and sparse retrieval, commercial reranking, and a citation verification\nloop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth\nbenchmark across five leading large language models (LLMs), analyzing\nperformance across key dimensions such as completeness, hallucination, and\ncontext utilization. Results demonstrate that eSapiens outperforms a FAISS\nbaseline in contextual relevance and generation quality, with optional\nstrict-grounding controls for high-stakes scenarios. This work provides a\ndeployable framework for robust, citation-aware question answering in\nreal-world enterprise applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce eSapiens, a unified question-answering system designed for\nenterprise settings, which bridges structured databases and unstructured\ntextual corpora via a dual-module architecture. The system combines a\nText-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)\npipeline, enabling natural language access to both relational data and\nfree-form documents. To enhance answer faithfulness, the RAG module integrates\ndense and sparse retrieval, commercial reranking, and a citation verification\nloop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth\nbenchmark across five leading large language models (LLMs), analyzing\nperformance across key dimensions such as completeness, hallucination, and\ncontext utilization. Results demonstrate that eSapiens outperforms a FAISS\nbaseline in contextual relevance and generation quality, with optional\nstrict-grounding controls for high-stakes scenarios. This work provides a\ndeployable framework for robust, citation-aware question answering in\nreal-world enterprise applications."
                },
                "authors": [
                    {
                        "name": "Isaac Shi"
                    },
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Wenli Wang"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14103v3",
                "updated": "2025-06-20T06:05:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    6,
                    5,
                    48,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-18T10:18:07Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    10,
                    18,
                    7,
                    1,
                    77,
                    0
                ],
                "title": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DangerMaps: Personalized Safety Advice for Travel in Urban Environments\n  using a Retrieval-Augmented Language Model"
                },
                "summary": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning a trip into a potentially unsafe area is a difficult task. We\nconducted a formative study on travelers' information needs, finding that most\nof them turn to search engines for trip planning. Search engines, however, fail\nto provide easily interpretable results adapted to the context and personal\ninformation needs of a traveler. Large language models (LLMs) create new\npossibilities for providing personalized travel safety advice. To explore this\nidea, we developed DangerMaps, a mapping system that assists its users in\nresearching the safety of an urban travel destination, whether it is pre-travel\nor on-location. DangerMaps plots safety ratings onto a map and provides\nexplanations on demand. This late breaking work specifically emphasizes the\nchallenges of designing real-world applications with large language models. We\nprovide a detailed description of our approach to prompt design and highlight\nfuture areas of research."
                },
                "authors": [
                    {
                        "name": "Jonas Oppenlaender"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Oppenlaender"
                },
                "author": "Jonas Oppenlaender",
                "arxiv_comment": "14 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08558v3",
                "updated": "2025-06-20T05:51:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    5,
                    51,
                    24,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-11T15:47:12Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies"
                },
                "summary": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Tony Khuong Nguyen"
                    },
                    {
                        "name": "Emma Dixon"
                    },
                    {
                        "name": "Christopher Rodriguez"
                    },
                    {
                        "name": "Patrick Miller"
                    },
                    {
                        "name": "Robert Lee"
                    },
                    {
                        "name": "Paarth Shah"
                    },
                    {
                        "name": "Rares Ambrus"
                    },
                    {
                        "name": "Haruki Nishimura"
                    },
                    {
                        "name": "Masha Itkina"
                    }
                ],
                "author_detail": {
                    "name": "Masha Itkina"
                },
                "author": "Masha Itkina",
                "arxiv_comment": "Accepted by Robotics: Science and Systems 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16029v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16029v3",
                "updated": "2025-06-20T05:23:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    5,
                    23,
                    34,
                    4,
                    171,
                    0
                ],
                "published": "2025-01-27T13:18:40Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    18,
                    40,
                    0,
                    27,
                    0
                ],
                "title": "FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting"
                },
                "summary": "Large Language Models (LLMs) are rapidly transforming the landscape of\ndigital content creation. However, the prevalent black-box Application\nProgramming Interface (API) access to many LLMs introduces significant\nchallenges in accountability, governance, and security. LLM fingerprinting,\nwhich aims to identify the source model by analyzing statistical and stylistic\nfeatures of generated text, offers a potential solution. Current progress in\nthis area is hindered by a lack of dedicated datasets and the need for\nefficient, practical methods that are robust against adversarial manipulations.\nTo address these challenges, we introduce FD-Dataset, a comprehensive bilingual\nfingerprinting benchmark comprising 90,000 text samples from 20 famous\nproprietary and open-source LLMs. Furthermore, we present FDLLM, a novel\nfingerprinting method that leverages parameter-efficient Low-Rank Adaptation\n(LoRA) to fine-tune a foundation model. This approach enables LoRA to extract\ndeep, persistent features that characterize each source LLM. Through our\nanalysis, we find that LoRA adaptation promotes the aggregation of outputs from\nthe same LLM in representation space while enhancing the separation between\ndifferent LLMs. This mechanism explains why LoRA proves particularly effective\nfor LLM fingerprinting. Extensive empirical evaluations on FD-Dataset\ndemonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than\nthe strongest baseline. FDLLM also exhibits strong generalization to newly\nreleased models, achieving an average accuracy of 95% on unseen models.\nNotably, FDLLM remains consistently robust under various adversarial attacks,\nincluding polishing, translation, and synonym substitution. Experimental\nresults show that FDLLM reduces the average attack success rate from 49.2%\n(LM-D) to 23.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly transforming the landscape of\ndigital content creation. However, the prevalent black-box Application\nProgramming Interface (API) access to many LLMs introduces significant\nchallenges in accountability, governance, and security. LLM fingerprinting,\nwhich aims to identify the source model by analyzing statistical and stylistic\nfeatures of generated text, offers a potential solution. Current progress in\nthis area is hindered by a lack of dedicated datasets and the need for\nefficient, practical methods that are robust against adversarial manipulations.\nTo address these challenges, we introduce FD-Dataset, a comprehensive bilingual\nfingerprinting benchmark comprising 90,000 text samples from 20 famous\nproprietary and open-source LLMs. Furthermore, we present FDLLM, a novel\nfingerprinting method that leverages parameter-efficient Low-Rank Adaptation\n(LoRA) to fine-tune a foundation model. This approach enables LoRA to extract\ndeep, persistent features that characterize each source LLM. Through our\nanalysis, we find that LoRA adaptation promotes the aggregation of outputs from\nthe same LLM in representation space while enhancing the separation between\ndifferent LLMs. This mechanism explains why LoRA proves particularly effective\nfor LLM fingerprinting. Extensive empirical evaluations on FD-Dataset\ndemonstrate FDLLM's superiority, achieving a Macro F1 score 22.1% higher than\nthe strongest baseline. FDLLM also exhibits strong generalization to newly\nreleased models, achieving an average accuracy of 95% on unseen models.\nNotably, FDLLM remains consistently robust under various adversarial attacks,\nincluding polishing, translation, and synonym substitution. Experimental\nresults show that FDLLM reduces the average attack success rate from 49.2%\n(LM-D) to 23.9%."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fu"
                    },
                    {
                        "name": "Junfan Chen"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Ting Yang"
                    },
                    {
                        "name": "Jun Niu"
                    },
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Ruidong Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jice Wang"
                    },
                    {
                        "name": "Fannv He"
                    },
                    {
                        "name": "Yuqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Zhang"
                },
                "author": "Yuqing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16029v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16029v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05731v2",
                "updated": "2025-06-20T05:05:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    5,
                    5,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-09T00:25:41Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    0,
                    25,
                    41,
                    6,
                    40,
                    0
                ],
                "title": "Visual Text Mining with Progressive Taxonomy Construction for\n  Environmental Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Text Mining with Progressive Taxonomy Construction for\n  Environmental Studies"
                },
                "summary": "Environmental experts have developed the DPSIR (Driver, Pressure, State,\nImpact, Response) framework to systematically study and communicate key\nrelationships between society and the environment. Using this framework\nrequires experts to construct a DPSIR taxonomy from a corpus, annotate the\ndocuments, and identify DPSIR variables and relationships, which is laborious\nand inflexible. Automating it with conventional text mining faces technical\nchallenges, primarily because the taxonomy often begins with abstract\ndefinitions, which experts progressively refine and contextualize as they\nannotate the corpus. In response, we develop GreenMine, a system that supports\ninteractive text mining with prompt engineering. The system implements a\nprompting pipeline consisting of three simple and evaluable subtasks. In each\nsubtask, the DPSIR taxonomy can be defined in natural language and iteratively\nrefined as experts analyze the corpus. To support users evaluate the taxonomy,\nwe introduce an uncertainty score based on response consistency. Then, we\ndesign a radial uncertainty chart that visualizes uncertainties and corpus\ntopics, which supports interleaved evaluation and exploration. Using the\nsystem, experts can progressively construct the DPSIR taxonomy and annotate the\ncorpus with LLMs. Using real-world interview transcripts, we present a case\nstudy to demonstrate the capability of the system in supporting interactive\nmining of DPSIR relationships, and an expert review in the form of\ncollaborative discussion to understand the potential and limitations of the\nsystem. We discuss the lessons learned from developing the system and future\nopportunities for supporting interactive text mining in knowledge-intensive\ntasks for other application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental experts have developed the DPSIR (Driver, Pressure, State,\nImpact, Response) framework to systematically study and communicate key\nrelationships between society and the environment. Using this framework\nrequires experts to construct a DPSIR taxonomy from a corpus, annotate the\ndocuments, and identify DPSIR variables and relationships, which is laborious\nand inflexible. Automating it with conventional text mining faces technical\nchallenges, primarily because the taxonomy often begins with abstract\ndefinitions, which experts progressively refine and contextualize as they\nannotate the corpus. In response, we develop GreenMine, a system that supports\ninteractive text mining with prompt engineering. The system implements a\nprompting pipeline consisting of three simple and evaluable subtasks. In each\nsubtask, the DPSIR taxonomy can be defined in natural language and iteratively\nrefined as experts analyze the corpus. To support users evaluate the taxonomy,\nwe introduce an uncertainty score based on response consistency. Then, we\ndesign a radial uncertainty chart that visualizes uncertainties and corpus\ntopics, which supports interleaved evaluation and exploration. Using the\nsystem, experts can progressively construct the DPSIR taxonomy and annotate the\ncorpus with LLMs. Using real-world interview transcripts, we present a case\nstudy to demonstrate the capability of the system in supporting interactive\nmining of DPSIR relationships, and an expert review in the form of\ncollaborative discussion to understand the potential and limitations of the\nsystem. We discuss the lessons learned from developing the system and future\nopportunities for supporting interactive text mining in knowledge-intensive\ntasks for other application scenarios."
                },
                "authors": [
                    {
                        "name": "Sam Yu-Te Lee"
                    },
                    {
                        "name": "Cheng-Wei Hung"
                    },
                    {
                        "name": "Mei-Hua Yuan"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_doi": "10.1109/PacificVis64226.2025.00037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PacificVis64226.2025.00037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE PacificVis 2025 Information systems, Information systems\n  applications, Data Mining; Human-centered computing, Visualization,\n  Visualization systems and tools",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20643v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20643v3",
                "updated": "2025-06-20T04:55:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    4,
                    55,
                    47,
                    4,
                    171,
                    0
                ],
                "published": "2024-10-28T00:39:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    0,
                    39,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenUP: Generative User Profilers as In-Context Learners for Next POI\n  Recommender Systems"
                },
                "summary": "Traditional Point-of-Interest (POI) recommendation systems often lack\ntransparency, interpretability, and scrutability due to their reliance on dense\nvector-based user embeddings. Furthermore, the cold-start problem -- where\nsystems have insufficient data for new users -- limits their ability to\ngenerate accurate recommendations. Existing methods often address this by\nleveraging similar trajectories from other users, but this approach can be\ncomputationally expensive and increases the context length for LLM-based\nmethods, making them difficult to scale. To address these limitations, we\npropose a method that generates natural language (NL) user profiles from\nlarge-scale, location-based social network (LBSN) check-ins, utilizing robust\npersonality assessments and behavioral theories. These NL profiles capture user\npreferences, routines, and behaviors, improving POI prediction accuracy while\noffering enhanced transparency. By incorporating NL profiles as system prompts\nto LLMs, our approach reduces reliance on extensive historical data, while\nremaining flexible, easily updated, and computationally efficient. Our method\nis not only competitive with other LLM-based methods but is also more scalable\nfor real-world POI recommender systems. Results demonstrate that our approach\nconsistently outperforms baseline methods, offering a more interpretable and\nresource-efficient solution for POI recommendation systems. Our source code is\navailable at: https://github.com/w11wo/GenUP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional Point-of-Interest (POI) recommendation systems often lack\ntransparency, interpretability, and scrutability due to their reliance on dense\nvector-based user embeddings. Furthermore, the cold-start problem -- where\nsystems have insufficient data for new users -- limits their ability to\ngenerate accurate recommendations. Existing methods often address this by\nleveraging similar trajectories from other users, but this approach can be\ncomputationally expensive and increases the context length for LLM-based\nmethods, making them difficult to scale. To address these limitations, we\npropose a method that generates natural language (NL) user profiles from\nlarge-scale, location-based social network (LBSN) check-ins, utilizing robust\npersonality assessments and behavioral theories. These NL profiles capture user\npreferences, routines, and behaviors, improving POI prediction accuracy while\noffering enhanced transparency. By incorporating NL profiles as system prompts\nto LLMs, our approach reduces reliance on extensive historical data, while\nremaining flexible, easily updated, and computationally efficient. Our method\nis not only competitive with other LLM-based methods but is also more scalable\nfor real-world POI recommender systems. Results demonstrate that our approach\nconsistently outperforms baseline methods, offering a more interpretable and\nresource-efficient solution for POI recommendation systems. Our source code is\navailable at: https://github.com/w11wo/GenUP/."
                },
                "authors": [
                    {
                        "name": "Wilson Wongso"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20643v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13030v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13030v4",
                "updated": "2025-06-20T04:42:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    4,
                    42,
                    54,
                    4,
                    171,
                    0
                ],
                "published": "2025-02-18T16:46:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization"
                },
                "summary": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark."
                },
                "authors": [
                    {
                        "name": "Sunay Joshi"
                    },
                    {
                        "name": "Shayan Kiyani"
                    },
                    {
                        "name": "George Pappas"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Hamed Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Hassani"
                },
                "author": "Hamed Hassani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13030v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13030v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07674v3",
                "updated": "2025-06-20T03:44:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    44,
                    20,
                    4,
                    171,
                    0
                ],
                "published": "2025-01-13T20:13:59Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    20,
                    13,
                    59,
                    0,
                    13,
                    0
                ],
                "title": "CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive\n  Diagnosis Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDS: Knowledge Component-Driven Data Synthesis Guided by Cognitive\n  Diagnosis Theory"
                },
                "summary": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub."
                },
                "authors": [
                    {
                        "name": "Haokun Zhao"
                    },
                    {
                        "name": "Jinyi Han"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Jiansheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jiansheng Wei"
                },
                "author": "Jiansheng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16724v1",
                "updated": "2025-06-20T03:43:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    43,
                    10,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T03:43:10Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    43,
                    10,
                    4,
                    171,
                    0
                ],
                "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Model Confidence on Bias Effects in Measured Uncertainties"
                },
                "summary": "With the growing adoption of Large Language Models (LLMs) for open-ended\ntasks, accurately assessing epistemic uncertainty, which reflects a model's\nlack of knowledge, has become crucial to ensuring reliable outcomes. However,\nquantifying epistemic uncertainty in such tasks is challenging due to the\npresence of aleatoric uncertainty, which arises from multiple valid answers.\nWhile bias can introduce noise into epistemic uncertainty estimation, it may\nalso reduce noise from aleatoric uncertainty. To investigate this trade-off, we\nconduct experiments on Visual Question Answering (VQA) tasks and find that\nmitigating prompt-introduced bias improves uncertainty quantification in\nGPT-4o. Building on prior work showing that LLMs tend to copy input information\nwhen model confidence is low, we further analyze how these prompt biases affect\nmeasured epistemic and aleatoric uncertainty across varying bias-free\nconfidence levels with GPT-4o and Qwen2-VL. We find that all considered biases\ninduce greater changes in both uncertainties when bias-free model confidence is\nlower. Moreover, lower bias-free model confidence leads to greater\nunderestimation of epistemic uncertainty (i.e. overconfidence) due to bias,\nwhereas it has no significant effect on the direction of changes in aleatoric\nuncertainty estimation. These distinct effects deepen our understanding of bias\nmitigation for uncertainty quantification and potentially inform the\ndevelopment of more advanced techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing adoption of Large Language Models (LLMs) for open-ended\ntasks, accurately assessing epistemic uncertainty, which reflects a model's\nlack of knowledge, has become crucial to ensuring reliable outcomes. However,\nquantifying epistemic uncertainty in such tasks is challenging due to the\npresence of aleatoric uncertainty, which arises from multiple valid answers.\nWhile bias can introduce noise into epistemic uncertainty estimation, it may\nalso reduce noise from aleatoric uncertainty. To investigate this trade-off, we\nconduct experiments on Visual Question Answering (VQA) tasks and find that\nmitigating prompt-introduced bias improves uncertainty quantification in\nGPT-4o. Building on prior work showing that LLMs tend to copy input information\nwhen model confidence is low, we further analyze how these prompt biases affect\nmeasured epistemic and aleatoric uncertainty across varying bias-free\nconfidence levels with GPT-4o and Qwen2-VL. We find that all considered biases\ninduce greater changes in both uncertainties when bias-free model confidence is\nlower. Moreover, lower bias-free model confidence leads to greater\nunderestimation of epistemic uncertainty (i.e. overconfidence) due to bias,\nwhereas it has no significant effect on the direction of changes in aleatoric\nuncertainty estimation. These distinct effects deepen our understanding of bias\nmitigation for uncertainty quantification and potentially inform the\ndevelopment of more advanced techniques."
                },
                "authors": [
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Weiguang Wang"
                    },
                    {
                        "name": "Hangfeng He"
                    }
                ],
                "author_detail": {
                    "name": "Hangfeng He"
                },
                "author": "Hangfeng He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04287v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04287v2",
                "updated": "2025-06-20T03:16:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    16,
                    30,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-04T10:04:21Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    10,
                    4,
                    21,
                    2,
                    155,
                    0
                ],
                "title": "Automated Skill Discovery for Language Agents through Exploration and\n  Iterative Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Skill Discovery for Language Agents through Exploration and\n  Iterative Feedback"
                },
                "summary": "Training large language model (LLM) agents to acquire necessary skills and\nperform diverse tasks within an environment is gaining interest as a means to\nenable open-endedness. However, creating the training dataset for their skill\nacquisition faces several challenges. Manual trajectory collection requires\nsignificant human effort. Another approach, where LLMs directly propose tasks\nto learn, is often invalid, as the LLMs lack knowledge of which tasks are\nactually feasible. Moreover, the generated data may not provide a meaningful\nlearning signal, as agents often already perform well on the proposed tasks. To\naddress this, we propose a novel automatic skill discovery framework EXIF for\nLLM-powered agents, designed to improve the feasibility of generated target\nbehaviors while accounting for the agents' capabilities. Our method adopts an\nexploration-first strategy by employing an exploration agent (Alice) to train\nthe target agent (Bob) to learn essential skills in the environment.\nSpecifically, Alice first interacts with the environment to retrospectively\ngenerate a feasible, environment-grounded skill dataset, which is then used to\ntrain Bob. Crucially, we incorporate an iterative feedback loop, where Alice\nevaluates Bob's performance to identify areas for improvement. This feedback\nthen guides Alice's next round of exploration, forming a closed-loop data\ngeneration process. Experiments on Webshop and Crafter demonstrate EXIF's\nability to effectively discover meaningful skills and iteratively expand the\ncapabilities of the trained agent without any human intervention, achieving\nsubstantial performance improvements. Interestingly, we observe that setting\nAlice to the same model as Bob also notably improves performance, demonstrating\nEXIF's potential for building a self-evolving system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language model (LLM) agents to acquire necessary skills and\nperform diverse tasks within an environment is gaining interest as a means to\nenable open-endedness. However, creating the training dataset for their skill\nacquisition faces several challenges. Manual trajectory collection requires\nsignificant human effort. Another approach, where LLMs directly propose tasks\nto learn, is often invalid, as the LLMs lack knowledge of which tasks are\nactually feasible. Moreover, the generated data may not provide a meaningful\nlearning signal, as agents often already perform well on the proposed tasks. To\naddress this, we propose a novel automatic skill discovery framework EXIF for\nLLM-powered agents, designed to improve the feasibility of generated target\nbehaviors while accounting for the agents' capabilities. Our method adopts an\nexploration-first strategy by employing an exploration agent (Alice) to train\nthe target agent (Bob) to learn essential skills in the environment.\nSpecifically, Alice first interacts with the environment to retrospectively\ngenerate a feasible, environment-grounded skill dataset, which is then used to\ntrain Bob. Crucially, we incorporate an iterative feedback loop, where Alice\nevaluates Bob's performance to identify areas for improvement. This feedback\nthen guides Alice's next round of exploration, forming a closed-loop data\ngeneration process. Experiments on Webshop and Crafter demonstrate EXIF's\nability to effectively discover meaningful skills and iteratively expand the\ncapabilities of the trained agent without any human intervention, achieving\nsubstantial performance improvements. Interestingly, we observe that setting\nAlice to the same model as Bob also notably improves performance, demonstrating\nEXIF's potential for building a self-evolving system."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Sinjae Kang"
                    },
                    {
                        "name": "Juyong Lee"
                    },
                    {
                        "name": "Dongjun Lee"
                    },
                    {
                        "name": "Se-Young Yun"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04287v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04287v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22960v2",
                "updated": "2025-06-20T03:07:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    7,
                    38,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-29T01:02:55Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    2,
                    55,
                    3,
                    149,
                    0
                ],
                "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study\n  of Conditional Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study\n  of Conditional Effectiveness"
                },
                "summary": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Euiin Yi"
                    },
                    {
                        "name": "Jongwoo Ko"
                    },
                    {
                        "name": "Kimin Lee"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17143v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17143v4",
                "updated": "2025-06-20T03:04:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    3,
                    4,
                    52,
                    4,
                    171,
                    0
                ],
                "published": "2023-10-26T04:35:00Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    4,
                    35,
                    0,
                    3,
                    299,
                    0
                ],
                "title": "Techniques for supercharging academic writing with generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques for supercharging academic writing with generative AI"
                },
                "summary": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic writing is an indispensable yet laborious part of the research\nenterprise. This Perspective maps out principles and methods for using\ngenerative artificial intelligence (AI), specifically large language models\n(LLMs), to elevate the quality and efficiency of academic writing. We introduce\na human-AI collaborative framework that delineates the rationale (why), process\n(how), and nature (what) of AI engagement in writing. The framework pinpoints\nboth short-term and long-term reasons for engagement and their underlying\nmechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals\nthe role of AI throughout the writing process, conceptualized through a\ntwo-stage model for human-AI collaborative writing, and the nature of AI\nassistance in writing, represented through a model of writing-assistance types\nand levels. Building on this framework, we describe effective prompting\ntechniques for incorporating AI into the writing routine (outlining, drafting,\nand editing) as well as strategies for maintaining rigorous scholarship,\nadhering to varied journal policies, and avoiding overreliance on AI.\nUltimately, the prudent integration of AI into academic writing can ease the\ncommunication burden, empower authors, accelerate discovery, and promote\ndiversity in science."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_doi": "10.1038/s41551-024-01185-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41551-024-01185-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.17143v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17143v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: Nature Biomedical Engineering, 2025",
                "arxiv_journal_ref": "Nature Biomedical Engineering, 9, 426-431 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v6",
                "updated": "2025-06-20T02:59:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    59,
                    45,
                    4,
                    171,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a \"Triple-Too\" problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches--principlism (reliance on abstract ethical\nprinciples), formalism (rigid application of rules), and technological\nsolutionism (overemphasis on technological fixes)--offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a \"Triple-Too\" problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches--principlism (reliance on abstract ethical\nprinciples), formalism (rigid application of rules), and technological\nsolutionism (overemphasis on technological fixes)--offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_doi": "10.1007/s43681-024-00585-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s43681-024-00585-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15284v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: AI and Ethics, 2025",
                "arxiv_journal_ref": "AI and Ethics, 5, 2719-2731 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04470v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04470v5",
                "updated": "2025-06-20T02:50:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    50,
                    33,
                    4,
                    171,
                    0
                ],
                "published": "2024-02-06T23:28:23Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    23,
                    28,
                    23,
                    1,
                    37,
                    0
                ],
                "title": "Six Fallacies in Substituting Large Language Models for Human\n  Participants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Six Fallacies in Substituting Large Language Models for Human\n  Participants"
                },
                "summary": "Can AI systems like large language models (LLMs) replace human participants\nin behavioral and psychological research? Here I critically evaluate the\n\"replacement\" perspective and identify six interpretive fallacies that\nundermine its validity. These fallacies are: (1) equating token prediction with\nhuman intelligence, (2) treating LLMs as the average human, (3) interpreting\nalignment as explanation, (4) anthropomorphizing AI systems, (5) essentializing\nidentities, and (6) substituting model data for human evidence. Each fallacy\nrepresents a potential misunderstanding about what LLMs are and what they can\ntell us about human cognition. The analysis distinguishes levels of similarity\nbetween LLMs and humans, particularly functional equivalence (outputs) versus\nmechanistic equivalence (processes), while highlighting both technical\nlimitations (addressable through engineering) and conceptual limitations\n(arising from fundamental differences between statistical and biological\nintelligence). For each fallacy, specific safeguards are provided to guide\nresponsible research practices. Ultimately, the analysis supports\nconceptualizing LLMs as pragmatic simulation tools--useful for role-play, rapid\nhypothesis testing, and computational modeling (provided their outputs are\nvalidated against human data)--rather than as replacements for human\nparticipants. This framework enables researchers to leverage language models\nproductively while respecting the fundamental differences between machine\nintelligence and human thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI systems like large language models (LLMs) replace human participants\nin behavioral and psychological research? Here I critically evaluate the\n\"replacement\" perspective and identify six interpretive fallacies that\nundermine its validity. These fallacies are: (1) equating token prediction with\nhuman intelligence, (2) treating LLMs as the average human, (3) interpreting\nalignment as explanation, (4) anthropomorphizing AI systems, (5) essentializing\nidentities, and (6) substituting model data for human evidence. Each fallacy\nrepresents a potential misunderstanding about what LLMs are and what they can\ntell us about human cognition. The analysis distinguishes levels of similarity\nbetween LLMs and humans, particularly functional equivalence (outputs) versus\nmechanistic equivalence (processes), while highlighting both technical\nlimitations (addressable through engineering) and conceptual limitations\n(arising from fundamental differences between statistical and biological\nintelligence). For each fallacy, specific safeguards are provided to guide\nresponsible research practices. Ultimately, the analysis supports\nconceptualizing LLMs as pragmatic simulation tools--useful for role-play, rapid\nhypothesis testing, and computational modeling (provided their outputs are\nvalidated against human data)--rather than as replacements for human\nparticipants. This framework enables researchers to leverage language models\nproductively while respecting the fundamental differences between machine\nintelligence and human thought."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04470v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04470v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16702v1",
                "updated": "2025-06-20T02:45:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    45,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T02:45:23Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    45,
                    23,
                    4,
                    171,
                    0
                ],
                "title": "Large Language Models as Psychological Simulators: A Methodological\n  Guide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Psychological Simulators: A Methodological\n  Guide"
                },
                "summary": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02404v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02404v3",
                "updated": "2025-06-20T02:42:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    42,
                    32,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-03T03:44:26Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    44,
                    26,
                    1,
                    154,
                    0
                ],
                "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating\n  Graph Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating\n  Graph Retrieval-Augmented Generation"
                },
                "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community."
                },
                "authors": [
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Qian-wen Zhang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02404v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02404v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16699v1",
                "updated": "2025-06-20T02:41:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    41,
                    23,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T02:41:23Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    41,
                    23,
                    4,
                    171,
                    0
                ],
                "title": "Exploring Traffic Simulation and Cybersecurity Strategies Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Traffic Simulation and Cybersecurity Strategies Using Large\n  Language Models"
                },
                "summary": "Intelligent Transportation Systems (ITS) are increasingly vulnerable to\nsophisticated cyberattacks due to their complex, interconnected nature.\nEnsuring the cybersecurity of these systems is paramount to maintaining road\nsafety and minimizing traffic disruptions. This study presents a novel\nmulti-agent framework leveraging Large Language Models (LLMs) to enhance\ntraffic simulation and cybersecurity testing. The framework automates the\ncreation of traffic scenarios, the design of cyberattack strategies, and the\ndevelopment of defense mechanisms. A case study demonstrates the framework's\nability to simulate a cyberattack targeting connected vehicle broadcasts,\nevaluate its impact, and implement a defense mechanism that significantly\nmitigates traffic delays. Results show a 10.2 percent increase in travel time\nduring an attack, which is reduced by 3.3 percent with the defense strategy.\nThis research highlights the potential of LLM-driven multi-agent systems in\nadvancing transportation cybersecurity and offers a scalable approach for\nfuture research in traffic simulation and cyber defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Transportation Systems (ITS) are increasingly vulnerable to\nsophisticated cyberattacks due to their complex, interconnected nature.\nEnsuring the cybersecurity of these systems is paramount to maintaining road\nsafety and minimizing traffic disruptions. This study presents a novel\nmulti-agent framework leveraging Large Language Models (LLMs) to enhance\ntraffic simulation and cybersecurity testing. The framework automates the\ncreation of traffic scenarios, the design of cyberattack strategies, and the\ndevelopment of defense mechanisms. A case study demonstrates the framework's\nability to simulate a cyberattack targeting connected vehicle broadcasts,\nevaluate its impact, and implement a defense mechanism that significantly\nmitigates traffic delays. Results show a 10.2 percent increase in travel time\nduring an attack, which is reduced by 3.3 percent with the defense strategy.\nThis research highlights the potential of LLM-driven multi-agent systems in\nadvancing transportation cybersecurity and offers a scalable approach for\nfuture research in traffic simulation and cyber defense."
                },
                "authors": [
                    {
                        "name": "Lu Gao"
                    },
                    {
                        "name": "Yongxin Liu"
                    },
                    {
                        "name": "Hongyun Chen"
                    },
                    {
                        "name": "Dahai Liu"
                    },
                    {
                        "name": "Yunpeng Zhang"
                    },
                    {
                        "name": "Jingran Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jingran Sun"
                },
                "author": "Jingran Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16697v1",
                "updated": "2025-06-20T02:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    38,
                    42,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T02:38:42Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    38,
                    42,
                    4,
                    171,
                    0
                ],
                "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research\n  in Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research\n  in Psychology"
                },
                "summary": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10486v2",
                "updated": "2025-06-20T02:33:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    2,
                    33,
                    3,
                    4,
                    171,
                    0
                ],
                "published": "2025-03-13T15:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    54,
                    26,
                    3,
                    72,
                    0
                ],
                "title": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3\n  Mini Across Chronic Health Conditions"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing medical diagnostics by\nenhancing both disease classification and clinical decision-making. In this\nstudy, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek\nR1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We\nassessed their predictive accuracy at both the disease and category levels, as\nwell as the reliability of their confidence scores. DeepSeek R1 achieved a\ndisease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3\nMini, which attained 72% and 75% respectively. Notably, DeepSeek R1\ndemonstrated exceptional performance in Mental Health, Neurological Disorders,\nand Oncology, where it reached 100% accuracy, while O3 Mini excelled in\nAutoimmune Disease classification with 100% accuracy. Both models, however,\nstruggled with Respiratory Disease classification, recording accuracies of only\n40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of\nconfidence scores revealed that DeepSeek R1 provided high-confidence\npredictions in 92% of cases, compared to 68% for O3 Mini. Ethical\nconsiderations regarding bias, model interpretability, and data privacy are\nalso discussed to ensure the responsible integration of LLMs into clinical\npractice. Overall, our findings offer valuable insights into the strengths and\nlimitations of LLM-based diagnostic systems and provide a roadmap for future\nenhancements in AI-driven healthcare."
                },
                "authors": [
                    {
                        "name": "Gaurav Kumar Gupta"
                    },
                    {
                        "name": "Pranal Pande"
                    },
                    {
                        "name": "Nirajan Acharya"
                    },
                    {
                        "name": "Aniket Kumar Singh"
                    },
                    {
                        "name": "Suman Niroula"
                    }
                ],
                "author_detail": {
                    "name": "Suman Niroula"
                },
                "author": "Suman Niroula",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]