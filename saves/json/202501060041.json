[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v1",
                "updated": "2025-01-01T10:50:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rsler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v1",
                "updated": "2024-12-29T15:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v1",
                "updated": "2024-12-27T20:47:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jimnez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01428v1",
                "updated": "2025-01-02T18:59:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:59Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    59,
                    3,
                    2,
                    0
                ],
                "title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models"
                },
                "summary": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Zhangyang Qi"
                    },
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Ye Fang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "Project page: https://gpt4scene.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01426v1",
                "updated": "2025-01-02T18:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "title": "Unifying Specialized Visual Encoders for Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Specialized Visual Encoders for Video Language Models"
                },
                "summary": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding."
                },
                "authors": [
                    {
                        "name": "Jihoon Chung"
                    },
                    {
                        "name": "Tyler Zhu"
                    },
                    {
                        "name": "Max Gonzalez Saez-Diez"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Olga Russakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Olga Russakovsky"
                },
                "author": "Olga Russakovsky",
                "arxiv_comment": "Project page: https://tylerzhu.com/merv/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v2",
                "updated": "2025-01-02T18:46:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    46,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "This version has been updated with further clarification regarding\n  the model size estimates that were mined from public articles only and\n  provided to aid in contextualizing model performance. The authors cannot\n  vouch for the accuracy of those estimates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20280v2",
                "updated": "2025-01-02T18:31:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    31,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2024-03-29T16:49:40Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    16,
                    49,
                    40,
                    4,
                    89,
                    0
                ],
                "title": "Sparsely Multimodal Data Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsely Multimodal Data Fusion"
                },
                "summary": "Multimodal data fusion is essential for applications requiring the\nintegration of diverse data sources, especially in the presence of incomplete\nor sparsely available modalities. This paper presents a comparative study of\nthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,\nand Everything at Once (EAO), to evaluate their performance on sparsely\nmultimodal data. MCA introduces fusion embeddings for all combinations of input\nmodalities and uses attention masking to create distinct attention channels,\nenabling flexible and efficient data fusion. Experiments on two datasets with\nfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperforms\nZorro across ranking, recall, regression, and classification tasks and\noutperforms EAO across regression and classification tasks. MCA achieves\nsuperior performance by maintaining robust uniformity across unimodal and\nfusion embeddings. While EAO performs best in ranking metrics due to its\napproach of forming fusion embeddings post-inference, it underperforms in\ndownstream tasks requiring multimodal interactions. These results highlight the\nimportance of contrasting all modality combinations in constructing embedding\nspaces and offers insights into the design of multimodal architectures for\nreal-world applications with incomplete data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal data fusion is essential for applications requiring the\nintegration of diverse data sources, especially in the presence of incomplete\nor sparsely available modalities. This paper presents a comparative study of\nthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,\nand Everything at Once (EAO), to evaluate their performance on sparsely\nmultimodal data. MCA introduces fusion embeddings for all combinations of input\nmodalities and uses attention masking to create distinct attention channels,\nenabling flexible and efficient data fusion. Experiments on two datasets with\nfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperforms\nZorro across ranking, recall, regression, and classification tasks and\noutperforms EAO across regression and classification tasks. MCA achieves\nsuperior performance by maintaining robust uniformity across unimodal and\nfusion embeddings. While EAO performs best in ranking metrics due to its\napproach of forming fusion embeddings post-inference, it underperforms in\ndownstream tasks requiring multimodal interactions. These results highlight the\nimportance of contrasting all modality combinations in constructing embedding\nspaces and offers insights into the design of multimodal architectures for\nreal-world applications with incomplete data."
                },
                "authors": [
                    {
                        "name": "Josiah Bjorgaard"
                    }
                ],
                "author_detail": {
                    "name": "Josiah Bjorgaard"
                },
                "author": "Josiah Bjorgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01366v1",
                "updated": "2025-01-02T17:20:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T17:20:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding"
                },
                "summary": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications."
                },
                "authors": [
                    {
                        "name": "Austin T. Wang"
                    },
                    {
                        "name": "ZeMing Gong"
                    },
                    {
                        "name": "Angel X. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Angel X. Chang"
                },
                "author": "Angel X. Chang",
                "arxiv_comment": "20 pages with 5 figures and 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v3",
                "updated": "2025-01-02T17:17:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    17,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "9 pages, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01346v1",
                "updated": "2025-01-02T16:53:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:53:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and linguistic representations\nis not fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and linguistic representations\nis not fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Weiru Liu"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01336v1",
                "updated": "2025-01-02T16:38:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:38:21Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "title": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git"
                },
                "authors": [
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01335v1",
                "updated": "2025-01-02T16:37:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:37:04Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models"
                },
                "summary": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Johan Wahrus"
                    },
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01332v1",
                "updated": "2025-01-02T16:34:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:34:10Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "title": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension"
                },
                "summary": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers."
                },
                "authors": [
                    {
                        "name": "Yanbo Fang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01329v1",
                "updated": "2025-01-02T16:30:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:30:05Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation"
                },
                "summary": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task."
                },
                "authors": [
                    {
                        "name": "Shuzheng Gao"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xiaoqian Jiao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Michael Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lyu"
                },
                "author": "Michael Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01312v1",
                "updated": "2025-01-02T15:53:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:53:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Learning Spectral Methods by Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spectral Methods by Transformers"
                },
                "summary": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Yihan He"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Hong-Yu Chen"
                    },
                    {
                        "name": "Dennis Wu"
                    },
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "77 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01306v1",
                "updated": "2025-01-02T15:36:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:36:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02064v2",
                "updated": "2025-01-02T15:34:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    23,
                    3,
                    2,
                    0
                ],
                "published": "2024-11-04T13:06:46Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    6,
                    46,
                    0,
                    309,
                    0
                ],
                "title": "Amortized Bayesian Experimental Design for Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Experimental Design for Decision-Making"
                },
                "summary": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many critical decisions, such as personalized medical diagnoses and product\npricing, are made based on insights gained from designing, observing, and\nanalyzing a series of experiments. This highlights the crucial role of\nexperimental design, which goes beyond merely collecting information on system\nparameters as in traditional Bayesian experimental design (BED), but also plays\na key part in facilitating downstream decision-making. Most recent BED methods\nuse an amortized policy network to rapidly design experiments. However, the\ninformation gathered through these methods is suboptimal for down-the-line\ndecision-making, as the experiments are not inherently designed with downstream\nobjectives in mind. In this paper, we present an amortized decision-aware BED\nframework that prioritizes maximizing downstream decision utility. We introduce\na novel architecture, the Transformer Neural Decision Process (TNDP), capable\nof instantly proposing the next experimental design, whilst inferring the\ndownstream decision, thus effectively amortizing both tasks within a unified\nworkflow. We demonstrate the performance of our method across several tasks,\nshowing that it can deliver informative designs and facilitate accurate\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Yujia Guo"
                    },
                    {
                        "name": "Luigi Acerbi"
                    },
                    {
                        "name": "Samuel Kaski"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kaski"
                },
                "author": "Samuel Kaski",
                "arxiv_comment": "20 pages, 6 figures. Accepted at the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01305v1",
                "updated": "2025-01-02T15:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:34:02Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "title": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case"
                },
                "summary": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b."
                },
                "authors": [
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Harshul Surana"
                    },
                    {
                        "name": "Darssan Eswaramoorthi"
                    },
                    {
                        "name": "Yuxin Zi"
                    },
                    {
                        "name": "Vedant Palit"
                    },
                    {
                        "name": "Ritvik Garimella"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01303v1",
                "updated": "2025-01-02T15:32:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Citations and Trust in LLM Generated Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citations and Trust in LLM Generated Responses"
                },
                "summary": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Matthew Facciani"
                    },
                    {
                        "name": "Amrit Poudel"
                    },
                    {
                        "name": "Ellen Joyce"
                    },
                    {
                        "name": "Salvador Aguinaga"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    },
                    {
                        "name": "Tim Weninger"
                    }
                ],
                "author_detail": {
                    "name": "Tim Weninger"
                },
                "author": "Tim Weninger",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01290v1",
                "updated": "2025-01-02T15:10:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    10,
                    52,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:10:52Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    10,
                    52,
                    3,
                    2,
                    0
                ],
                "title": "ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark"
                },
                "summary": "Despite recent advances in AI, the development of systems capable of\nexecuting complex, multi-step reasoning tasks involving multiple tools remains\na significant challenge. Current benchmarks fall short in capturing the\nreal-world complexity of tool-use reasoning, where verifying the correctness of\nnot only the final answer but also the intermediate steps is important for\nevaluation, development, and identifying failures during inference time. To\nbridge this gap, we introduce ToolComp, a comprehensive benchmark designed to\nevaluate multi-step tool-use reasoning. ToolComp is developed through a\ncollaboration between models and human annotators, featuring\nhuman-edited/verified prompts, final answers, and process supervision labels,\nallowing for the evaluation of both final outcomes and intermediate reasoning.\nEvaluation across six different model families demonstrates the challenging\nnature of our dataset, with the majority of models achieving less than 50%\naccuracy. Additionally, we generate synthetic training data to compare the\nperformance of outcome-supervised reward models (ORMs) with process-supervised\nreward models (PRMs) to assess their ability to improve complex tool-use\nreasoning as evaluated by ToolComp. Our results show that PRMs generalize\nsignificantly better than ORMs, achieving a 19% and 11% improvement in rank@1\naccuracy for ranking base and fine-tuned model trajectories, respectively.\nThese findings highlight the critical role of process supervision in both the\nevaluation and training of AI models, paving the way for more robust and\ncapable systems in complex, multi-step tool-use tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in AI, the development of systems capable of\nexecuting complex, multi-step reasoning tasks involving multiple tools remains\na significant challenge. Current benchmarks fall short in capturing the\nreal-world complexity of tool-use reasoning, where verifying the correctness of\nnot only the final answer but also the intermediate steps is important for\nevaluation, development, and identifying failures during inference time. To\nbridge this gap, we introduce ToolComp, a comprehensive benchmark designed to\nevaluate multi-step tool-use reasoning. ToolComp is developed through a\ncollaboration between models and human annotators, featuring\nhuman-edited/verified prompts, final answers, and process supervision labels,\nallowing for the evaluation of both final outcomes and intermediate reasoning.\nEvaluation across six different model families demonstrates the challenging\nnature of our dataset, with the majority of models achieving less than 50%\naccuracy. Additionally, we generate synthetic training data to compare the\nperformance of outcome-supervised reward models (ORMs) with process-supervised\nreward models (PRMs) to assess their ability to improve complex tool-use\nreasoning as evaluated by ToolComp. Our results show that PRMs generalize\nsignificantly better than ORMs, achieving a 19% and 11% improvement in rank@1\naccuracy for ranking base and fine-tuned model trajectories, respectively.\nThese findings highlight the critical role of process supervision in both the\nevaluation and training of AI models, paving the way for more robust and\ncapable systems in complex, multi-step tool-use tasks."
                },
                "authors": [
                    {
                        "name": "Vaskar Nath"
                    },
                    {
                        "name": "Pranav Raja"
                    },
                    {
                        "name": "Claire Yoon"
                    },
                    {
                        "name": "Sean Hendryx"
                    }
                ],
                "author_detail": {
                    "name": "Sean Hendryx"
                },
                "author": "Sean Hendryx",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21079v2",
                "updated": "2025-01-02T15:00:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    0,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-30T16:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    56,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "Edicho: Consistent Image Editing in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edicho: Consistent Image Editing in the Wild"
                },
                "summary": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies."
                },
                "authors": [
                    {
                        "name": "Qingyan Bai"
                    },
                    {
                        "name": "Hao Ouyang"
                    },
                    {
                        "name": "Yinghao Xu"
                    },
                    {
                        "name": "Qiuyu Wang"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ka Leong Cheng"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "Project page: https://github.com/EzioBy/edicho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01273v1",
                "updated": "2025-01-02T14:13:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T14:13:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Does a Large Language Model Really Speak in Human-Like Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does a Large Language Model Really Speak in Human-Like Language?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text."
                },
                "authors": [
                    {
                        "name": "Mose Park"
                    },
                    {
                        "name": "Yunjin Choi"
                    },
                    {
                        "name": "Jong-June Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jong-June Jeon"
                },
                "author": "Jong-June Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01264v1",
                "updated": "2025-01-02T13:59:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:59:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgCo: Program Helps Self-Correction of Large Language Models"
                },
                "summary": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools."
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01887v3",
                "updated": "2025-01-02T13:49:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-02T02:18:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    18,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"
                },
                "summary": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Fanzeng Xia"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01257v1",
                "updated": "2025-01-02T13:49:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings"
                },
                "summary": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 20 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 20 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07640v3",
                "updated": "2025-01-02T13:46:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    46,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-05-13T11:00:25Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    11,
                    0,
                    25,
                    0,
                    134,
                    0
                ],
                "title": "Hyperparameter Importance Analysis for Multi-Objective AutoML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter Importance Analysis for Multi-Objective AutoML"
                },
                "summary": "Hyperparameter optimization plays a pivotal role in enhancing the predictive\nperformance and generalization capabilities of ML models. However, in many\napplications, we do not only care about predictive performance but also about\nadditional objectives such as inference time, memory, or energy consumption. In\nsuch multi-objective scenarios, determining the importance of hyperparameters\nposes a significant challenge due to the complex interplay between the\nconflicting objectives. In this paper, we propose the first method for\nassessing the importance of hyperparameters in multi-objective hyperparameter\noptimization. Our approach leverages surrogate-based hyperparameter importance\nmeasures, i.e., fANOVA and ablation paths, to provide insights into the impact\nof hyperparameters on the optimization objectives. Specifically, we compute the\na-priori scalarization of the objectives and determine the importance of the\nhyperparameters for different objective tradeoffs. Through extensive empirical\nevaluations on diverse benchmark datasets with three different objective pairs,\neach combined with accuracy, namely time, demographic parity loss, and energy\nconsumption, we demonstrate the effectiveness and robustness of our proposed\nmethod. Our findings not only offer valuable guidance for hyperparameter tuning\nin multi-objective optimization tasks but also contribute to advancing the\nunderstanding of hyperparameter importance in complex optimization scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperparameter optimization plays a pivotal role in enhancing the predictive\nperformance and generalization capabilities of ML models. However, in many\napplications, we do not only care about predictive performance but also about\nadditional objectives such as inference time, memory, or energy consumption. In\nsuch multi-objective scenarios, determining the importance of hyperparameters\nposes a significant challenge due to the complex interplay between the\nconflicting objectives. In this paper, we propose the first method for\nassessing the importance of hyperparameters in multi-objective hyperparameter\noptimization. Our approach leverages surrogate-based hyperparameter importance\nmeasures, i.e., fANOVA and ablation paths, to provide insights into the impact\nof hyperparameters on the optimization objectives. Specifically, we compute the\na-priori scalarization of the objectives and determine the importance of the\nhyperparameters for different objective tradeoffs. Through extensive empirical\nevaluations on diverse benchmark datasets with three different objective pairs,\neach combined with accuracy, namely time, demographic parity loss, and energy\nconsumption, we demonstrate the effectiveness and robustness of our proposed\nmethod. Our findings not only offer valuable guidance for hyperparameter tuning\nin multi-objective optimization tasks but also contribute to advancing the\nunderstanding of hyperparameter importance in complex optimization scenarios."
                },
                "authors": [
                    {
                        "name": "Daphne Theodorakopoulos"
                    },
                    {
                        "name": "Frederic Stahl"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_doi": "10.3233/FAIA240602",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240602",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Presented at the 27th European Conference on Artificial Intelligence,\n  19-24 October 2024, Santiago de Compostela, Spain",
                "arxiv_journal_ref": "Frontiers in Artificial Intelligence and Applications: Proceedings\n  of the European Conference on AI (ECAI) 2024, vol. 392, 1100-1107, IOS Press",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17440v2",
                "updated": "2025-01-02T13:32:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    32,
                    4,
                    3,
                    2,
                    0
                ],
                "published": "2023-10-26T14:50:07Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    14,
                    50,
                    7,
                    3,
                    299,
                    0
                ],
                "title": "Gibbs optimal design of experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gibbs optimal design of experiments"
                },
                "summary": "Bayesian optimal design is a well-established approach to planning\nexperiments. A distribution for the responses, i.e. a statistical model, is\nassumed which is dependent on unknown parameters. A utility function is then\nspecified giving gain in information in estimating the true values of the\nparameters, using the Bayesian posterior distribution. A Bayesian optimal\ndesign is given by maximising expectation of the utility with respect to the\ndistribution implied by statistical model and prior distribution for the true\nparameter values. The approach accounts for the experimental aim, via\nspecification of the utility, and of assumed sources of uncertainty. However,\nit is predicated on the statistical model being correct. Recently, a new type\nof statistical inference, known as Gibbs inference, has been proposed. This is\nBayesian-like, i.e. uncertainty for unknown quantities is represented by a\nposterior distribution, but does not necessarily require specification of a\nstatistical model. The resulting inference is less sensitive to\nmisspecification of the statistical model. This paper introduces Gibbs optimal\ndesign: a framework for optimal design of experiments under Gibbs inference. A\ncomputational approach to find designs in practice is outlined and the\nframework is demonstrated on exemplars including linear models, and experiments\nwith count and time-to-event responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimal design is a well-established approach to planning\nexperiments. A distribution for the responses, i.e. a statistical model, is\nassumed which is dependent on unknown parameters. A utility function is then\nspecified giving gain in information in estimating the true values of the\nparameters, using the Bayesian posterior distribution. A Bayesian optimal\ndesign is given by maximising expectation of the utility with respect to the\ndistribution implied by statistical model and prior distribution for the true\nparameter values. The approach accounts for the experimental aim, via\nspecification of the utility, and of assumed sources of uncertainty. However,\nit is predicated on the statistical model being correct. Recently, a new type\nof statistical inference, known as Gibbs inference, has been proposed. This is\nBayesian-like, i.e. uncertainty for unknown quantities is represented by a\nposterior distribution, but does not necessarily require specification of a\nstatistical model. The resulting inference is less sensitive to\nmisspecification of the statistical model. This paper introduces Gibbs optimal\ndesign: a framework for optimal design of experiments under Gibbs inference. A\ncomputational approach to find designs in practice is outlined and the\nframework is demonstrated on exemplars including linear models, and experiments\nwith count and time-to-event responses."
                },
                "authors": [
                    {
                        "name": "Antony M. Overstall"
                    },
                    {
                        "name": "Jacinta Holloway-Brown"
                    },
                    {
                        "name": "James M. McGree"
                    }
                ],
                "author_detail": {
                    "name": "James M. McGree"
                },
                "author": "James M. McGree",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01246v1",
                "updated": "2025-01-02T13:14:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    14,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:14:28Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    14,
                    28,
                    3,
                    2,
                    0
                ],
                "title": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base\n  Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base\n  Completion"
                },
                "summary": "Integrating large language models (LLMs) with rule-based reasoning offers a\npowerful solution for improving the flexibility and reliability of Knowledge\nBase Completion (KBC). Traditional rule-based KBC methods offer verifiable\nreasoning yet lack flexibility, while LLMs provide strong semantic\nunderstanding yet suffer from hallucinations. With the aim of combining LLMs'\nunderstanding capability with the logical and rigor of rule-based approaches,\nwe propose a novel framework consisting of a Subgraph Extractor, an LLM\nProposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs\nfrom the KB. Then, the LLM uses these subgraphs to propose diverse and\nmeaningful rules that are helpful for inferring missing facts. To effectively\navoid hallucination in LLMs' generations, these proposed rules are further\nrefined by a Rule Reasoner to pinpoint the most significant rules in the KB for\nKnowledge Base Completion. Our approach offers several key benefits: the\nutilization of LLMs to enhance the richness and diversity of the proposed rules\nand the integration with rule-based reasoning to improve reliability. Our\nmethod also demonstrates strong performance across diverse KB datasets,\nhighlighting the robustness and generalizability of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) with rule-based reasoning offers a\npowerful solution for improving the flexibility and reliability of Knowledge\nBase Completion (KBC). Traditional rule-based KBC methods offer verifiable\nreasoning yet lack flexibility, while LLMs provide strong semantic\nunderstanding yet suffer from hallucinations. With the aim of combining LLMs'\nunderstanding capability with the logical and rigor of rule-based approaches,\nwe propose a novel framework consisting of a Subgraph Extractor, an LLM\nProposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs\nfrom the KB. Then, the LLM uses these subgraphs to propose diverse and\nmeaningful rules that are helpful for inferring missing facts. To effectively\navoid hallucination in LLMs' generations, these proposed rules are further\nrefined by a Rule Reasoner to pinpoint the most significant rules in the KB for\nKnowledge Base Completion. Our approach offers several key benefits: the\nutilization of LLMs to enhance the richness and diversity of the proposed rules\nand the integration with rule-based reasoning to improve reliability. Our\nmethod also demonstrates strong performance across diverse KB datasets,\nhighlighting the robustness and generalizability of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Qiyuan He"
                    },
                    {
                        "name": "Jianfei Yu"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01245v1",
                "updated": "2025-01-02T13:12:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    12,
                    12,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:12:12Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    12,
                    12,
                    3,
                    2,
                    0
                ],
                "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal\n  Perturbation and Learning Stabilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal\n  Perturbation and Learning Stabilization"
                },
                "summary": "Human action understanding is crucial for the advancement of multimodal\nsystems. While recent developments, driven by powerful large language models\n(LLMs), aim to be general enough to cover a wide range of categories, they\noften overlook the need for more specific capabilities. In this work, we\naddress the more challenging task of Fine-grained Action Recognition (FAR),\nwhich focuses on detailed semantic labels within shorter temporal duration\n(e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating\nfine-grained labels and the substantial data needed for fine-tuning LLMs, we\npropose to adopt semi-supervised learning (SSL). Our framework, SeFAR,\nincorporates several innovative designs to tackle these challenges.\nSpecifically, to capture sufficient visual details, we construct Dual-level\ntemporal elements as more effective representations, based on which we design a\nnew strong augmentation strategy for the Teacher-Student learning paradigm\nthrough involving moderate temporal perturbation. Furthermore, to handle the\nhigh uncertainty within the teacher model's predictions for FAR, we propose the\nAdaptive Regulation to stabilize the learning process. Experiments show that\nSeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and\nFineDiving, across various data scopes. It also outperforms other\nsemi-supervised methods on two classical coarse-grained datasets, UCF101 and\nHMDB51. Further analysis and ablation studies validate the effectiveness of our\ndesigns. Additionally, we show that the features extracted by our SeFAR could\nlargely promote the ability of multimodal foundation models to understand\nfine-grained and domain-specific semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human action understanding is crucial for the advancement of multimodal\nsystems. While recent developments, driven by powerful large language models\n(LLMs), aim to be general enough to cover a wide range of categories, they\noften overlook the need for more specific capabilities. In this work, we\naddress the more challenging task of Fine-grained Action Recognition (FAR),\nwhich focuses on detailed semantic labels within shorter temporal duration\n(e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating\nfine-grained labels and the substantial data needed for fine-tuning LLMs, we\npropose to adopt semi-supervised learning (SSL). Our framework, SeFAR,\nincorporates several innovative designs to tackle these challenges.\nSpecifically, to capture sufficient visual details, we construct Dual-level\ntemporal elements as more effective representations, based on which we design a\nnew strong augmentation strategy for the Teacher-Student learning paradigm\nthrough involving moderate temporal perturbation. Furthermore, to handle the\nhigh uncertainty within the teacher model's predictions for FAR, we propose the\nAdaptive Regulation to stabilize the learning process. Experiments show that\nSeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and\nFineDiving, across various data scopes. It also outperforms other\nsemi-supervised methods on two classical coarse-grained datasets, UCF101 and\nHMDB51. Further analysis and ablation studies validate the effectiveness of our\ndesigns. Additionally, we show that the features extracted by our SeFAR could\nlargely promote the ability of multimodal foundation models to understand\nfine-grained and domain-specific semantics."
                },
                "authors": [
                    {
                        "name": "Yongle Huang"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zhenbang Xu"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Haozhou Sun"
                    },
                    {
                        "name": "Dian Shao"
                    }
                ],
                "author_detail": {
                    "name": "Dian Shao"
                },
                "author": "Dian Shao",
                "arxiv_comment": "AAAI 2025; Code: https://github.com/KyleHuang9/SeFAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v4",
                "updated": "2025-01-02T13:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    11,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01237v1",
                "updated": "2025-01-02T12:55:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T12:55:27Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "title": "Automated Self-Refinement and Self-Correction for LLM-based Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Self-Refinement and Self-Correction for LLM-based Product\n  Attribute Value Extraction"
                },
                "summary": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques, error-based prompt rewriting and self-correction,\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques have only a marginal impact on the model's performance across the\ndifferent scenarios, while significantly increasing processing costs. For\nscenarios with training data, fine-tuning yields the highest performance, while\nthe ramp-up costs of fine-tuning are balanced out as the amount of product\ndescriptions increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques, error-based prompt rewriting and self-correction,\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques have only a marginal impact on the model's performance across the\ndifferent scenarios, while significantly increasing processing costs. For\nscenarios with training data, fine-tuning yields the highest performance, while\nthe ramp-up costs of fine-tuning are balanced out as the amount of product\ndescriptions increases."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01235v1",
                "updated": "2025-01-02T12:51:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    51,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T12:51:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    51,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "SVFR: A Unified Framework for Generalized Video Face Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVFR: A Unified Framework for Generalized Video Face Restoration"
                },
                "summary": "Face Restoration (FR) is a crucial area within image and video processing,\nfocusing on reconstructing high-quality portraits from degraded inputs. Despite\nadvancements in image FR, video FR remains relatively under-explored, primarily\ndue to challenges related to temporal consistency, motion artifacts, and the\nlimited availability of high-quality video data. Moreover, traditional face\nrestoration typically prioritizes enhancing resolution and may not give as much\nconsideration to related tasks such as facial colorization and inpainting. In\nthis paper, we propose a novel approach for the Generalized Video Face\nRestoration (GVFR) task, which integrates video BFR, inpainting, and\ncolorization tasks that we empirically show to benefit each other. We present a\nunified framework, termed as stable video face restoration (SVFR), which\nleverages the generative and motion priors of Stable Video Diffusion (SVD) and\nincorporates task-specific information through a unified face restoration\nframework. A learnable task embedding is introduced to enhance task\nidentification. Meanwhile, a novel Unified Latent Regularization (ULR) is\nemployed to encourage the shared feature representation learning among\ndifferent subtasks. To further enhance the restoration quality and temporal\nstability, we introduce the facial prior learning and the self-referred\nrefinement as auxiliary strategies used for both training and inference. The\nproposed framework effectively combines the complementary strengths of these\ntasks, enhancing temporal coherence and achieving superior restoration quality.\nThis work advances the state-of-the-art in video FR and establishes a new\nparadigm for generalized video face restoration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Restoration (FR) is a crucial area within image and video processing,\nfocusing on reconstructing high-quality portraits from degraded inputs. Despite\nadvancements in image FR, video FR remains relatively under-explored, primarily\ndue to challenges related to temporal consistency, motion artifacts, and the\nlimited availability of high-quality video data. Moreover, traditional face\nrestoration typically prioritizes enhancing resolution and may not give as much\nconsideration to related tasks such as facial colorization and inpainting. In\nthis paper, we propose a novel approach for the Generalized Video Face\nRestoration (GVFR) task, which integrates video BFR, inpainting, and\ncolorization tasks that we empirically show to benefit each other. We present a\nunified framework, termed as stable video face restoration (SVFR), which\nleverages the generative and motion priors of Stable Video Diffusion (SVD) and\nincorporates task-specific information through a unified face restoration\nframework. A learnable task embedding is introduced to enhance task\nidentification. Meanwhile, a novel Unified Latent Regularization (ULR) is\nemployed to encourage the shared feature representation learning among\ndifferent subtasks. To further enhance the restoration quality and temporal\nstability, we introduce the facial prior learning and the self-referred\nrefinement as auxiliary strategies used for both training and inference. The\nproposed framework effectively combines the complementary strengths of these\ntasks, enhancing temporal coherence and achieving superior restoration quality.\nThis work advances the state-of-the-art in video FR and establishes a new\nparadigm for generalized video face restoration."
                },
                "authors": [
                    {
                        "name": "Zhiyao Wang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chengming Xu"
                    },
                    {
                        "name": "Junwei Zhu"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05798v4",
                "updated": "2025-01-02T12:00:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    0,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-09T17:02:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    2,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Preference-based Linear Bandits via Human Response Time"
                },
                "summary": "Interactive preference learning systems infer human preferences by presenting\nqueries as pairs of options and collecting binary choices. Although binary\nchoices are simple and widely used, they provide limited information about\npreference strength. To address this, we leverage human response times, which\nare inversely related to preference strength, as an additional signal. We\npropose a computationally efficient method that combines choices and response\ntimes to estimate human utility functions, grounded in the EZ diffusion model\nfrom psychology. Theoretical and empirical analyses show that for queries with\nstrong preferences, response times complement choices by providing extra\ninformation about preference strength, leading to significantly improved\nutility estimation. We incorporate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that using response times significantly\naccelerates preference learning compared to choice-only approaches. Additional\nmaterials, such as code, slides, and talk video, are available at\nhttps://shenlirobot.github.io/pages/NeurIPS24.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive preference learning systems infer human preferences by presenting\nqueries as pairs of options and collecting binary choices. Although binary\nchoices are simple and widely used, they provide limited information about\npreference strength. To address this, we leverage human response times, which\nare inversely related to preference strength, as an additional signal. We\npropose a computationally efficient method that combines choices and response\ntimes to estimate human utility functions, grounded in the EZ diffusion model\nfrom psychology. Theoretical and empirical analyses show that for queries with\nstrong preferences, response times complement choices by providing extra\ninformation about preference strength, leading to significantly improved\nutility estimation. We incorporate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that using response times significantly\naccelerates preference learning compared to choice-only approaches. Additional\nmaterials, such as code, slides, and talk video, are available at\nhttps://shenlirobot.github.io/pages/NeurIPS24.html"
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Yuyang Zhang"
                    },
                    {
                        "name": "Zhaolin Ren"
                    },
                    {
                        "name": "Claire Liang"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Julie A. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Julie A. Shah"
                },
                "author": "Julie A. Shah",
                "arxiv_comment": "NeurIPS 2024 (Oral) camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01212v1",
                "updated": "2025-01-02T11:41:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    41,
                    43,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T11:41:43Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    41,
                    43,
                    3,
                    2,
                    0
                ],
                "title": "Real-time Cross-modal Cybersickness Prediction in Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Cross-modal Cybersickness Prediction in Virtual Reality"
                },
                "summary": "Cybersickness remains a significant barrier to the widespread adoption of\nimmersive virtual reality (VR) experiences, as it can greatly disrupt user\nengagement and comfort. Research has shown that cybersickness can significantly\nbe reflected in head and eye tracking data, along with other physiological data\n(e.g., TMP, EDA, and BMP). Despite the application of deep learning techniques\nsuch as CNNs and LSTMs, these models often struggle to capture the complex\ninteractions between multiple data modalities and lack the capacity for\nreal-time inference, limiting their practical application. Addressing this gap,\nwe propose a lightweight model that leverages a transformer-based encoder with\nsparse self-attention to process bio-signal features and a PP-TSN network for\nvideo feature extraction. These features are then integrated via a cross-modal\nfusion module, creating a video-aware bio-signal representation that supports\ncybersickness prediction based on both visual and bio-signal inputs. Our model,\ntrained with a lightweight framework, was validated on a public dataset\ncontaining eye and head tracking data, physiological data, and VR video, and\ndemonstrated state-of-the-art performance in cybersickness prediction,\nachieving a high accuracy of 93.13\\% using only VR video inputs. These findings\nsuggest that our approach not only enables effective, real-time cybersickness\nprediction but also addresses the longstanding issue of modality interaction in\nVR environments. This advancement provides a foundation for future research on\nmultimodal data integration in VR, potentially leading to more personalized,\ncomfortable and widely accessible VR experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersickness remains a significant barrier to the widespread adoption of\nimmersive virtual reality (VR) experiences, as it can greatly disrupt user\nengagement and comfort. Research has shown that cybersickness can significantly\nbe reflected in head and eye tracking data, along with other physiological data\n(e.g., TMP, EDA, and BMP). Despite the application of deep learning techniques\nsuch as CNNs and LSTMs, these models often struggle to capture the complex\ninteractions between multiple data modalities and lack the capacity for\nreal-time inference, limiting their practical application. Addressing this gap,\nwe propose a lightweight model that leverages a transformer-based encoder with\nsparse self-attention to process bio-signal features and a PP-TSN network for\nvideo feature extraction. These features are then integrated via a cross-modal\nfusion module, creating a video-aware bio-signal representation that supports\ncybersickness prediction based on both visual and bio-signal inputs. Our model,\ntrained with a lightweight framework, was validated on a public dataset\ncontaining eye and head tracking data, physiological data, and VR video, and\ndemonstrated state-of-the-art performance in cybersickness prediction,\nachieving a high accuracy of 93.13\\% using only VR video inputs. These findings\nsuggest that our approach not only enables effective, real-time cybersickness\nprediction but also addresses the longstanding issue of modality interaction in\nVR environments. This advancement provides a foundation for future research on\nmultimodal data integration in VR, potentially leading to more personalized,\ncomfortable and widely accessible VR experiences."
                },
                "authors": [
                    {
                        "name": "Yitong Zhu"
                    },
                    {
                        "name": "Tangyao Li"
                    },
                    {
                        "name": "Yuyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuyang Wang"
                },
                "author": "Yuyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01205v1",
                "updated": "2025-01-02T11:25:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    45,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T11:25:45Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    45,
                    3,
                    2,
                    0
                ],
                "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A\n  Framework for Senior Design Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A\n  Framework for Senior Design Projects"
                },
                "summary": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of..."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Muhammad Rafay Naeem"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Muhammad Imran Taj"
                    },
                    {
                        "name": "Imran Hashmi"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01203v1",
                "updated": "2025-01-02T11:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T11:25:28Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    28,
                    3,
                    2,
                    0
                ],
                "title": "HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning\n  for Journal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning\n  for Journal Recommendation"
                },
                "summary": "Academic journal recommendation requires effectively combining structural\nunderstanding of scholarly networks with interpretable recommendations. While\ngraph neural networks (GNNs) and large language models (LLMs) excel in their\nrespective domains, current approaches often fail to achieve true integration\nat the reasoning level. We propose HetGCoT-Rec, a framework that deeply\nintegrates heterogeneous graph transformer with LLMs through chain-of-thought\nreasoning. Our framework features two key technical innovations: (1) a\nstructure-aware mechanism that transforms heterogeneous graph neural network\nlearned subgraph information into natural language contexts, utilizing\npredefined metapaths to capture academic relationships, and (2) a multi-step\nreasoning strategy that systematically embeds graph-derived contexts into the\nLLM's stage-wise reasoning process. Experiments on a dataset collected from\nOpenAlex demonstrate that our approach significantly outperforms baseline\nmethods, achieving 96.48% Hit rate and 92.21% H@1 accuracy. Furthermore, we\nvalidate the framework's adaptability across different LLM architectures,\nshowing consistent improvements in both recommendation accuracy and explanation\nquality. Our work demonstrates an effective approach for combining\ngraph-structured reasoning with language models for interpretable academic\nvenue recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic journal recommendation requires effectively combining structural\nunderstanding of scholarly networks with interpretable recommendations. While\ngraph neural networks (GNNs) and large language models (LLMs) excel in their\nrespective domains, current approaches often fail to achieve true integration\nat the reasoning level. We propose HetGCoT-Rec, a framework that deeply\nintegrates heterogeneous graph transformer with LLMs through chain-of-thought\nreasoning. Our framework features two key technical innovations: (1) a\nstructure-aware mechanism that transforms heterogeneous graph neural network\nlearned subgraph information into natural language contexts, utilizing\npredefined metapaths to capture academic relationships, and (2) a multi-step\nreasoning strategy that systematically embeds graph-derived contexts into the\nLLM's stage-wise reasoning process. Experiments on a dataset collected from\nOpenAlex demonstrate that our approach significantly outperforms baseline\nmethods, achieving 96.48% Hit rate and 92.21% H@1 accuracy. Furthermore, we\nvalidate the framework's adaptability across different LLM architectures,\nshowing consistent improvements in both recommendation accuracy and explanation\nquality. Our work demonstrates an effective approach for combining\ngraph-structured reasoning with language models for interpretable academic\nvenue recommendations."
                },
                "authors": [
                    {
                        "name": "Runsong Jia"
                    },
                    {
                        "name": "Mengjia Wu"
                    },
                    {
                        "name": "Ying Ding"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15270v2",
                "updated": "2025-01-02T11:21:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    21,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-17T08:05:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    5,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Baichuan4-Finance Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan4-Finance Technical Report"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Boyu Qiu"
                    },
                    {
                        "name": "Yuhao Feng"
                    },
                    {
                        "name": "Shuqi Li"
                    },
                    {
                        "name": "Qian Ma"
                    },
                    {
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "name": "Qiang Ju"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xie"
                },
                "author": "Jian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v3",
                "updated": "2025-01-02T11:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    16,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v3",
                "updated": "2025-01-02T11:04:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    4,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations and clarify findings of\n  evaluations of the o1 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14674v3",
                "updated": "2025-01-02T11:00:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    0,
                    24,
                    3,
                    2,
                    0
                ],
                "published": "2024-11-22T02:15:38Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    15,
                    38,
                    4,
                    327,
                    0
                ],
                "title": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures"
                },
                "summary": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection."
                },
                "authors": [
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Peter Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Peter Mueller"
                },
                "author": "Peter Mueller",
                "arxiv_comment": "38 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v2",
                "updated": "2025-01-02T10:56:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "16 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01192v1",
                "updated": "2025-01-02T10:55:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T10:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education"
                },
                "summary": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly science literacy gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly science literacy gap."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Amin Alibakhshi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Alibakhshi"
                },
                "author": "Amin Alibakhshi",
                "arxiv_comment": "CHI late-breaking work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20367v2",
                "updated": "2025-01-02T09:43:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    43,
                    43,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-29T06:15:41Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    15,
                    41,
                    6,
                    364,
                    0
                ],
                "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey"
                },
                "summary": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques."
                },
                "authors": [
                    {
                        "name": "Junqiao Wang"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Kunyu Wu"
                    },
                    {
                        "name": "Guangwu Qian"
                    },
                    {
                        "name": "Qiuwu Chen"
                    },
                    {
                        "name": "Lewei He"
                    }
                ],
                "author_detail": {
                    "name": "Lewei He"
                },
                "author": "Lewei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01163v1",
                "updated": "2025-01-02T09:33:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    33,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T09:33:13Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    33,
                    13,
                    3,
                    2,
                    0
                ],
                "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer"
                },
                "summary": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks. The code and model will be\nreleased to promote future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks. The code and model will be\nreleased to promote future exploration."
                },
                "authors": [
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Feras Dayoub"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03603v3",
                "updated": "2025-01-02T09:13:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    13,
                    42,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-03T23:52:37Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    52,
                    37,
                    1,
                    338,
                    0
                ],
                "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HunyuanVideo: A Systematic Framework For Large Video Generative Models"
                },
                "summary": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in video generation have significantly impacted daily\nlife for both individuals and industries. However, the leading video generation\nmodels remain closed-source, resulting in a notable performance gap between\nindustry capabilities and those available to the public. In this report, we\nintroduce HunyuanVideo, an innovative open-source video foundation model that\ndemonstrates performance in video generation comparable to, or even surpassing,\nthat of leading closed-source models. HunyuanVideo encompasses a comprehensive\nframework that integrates several key elements, including data curation,\nadvanced architectural design, progressive model scaling and training, and an\nefficient infrastructure tailored for large-scale model training and inference.\nAs a result, we successfully trained a video generative model with over 13\nbillion parameters, making it the largest among all open-source models. We\nconducted extensive experiments and implemented a series of targeted designs to\nensure high visual quality, motion dynamics, text-video alignment, and advanced\nfilming techniques. According to evaluations by professionals, HunyuanVideo\noutperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6,\nand three top-performing Chinese video generative models. By releasing the code\nfor the foundation model and its applications, we aim to bridge the gap between\nclosed-source and open-source communities. This initiative will empower\nindividuals within the community to experiment with their ideas, fostering a\nmore dynamic and vibrant video generation ecosystem. The code is publicly\navailable at https://github.com/Tencent/HunyuanVideo."
                },
                "authors": [
                    {
                        "name": "Weijie Kong"
                    },
                    {
                        "name": "Qi Tian"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Rox Min"
                    },
                    {
                        "name": "Zuozhuo Dai"
                    },
                    {
                        "name": "Jin Zhou"
                    },
                    {
                        "name": "Jiangfeng Xiong"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Kathrina Wu"
                    },
                    {
                        "name": "Qin Lin"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Yanxin Long"
                    },
                    {
                        "name": "Aladdin Wang"
                    },
                    {
                        "name": "Andong Wang"
                    },
                    {
                        "name": "Changlin Li"
                    },
                    {
                        "name": "Duojun Huang"
                    },
                    {
                        "name": "Fang Yang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Hongmei Wang"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Jiawang Bai"
                    },
                    {
                        "name": "Jianbing Wu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Joey Wang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Mengyang Liu"
                    },
                    {
                        "name": "Pengyu Li"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Weiyan Wang"
                    },
                    {
                        "name": "Wenqing Yu"
                    },
                    {
                        "name": "Xinchi Deng"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Yutao Cui"
                    },
                    {
                        "name": "Yuanbo Peng"
                    },
                    {
                        "name": "Zhentao Yu"
                    },
                    {
                        "name": "Zhiyu He"
                    },
                    {
                        "name": "Zhiyong Xu"
                    },
                    {
                        "name": "Zixiang Zhou"
                    },
                    {
                        "name": "Zunnan Xu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Qinglin Lu"
                    },
                    {
                        "name": "Songtao Liu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Caesar Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Caesar Zhong"
                },
                "arxiv_affiliation": "refer to the report for detailed contributions",
                "author": "Caesar Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01149v1",
                "updated": "2025-01-02T09:03:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    3,
                    56,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T09:03:56Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    3,
                    56,
                    3,
                    2,
                    0
                ],
                "title": "A3: Android Agent Arena for Mobile GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3: Android Agent Arena for Mobile GUI Agents"
                },
                "summary": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}."
                },
                "authors": [
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Hanhao Li"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Guozhi Wang"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01148v1",
                "updated": "2025-01-02T09:01:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    1,
                    9,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T09:01:09Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    1,
                    9,
                    3,
                    2,
                    0
                ],
                "title": "Adaptive posterior distributions for uncertainty analysis of covariance\n  matrices in Bayesian inversion problems for multioutput signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive posterior distributions for uncertainty analysis of covariance\n  matrices in Bayesian inversion problems for multioutput signals"
                },
                "summary": "In this paper we address the problem of performing Bayesian inference for the\nparameters of a nonlinear multi-output model and the covariance matrix of the\ndifferent output signals. We propose an adaptive importance sampling (AIS)\nscheme for multivariate Bayesian inversion problems, which is based in two main\nideas: the variables of interest are split in two blocks and the inference\ntakes advantage of known analytical optimization formulas. We estimate both the\nunknown parameters of the multivariate non-linear model and the covariance\nmatrix of the noise. In the first part of the proposed inference scheme, a\nnovel AIS technique called adaptive target adaptive importance sampling (ATAIS)\nis designed, which alternates iteratively between an IS technique over the\nparameters of the non-linear model and a frequentist approach for the\ncovariance matrix of the noise. In the second part of the proposed inference\nscheme, a prior density over the covariance matrix is considered and the cloud\nof samples obtained by ATAIS are recycled and re-weighted to obtain a complete\nBayesian study over the model parameters and covariance matrix. ATAIS is the\nmain contribution of the work. Additionally, the inverted layered importance\nsampling (ILIS) is presented as a possible compelling algorithm (but based on a\nconceptually simpler idea). Different numerical examples show the benefits of\nthe proposed approaches",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we address the problem of performing Bayesian inference for the\nparameters of a nonlinear multi-output model and the covariance matrix of the\ndifferent output signals. We propose an adaptive importance sampling (AIS)\nscheme for multivariate Bayesian inversion problems, which is based in two main\nideas: the variables of interest are split in two blocks and the inference\ntakes advantage of known analytical optimization formulas. We estimate both the\nunknown parameters of the multivariate non-linear model and the covariance\nmatrix of the noise. In the first part of the proposed inference scheme, a\nnovel AIS technique called adaptive target adaptive importance sampling (ATAIS)\nis designed, which alternates iteratively between an IS technique over the\nparameters of the non-linear model and a frequentist approach for the\ncovariance matrix of the noise. In the second part of the proposed inference\nscheme, a prior density over the covariance matrix is considered and the cloud\nof samples obtained by ATAIS are recycled and re-weighted to obtain a complete\nBayesian study over the model parameters and covariance matrix. ATAIS is the\nmain contribution of the work. Additionally, the inverted layered importance\nsampling (ILIS) is presented as a possible compelling algorithm (but based on a\nconceptually simpler idea). Different numerical examples show the benefits of\nthe proposed approaches"
                },
                "authors": [
                    {
                        "name": "E. Curbelo"
                    },
                    {
                        "name": "L. Martino"
                    },
                    {
                        "name": "F. Llorente"
                    },
                    {
                        "name": "D. Delgado-Gomez"
                    }
                ],
                "author_detail": {
                    "name": "D. Delgado-Gomez"
                },
                "author": "D. Delgado-Gomez",
                "arxiv_doi": "10.1016/j.jfranklin.2024.107441",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jfranklin.2024.107441",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of the Franklin Institute, Volume 362, Issue 2, 2024.\n  107441",
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v1",
                "updated": "2025-01-02T08:57:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. Importantly, DialectFP4 ensures hardware\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. Furthermore, we propose a two-stage approach\nfor online DialectFP4 activation quantization. BlockDialect achieves 11.40%\n(6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4\nformat with a comparable bit usage per data, while being only 5.89% (3.31%)\nbelow full precision even when quantizing full-path matrix multiplication.\nFocusing on how to represent over how to scale, our work presents a promising\npath for energy-efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. Importantly, DialectFP4 ensures hardware\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. Furthermore, we propose a two-stage approach\nfor online DialectFP4 activation quantization. BlockDialect achieves 11.40%\n(6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4\nformat with a comparable bit usage per data, while being only 5.89% (3.31%)\nbelow full precision even when quantizing full-path matrix multiplication.\nFocusing on how to represent over how to scale, our work presents a promising\npath for energy-efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00927v2",
                "updated": "2025-01-02T08:53:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-30T16:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    57,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Text Clustering as Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering as Classification with LLMs"
                },
                "summary": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14528v3",
                "updated": "2025-01-02T08:49:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    49,
                    12,
                    3,
                    2,
                    0
                ],
                "published": "2023-05-23T21:10:17Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    21,
                    10,
                    17,
                    1,
                    143,
                    0
                ],
                "title": "Function Basis Encoding of Numerical Features in Factorization Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function Basis Encoding of Numerical Features in Factorization Machines"
                },
                "summary": "Factorization machine (FM) variants are widely used for large scale real-time\ncontent recommendation systems, since they offer an excellent balance between\nmodel accuracy and low computational costs for training and inference. These\nsystems are trained on tabular data with both numerical and categorical\ncolumns. Incorporating numerical columns poses a challenge, and they are\ntypically incorporated using a scalar transformation or binning, which can be\neither learned or chosen a-priori. In this work, we provide a systematic and\ntheoretically-justified way to incorporate numerical features into FM variants\nby encoding them into a vector of function values for a set of functions of\none's choice.\n  We view factorization machines as approximators of segmentized functions,\nnamely, functions from a field's value to the real numbers, assuming the\nremaining fields are assigned some given constants, which we refer to as the\nsegment. From this perspective, we show that our technique yields a model that\nlearns segmentized functions of the numerical feature spanned by the set of\nfunctions of one's choice, namely, the spanning coefficients vary between\nsegments. Hence, to improve model accuracy we advocate the use of functions\nknown to have strong approximation power, and offer the B-Spline basis due to\nits well-known approximation power, availability in software libraries, and\nefficiency. Our technique preserves fast training and inference, and requires\nonly a small modification of the computational graph of an FM model. Therefore,\nit is easy to incorporate into an existing system to improve its performance.\nFinally, we back our claims with a set of experiments, including synthetic,\nperformance evaluation on several data-sets, and an A/B test on a real online\nadvertising system which shows improved performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factorization machine (FM) variants are widely used for large scale real-time\ncontent recommendation systems, since they offer an excellent balance between\nmodel accuracy and low computational costs for training and inference. These\nsystems are trained on tabular data with both numerical and categorical\ncolumns. Incorporating numerical columns poses a challenge, and they are\ntypically incorporated using a scalar transformation or binning, which can be\neither learned or chosen a-priori. In this work, we provide a systematic and\ntheoretically-justified way to incorporate numerical features into FM variants\nby encoding them into a vector of function values for a set of functions of\none's choice.\n  We view factorization machines as approximators of segmentized functions,\nnamely, functions from a field's value to the real numbers, assuming the\nremaining fields are assigned some given constants, which we refer to as the\nsegment. From this perspective, we show that our technique yields a model that\nlearns segmentized functions of the numerical feature spanned by the set of\nfunctions of one's choice, namely, the spanning coefficients vary between\nsegments. Hence, to improve model accuracy we advocate the use of functions\nknown to have strong approximation power, and offer the B-Spline basis due to\nits well-known approximation power, availability in software libraries, and\nefficiency. Our technique preserves fast training and inference, and requires\nonly a small modification of the computational graph of an FM model. Therefore,\nit is easy to incorporate into an existing system to improve its performance.\nFinally, we back our claims with a set of experiments, including synthetic,\nperformance evaluation on several data-sets, and an A/B test on a real online\nadvertising system which shows improved performance."
                },
                "authors": [
                    {
                        "name": "Alex Shtoff"
                    },
                    {
                        "name": "Elie Abboud"
                    },
                    {
                        "name": "Rotem Stram"
                    },
                    {
                        "name": "Oren Somekh"
                    }
                ],
                "author_detail": {
                    "name": "Oren Somekh"
                },
                "author": "Oren Somekh",
                "arxiv_comment": "Published in TMLR, '2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01141v1",
                "updated": "2025-01-02T08:48:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    48,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T08:48:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    48,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language\n  Models and Reinforcement Learning Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language\n  Models and Reinforcement Learning Method"
                },
                "summary": "This paper investigates adaptive transmission strategies in embodied\nAI-enhanced vehicular networks by integrating large language models (LLMs) for\nsemantic information extraction and deep reinforcement learning (DRL) for\ndecision-making. The proposed framework aims to optimize both data transmission\nefficiency and decision accuracy by formulating an optimization problem that\nincorporates the Weber-Fechner law, serving as a metric for balancing bandwidth\nutilization and quality of experience (QoE). Specifically, we employ the large\nlanguage and vision assistant (LLAVA) model to extract critical semantic\ninformation from raw image data captured by embodied AI agents (i.e.,\nvehicles), reducing transmission data size by approximately more than 90\\%\nwhile retaining essential content for vehicular communication and\ndecision-making. In the dynamic vehicular environment, we employ a generalized\nadvantage estimation-based proximal policy optimization (GAE-PPO) method to\nstabilize decision-making under uncertainty. Simulation results show that\nattention maps from LLAVA highlight the model's focus on relevant image\nregions, enhancing semantic representation accuracy. Additionally, our proposed\ntransmission strategy improves QoE by up to 36\\% compared to DDPG and\naccelerates convergence by reducing required steps by up to 47\\% compared to\npure PPO. Further analysis indicates that adapting semantic symbol length\nprovides an effective trade-off between transmission quality and bandwidth,\nachieving up to a 61.4\\% improvement in QoE when scaling from 4 to 8 vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates adaptive transmission strategies in embodied\nAI-enhanced vehicular networks by integrating large language models (LLMs) for\nsemantic information extraction and deep reinforcement learning (DRL) for\ndecision-making. The proposed framework aims to optimize both data transmission\nefficiency and decision accuracy by formulating an optimization problem that\nincorporates the Weber-Fechner law, serving as a metric for balancing bandwidth\nutilization and quality of experience (QoE). Specifically, we employ the large\nlanguage and vision assistant (LLAVA) model to extract critical semantic\ninformation from raw image data captured by embodied AI agents (i.e.,\nvehicles), reducing transmission data size by approximately more than 90\\%\nwhile retaining essential content for vehicular communication and\ndecision-making. In the dynamic vehicular environment, we employ a generalized\nadvantage estimation-based proximal policy optimization (GAE-PPO) method to\nstabilize decision-making under uncertainty. Simulation results show that\nattention maps from LLAVA highlight the model's focus on relevant image\nregions, enhancing semantic representation accuracy. Additionally, our proposed\ntransmission strategy improves QoE by up to 36\\% compared to DDPG and\naccelerates convergence by reducing required steps by up to 47\\% compared to\npure PPO. Further analysis indicates that adapting semantic symbol length\nprovides an effective trade-off between transmission quality and bandwidth,\nachieving up to a 61.4\\% improvement in QoE when scaling from 4 to 8 vehicles."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Suttinee Sawadsitang"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v3",
                "updated": "2025-01-02T08:43:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    43,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01127v1",
                "updated": "2025-01-02T07:58:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    58,
                    26,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T07:58:26Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    58,
                    26,
                    3,
                    2,
                    0
                ],
                "title": "InDeed: Interpretable image deep decomposition with guaranteed\n  generalizability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InDeed: Interpretable image deep decomposition with guaranteed\n  generalizability"
                },
                "summary": "Image decomposition aims to analyze an image into elementary components,\nwhich is essential for numerous downstream tasks and also by nature provides\ncertain interpretability to the analysis. Deep learning can be powerful for\nsuch tasks, but surprisingly their combination with a focus on interpretability\nand generalizability is rarely explored. In this work, we introduce a novel\nframework for interpretable deep image decomposition, combining hierarchical\nBayesian modeling and deep learning to create an architecture-modularized and\nmodel-generalizable deep neural network (DNN). The proposed framework includes\nthree steps: (1) hierarchical Bayesian modeling of image decomposition, (2)\ntransforming the inference problem into optimization tasks, and (3) deep\ninference via a modularized Bayesian DNN. We further establish a theoretical\nconnection between the loss function and the generalization error bound, which\ninspires a new test-time adaptation approach for out-of-distribution scenarios.\nWe instantiated the application using two downstream tasks, \\textit{i.e.},\nimage denoising, and unsupervised anomaly detection, and the results\ndemonstrated improved generalizability as well as interpretability of our\nmethods. The source code will be released upon the acceptance of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image decomposition aims to analyze an image into elementary components,\nwhich is essential for numerous downstream tasks and also by nature provides\ncertain interpretability to the analysis. Deep learning can be powerful for\nsuch tasks, but surprisingly their combination with a focus on interpretability\nand generalizability is rarely explored. In this work, we introduce a novel\nframework for interpretable deep image decomposition, combining hierarchical\nBayesian modeling and deep learning to create an architecture-modularized and\nmodel-generalizable deep neural network (DNN). The proposed framework includes\nthree steps: (1) hierarchical Bayesian modeling of image decomposition, (2)\ntransforming the inference problem into optimization tasks, and (3) deep\ninference via a modularized Bayesian DNN. We further establish a theoretical\nconnection between the loss function and the generalization error bound, which\ninspires a new test-time adaptation approach for out-of-distribution scenarios.\nWe instantiated the application using two downstream tasks, \\textit{i.e.},\nimage denoising, and unsupervised anomaly detection, and the results\ndemonstrated improved generalizability as well as interpretability of our\nmethods. The source code will be released upon the acceptance of this paper."
                },
                "authors": [
                    {
                        "name": "Sihan Wang"
                    },
                    {
                        "name": "Shangqi Gao"
                    },
                    {
                        "name": "Fuping Wu"
                    },
                    {
                        "name": "Xiahai Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Xiahai Zhuang"
                },
                "author": "Xiahai Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01124v1",
                "updated": "2025-01-02T07:45:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    45,
                    34,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T07:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    45,
                    34,
                    3,
                    2,
                    0
                ],
                "title": "Graph2text or Graph2token: A Perspective of Large Language Models for\n  Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph2text or Graph2token: A Perspective of Large Language Models for\n  Graph Learning"
                },
                "summary": "Graphs are data structures used to represent irregular networks and are\nprevalent in numerous real-world applications. Previous methods directly model\ngraph structures and achieve significant success. However, these methods\nencounter bottlenecks due to the inherent irregularity of graphs. An innovative\nsolution is converting graphs into textual representations, thereby harnessing\nthe powerful capabilities of Large Language Models (LLMs) to process and\ncomprehend graphs. In this paper, we present a comprehensive review of\nmethodologies for applying LLMs to graphs, termed LLM4graph. The core of\nLLM4graph lies in transforming graphs into texts for LLMs to understand and\nanalyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of\nthe transformation. Specifically, existing methods can be divided into two\nparadigms: Graph2text and Graph2token, which transform graphs into texts or\ntokens as the input of LLMs, respectively. We point out four challenges during\nthe transformation to systematically present existing methods in a\nproblem-oriented perspective. For practical concerns, we provide a guideline\nfor researchers on selecting appropriate models and LLMs for different graphs\nand hardware constraints. We also identify five future research directions for\nLLM4graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are data structures used to represent irregular networks and are\nprevalent in numerous real-world applications. Previous methods directly model\ngraph structures and achieve significant success. However, these methods\nencounter bottlenecks due to the inherent irregularity of graphs. An innovative\nsolution is converting graphs into textual representations, thereby harnessing\nthe powerful capabilities of Large Language Models (LLMs) to process and\ncomprehend graphs. In this paper, we present a comprehensive review of\nmethodologies for applying LLMs to graphs, termed LLM4graph. The core of\nLLM4graph lies in transforming graphs into texts for LLMs to understand and\nanalyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of\nthe transformation. Specifically, existing methods can be divided into two\nparadigms: Graph2text and Graph2token, which transform graphs into texts or\ntokens as the input of LLMs, respectively. We point out four challenges during\nthe transformation to systematically present existing methods in a\nproblem-oriented perspective. For practical concerns, we provide a guideline\nfor researchers on selecting appropriate models and LLMs for different graphs\nand hardware constraints. We also identify five future research directions for\nLLM4graph."
                },
                "authors": [
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Ruolin Li"
                    },
                    {
                        "name": "Guchun Liu"
                    },
                    {
                        "name": "Yanming Shen"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Fengling Han"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01121v1",
                "updated": "2025-01-02T07:41:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    41,
                    27,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T07:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    41,
                    27,
                    3,
                    2,
                    0
                ],
                "title": "PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric\n  Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatchRefiner V2: Fast and Lightweight Real-Domain High-Resolution Metric\n  Depth Estimation"
                },
                "summary": "While current high-resolution depth estimation methods achieve strong\nresults, they often suffer from computational inefficiencies due to reliance on\nheavyweight models and multiple inference steps, increasing inference time. To\naddress this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner\nmodels with lightweight encoders. This reduces model size and inference time\nbut introduces noisy features. To overcome this, we propose a Coarse-to-Fine\n(C2F) module with a Guided Denoising Unit for refining and denoising the\nrefiner features and a Noisy Pretraining strategy to pretrain the refiner\nbranch to fully exploit the potential of the lightweight refiner branch.\nAdditionally, we introduce a Scale-and-Shift Invariant Gradient Matching\n(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms\nstate-of-the-art depth estimation methods on UnrealStereo4K in both accuracy\nand speed, using fewer parameters and faster inference. It also shows improved\ndepth boundary delineation on real-world datasets like CityScape, ScanNet++,\nand KITTI, demonstrating its versatility across domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While current high-resolution depth estimation methods achieve strong\nresults, they often suffer from computational inefficiencies due to reliance on\nheavyweight models and multiple inference steps, increasing inference time. To\naddress this, we introduce PatchRefiner V2 (PRV2), which replaces heavy refiner\nmodels with lightweight encoders. This reduces model size and inference time\nbut introduces noisy features. To overcome this, we propose a Coarse-to-Fine\n(C2F) module with a Guided Denoising Unit for refining and denoising the\nrefiner features and a Noisy Pretraining strategy to pretrain the refiner\nbranch to fully exploit the potential of the lightweight refiner branch.\nAdditionally, we introduce a Scale-and-Shift Invariant Gradient Matching\n(SSIGM) loss to enhance synthetic-to-real domain transfer. PRV2 outperforms\nstate-of-the-art depth estimation methods on UnrealStereo4K in both accuracy\nand speed, using fewer parameters and faster inference. It also shows improved\ndepth boundary delineation on real-world datasets like CityScape, ScanNet++,\nand KITTI, demonstrating its versatility across domains."
                },
                "authors": [
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Wenqing Cui"
                    },
                    {
                        "name": "Shariq Farooq Bhat"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01120v1",
                "updated": "2025-01-02T07:39:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    39,
                    48,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T07:39:48Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    39,
                    48,
                    3,
                    2,
                    0
                ],
                "title": "Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Dynamic Prompt Tuning for Incomplete Multimodal\n  Learning"
                },
                "summary": "Multimodal learning with incomplete modality is practical and challenging.\nRecently, researchers have focused on enhancing the robustness of pre-trained\nMultiModal Transformers (MMTs) under missing modality conditions by applying\nlearnable prompts. However, these prompt-based methods face several\nlimitations: (1) incomplete modalities provide restricted modal cues for\ntask-specific inference, (2) dummy imputation for missing content causes\ninformation loss and introduces noise, and (3) static prompts are\ninstance-agnostic, offering limited knowledge for instances with various\nmissing conditions. To address these issues, we propose RAGPT, a novel\nRetrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three\nmodules: (I) the multi-channel retriever, which identifies similar instances\nthrough a within-modality retrieval strategy, (II) the missing modality\ngenerator, which recovers missing information using retrieved contexts, and\n(III) the context-aware prompter, which captures contextual knowledge from\nrelevant instances and generates dynamic prompts to largely enhance the MMT's\nrobustness. Extensive experiments conducted on three real-world datasets show\nthat RAGPT consistently outperforms all competitive baselines in handling\nincomplete modality problems. The code of our work and prompt-based baselines\nis available at https://github.com/Jian-Lang/RAGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal learning with incomplete modality is practical and challenging.\nRecently, researchers have focused on enhancing the robustness of pre-trained\nMultiModal Transformers (MMTs) under missing modality conditions by applying\nlearnable prompts. However, these prompt-based methods face several\nlimitations: (1) incomplete modalities provide restricted modal cues for\ntask-specific inference, (2) dummy imputation for missing content causes\ninformation loss and introduces noise, and (3) static prompts are\ninstance-agnostic, offering limited knowledge for instances with various\nmissing conditions. To address these issues, we propose RAGPT, a novel\nRetrieval-AuGmented dynamic Prompt Tuning framework. RAGPT comprises three\nmodules: (I) the multi-channel retriever, which identifies similar instances\nthrough a within-modality retrieval strategy, (II) the missing modality\ngenerator, which recovers missing information using retrieved contexts, and\n(III) the context-aware prompter, which captures contextual knowledge from\nrelevant instances and generates dynamic prompts to largely enhance the MMT's\nrobustness. Extensive experiments conducted on three real-world datasets show\nthat RAGPT consistently outperforms all competitive baselines in handling\nincomplete modality problems. The code of our work and prompt-based baselines\nis available at https://github.com/Jian-Lang/RAGPT."
                },
                "authors": [
                    {
                        "name": "Jian Lang"
                    },
                    {
                        "name": "Zhangtao Cheng"
                    },
                    {
                        "name": "Ting Zhong"
                    },
                    {
                        "name": "Fan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Fan Zhou"
                },
                "author": "Fan Zhou",
                "arxiv_comment": "9 pages, 8 figures. Accepted by AAAI 2025. Codes are released at\n  https://github.com/Jian-Lang/RAGPT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v2",
                "updated": "2025-01-02T07:29:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    29,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20265v2",
                "updated": "2025-01-02T07:26:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    26,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-28T20:40:23Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    20,
                    40,
                    23,
                    5,
                    363,
                    0
                ],
                "title": "Overcoming Intensity Limits for Long-Distance Quantum Key Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming Intensity Limits for Long-Distance Quantum Key Distribution"
                },
                "summary": "Quantum Key Distribution (QKD) enables the sharing of cryptographic keys\nsecured by quantum mechanics. The BB84 protocol assumed single-photon sources,\nbut practical systems rely on weak coherent pulses vulnerable to\nphoton-number-splitting (PNS) attacks. The Gottesman-Lo-L\\\"utkenhaus-Preskill\n(GLLP) framework addressed these imperfections, deriving secure key rate bounds\nunder limited PNS. The Decoy-state protocol further improved performance by\nrefining single-photon yield estimates, but still considered multi-photon\nstates as insecure, limiting intensities and thereby constraining key rate and\ndistance. Here, we show that higher intensities can be securely permitted by\napplying Bayesian inference to estimate key parameters directly from observed\ndata rather than relying on worst-case assumptions. By raising the pulse\nintensity to 10 photons, we achieve 50 times the key rate and a 62.2% increase\nin operational range (about 200 km) compared to the decoy-state protocol.\nFurthermore, we accurately model after-pulsing using a Hidden Markov Model and\nreveal inaccuracies in decoy-state calculations that may produce erroneous\nkey-rate estimates. By bridging theoretical security and real-world conditions,\nthis Bayesian methodology provides a versatile post-processing step for many\ndiscrete-variable QKD protocols, advancing their reach, efficiency, and\nfacilitating broader adoption of quantum-secured communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) enables the sharing of cryptographic keys\nsecured by quantum mechanics. The BB84 protocol assumed single-photon sources,\nbut practical systems rely on weak coherent pulses vulnerable to\nphoton-number-splitting (PNS) attacks. The Gottesman-Lo-L\\\"utkenhaus-Preskill\n(GLLP) framework addressed these imperfections, deriving secure key rate bounds\nunder limited PNS. The Decoy-state protocol further improved performance by\nrefining single-photon yield estimates, but still considered multi-photon\nstates as insecure, limiting intensities and thereby constraining key rate and\ndistance. Here, we show that higher intensities can be securely permitted by\napplying Bayesian inference to estimate key parameters directly from observed\ndata rather than relying on worst-case assumptions. By raising the pulse\nintensity to 10 photons, we achieve 50 times the key rate and a 62.2% increase\nin operational range (about 200 km) compared to the decoy-state protocol.\nFurthermore, we accurately model after-pulsing using a Hidden Markov Model and\nreveal inaccuracies in decoy-state calculations that may produce erroneous\nkey-rate estimates. By bridging theoretical security and real-world conditions,\nthis Bayesian methodology provides a versatile post-processing step for many\ndiscrete-variable QKD protocols, advancing their reach, efficiency, and\nfacilitating broader adoption of quantum-secured communication."
                },
                "authors": [
                    {
                        "name": "Ibrahim Almosallam"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Almosallam"
                },
                "author": "Ibrahim Almosallam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01090v1",
                "updated": "2025-01-02T06:23:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    6,
                    23,
                    51,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T06:23:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    6,
                    23,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "HoneypotNet: Backdoor Attacks Against Model Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HoneypotNet: Backdoor Attacks Against Model Extraction"
                },
                "summary": "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks."
                },
                "authors": [
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "arxiv_comment": "Accepted to the AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01078v1",
                "updated": "2025-01-02T05:53:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    53,
                    14,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T05:53:14Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    53,
                    14,
                    3,
                    2,
                    0
                ],
                "title": "Communication-and-Computation Efficient Split Federated Learning:\n  Gradient Aggregation and Resource Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-and-Computation Efficient Split Federated Learning:\n  Gradient Aggregation and Resource Management"
                },
                "summary": "With the prevalence of Large Learning Models (LLM), Split Federated Learning\n(SFL), which divides a learning model into server-side and client-side models,\nhas emerged as an appealing technology to deal with the heavy computational\nburden for network edge clients. However, existing SFL frameworks would\nfrequently upload smashed data and download gradients between the server and\neach client, leading to severe communication overheads. To address this issue,\nthis work proposes a novel communication-and-computation efficient SFL\nframework, which allows dynamic model splitting (server- and client-side model\ncutting point selection) and broadcasting of aggregated smashed data gradients.\nWe theoretically analyze the impact of the cutting point selection on the\nconvergence rate of the proposed framework, revealing that model splitting with\na smaller client-side model size leads to a better convergence performance and\nvise versa. Based on the above insights, we formulate an optimization problem\nto minimize the model convergence rate and latency under the consideration of\ndata privacy via a joint Cutting point selection, Communication and Computation\nresource allocation (CCC) strategy. To deal with the proposed mixed integer\nnonlinear programming optimization problem, we develop an algorithm by\nintegrating the Double Deep Q-learning Network (DDQN) with convex optimization\nmethods. Extensive experiments validate our theoretical analyses across various\ndatasets, and the numerical results demonstrate the effectiveness and\nsuperiority of the proposed communication-efficient SFL compared with existing\nschemes, including parallel split learning and traditional SFL mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the prevalence of Large Learning Models (LLM), Split Federated Learning\n(SFL), which divides a learning model into server-side and client-side models,\nhas emerged as an appealing technology to deal with the heavy computational\nburden for network edge clients. However, existing SFL frameworks would\nfrequently upload smashed data and download gradients between the server and\neach client, leading to severe communication overheads. To address this issue,\nthis work proposes a novel communication-and-computation efficient SFL\nframework, which allows dynamic model splitting (server- and client-side model\ncutting point selection) and broadcasting of aggregated smashed data gradients.\nWe theoretically analyze the impact of the cutting point selection on the\nconvergence rate of the proposed framework, revealing that model splitting with\na smaller client-side model size leads to a better convergence performance and\nvise versa. Based on the above insights, we formulate an optimization problem\nto minimize the model convergence rate and latency under the consideration of\ndata privacy via a joint Cutting point selection, Communication and Computation\nresource allocation (CCC) strategy. To deal with the proposed mixed integer\nnonlinear programming optimization problem, we develop an algorithm by\nintegrating the Double Deep Q-learning Network (DDQN) with convex optimization\nmethods. Extensive experiments validate our theoretical analyses across various\ndatasets, and the numerical results demonstrate the effectiveness and\nsuperiority of the proposed communication-efficient SFL compared with existing\nschemes, including parallel split learning and traditional SFL mechanisms."
                },
                "authors": [
                    {
                        "name": "Yipeng Liang"
                    },
                    {
                        "name": "Qimei Chen"
                    },
                    {
                        "name": "Guangxu Zhu"
                    },
                    {
                        "name": "Muhammad Kaleem Awan"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "13 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19376v2",
                "updated": "2025-01-02T05:34:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    34,
                    51,
                    3,
                    2,
                    0
                ],
                "published": "2024-02-29T17:24:22Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    17,
                    24,
                    22,
                    3,
                    60,
                    0
                ],
                "title": "Commercial Evaluation of Zero-Skipping MAC Design for Bit Sparsity\n  Exploitation in DL Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial Evaluation of Zero-Skipping MAC Design for Bit Sparsity\n  Exploitation in DL Inference"
                },
                "summary": "General Matrix Multiply (GEMM) units, consisting of multiply-accumulate (MAC)\narrays, perform bulk of the computation in deep learning (DL). Recent work has\nproposed a novel MAC design, Bit-Pragmatic (PRA), capable of dynamically\nexploiting bit sparsity. This work presents OzMAC (Omit-zero-MAC), a modified\nre-implementation of PRA, but extends beyond earlier works by performing\nrigorous post-synthesis evaluation against binary MAC design across multiple\nbitwidths and clock frequencies using TSMC N5 process node to assess commercial\nimplementation potential. We demonstrate the existence of high bit sparsity in\neight pretrained INT8 DL workloads and show that 8-bit OzMAC improves all three\nmetrics of area, power, and energy significantly by 21%, 70%, and 28%,\nrespectively. Similar improvements are achieved when scaling data precisions\n(4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit\nOzMAC, scaling its frequency to normalize the throughput, it still achieves 30%\nimprovement on both power and energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Matrix Multiply (GEMM) units, consisting of multiply-accumulate (MAC)\narrays, perform bulk of the computation in deep learning (DL). Recent work has\nproposed a novel MAC design, Bit-Pragmatic (PRA), capable of dynamically\nexploiting bit sparsity. This work presents OzMAC (Omit-zero-MAC), a modified\nre-implementation of PRA, but extends beyond earlier works by performing\nrigorous post-synthesis evaluation against binary MAC design across multiple\nbitwidths and clock frequencies using TSMC N5 process node to assess commercial\nimplementation potential. We demonstrate the existence of high bit sparsity in\neight pretrained INT8 DL workloads and show that 8-bit OzMAC improves all three\nmetrics of area, power, and energy significantly by 21%, 70%, and 28%,\nrespectively. Similar improvements are achieved when scaling data precisions\n(4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit\nOzMAC, scaling its frequency to normalize the throughput, it still achieves 30%\nimprovement on both power and energy."
                },
                "authors": [
                    {
                        "name": "Harideep Nair"
                    },
                    {
                        "name": "Prabhu Vellaisamy"
                    },
                    {
                        "name": "Tsung-Han Lin"
                    },
                    {
                        "name": "Perry Wang"
                    },
                    {
                        "name": "Shawn Blanton"
                    },
                    {
                        "name": "John Paul Shen"
                    }
                ],
                "author_detail": {
                    "name": "John Paul Shen"
                },
                "author": "John Paul Shen",
                "arxiv_doi": "10.1109/VLSI-SoC62099.2024.10767792",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/VLSI-SoC62099.2024.10767792",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.19376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Pre-print version of the publication in VLSI-SoC 2024",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01059v1",
                "updated": "2025-01-02T05:07:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    7,
                    6,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T05:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    7,
                    6,
                    3,
                    2,
                    0
                ],
                "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models"
                },
                "summary": "Large language models (LLMs) often suffer from context faithfulness\nhallucinations, where outputs deviate from retrieved information due to\ninsufficient context utilization and high output uncertainty. Our uncertainty\nevaluation experiments reveal a strong correlation between high uncertainty and\nhallucinations. We hypothesize that attention mechanisms encode signals\nindicative of contextual utilization, validated through probing analysis. Based\non these insights, we propose Dynamic Attention-Guided Context Decoding\n(DAGCD), a lightweight framework that integrates attention distributions and\nuncertainty signals in a single-pass decoding process. Experiments across QA\ndatasets demonstrate DAGCD's effectiveness, achieving significant improvements\nin faithfulness and robustness while maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from context faithfulness\nhallucinations, where outputs deviate from retrieved information due to\ninsufficient context utilization and high output uncertainty. Our uncertainty\nevaluation experiments reveal a strong correlation between high uncertainty and\nhallucinations. We hypothesize that attention mechanisms encode signals\nindicative of contextual utilization, validated through probing analysis. Based\non these insights, we propose Dynamic Attention-Guided Context Decoding\n(DAGCD), a lightweight framework that integrates attention distributions and\nuncertainty signals in a single-pass decoding process. Experiments across QA\ndatasets demonstrate DAGCD's effectiveness, achieving significant improvements\nin faithfulness and robustness while maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Ning Cheng"
                    },
                    {
                        "name": "Zhitao Li"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01054v1",
                "updated": "2025-01-02T04:33:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    33,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T04:33:31Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    33,
                    31,
                    3,
                    2,
                    0
                ],
                "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Scaling of Unit Tests for Code Reward Modeling"
                },
                "summary": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus)."
                },
                "authors": [
                    {
                        "name": "Zeyao Ma"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Sijia Luo"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "Homepage: https://code-reward-model.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01985v2",
                "updated": "2025-01-02T04:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    15,
                    37,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-02T19:45:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    45,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks"
                },
                "summary": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size."
                },
                "authors": [
                    {
                        "name": "Hamed Firooz"
                    },
                    {
                        "name": "Maziar Sanjabi"
                    },
                    {
                        "name": "Wenlong Jiang"
                    },
                    {
                        "name": "Xiaoling Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Zhai"
                },
                "author": "Xiaoling Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01046v1",
                "updated": "2025-01-02T04:11:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    11,
                    23,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T04:11:23Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    11,
                    23,
                    3,
                    2,
                    0
                ],
                "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FED: Fast and Efficient Dataset Deduplication Framework with GPU\n  Acceleration"
                },
                "summary": "Dataset deduplication plays a crucial role in enhancing data quality,\nultimately improving training performance and efficiency of LLMs. A commonly\nused method for data deduplication is the MinHash LSH algorithm. Recently,\nNVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains\nsuboptimal, leaving room for further improvement in processing efficiency. This\npaper proposes a GPU-accelerated deduplication framework \\sys that optimizes\nMinHash LSH for GPU clusters and leverages computationally efficient and\npartially reusable non-cryptographic hash functions. \\sys significantly\noutperforms the CPU-based deduplication tool included in SlimPajama by up to\n58.3 times and the GPU-based deduplication tool included in NVIDIA NeMo Curator\nby up to 8.6 times when processing 1 million documents with a node of four\nGPUs. Deduplication of 1.2 trillion tokens is completed in just 5.1 hours in a\nfour-node, 16-GPU environment. The related code is publicly available on GitHub\n(https://github.com/mcrl/FED).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset deduplication plays a crucial role in enhancing data quality,\nultimately improving training performance and efficiency of LLMs. A commonly\nused method for data deduplication is the MinHash LSH algorithm. Recently,\nNVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains\nsuboptimal, leaving room for further improvement in processing efficiency. This\npaper proposes a GPU-accelerated deduplication framework \\sys that optimizes\nMinHash LSH for GPU clusters and leverages computationally efficient and\npartially reusable non-cryptographic hash functions. \\sys significantly\noutperforms the CPU-based deduplication tool included in SlimPajama by up to\n58.3 times and the GPU-based deduplication tool included in NVIDIA NeMo Curator\nby up to 8.6 times when processing 1 million documents with a node of four\nGPUs. Deduplication of 1.2 trillion tokens is completed in just 5.1 hours in a\nfour-node, 16-GPU environment. The related code is publicly available on GitHub\n(https://github.com/mcrl/FED)."
                },
                "authors": [
                    {
                        "name": "Youngjun Son"
                    },
                    {
                        "name": "Chaewon Kim"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13334v3",
                "updated": "2025-01-02T04:06:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    6,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-17T08:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    46,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models"
                },
                "summary": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs."
                },
                "authors": [
                    {
                        "name": "Isack Lee"
                    },
                    {
                        "name": "Haebin Seong"
                    }
                ],
                "author_detail": {
                    "name": "Haebin Seong"
                },
                "author": "Haebin Seong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01041v1",
                "updated": "2025-01-02T03:49:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    49,
                    21,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:49:21Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    49,
                    21,
                    3,
                    2,
                    0
                ],
                "title": "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies"
                },
                "summary": "ntegrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) (Guha et al., 2024) addresses a critical gap in the\nimplementation of integrative weighting approaches for multiple observational\nstudies and causal inferences about various groups of subjects, such as disease\nsubtypes. The package features three weighting approaches, each representing a\nspecial case of the unified weighting framework introduced by Guha and Li\n(2024), which includes an extension of inverse probability weights for data\nintegration settings. It performs meta-analysis on user-inputted datasets as\nfollows: (i) it first estimates the propensity scores for study-group\ncombinations, calculates subject balancing weights, and determines the\neffective sample size (ESS) for a user-specified weighting method; and (ii) it\nthen estimates various features of multiple counterfactual group outcomes, such\nas group medians and differences in group means for the mRNA expression of\neight genes. Additionally, bootstrap variability estimates are provided. Among\nthe implemented weighting methods, we highlight the FLEXible, O ptimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ntegrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) (Guha et al., 2024) addresses a critical gap in the\nimplementation of integrative weighting approaches for multiple observational\nstudies and causal inferences about various groups of subjects, such as disease\nsubtypes. The package features three weighting approaches, each representing a\nspecial case of the unified weighting framework introduced by Guha and Li\n(2024), which includes an extension of inverse probability weights for data\nintegration settings. It performs meta-analysis on user-inputted datasets as\nfollows: (i) it first estimates the propensity scores for study-group\ncombinations, calculates subject balancing weights, and determines the\neffective sample size (ESS) for a user-specified weighting method; and (ii) it\nthen estimates various features of multiple counterfactual group outcomes, such\nas group medians and differences in group means for the mRNA expression of\neight genes. Additionally, bootstrap variability estimates are provided. Among\nthe implemented weighting methods, we highlight the FLEXible, O ptimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers."
                },
                "authors": [
                    {
                        "name": "Subharup Guha"
                    },
                    {
                        "name": "Mengqi Xu"
                    },
                    {
                        "name": "Kashish Priyam"
                    },
                    {
                        "name": "Yi Li"
                    }
                ],
                "author_detail": {
                    "name": "Yi Li"
                },
                "author": "Yi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v2",
                "updated": "2025-01-02T03:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00314v3",
                "updated": "2025-01-02T03:29:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2023-01-01T00:47:03Z",
                "published_parsed": [
                    2023,
                    1,
                    1,
                    0,
                    47,
                    3,
                    6,
                    1,
                    0
                ],
                "title": "Causal Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Deep Learning"
                },
                "summary": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We derive a set of causal deep neural networks whose architectures are a\nconsequence of tensor (multilinear) factor analysis, a framework that\nfacilitates forward and inverse causal inference. Forward causal questions are\naddressed with a neural architecture composed of causal capsules and a tensor\ntransformer. Causal capsules compute a set of invariant causal factor\nrepresentations, whose interactions are governed by a tensor transformation.\nInverse causal questions are addressed with a neural network that implements\nthe multilinear projection algorithm. The architecture reverses the order of\nthe operations of a forward neural network and estimates the causes of effects.\nAs an alternative to aggressive bottleneck dimension reduction or regularized\nregression that may camouflage an inherently underdetermined inverse problem,\nwe prescribe modeling different aspects of the mechanism of data formation with\npiecewise tensor models whose multilinear projections produce multiple\ncandidate solutions. Our forward and inverse questions may be addressed with\nshallow architectures, but for computationally scalable solutions, we derive a\nset of deep neural networks by taking advantage of block algebra. An\ninterleaved kernel hierarchy results in a doubly non-linear tensor factor\nmodels. The causal neural networks that are a consequence of tensor factor\nanalysis are data agnostic, but are illustrated with facial images. Sequential,\nparallel and asynchronous parallel computation strategies are described."
                },
                "authors": [
                    {
                        "name": "M. Alex O. Vasilescu"
                    }
                ],
                "author_detail": {
                    "name": "M. Alex O. Vasilescu"
                },
                "author": "M. Alex O. Vasilescu",
                "arxiv_doi": "10.1007/978-3-031-78189-6_27",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-78189-6_27",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.00314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Antonacopoulos, A., Chaudhuri, S., Chellappa, R., Liu, CL.,\n  Bhattacharya, S., Pal, U. (eds) Pattern Recognition. (ICPR 2024). Lecture\n  Notes in Computer Science, vol 15309, pp 420-438(LNCS 2025). Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary) 68T30, 68T45, 62H25, 62H30, 62H35, 62D20, 62J10,\n  15A72, 15A69, 15A09 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.1; I.2.6; I.2.4; G.3; I.2.10; I.5.2; I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01031v1",
                "updated": "2025-01-02T03:26:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    26,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:26:13Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    26,
                    13,
                    3,
                    2,
                    0
                ],
                "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning"
                },
                "summary": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nin-context learning to integrate cultural and demographic knowledge dynamically\nduring text generation. Leveraging the World Values Survey (WVS) dataset,\nValuesRAG first generates summaries of values for each individual.\nSubsequently, we curated several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided, highlighting ValuesRAG's potential to foster culturally aligned AI\nsystems and enhance the inclusivity of AI-driven applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nin-context learning to integrate cultural and demographic knowledge dynamically\nduring text generation. Leveraging the World Values Survey (WVS) dataset,\nValuesRAG first generates summaries of values for each individual.\nSubsequently, we curated several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided, highlighting ValuesRAG's potential to foster culturally aligned AI\nsystems and enhance the inclusivity of AI-driven applications."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Zonghao Yuan"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v1",
                "updated": "2025-01-02T03:17:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01025v1",
                "updated": "2025-01-02T03:15:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    15,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:15:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    15,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Towards Adversarially Robust Deep Metric Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adversarially Robust Deep Metric Learning"
                },
                "summary": "Deep Metric Learning (DML) has shown remarkable successes in many domains by\ntaking advantage of powerful deep neural networks. Deep neural networks are\nprone to adversarial attacks and could be easily fooled by adversarial\nexamples. The current progress on this robustness issue is mainly about deep\nclassification models but pays little attention to DML models. Existing works\nfail to thoroughly inspect the robustness of DML and neglect an important DML\nscenario, the clustering-based inference. In this work, we first point out the\nrobustness issue of DML models in clustering-based inference scenarios. We find\nthat, for the clustering-based inference, existing defenses designed DML are\nunable to be reused and the adaptions of defenses designed for deep\nclassification models cannot achieve satisfactory robustness performance. To\nalleviate the hazard of adversarial examples, we propose a new defense, the\nEnsemble Adversarial Training (EAT), which exploits ensemble learning and\nadversarial training. EAT promotes the diversity of the ensemble, encouraging\neach model in the ensemble to have different robustness features, and employs a\nself-transferring mechanism to make full use of the robustness statistics of\nthe whole ensemble in the update of every single model. We evaluate the EAT\nmethod on three widely-used datasets with two popular model architectures. The\nresults show that the proposed EAT method greatly outperforms the adaptions of\ndefenses designed for deep classification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Metric Learning (DML) has shown remarkable successes in many domains by\ntaking advantage of powerful deep neural networks. Deep neural networks are\nprone to adversarial attacks and could be easily fooled by adversarial\nexamples. The current progress on this robustness issue is mainly about deep\nclassification models but pays little attention to DML models. Existing works\nfail to thoroughly inspect the robustness of DML and neglect an important DML\nscenario, the clustering-based inference. In this work, we first point out the\nrobustness issue of DML models in clustering-based inference scenarios. We find\nthat, for the clustering-based inference, existing defenses designed DML are\nunable to be reused and the adaptions of defenses designed for deep\nclassification models cannot achieve satisfactory robustness performance. To\nalleviate the hazard of adversarial examples, we propose a new defense, the\nEnsemble Adversarial Training (EAT), which exploits ensemble learning and\nadversarial training. EAT promotes the diversity of the ensemble, encouraging\neach model in the ensemble to have different robustness features, and employs a\nself-transferring mechanism to make full use of the robustness statistics of\nthe whole ensemble in the update of every single model. We evaluate the EAT\nmethod on three widely-used datasets with two popular model architectures. The\nresults show that the proposed EAT method greatly outperforms the adaptions of\ndefenses designed for deep classification models."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Ke"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Ke"
                },
                "author": "Xiaopeng Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v4",
                "updated": "2025-01-02T03:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    14,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16668v2",
                "updated": "2025-01-02T03:02:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    2,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-22T03:53:46Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    53,
                    46,
                    1,
                    296,
                    0
                ],
                "title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling"
                },
                "summary": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance."
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Gromit Yeuk-Yin Chan"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Sonia Castelo Quispe"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Leslie Welch"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Jing Qian"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qian"
                },
                "author": "Jing Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01023v1",
                "updated": "2025-01-02T02:51:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    51,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:51:16Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    51,
                    16,
                    3,
                    2,
                    0
                ],
                "title": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer"
                },
                "summary": "In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the \\textbf{H}adamard \\textbf{A}ttention\n\\textbf{R}ecurrent Stereo \\textbf{T}ransformer (HART) that incorporates the\nfollowing components: 1) For faster inference, we present a Hadamard product\nparadigm for the attention mechanism, achieving linear computational\ncomplexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the\ndifferences between relevant and irrelevant feature responses. This allows HART\nto focus on important details. DAK also converts zero elements to non-zero\nelements to mitigate the reduced expressiveness caused by the low-rank\nbottleneck. 3) To compensate for the spatial and channel interaction missing in\nthe Hadamard product, we propose MKOI to capture both global and local\ninformation through the interleaving of large and small kernel convolutions.\nExperimental results demonstrate the effectiveness of our HART. In reflective\narea, HART ranked \\textbf{1st} on the KITTI 2012 benchmark among all published\nmethods at the time of submission. Code is available at\n\\url{https://github.com/ZYangChen/HART}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the \\textbf{H}adamard \\textbf{A}ttention\n\\textbf{R}ecurrent Stereo \\textbf{T}ransformer (HART) that incorporates the\nfollowing components: 1) For faster inference, we present a Hadamard product\nparadigm for the attention mechanism, achieving linear computational\ncomplexity. 2) We designed a Dense Attention Kernel (DAK) to amplify the\ndifferences between relevant and irrelevant feature responses. This allows HART\nto focus on important details. DAK also converts zero elements to non-zero\nelements to mitigate the reduced expressiveness caused by the low-rank\nbottleneck. 3) To compensate for the spatial and channel interaction missing in\nthe Hadamard product, we propose MKOI to capture both global and local\ninformation through the interleaving of large and small kernel convolutions.\nExperimental results demonstrate the effectiveness of our HART. In reflective\narea, HART ranked \\textbf{1st} on the KITTI 2012 benchmark among all published\nmethods at the time of submission. Code is available at\n\\url{https://github.com/ZYangChen/HART}."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yongjun Zhang"
                    },
                    {
                        "name": "Wenting Li"
                    },
                    {
                        "name": "Bingshu Wang"
                    },
                    {
                        "name": "Yabo Wu"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01014v1",
                "updated": "2025-01-02T02:35:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    35,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:35:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    35,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based\n  on Large language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based\n  on Large language Model"
                },
                "summary": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail."
                },
                "authors": [
                    {
                        "name": "Chengze Zhang"
                    },
                    {
                        "name": "Changshan Li"
                    },
                    {
                        "name": "Shiyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyang Gao"
                },
                "author": "Shiyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.07722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.07722v2",
                "updated": "2025-01-02T02:24:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    24,
                    58,
                    3,
                    2,
                    0
                ],
                "published": "2021-10-12T15:55:37Z",
                "published_parsed": [
                    2021,
                    10,
                    12,
                    15,
                    55,
                    37,
                    1,
                    285,
                    0
                ],
                "title": "The Sigma-max System Induced from Randomness & Fuzziness and its\n  Application in Time Series Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sigma-max System Induced from Randomness & Fuzziness and its\n  Application in Time Series Prediction"
                },
                "summary": "This paper managed to induce probability theory (sigma system) and\npossibility theory (max system) respectively from the clearly-defined\nrandomness and fuzziness, while focusing the question why the key axiom of\n\"maxitivity\" is adopted for possibility measure. Such an objective is achieved\nby following three steps: a) the establishment of mathematical definitions of\nrandomness and fuzziness; b) the development of intuitive definition of\npossibility as measure of fuzziness based on compatibility interpretation; c)\nthe abstraction of the axiomatic definitions of probability/ possibility from\ntheir intuitive definitions, by taking advantage of properties of the\nwell-defined randomness and fuzziness. We derived the conclusion that \"max\" is\nthe only but un-strict disjunctive operator that is applicable across the fuzzy\nevent space, and is an exact operator for extracting the value from the fuzzy\nsample space that leads to the largest possibility of one. Then a demonstration\nexample of stock price prediction is presented, which confirms that max\ninference indeed exhibits distinctive performance, with an improvement up to\n18.99%, over sigma inference for the investigated application. Our work\nprovides a physical foundation for the axiomatic definition of possibility for\nthe measure of fuzziness, which hopefully would facilitate wider adoption of\npossibility theory in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper managed to induce probability theory (sigma system) and\npossibility theory (max system) respectively from the clearly-defined\nrandomness and fuzziness, while focusing the question why the key axiom of\n\"maxitivity\" is adopted for possibility measure. Such an objective is achieved\nby following three steps: a) the establishment of mathematical definitions of\nrandomness and fuzziness; b) the development of intuitive definition of\npossibility as measure of fuzziness based on compatibility interpretation; c)\nthe abstraction of the axiomatic definitions of probability/ possibility from\ntheir intuitive definitions, by taking advantage of properties of the\nwell-defined randomness and fuzziness. We derived the conclusion that \"max\" is\nthe only but un-strict disjunctive operator that is applicable across the fuzzy\nevent space, and is an exact operator for extracting the value from the fuzzy\nsample space that leads to the largest possibility of one. Then a demonstration\nexample of stock price prediction is presented, which confirms that max\ninference indeed exhibits distinctive performance, with an improvement up to\n18.99%, over sigma inference for the investigated application. Our work\nprovides a physical foundation for the axiomatic definition of possibility for\nthe measure of fuzziness, which hopefully would facilitate wider adoption of\npossibility theory in practice."
                },
                "authors": [
                    {
                        "name": "Wei Mei"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Yuanzeng Cheng"
                    },
                    {
                        "name": "Limin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Limin Liu"
                },
                "author": "Limin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.07722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.07722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B48, 03B52, 60A05, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01009v1",
                "updated": "2025-01-02T02:14:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    14,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:14:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    14,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "Detection of \"diffuse\" coronal He I 1083 during the April 8 2024 Solar\n  Eclipse: evidence for terrestrial atmospheric scattering origin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of \"diffuse\" coronal He I 1083 during the April 8 2024 Solar\n  Eclipse: evidence for terrestrial atmospheric scattering origin"
                },
                "summary": "Strong He I 1083 nm atomic line signals have been previously measured during\ntotal solar eclipses at coronal heights above the lunar limb. This rather\nunexpected measurement has kindled a discussion about the hypothesized presence\nof significant amounts of neutral helium at coronal conditions. We performed\nspectroscopic observations of the He I 1083 nm spectroscopic region with the\nnewly built CHEESE instrument during the April 8th 2024 total solar eclipse to\ntest the presence of He I 1083 in the solar corona. We detected the He I 1083,\nthe forbidden coronal line Fe XIII 1074.7 nm, as well as the chromospheric H I\n1093.8 nm Paschen-{\\gamma} line in our eclipse observations. The chromospheric\nHe I 1083 and H I 1093.8 nm Paschen-{\\gamma} lines are detected in the corona\nas well as on the lunar disc. Our findings point toward a non-solar origin of\nthe He I 1083 signal during the April 8th 2024 eclipse that challenge the\nnotion of abundant neutral helium in the solar corona inferred from eclipse\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong He I 1083 nm atomic line signals have been previously measured during\ntotal solar eclipses at coronal heights above the lunar limb. This rather\nunexpected measurement has kindled a discussion about the hypothesized presence\nof significant amounts of neutral helium at coronal conditions. We performed\nspectroscopic observations of the He I 1083 nm spectroscopic region with the\nnewly built CHEESE instrument during the April 8th 2024 total solar eclipse to\ntest the presence of He I 1083 in the solar corona. We detected the He I 1083,\nthe forbidden coronal line Fe XIII 1074.7 nm, as well as the chromospheric H I\n1093.8 nm Paschen-{\\gamma} line in our eclipse observations. The chromospheric\nHe I 1083 and H I 1093.8 nm Paschen-{\\gamma} lines are detected in the corona\nas well as on the lunar disc. Our findings point toward a non-solar origin of\nthe He I 1083 signal during the April 8th 2024 eclipse that challenge the\nnotion of abundant neutral helium in the solar corona inferred from eclipse\nobservations."
                },
                "authors": [
                    {
                        "name": "M. E. Molnar"
                    },
                    {
                        "name": "R. Casini"
                    },
                    {
                        "name": "P. Bryans"
                    },
                    {
                        "name": "B. Berkey"
                    },
                    {
                        "name": "K. Tyson"
                    }
                ],
                "author_detail": {
                    "name": "K. Tyson"
                },
                "author": "K. Tyson",
                "arxiv_comment": "Submitted to Solar Physics; comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00999v1",
                "updated": "2025-01-02T01:33:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T01:33:58Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "title": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment."
                },
                "authors": [
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Zhengyu Qi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v2",
                "updated": "2025-01-02T01:11:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    11,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00982v1",
                "updated": "2025-01-02T00:01:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    0,
                    1,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T00:01:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    0,
                    1,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for\n  interpretable mental health screening through psychometric practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for\n  interpretable mental health screening through psychometric practice"
                },
                "summary": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices."
                },
                "authors": [
                    {
                        "name": "Federico Ravenda"
                    },
                    {
                        "name": "Seyed Ali Bahrainian"
                    },
                    {
                        "name": "Andrea Raballo"
                    },
                    {
                        "name": "Antonietta Mira"
                    },
                    {
                        "name": "Noriko Kando"
                    }
                ],
                "author_detail": {
                    "name": "Noriko Kando"
                },
                "author": "Noriko Kando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13184v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13184v5",
                "updated": "2025-01-01T23:34:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    34,
                    53,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-20T17:49:46Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    17,
                    49,
                    46,
                    1,
                    51,
                    0
                ],
                "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents"
                },
                "summary": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents."
                },
                "authors": [
                    {
                        "name": "Zhaoqian Xue"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Beichen Wang"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hua Tang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13184v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13184v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12944v2",
                "updated": "2025-01-01T23:21:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    21,
                    43,
                    2,
                    1,
                    0
                ],
                "published": "2024-11-20T00:28:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    28,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials"
                },
                "summary": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nits flexibility introduces inferential challenges, with two fundamental ones\nbeing the precise definition of treatment effects and robust, efficient\ninference on these effects. Central to these challenges is the definition of an\nappropriate target population for the estimand, as some commonly used\npopulations can be unexpectedly problematic. This article, for the first time,\npresents a clear framework for constructing a clinically meaningful estimand\nwith precise specificity regarding the population of interest. The proposed\nestimand defines the treatment effect as a contrast of expected outcomes\nbetween two treatments within the entire concurrently eligible (ECE) population\n- the largest population that preserves the integrity of randomization -\nestablishing a foundation for future research in platform trials. Then, we\ndevelop weighting and post-stratification methods for estimation of treatment\neffects with minimal assumptions. To fully leverage the efficiency potential of\nplatform trials, we also consider a model-assisted approach for baseline\ncovariate adjustment to gain efficiency while maintaining robustness against\nmodel misspecification. We derive and compare asymptotic distributions of\nproposed estimators in theory and propose robust variance estimators. The\nproposed estimators are empirically evaluated in a simulation study and\nillustrated in the SIMPLIFY trial, using the R package RobinCID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nits flexibility introduces inferential challenges, with two fundamental ones\nbeing the precise definition of treatment effects and robust, efficient\ninference on these effects. Central to these challenges is the definition of an\nappropriate target population for the estimand, as some commonly used\npopulations can be unexpectedly problematic. This article, for the first time,\npresents a clear framework for constructing a clinically meaningful estimand\nwith precise specificity regarding the population of interest. The proposed\nestimand defines the treatment effect as a contrast of expected outcomes\nbetween two treatments within the entire concurrently eligible (ECE) population\n- the largest population that preserves the integrity of randomization -\nestablishing a foundation for future research in platform trials. Then, we\ndevelop weighting and post-stratification methods for estimation of treatment\neffects with minimal assumptions. To fully leverage the efficiency potential of\nplatform trials, we also consider a model-assisted approach for baseline\ncovariate adjustment to gain efficiency while maintaining robustness against\nmodel misspecification. We derive and compare asymptotic distributions of\nproposed estimators in theory and propose robust variance estimators. The\nproposed estimators are empirically evaluated in a simulation study and\nillustrated in the SIMPLIFY trial, using the R package RobinCID."
                },
                "authors": [
                    {
                        "name": "Yuhan Qian"
                    },
                    {
                        "name": "Yifan Yi"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Yanyao Yi"
                    },
                    {
                        "name": "Nicole Mayer-Hamblett"
                    },
                    {
                        "name": "Patrick J. Heagerty"
                    },
                    {
                        "name": "Ting Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ting Ye"
                },
                "author": "Ting Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00959v1",
                "updated": "2025-01-01T21:31:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:31:47Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "title": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs"
                },
                "summary": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v1",
                "updated": "2025-01-01T21:29:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00957v1",
                "updated": "2025-01-01T21:23:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors"
                },
                "summary": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15166v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15166v3",
                "updated": "2025-01-01T20:57:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    57,
                    38,
                    2,
                    1,
                    0
                ],
                "published": "2024-10-19T17:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    35,
                    12,
                    5,
                    293,
                    0
                ],
                "title": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso"
                },
                "summary": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings."
                },
                "authors": [
                    {
                        "name": "Alexandre Belloni"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Matthew Harding"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Harding"
                },
                "author": "Matthew Harding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15166v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00952v1",
                "updated": "2025-01-01T20:48:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    48,
                    26,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:48:26Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    48,
                    26,
                    2,
                    1,
                    0
                ],
                "title": "Active and transfer learning with partially Bayesian neural networks for\n  materials and chemicals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active and transfer learning with partially Bayesian neural networks for\n  materials and chemicals"
                },
                "summary": "Active learning, an iterative process of selecting the most informative data\npoints for exploration, is crucial for efficient characterization of materials\nand chemicals property space. Neural networks excel at predicting these\nproperties but lack the uncertainty quantification needed for active\nlearning-driven exploration. Fully Bayesian neural networks, in which weights\nare treated as probability distributions inferred via advanced Markov Chain\nMonte Carlo methods, offer robust uncertainty quantification but at high\ncomputational cost. Here, we show that partially Bayesian neural networks\n(PBNNs), where only selected layers have probabilistic weights while others\nremain deterministic, can achieve accuracy and uncertainty estimates on active\nlearning tasks comparable to fully Bayesian networks at lower computational\ncost. Furthermore, by initializing prior distributions with weights pre-trained\non theoretical calculations, we demonstrate that PBNNs can effectively leverage\ncomputational predictions to accelerate active learning of experimental data.\nWe validate these approaches on both molecular property prediction and\nmaterials science tasks, establishing PBNNs as a practical tool for active\nlearning with limited, complex datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active learning, an iterative process of selecting the most informative data\npoints for exploration, is crucial for efficient characterization of materials\nand chemicals property space. Neural networks excel at predicting these\nproperties but lack the uncertainty quantification needed for active\nlearning-driven exploration. Fully Bayesian neural networks, in which weights\nare treated as probability distributions inferred via advanced Markov Chain\nMonte Carlo methods, offer robust uncertainty quantification but at high\ncomputational cost. Here, we show that partially Bayesian neural networks\n(PBNNs), where only selected layers have probabilistic weights while others\nremain deterministic, can achieve accuracy and uncertainty estimates on active\nlearning tasks comparable to fully Bayesian networks at lower computational\ncost. Furthermore, by initializing prior distributions with weights pre-trained\non theoretical calculations, we demonstrate that PBNNs can effectively leverage\ncomputational predictions to accelerate active learning of experimental data.\nWe validate these approaches on both molecular property prediction and\nmaterials science tasks, establishing PBNNs as a practical tool for active\nlearning with limited, complex datasets."
                },
                "authors": [
                    {
                        "name": "Sarah I. Allec"
                    },
                    {
                        "name": "Maxim Ziatdinov"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Ziatdinov"
                },
                "author": "Maxim Ziatdinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00940v1",
                "updated": "2025-01-01T19:44:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    19,
                    44,
                    30,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T19:44:30Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    19,
                    44,
                    30,
                    2,
                    1,
                    0
                ],
                "title": "SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI\n  and Structured Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI\n  and Structured Prompt Engineering"
                },
                "summary": "The rapid evolution of modern malware presents significant challenges to the\ndevelopment of effective defense mechanisms. Traditional cyber deception\ntechniques often rely on static or manually configured parameters, limiting\ntheir adaptability to dynamic and sophisticated threats. This study leverages\nGenerative AI (GenAI) models to automate the creation of adaptive cyber\ndeception ploys, focusing on structured prompt engineering (PE) to enhance\nrelevance, actionability, and deployability. We introduce a systematic\nframework (SPADE) to address inherent challenges large language models (LLMs)\npose to adaptive deceptions, including generalized outputs, ambiguity,\nunder-utilization of contextual information, and scalability constraints.\nEvaluations across diverse malware scenarios using metrics such as Recall,\nExact Match (EM), BLEU Score, and expert quality assessments identified\nChatGPT-4o as the top performer. Additionally, it achieved high engagement\n(93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini\ndemonstrated competitive performance, with Llama3.2 showing promise despite\nrequiring further optimization. These findings highlight the transformative\npotential of GenAI in automating scalable, adaptive deception strategies and\nunderscore the critical role of structured PE in advancing real-world\ncybersecurity applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of modern malware presents significant challenges to the\ndevelopment of effective defense mechanisms. Traditional cyber deception\ntechniques often rely on static or manually configured parameters, limiting\ntheir adaptability to dynamic and sophisticated threats. This study leverages\nGenerative AI (GenAI) models to automate the creation of adaptive cyber\ndeception ploys, focusing on structured prompt engineering (PE) to enhance\nrelevance, actionability, and deployability. We introduce a systematic\nframework (SPADE) to address inherent challenges large language models (LLMs)\npose to adaptive deceptions, including generalized outputs, ambiguity,\nunder-utilization of contextual information, and scalability constraints.\nEvaluations across diverse malware scenarios using metrics such as Recall,\nExact Match (EM), BLEU Score, and expert quality assessments identified\nChatGPT-4o as the top performer. Additionally, it achieved high engagement\n(93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini\ndemonstrated competitive performance, with Llama3.2 showing promise despite\nrequiring further optimization. These findings highlight the transformative\npotential of GenAI in automating scalable, adaptive deception strategies and\nunderscore the critical role of structured PE in advancing real-world\ncybersecurity applications."
                },
                "authors": [
                    {
                        "name": "Shihab Ahmed"
                    },
                    {
                        "name": "A B M Mohaimenur Rahman"
                    },
                    {
                        "name": "Md Morshed Alam"
                    },
                    {
                        "name": "Md Sajidul Islam Sajid"
                    }
                ],
                "author_detail": {
                    "name": "Md Sajidul Islam Sajid"
                },
                "author": "Md Sajidul Islam Sajid",
                "arxiv_journal_ref": "2025 IEEE 15th Annual Computing and Communication Workshop and\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00915v1",
                "updated": "2025-01-01T18:22:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    18,
                    22,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T18:22:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    18,
                    22,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "Diffusion Policies for Generative Modeling of Spacecraft Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies for Generative Modeling of Spacecraft Trajectories"
                },
                "summary": "Machine learning has demonstrated remarkable promise for solving the\ntrajectory generation problem and in paving the way for online use of\ntrajectory optimization for resource-constrained spacecraft. However, a key\nshortcoming in current machine learning-based methods for trajectory generation\nis that they require large datasets and even small changes to the original\ntrajectory design requirements necessitate retraining new models to learn the\nparameter-to-solution mapping. In this work, we leverage compositional\ndiffusion modeling to efficiently adapt out-of-distribution data and problem\nvariations in a few-shot framework for 6 degree-of-freedom (DoF) powered\ndescent trajectory generation. Unlike traditional deep learning methods that\ncan only learn the underlying structure of one specific trajectory optimization\nproblem, diffusion models are a powerful generative modeling framework that\nrepresents the solution as a probability density function (PDF) and this allows\nfor the composition of PDFs encompassing a variety of trajectory design\nspecifications and constraints. We demonstrate the capability of compositional\ndiffusion models for inference-time 6 DoF minimum-fuel landing site selection\nand composable constraint representations. Using these samples as initial\nguesses for 6 DoF powered descent guidance enables dynamically feasible and\ncomputationally efficient trajectory generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has demonstrated remarkable promise for solving the\ntrajectory generation problem and in paving the way for online use of\ntrajectory optimization for resource-constrained spacecraft. However, a key\nshortcoming in current machine learning-based methods for trajectory generation\nis that they require large datasets and even small changes to the original\ntrajectory design requirements necessitate retraining new models to learn the\nparameter-to-solution mapping. In this work, we leverage compositional\ndiffusion modeling to efficiently adapt out-of-distribution data and problem\nvariations in a few-shot framework for 6 degree-of-freedom (DoF) powered\ndescent trajectory generation. Unlike traditional deep learning methods that\ncan only learn the underlying structure of one specific trajectory optimization\nproblem, diffusion models are a powerful generative modeling framework that\nrepresents the solution as a probability density function (PDF) and this allows\nfor the composition of PDFs encompassing a variety of trajectory design\nspecifications and constraints. We demonstrate the capability of compositional\ndiffusion models for inference-time 6 DoF minimum-fuel landing site selection\nand composable constraint representations. Using these samples as initial\nguesses for 6 DoF powered descent guidance enables dynamically feasible and\ncomputationally efficient trajectory generation."
                },
                "authors": [
                    {
                        "name": "Julia Briden"
                    },
                    {
                        "name": "Breanna Johnson"
                    },
                    {
                        "name": "Richard Linares"
                    },
                    {
                        "name": "Abhishek Cauligi"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Cauligi"
                },
                "author": "Abhishek Cauligi",
                "arxiv_comment": "AIAA SCITECH 2025 Forum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00911v1",
                "updated": "2025-01-01T17:58:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    58,
                    31,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T17:58:31Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    58,
                    31,
                    2,
                    1,
                    0
                ],
                "title": "Aligning LLMs with Domain Invariant Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs with Domain Invariant Reward Models"
                },
                "summary": "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https://github.com/portal-cornell/dial}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https://github.com/portal-cornell/dial}."
                },
                "authors": [
                    {
                        "name": "David Wu"
                    },
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00906v1",
                "updated": "2025-01-01T17:38:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T17:38:40Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "title": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things"
                },
                "summary": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures."
                },
                "authors": [
                    {
                        "name": "Talha Zeeshan"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Susanna Pirttikangas"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    }
                ],
                "author_detail": {
                    "name": "Sasu Tarkoma"
                },
                "author": "Sasu Tarkoma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00895v1",
                "updated": "2025-01-01T16:56:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    56,
                    43,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:56:43Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    56,
                    43,
                    2,
                    1,
                    0
                ],
                "title": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a\n  Global-Scale Dataset and a Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a\n  Global-Scale Dataset and a Foundation Model"
                },
                "summary": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is \\url{https://chen-yang-liu.github.io/Text2Earth}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is \\url{https://chen-yang-liu.github.io/Text2Earth}"
                },
                "authors": [
                    {
                        "name": "Chenyang Liu"
                    },
                    {
                        "name": "Keyan Chen"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Zhenwei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zhenwei Shi"
                },
                "author": "Zhenwei Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06970v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06970v3",
                "updated": "2025-01-01T16:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    32,
                    6,
                    2,
                    1,
                    0
                ],
                "published": "2024-08-13T15:27:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    27,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2"
                },
                "summary": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial imagery. The study evaluates these models\nacross diverse lighting conditions, spatial resolutions, and prompt strategies.\nSAM 2 showed slight improvements over SAM, while SAM 2.1 demonstrated notable\nimprovements, particularly in sub-optimal lighting and low resolution\nconditions. SAM models, when prompted by user-defined boxes, outperformed CNN\nin all scenarios; in particular, user-box prompts were found crucial for\nachieving reasonable performance in low resolution data. Additionally, under\nhigh resolution, YOLOv9 automatic prompting outperformed user-points prompting\nby providing reliable prompts to SAM. Under low resolution, SAM 2.1 prompted by\nuser points showed similar performance to SAM 2.1 prompted by YOLOv9,\nhighlighting its zero shot improvements with a single click. In high resolution\nwith optimal lighting imagery, Eff-UNet outperformed SAMs prompted by YOLOv9,\nwhile under sub-optimal lighting conditions, Eff-UNet, and SAM 2.1 prompted by\nYOLOv9, had similar performance. However, SAM is more resource-intensive, and\ndespite improved inference time of SAM 2.1, Eff-UNet is more suitable for\nautomatic segmentation in high resolution data. This research details strengths\nand limitations of each model and outlines the robustness of user-prompted\nimage segmentation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial imagery. The study evaluates these models\nacross diverse lighting conditions, spatial resolutions, and prompt strategies.\nSAM 2 showed slight improvements over SAM, while SAM 2.1 demonstrated notable\nimprovements, particularly in sub-optimal lighting and low resolution\nconditions. SAM models, when prompted by user-defined boxes, outperformed CNN\nin all scenarios; in particular, user-box prompts were found crucial for\nachieving reasonable performance in low resolution data. Additionally, under\nhigh resolution, YOLOv9 automatic prompting outperformed user-points prompting\nby providing reliable prompts to SAM. Under low resolution, SAM 2.1 prompted by\nuser points showed similar performance to SAM 2.1 prompted by YOLOv9,\nhighlighting its zero shot improvements with a single click. In high resolution\nwith optimal lighting imagery, Eff-UNet outperformed SAMs prompted by YOLOv9,\nwhile under sub-optimal lighting conditions, Eff-UNet, and SAM 2.1 prompted by\nYOLOv9, had similar performance. However, SAM is more resource-intensive, and\ndespite improved inference time of SAM 2.1, Eff-UNet is more suitable for\nautomatic segmentation in high resolution data. This research details strengths\nand limitations of each model and outlines the robustness of user-prompted\nimage segmentation models."
                },
                "authors": [
                    {
                        "name": "Osher Rafaeli"
                    },
                    {
                        "name": "Tal Svoray"
                    },
                    {
                        "name": "Roni Blushtein-Livnon"
                    },
                    {
                        "name": "Ariel Nahlieli"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Nahlieli"
                },
                "author": "Ariel Nahlieli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06970v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06970v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00888v1",
                "updated": "2025-01-01T16:28:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    28,
                    21,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:28:21Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    28,
                    21,
                    2,
                    1,
                    0
                ],
                "title": "Unfolding the Headline: Iterative Self-Questioning for News Retrieval\n  and Timeline Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unfolding the Headline: Iterative Self-Questioning for News Retrieval\n  and Timeline Summarization"
                },
                "summary": "In the fast-changing realm of information, the capacity to construct coherent\ntimelines from extensive event-related content has become increasingly\nsignificant and challenging. The complexity arises in aggregating related\ndocuments to build a meaningful event graph around a central topic. This paper\nproposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline\nSummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective\non the integration of Large Language Models (LLMs) to tackle the task of\nTimeline Summarization (TLS). By iteratively reflecting on how events are\nlinked and posing new questions regarding a specific news topic to gather\ninformation online or from an offline knowledge base, LLMs produce and refresh\nchronological summaries based on documents retrieved in each round.\nFurthermore, we curate Open-TLS, a novel dataset of timelines on recent news\ntopics authored by professional journalists to evaluate open-domain TLS where\ninformation overload makes it impossible to find comprehensive relevant\ndocuments from the web. Our experiments indicate that CHRONOS is not only adept\nat open-domain timeline summarization, but it also rivals the performance of\nexisting state-of-the-art systems designed for closed-domain applications,\nwhere a related news corpus is provided for summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-changing realm of information, the capacity to construct coherent\ntimelines from extensive event-related content has become increasingly\nsignificant and challenging. The complexity arises in aggregating related\ndocuments to build a meaningful event graph around a central topic. This paper\nproposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline\nSummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective\non the integration of Large Language Models (LLMs) to tackle the task of\nTimeline Summarization (TLS). By iteratively reflecting on how events are\nlinked and posing new questions regarding a specific news topic to gather\ninformation online or from an offline knowledge base, LLMs produce and refresh\nchronological summaries based on documents retrieved in each round.\nFurthermore, we curate Open-TLS, a novel dataset of timelines on recent news\ntopics authored by professional journalists to evaluate open-domain TLS where\ninformation overload makes it impossible to find comprehensive relevant\ndocuments from the web. Our experiments indicate that CHRONOS is not only adept\nat open-domain timeline summarization, but it also rivals the performance of\nexisting state-of-the-art systems designed for closed-domain applications,\nwhere a related news corpus is provided for summarization."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00885v1",
                "updated": "2025-01-01T16:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    19,
                    48,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    19,
                    48,
                    2,
                    1,
                    0
                ],
                "title": "Representation in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation in large language models"
                },
                "summary": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors."
                },
                "authors": [
                    {
                        "name": "Cameron C. Yetman"
                    }
                ],
                "author_detail": {
                    "name": "Cameron C. Yetman"
                },
                "author": "Cameron C. Yetman",
                "arxiv_comment": "Draft of paper under review. 27 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00881v1",
                "updated": "2025-01-01T16:00:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    0,
                    18,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:00:18Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    0,
                    18,
                    2,
                    1,
                    0
                ],
                "title": "Agentic Systems: A Guide to Transforming Industries with Vertical AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Systems: A Guide to Transforming Industries with Vertical AI\n  Agents"
                },
                "summary": "The evolution of agentic systems represents a significant milestone in\nartificial intelligence and modern software systems, driven by the demand for\nvertical intelligence tailored to diverse industries. These systems enhance\nbusiness outcomes through adaptability, learning, and interaction with dynamic\nenvironments. At the forefront of this revolution are Large Language Model\n(LLM) agents, which serve as the cognitive backbone of these intelligent\nsystems. In response to the need for consistency and scalability, this work\nattempts to define a level of standardization for Vertical AI agent design\npatterns by identifying core building blocks and proposing a \\textbf{Cognitive\nSkills } Module, which incorporates domain-specific, purpose-built inference\ncapabilities. Building on these foundational concepts, this paper offers a\ncomprehensive introduction to agentic systems, detailing their core components,\noperational patterns, and implementation strategies. It further explores\npractical use cases and examples across various industries, highlighting the\ntransformative potential of LLM agents in driving industry-specific\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of agentic systems represents a significant milestone in\nartificial intelligence and modern software systems, driven by the demand for\nvertical intelligence tailored to diverse industries. These systems enhance\nbusiness outcomes through adaptability, learning, and interaction with dynamic\nenvironments. At the forefront of this revolution are Large Language Model\n(LLM) agents, which serve as the cognitive backbone of these intelligent\nsystems. In response to the need for consistency and scalability, this work\nattempts to define a level of standardization for Vertical AI agent design\npatterns by identifying core building blocks and proposing a \\textbf{Cognitive\nSkills } Module, which incorporates domain-specific, purpose-built inference\ncapabilities. Building on these foundational concepts, this paper offers a\ncomprehensive introduction to agentic systems, detailing their core components,\noperational patterns, and implementation strategies. It further explores\npractical use cases and examples across various industries, highlighting the\ntransformative potential of LLM agents in driving industry-specific\napplications."
                },
                "authors": [
                    {
                        "name": "Fouad Bousetouane"
                    }
                ],
                "author_detail": {
                    "name": "Fouad Bousetouane"
                },
                "author": "Fouad Bousetouane",
                "arxiv_comment": "31 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00880v1",
                "updated": "2025-01-01T15:58:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    58,
                    51,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:58:51Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    58,
                    51,
                    2,
                    1,
                    0
                ],
                "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Autoregressive Visual Generation with Cluster-Oriented Token\n  Prediction"
                },
                "summary": "Employing LLMs for visual generation has recently become a research focus.\nHowever, the existing methods primarily transfer the LLM architecture to visual\ngeneration but rarely investigate the fundamental differences between language\nand vision. This oversight may lead to suboptimal utilization of visual\ngeneration capabilities within the LLM framework. In this paper, we explore the\ncharacteristics of visual embedding space under the LLM framework and discover\nthat the correlation between visual embeddings can help achieve more stable and\nrobust generation results. We present IAR, an Improved AutoRegressive Visual\nGeneration Method that enhances the training efficiency and generation quality\nof LLM-based visual generation models. Firstly, we propose a Codebook\nRearrangement strategy that uses balanced k-means clustering algorithm to\nrearrange the visual codebook into clusters, ensuring high similarity among\nvisual features within each cluster. Leveraging the rearranged codebook, we\npropose a Cluster-oriented Cross-entropy Loss that guides the model to\ncorrectly predict the cluster where the token is located. This approach ensures\nthat even if the model predicts the wrong token index, there is a high\nprobability the predicted token is located in the correct cluster, which\nsignificantly enhances the generation quality and robustness. Extensive\nexperiments demonstrate that our method consistently enhances the model\ntraining efficiency and performance from 100M to 1.4B, reducing the training\ntime by half while achieving the same FID. Additionally, our approach can be\napplied to various LLM-based visual generation models and adheres to the\nscaling law, providing a promising direction for future research in LLM-based\nvisual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing LLMs for visual generation has recently become a research focus.\nHowever, the existing methods primarily transfer the LLM architecture to visual\ngeneration but rarely investigate the fundamental differences between language\nand vision. This oversight may lead to suboptimal utilization of visual\ngeneration capabilities within the LLM framework. In this paper, we explore the\ncharacteristics of visual embedding space under the LLM framework and discover\nthat the correlation between visual embeddings can help achieve more stable and\nrobust generation results. We present IAR, an Improved AutoRegressive Visual\nGeneration Method that enhances the training efficiency and generation quality\nof LLM-based visual generation models. Firstly, we propose a Codebook\nRearrangement strategy that uses balanced k-means clustering algorithm to\nrearrange the visual codebook into clusters, ensuring high similarity among\nvisual features within each cluster. Leveraging the rearranged codebook, we\npropose a Cluster-oriented Cross-entropy Loss that guides the model to\ncorrectly predict the cluster where the token is located. This approach ensures\nthat even if the model predicts the wrong token index, there is a high\nprobability the predicted token is located in the correct cluster, which\nsignificantly enhances the generation quality and robustness. Extensive\nexperiments demonstrate that our method consistently enhances the model\ntraining efficiency and performance from 100M to 1.4B, reducing the training\ntime by half while achieving the same FID. Additionally, our approach can be\napplied to various LLM-based visual generation models and adheres to the\nscaling law, providing a promising direction for future research in LLM-based\nvisual generation."
                },
                "authors": [
                    {
                        "name": "Teng Hu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Jieyu Weng"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01426v1",
                "updated": "2025-01-02T18:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "title": "Unifying Specialized Visual Encoders for Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Specialized Visual Encoders for Video Language Models"
                },
                "summary": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding."
                },
                "authors": [
                    {
                        "name": "Jihoon Chung"
                    },
                    {
                        "name": "Tyler Zhu"
                    },
                    {
                        "name": "Max Gonzalez Saez-Diez"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Olga Russakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Olga Russakovsky"
                },
                "author": "Olga Russakovsky",
                "arxiv_comment": "Project page: https://tylerzhu.com/merv/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01417v1",
                "updated": "2025-01-02T18:58:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    58,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:58:03Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    58,
                    3,
                    3,
                    2,
                    0
                ],
                "title": "The Bayesian Global Sky Model (B-GSM): Validation of a Data Driven\n  Bayesian Simultaneous Component Separation and Calibration Algorithm for EoR\n  Foreground Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian Global Sky Model (B-GSM): Validation of a Data Driven\n  Bayesian Simultaneous Component Separation and Calibration Algorithm for EoR\n  Foreground Modelling"
                },
                "summary": "We introduce the Bayesian Global Sky Model (B-GSM), a novel data-driven\nBayesian approach to modelling radio foregrounds at frequencies <400~MHz. B-GSM\naims to address the limitations of previous models by incorporating robust\nerror quantification and calibration. Using nested sampling, we compute\nBayesian evidence and posterior distributions for the spectral behaviour and\nspatial amplitudes of diffuse emission components. Bayesian model comparison is\nused to determine the optimal number of emission components and their spectral\nparametrisation. Posterior sky predictions are conditioned on both diffuse\nemission and absolute temperature datasets, enabling simultaneous component\nseparation and calibration. B-GSM is validated against a synthetic dataset\ndesigned to mimic the partial sky coverage, thermal noise, and calibration\nuncertainties present in real observations of the diffuse sky at low\nfrequencies. B-GSM correctly identifies a model parametrisation with two\nemission components featuring curved power-law spectra. The posterior sky\npredictions agree with the true synthetic sky within statistical uncertainty.\nWe find that the root-mean-square (RMS) residuals between the true and\nposterior predictions for the sky temperature as a function of LST are\nsignificantly reduced, when compared to the uncalibrated dataset. This\nindicates that B-GSM is able to correctly calibrate its posterior sky\nprediction to the independent absolute temperature dataset. We find that while\nthe spectral parameters and component amplitudes exhibit some sensitivity to\nprior assumptions, the posterior sky predictions remain robust across a\nselection of different priors. This is the first of two papers, and is focused\non validation of B-GSMs Bayesian framework, the second paper will present\nresults of deployment on real data and introduce the low-frequency sky model\nwhich will be available for public download.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Bayesian Global Sky Model (B-GSM), a novel data-driven\nBayesian approach to modelling radio foregrounds at frequencies <400~MHz. B-GSM\naims to address the limitations of previous models by incorporating robust\nerror quantification and calibration. Using nested sampling, we compute\nBayesian evidence and posterior distributions for the spectral behaviour and\nspatial amplitudes of diffuse emission components. Bayesian model comparison is\nused to determine the optimal number of emission components and their spectral\nparametrisation. Posterior sky predictions are conditioned on both diffuse\nemission and absolute temperature datasets, enabling simultaneous component\nseparation and calibration. B-GSM is validated against a synthetic dataset\ndesigned to mimic the partial sky coverage, thermal noise, and calibration\nuncertainties present in real observations of the diffuse sky at low\nfrequencies. B-GSM correctly identifies a model parametrisation with two\nemission components featuring curved power-law spectra. The posterior sky\npredictions agree with the true synthetic sky within statistical uncertainty.\nWe find that the root-mean-square (RMS) residuals between the true and\nposterior predictions for the sky temperature as a function of LST are\nsignificantly reduced, when compared to the uncalibrated dataset. This\nindicates that B-GSM is able to correctly calibrate its posterior sky\nprediction to the independent absolute temperature dataset. We find that while\nthe spectral parameters and component amplitudes exhibit some sensitivity to\nprior assumptions, the posterior sky predictions remain robust across a\nselection of different priors. This is the first of two papers, and is focused\non validation of B-GSMs Bayesian framework, the second paper will present\nresults of deployment on real data and introduce the low-frequency sky model\nwhich will be available for public download."
                },
                "authors": [
                    {
                        "name": "George Carter"
                    },
                    {
                        "name": "Will Handley"
                    },
                    {
                        "name": "Mark Ashdown"
                    },
                    {
                        "name": "Nima Razavi-Ghods"
                    }
                ],
                "author_detail": {
                    "name": "Nima Razavi-Ghods"
                },
                "author": "Nima Razavi-Ghods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v2",
                "updated": "2025-01-02T18:46:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    46,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "This version has been updated with further clarification regarding\n  the model size estimates that were mined from public articles only and\n  provided to aid in contextualizing model performance. The authors cannot\n  vouch for the accuracy of those estimates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04655v2",
                "updated": "2025-01-02T17:21:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    21,
                    22,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-05T22:59:26Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    59,
                    26,
                    3,
                    340,
                    0
                ],
                "title": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems"
                },
                "summary": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework."
                },
                "authors": [
                    {
                        "name": "Brian Hsu"
                    },
                    {
                        "name": "Cyrus DiCiccio"
                    },
                    {
                        "name": "Natesh Sivasubramoniapillai"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01366v1",
                "updated": "2025-01-02T17:20:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T17:20:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding"
                },
                "summary": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications."
                },
                "authors": [
                    {
                        "name": "Austin T. Wang"
                    },
                    {
                        "name": "ZeMing Gong"
                    },
                    {
                        "name": "Angel X. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Angel X. Chang"
                },
                "author": "Angel X. Chang",
                "arxiv_comment": "20 pages with 5 figures and 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v3",
                "updated": "2025-01-02T17:17:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    17,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "9 pages, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01336v1",
                "updated": "2025-01-02T16:38:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:38:21Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "title": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git"
                },
                "authors": [
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01335v1",
                "updated": "2025-01-02T16:37:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:37:04Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models"
                },
                "summary": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Johan Wahrus"
                    },
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01332v1",
                "updated": "2025-01-02T16:34:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:34:10Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "title": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension"
                },
                "summary": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers."
                },
                "authors": [
                    {
                        "name": "Yanbo Fang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01329v1",
                "updated": "2025-01-02T16:30:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:30:05Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation"
                },
                "summary": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task."
                },
                "authors": [
                    {
                        "name": "Shuzheng Gao"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xiaoqian Jiao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Michael Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lyu"
                },
                "author": "Michael Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06083v2",
                "updated": "2025-01-02T16:14:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    14,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-04T09:50:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    50,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval"
                },
                "summary": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01312v1",
                "updated": "2025-01-02T15:53:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:53:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Learning Spectral Methods by Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spectral Methods by Transformers"
                },
                "summary": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Yihan He"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Hong-Yu Chen"
                    },
                    {
                        "name": "Dennis Wu"
                    },
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "77 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01306v1",
                "updated": "2025-01-02T15:36:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:36:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01305v1",
                "updated": "2025-01-02T15:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:34:02Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "title": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case"
                },
                "summary": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b."
                },
                "authors": [
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Harshul Surana"
                    },
                    {
                        "name": "Darssan Eswaramoorthi"
                    },
                    {
                        "name": "Yuxin Zi"
                    },
                    {
                        "name": "Vedant Palit"
                    },
                    {
                        "name": "Ritvik Garimella"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01303v1",
                "updated": "2025-01-02T15:32:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Citations and Trust in LLM Generated Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citations and Trust in LLM Generated Responses"
                },
                "summary": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Matthew Facciani"
                    },
                    {
                        "name": "Amrit Poudel"
                    },
                    {
                        "name": "Ellen Joyce"
                    },
                    {
                        "name": "Salvador Aguinaga"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    },
                    {
                        "name": "Tim Weninger"
                    }
                ],
                "author_detail": {
                    "name": "Tim Weninger"
                },
                "author": "Tim Weninger",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01293v1",
                "updated": "2025-01-02T15:19:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    19,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:19:16Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    19,
                    16,
                    3,
                    2,
                    0
                ],
                "title": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite\n  Networks"
                },
                "summary": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "13 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07259v2",
                "updated": "2025-01-02T14:57:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    57,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2024-02-11T18:04:06Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    18,
                    4,
                    6,
                    6,
                    42,
                    0
                ],
                "title": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection"
                },
                "summary": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system."
                },
                "authors": [
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Aymen Fakhreddine"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos",
                "arxiv_comment": "6 pages, 6 figures, accepted by IEEE PIMRC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01285v1",
                "updated": "2025-01-02T14:53:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    53,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T14:53:11Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    53,
                    11,
                    3,
                    2,
                    0
                ],
                "title": "SARA: A Microservice-Based Architecture for Cross-Platform Collaborative\n  Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARA: A Microservice-Based Architecture for Cross-Platform Collaborative\n  Augmented Reality"
                },
                "summary": "Augmented Reality (AR) functionalities may be effectively leveraged in\ncollaborative service scenarios (e.g., remote maintenance, on-site building,\nstreet gaming, etc.). Standard development cycles for collaborative AR require\nto code for each specific visualization platform and implement the necessary\ncontrol mechanisms over the shared assets. This paper describes SARA, an\narchitecture to support cross-platform collaborative Augmented Reality\napplications based on microservices. The architecture is designed to work over\nthe concept of collaboration models (turn, layer, ownership,hierarchy-based and\nunconstrained examples) which regulate the interaction and permissions of each\nuser over the AR assets. Thanks to the reusability of its components, during\nthe development of an application, SARA enables focusing on the application\nlogic while avoiding the implementation of the communication protocol, data\nmodel handling and orchestration between the different, possibly\nheterogeneous,devices involved in the collaboration (i.e., mobile or wearable\nAR devices using different operating systems). To describe how to build an\napplication based on SARA, a prototype for HoloLens and iOS devices has been\nimplemented. the prototype is a collaborative voxel-based game in which several\nplayers work real time together on a piece of land, adding or eliminating cubes\nin a collaborative manner to create buildings and landscapes. Turn-based and\nunconstrained collaboration models are applied to regulate the interaction, the\ndevelopment workflow for this case study shows how the architecture serves as a\nframework to support the deployment of collaborative AR services, enabling the\nreuse of collaboration model components, agnostically handling client\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality (AR) functionalities may be effectively leveraged in\ncollaborative service scenarios (e.g., remote maintenance, on-site building,\nstreet gaming, etc.). Standard development cycles for collaborative AR require\nto code for each specific visualization platform and implement the necessary\ncontrol mechanisms over the shared assets. This paper describes SARA, an\narchitecture to support cross-platform collaborative Augmented Reality\napplications based on microservices. The architecture is designed to work over\nthe concept of collaboration models (turn, layer, ownership,hierarchy-based and\nunconstrained examples) which regulate the interaction and permissions of each\nuser over the AR assets. Thanks to the reusability of its components, during\nthe development of an application, SARA enables focusing on the application\nlogic while avoiding the implementation of the communication protocol, data\nmodel handling and orchestration between the different, possibly\nheterogeneous,devices involved in the collaboration (i.e., mobile or wearable\nAR devices using different operating systems). To describe how to build an\napplication based on SARA, a prototype for HoloLens and iOS devices has been\nimplemented. the prototype is a collaborative voxel-based game in which several\nplayers work real time together on a piece of land, adding or eliminating cubes\nin a collaborative manner to create buildings and landscapes. Turn-based and\nunconstrained collaboration models are applied to regulate the interaction, the\ndevelopment workflow for this case study shows how the architecture serves as a\nframework to support the deployment of collaborative AR services, enabling the\nreuse of collaboration model components, agnostically handling client\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Diego Vaquero-Melchor"
                    },
                    {
                        "name": "Ana M. Bernardos"
                    },
                    {
                        "name": "Luca Bergesio"
                    }
                ],
                "author_detail": {
                    "name": "Luca Bergesio"
                },
                "author": "Luca Bergesio",
                "arxiv_doi": "10.3390/app10062074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/app10062074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Applied Sciences 2020, 10(6), 2074",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01273v1",
                "updated": "2025-01-02T14:13:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T14:13:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Does a Large Language Model Really Speak in Human-Like Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does a Large Language Model Really Speak in Human-Like Language?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text."
                },
                "authors": [
                    {
                        "name": "Mose Park"
                    },
                    {
                        "name": "Yunjin Choi"
                    },
                    {
                        "name": "Jong-June Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jong-June Jeon"
                },
                "author": "Jong-June Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01264v1",
                "updated": "2025-01-02T13:59:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:59:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgCo: Program Helps Self-Correction of Large Language Models"
                },
                "summary": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools."
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19530v2",
                "updated": "2025-01-02T13:54:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    54,
                    17,
                    3,
                    2,
                    0
                ],
                "published": "2024-03-28T16:06:06Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    16,
                    6,
                    6,
                    3,
                    88,
                    0
                ],
                "title": "Detecting Financial Bots on the Ethereum Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Financial Bots on the Ethereum Blockchain"
                },
                "summary": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape."
                },
                "authors": [
                    {
                        "name": "Thomas Niedermayer"
                    },
                    {
                        "name": "Pietro Saggese"
                    },
                    {
                        "name": "Bernhard Haslhofer"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Haslhofer"
                },
                "author": "Bernhard Haslhofer",
                "arxiv_doi": "10.1145/3589335.3651959",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3589335.3651959",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.19530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01887v3",
                "updated": "2025-01-02T13:49:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-02T02:18:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    18,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"
                },
                "summary": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Fanzeng Xia"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01257v1",
                "updated": "2025-01-02T13:49:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings"
                },
                "summary": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 20 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 20 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01246v1",
                "updated": "2025-01-02T13:14:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    14,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:14:28Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    14,
                    28,
                    3,
                    2,
                    0
                ],
                "title": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base\n  Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base\n  Completion"
                },
                "summary": "Integrating large language models (LLMs) with rule-based reasoning offers a\npowerful solution for improving the flexibility and reliability of Knowledge\nBase Completion (KBC). Traditional rule-based KBC methods offer verifiable\nreasoning yet lack flexibility, while LLMs provide strong semantic\nunderstanding yet suffer from hallucinations. With the aim of combining LLMs'\nunderstanding capability with the logical and rigor of rule-based approaches,\nwe propose a novel framework consisting of a Subgraph Extractor, an LLM\nProposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs\nfrom the KB. Then, the LLM uses these subgraphs to propose diverse and\nmeaningful rules that are helpful for inferring missing facts. To effectively\navoid hallucination in LLMs' generations, these proposed rules are further\nrefined by a Rule Reasoner to pinpoint the most significant rules in the KB for\nKnowledge Base Completion. Our approach offers several key benefits: the\nutilization of LLMs to enhance the richness and diversity of the proposed rules\nand the integration with rule-based reasoning to improve reliability. Our\nmethod also demonstrates strong performance across diverse KB datasets,\nhighlighting the robustness and generalizability of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) with rule-based reasoning offers a\npowerful solution for improving the flexibility and reliability of Knowledge\nBase Completion (KBC). Traditional rule-based KBC methods offer verifiable\nreasoning yet lack flexibility, while LLMs provide strong semantic\nunderstanding yet suffer from hallucinations. With the aim of combining LLMs'\nunderstanding capability with the logical and rigor of rule-based approaches,\nwe propose a novel framework consisting of a Subgraph Extractor, an LLM\nProposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs\nfrom the KB. Then, the LLM uses these subgraphs to propose diverse and\nmeaningful rules that are helpful for inferring missing facts. To effectively\navoid hallucination in LLMs' generations, these proposed rules are further\nrefined by a Rule Reasoner to pinpoint the most significant rules in the KB for\nKnowledge Base Completion. Our approach offers several key benefits: the\nutilization of LLMs to enhance the richness and diversity of the proposed rules\nand the integration with rule-based reasoning to improve reliability. Our\nmethod also demonstrates strong performance across diverse KB datasets,\nhighlighting the robustness and generalizability of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Qiyuan He"
                    },
                    {
                        "name": "Jianfei Yu"
                    },
                    {
                        "name": "Wenya Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenya Wang"
                },
                "author": "Wenya Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01245v1",
                "updated": "2025-01-02T13:12:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    12,
                    12,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:12:12Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    12,
                    12,
                    3,
                    2,
                    0
                ],
                "title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal\n  Perturbation and Learning Stabilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal\n  Perturbation and Learning Stabilization"
                },
                "summary": "Human action understanding is crucial for the advancement of multimodal\nsystems. While recent developments, driven by powerful large language models\n(LLMs), aim to be general enough to cover a wide range of categories, they\noften overlook the need for more specific capabilities. In this work, we\naddress the more challenging task of Fine-grained Action Recognition (FAR),\nwhich focuses on detailed semantic labels within shorter temporal duration\n(e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating\nfine-grained labels and the substantial data needed for fine-tuning LLMs, we\npropose to adopt semi-supervised learning (SSL). Our framework, SeFAR,\nincorporates several innovative designs to tackle these challenges.\nSpecifically, to capture sufficient visual details, we construct Dual-level\ntemporal elements as more effective representations, based on which we design a\nnew strong augmentation strategy for the Teacher-Student learning paradigm\nthrough involving moderate temporal perturbation. Furthermore, to handle the\nhigh uncertainty within the teacher model's predictions for FAR, we propose the\nAdaptive Regulation to stabilize the learning process. Experiments show that\nSeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and\nFineDiving, across various data scopes. It also outperforms other\nsemi-supervised methods on two classical coarse-grained datasets, UCF101 and\nHMDB51. Further analysis and ablation studies validate the effectiveness of our\ndesigns. Additionally, we show that the features extracted by our SeFAR could\nlargely promote the ability of multimodal foundation models to understand\nfine-grained and domain-specific semantics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human action understanding is crucial for the advancement of multimodal\nsystems. While recent developments, driven by powerful large language models\n(LLMs), aim to be general enough to cover a wide range of categories, they\noften overlook the need for more specific capabilities. In this work, we\naddress the more challenging task of Fine-grained Action Recognition (FAR),\nwhich focuses on detailed semantic labels within shorter temporal duration\n(e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating\nfine-grained labels and the substantial data needed for fine-tuning LLMs, we\npropose to adopt semi-supervised learning (SSL). Our framework, SeFAR,\nincorporates several innovative designs to tackle these challenges.\nSpecifically, to capture sufficient visual details, we construct Dual-level\ntemporal elements as more effective representations, based on which we design a\nnew strong augmentation strategy for the Teacher-Student learning paradigm\nthrough involving moderate temporal perturbation. Furthermore, to handle the\nhigh uncertainty within the teacher model's predictions for FAR, we propose the\nAdaptive Regulation to stabilize the learning process. Experiments show that\nSeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and\nFineDiving, across various data scopes. It also outperforms other\nsemi-supervised methods on two classical coarse-grained datasets, UCF101 and\nHMDB51. Further analysis and ablation studies validate the effectiveness of our\ndesigns. Additionally, we show that the features extracted by our SeFAR could\nlargely promote the ability of multimodal foundation models to understand\nfine-grained and domain-specific semantics."
                },
                "authors": [
                    {
                        "name": "Yongle Huang"
                    },
                    {
                        "name": "Haodong Chen"
                    },
                    {
                        "name": "Zhenbang Xu"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Haozhou Sun"
                    },
                    {
                        "name": "Dian Shao"
                    }
                ],
                "author_detail": {
                    "name": "Dian Shao"
                },
                "author": "Dian Shao",
                "arxiv_comment": "AAAI 2025; Code: https://github.com/KyleHuang9/SeFAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v4",
                "updated": "2025-01-02T13:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    11,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "Paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01237v1",
                "updated": "2025-01-02T12:55:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T12:55:27Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "title": "Automated Self-Refinement and Self-Correction for LLM-based Product\n  Attribute Value Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Self-Refinement and Self-Correction for LLM-based Product\n  Attribute Value Extraction"
                },
                "summary": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques, error-based prompt rewriting and self-correction,\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques have only a marginal impact on the model's performance across the\ndifferent scenarios, while significantly increasing processing costs. For\nscenarios with training data, fine-tuning yields the highest performance, while\nthe ramp-up costs of fine-tuning are balanced out as the amount of product\ndescriptions increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques, error-based prompt rewriting and self-correction,\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques have only a marginal impact on the model's performance across the\ndifferent scenarios, while significantly increasing processing costs. For\nscenarios with training data, fine-tuning yields the highest performance, while\nthe ramp-up costs of fine-tuning are balanced out as the amount of product\ndescriptions increases."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01224v1",
                "updated": "2025-01-02T12:15:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    15,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T12:15:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    15,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems"
                },
                "summary": "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling."
                },
                "authors": [
                    {
                        "name": "Raphal Ollando"
                    },
                    {
                        "name": "Seung Yeob Shin"
                    },
                    {
                        "name": "Mario Minardi"
                    },
                    {
                        "name": "Nikolas Sidiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Nikolas Sidiropoulos"
                },
                "author": "Nikolas Sidiropoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01205v1",
                "updated": "2025-01-02T11:25:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    45,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T11:25:45Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    45,
                    3,
                    2,
                    0
                ],
                "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A\n  Framework for Senior Design Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A\n  Framework for Senior Design Projects"
                },
                "summary": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of..."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Muhammad Rafay Naeem"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Muhammad Imran Taj"
                    },
                    {
                        "name": "Imran Hashmi"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01203v1",
                "updated": "2025-01-02T11:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T11:25:28Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    25,
                    28,
                    3,
                    2,
                    0
                ],
                "title": "HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning\n  for Journal Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HetGCoT-Rec: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning\n  for Journal Recommendation"
                },
                "summary": "Academic journal recommendation requires effectively combining structural\nunderstanding of scholarly networks with interpretable recommendations. While\ngraph neural networks (GNNs) and large language models (LLMs) excel in their\nrespective domains, current approaches often fail to achieve true integration\nat the reasoning level. We propose HetGCoT-Rec, a framework that deeply\nintegrates heterogeneous graph transformer with LLMs through chain-of-thought\nreasoning. Our framework features two key technical innovations: (1) a\nstructure-aware mechanism that transforms heterogeneous graph neural network\nlearned subgraph information into natural language contexts, utilizing\npredefined metapaths to capture academic relationships, and (2) a multi-step\nreasoning strategy that systematically embeds graph-derived contexts into the\nLLM's stage-wise reasoning process. Experiments on a dataset collected from\nOpenAlex demonstrate that our approach significantly outperforms baseline\nmethods, achieving 96.48% Hit rate and 92.21% H@1 accuracy. Furthermore, we\nvalidate the framework's adaptability across different LLM architectures,\nshowing consistent improvements in both recommendation accuracy and explanation\nquality. Our work demonstrates an effective approach for combining\ngraph-structured reasoning with language models for interpretable academic\nvenue recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic journal recommendation requires effectively combining structural\nunderstanding of scholarly networks with interpretable recommendations. While\ngraph neural networks (GNNs) and large language models (LLMs) excel in their\nrespective domains, current approaches often fail to achieve true integration\nat the reasoning level. We propose HetGCoT-Rec, a framework that deeply\nintegrates heterogeneous graph transformer with LLMs through chain-of-thought\nreasoning. Our framework features two key technical innovations: (1) a\nstructure-aware mechanism that transforms heterogeneous graph neural network\nlearned subgraph information into natural language contexts, utilizing\npredefined metapaths to capture academic relationships, and (2) a multi-step\nreasoning strategy that systematically embeds graph-derived contexts into the\nLLM's stage-wise reasoning process. Experiments on a dataset collected from\nOpenAlex demonstrate that our approach significantly outperforms baseline\nmethods, achieving 96.48% Hit rate and 92.21% H@1 accuracy. Furthermore, we\nvalidate the framework's adaptability across different LLM architectures,\nshowing consistent improvements in both recommendation accuracy and explanation\nquality. Our work demonstrates an effective approach for combining\ngraph-structured reasoning with language models for interpretable academic\nvenue recommendations."
                },
                "authors": [
                    {
                        "name": "Runsong Jia"
                    },
                    {
                        "name": "Mengjia Wu"
                    },
                    {
                        "name": "Ying Ding"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15270v2",
                "updated": "2025-01-02T11:21:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    21,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-17T08:05:32Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    5,
                    32,
                    1,
                    352,
                    0
                ],
                "title": "Baichuan4-Finance Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baichuan4-Finance Technical Report"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field."
                },
                "authors": [
                    {
                        "name": "Hanyu Zhang"
                    },
                    {
                        "name": "Boyu Qiu"
                    },
                    {
                        "name": "Yuhao Feng"
                    },
                    {
                        "name": "Shuqi Li"
                    },
                    {
                        "name": "Qian Ma"
                    },
                    {
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "name": "Qiang Ju"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Jian Xie"
                },
                "author": "Jian Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21349v3",
                "updated": "2025-01-02T11:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    16,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-28T12:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    12,
                    18,
                    22,
                    0,
                    302,
                    0
                ],
                "title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system"
                },
                "summary": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON."
                },
                "authors": [
                    {
                        "name": "Zeyuan Li"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Qiuwu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwu Chen"
                },
                "author": "Qiuwu Chen",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v3",
                "updated": "2025-01-02T11:04:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    11,
                    4,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations and clarify findings of\n  evaluations of the o1 model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05411v2",
                "updated": "2025-01-02T10:56:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-07T18:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    18,
                    23,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Filtering Discomforting Recommendations with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering Discomforting Recommendations with Large Language Models"
                },
                "summary": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized algorithms can inadvertently expose users to discomforting\nrecommendations, potentially triggering negative consequences. The subjectivity\nof discomfort and the black-box nature of these algorithms make it challenging\nto effectively identify and filter such content. To address this, we first\nconducted a formative study to understand users' practices and expectations\nregarding discomforting recommendation filtering. Then, we designed a Large\nLanguage Model (LLM)-based tool named DiscomfortFilter, which constructs an\neditable preference profile for a user and helps the user express filtering\nneeds through conversation to mask discomforting preferences within the\nprofile. Based on the edited profile, DiscomfortFilter facilitates the\ndiscomforting recommendations filtering in a plug-and-play manner, maintaining\nflexibility and transparency. The constructed preference profile improves LLM\nreasoning and simplifies user alignment, enabling a 3.8B open-source LLM to\nrival top commercial models in an offline proxy task. A one-week user study\nwith 24 participants demonstrated the effectiveness of DiscomfortFilter, while\nalso highlighting its potential impact on platform recommendation outcomes. We\nconclude by discussing the ongoing challenges, highlighting its relevance to\nbroader research, assessing stakeholder impact, and outlining future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Yiyang Shao"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Longzhi Du"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "16 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16626v2",
                "updated": "2025-01-02T10:56:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    56,
                    7,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T13:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    43,
                    51,
                    5,
                    356,
                    0
                ],
                "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement"
                },
                "summary": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent speech enhancement (SE) research, transformer and its variants have\nemerged as the predominant methodologies. However, the quadratic complexity of\nthe self-attention mechanism imposes certain limitations on practical\ndeployment. Mamba, as a novel state-space model (SSM), has gained widespread\napplication in natural language processing and computer vision due to its\nstrong capabilities in modeling long sequences and relatively low computational\ncomplexity. In this work, we introduce Mamba-SEUNet, an innovative architecture\nthat integrates Mamba with U-Net for SE tasks. By leveraging bidirectional\nMamba to model forward and backward dependencies of speech signals at different\nresolutions, and incorporating skip connections to capture multi-scale\ninformation, our approach achieves state-of-the-art (SOTA) performance.\nExperimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet\nattains a PESQ score of 3.59, while maintaining low computational complexity.\nWhen combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet\nfurther improves the PESQ score to 3.73."
                },
                "authors": [
                    {
                        "name": "Junyu Wang"
                    },
                    {
                        "name": "Zizhen Lin"
                    },
                    {
                        "name": "Tianrui Wang"
                    },
                    {
                        "name": "Meng Ge"
                    },
                    {
                        "name": "Longbiao Wang"
                    },
                    {
                        "name": "Jianwu Dang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Dang"
                },
                "author": "Jianwu Dang",
                "arxiv_comment": "Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01192v1",
                "updated": "2025-01-02T10:55:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T10:55:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    10,
                    55,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating\n  Large Language Models as Tools for Early Childhood Science Education"
                },
                "summary": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly science literacy gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early childhood science education is crucial for developing scientific\nliteracy, yet translating complex scientific concepts into age-appropriate\ncontent remains challenging for educators. Our study evaluates four leading\nLarge Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their\nability to generate preschool-appropriate scientific explanations across\nbiology, chemistry, and physics. Through systematic evaluation by 30 nursery\nteachers using established pedagogical criteria, we identify significant\ndifferences in the models' capabilities to create engaging, accurate, and\ndevelopmentally appropriate content. Unexpectedly, Claude outperformed other\nmodels, particularly in biological topics, while all LLMs struggled with\nabstract chemical concepts. Our findings provide practical insights for\neducators leveraging AI in early science education and offer guidance for\ndevelopers working to enhance LLMs' educational applications. The results\nhighlight the potential and current limitations of using LLMs to bridge the\nearly science literacy gap."
                },
                "authors": [
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Amin Alibakhshi"
                    }
                ],
                "author_detail": {
                    "name": "Amin Alibakhshi"
                },
                "author": "Amin Alibakhshi",
                "arxiv_comment": "CHI late-breaking work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20367v2",
                "updated": "2025-01-02T09:43:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    43,
                    43,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-29T06:15:41Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    6,
                    15,
                    41,
                    6,
                    364,
                    0
                ],
                "title": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Code LLMs with Reinforcement Learning in Code Generation: A\n  Survey"
                },
                "summary": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid evolution of large language models (LLM), reinforcement\nlearning (RL) has emerged as a pivotal technique for code generation and\noptimization in various domains. This paper presents a systematic survey of the\napplication of RL in code optimization and generation, highlighting its role in\nenhancing compiler optimization, resource allocation, and the development of\nframeworks and tools. Subsequent sections first delve into the intricate\nprocesses of compiler optimization, where RL algorithms are leveraged to\nimprove efficiency and resource utilization. The discussion then progresses to\nthe function of RL in resource allocation, emphasizing register allocation and\nsystem optimization. We also explore the burgeoning role of frameworks and\ntools in code generation, examining how RL can be integrated to bolster their\ncapabilities. This survey aims to serve as a comprehensive resource for\nresearchers and practitioners interested in harnessing the power of RL to\nadvance code generation and optimization techniques."
                },
                "authors": [
                    {
                        "name": "Junqiao Wang"
                    },
                    {
                        "name": "Zeng Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Hengyuan Xu"
                    },
                    {
                        "name": "Kunyu Wu"
                    },
                    {
                        "name": "Guangwu Qian"
                    },
                    {
                        "name": "Qiuwu Chen"
                    },
                    {
                        "name": "Lewei He"
                    }
                ],
                "author_detail": {
                    "name": "Lewei He"
                },
                "author": "Lewei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01163v1",
                "updated": "2025-01-02T09:33:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    33,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T09:33:13Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    33,
                    13,
                    3,
                    2,
                    0
                ],
                "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer"
                },
                "summary": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks. The code and model will be\nreleased to promote future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential\nin 3D-vision-based dialogue and reasoning. However, how to further enhance 3D\nLMMs to achieve fine-grained scene understanding and facilitate flexible\nhuman-agent interaction remains a challenging problem. In this work, we\nintroduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an\nintelligent assistant in comprehending, reasoning, and interacting with the 3D\nworld. Unlike existing top-performing methods that rely on complicated\npipelines-such as offline multi-view feature extraction or additional\ntask-specific heads-3D-LLaVA adopts a minimalist design with integrated\narchitecture and only takes point clouds as input. At the core of 3D-LLaVA is a\nnew Omni Superpoint Transformer (OST), which integrates three functionalities:\n(1) a visual feature selector that converts and selects visual tokens, (2) a\nvisual prompt encoder that embeds interactive visual prompts into the visual\ntoken space, and (3) a referring mask decoder that produces 3D masks based on\ntext description. This versatile OST is empowered by the hybrid pretraining to\nobtain perception priors and leveraged as the visual connector that bridges the\n3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA\nreports impressive results on various benchmarks. The code and model will be\nreleased to promote future exploration."
                },
                "authors": [
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Feras Dayoub"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01149v1",
                "updated": "2025-01-02T09:03:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    3,
                    56,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T09:03:56Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    9,
                    3,
                    56,
                    3,
                    2,
                    0
                ],
                "title": "A3: Android Agent Arena for Mobile GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3: Android Agent Arena for Mobile GUI Agents"
                },
                "summary": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents have become increasingly prevalent in recent years, driven by\nsignificant advancements in the field of large language models (LLMs). Mobile\nGUI agents, a subset of AI agents, are designed to autonomously perform tasks\non mobile devices. While numerous studies have introduced agents, datasets, and\nbenchmarks to advance mobile GUI agent research, many existing datasets focus\non static frame evaluations and fail to provide a comprehensive platform for\nassessing performance on real-world, in-the-wild tasks. To address this gap, we\npresent Android Agent Arena (A3), a novel evaluation platform. Unlike existing\nin-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as\nreal-time online information retrieval and operational instructions; (2) a\nlarger, more flexible action space, enabling compatibility with agents trained\non any dataset; and (3) automated business-level LLM-based evaluation process.\nA3 includes 21 widely used general third-party apps and 201 tasks\nrepresentative of common user scenarios, providing a robust foundation for\nevaluating mobile GUI agents in real-world situations and a new autonomous\nevaluation process for less human labor and coding expertise. The project is\navailable at \\url{https://yuxiangchai.github.io/Android-Agent-Arena/}."
                },
                "authors": [
                    {
                        "name": "Yuxiang Chai"
                    },
                    {
                        "name": "Hanhao Li"
                    },
                    {
                        "name": "Jiayu Zhang"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Guozhi Wang"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v1",
                "updated": "2025-01-02T08:57:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. Importantly, DialectFP4 ensures hardware\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. Furthermore, we propose a two-stage approach\nfor online DialectFP4 activation quantization. BlockDialect achieves 11.40%\n(6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4\nformat with a comparable bit usage per data, while being only 5.89% (3.31%)\nbelow full precision even when quantizing full-path matrix multiplication.\nFocusing on how to represent over how to scale, our work presents a promising\npath for energy-efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. Importantly, DialectFP4 ensures hardware\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. Furthermore, we propose a two-stage approach\nfor online DialectFP4 activation quantization. BlockDialect achieves 11.40%\n(6.90%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4\nformat with a comparable bit usage per data, while being only 5.89% (3.31%)\nbelow full precision even when quantizing full-path matrix multiplication.\nFocusing on how to represent over how to scale, our work presents a promising\npath for energy-efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00927v2",
                "updated": "2025-01-02T08:53:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-09-30T16:57:34Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    57,
                    34,
                    0,
                    274,
                    0
                ],
                "title": "Text Clustering as Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Clustering as Classification with LLMs"
                },
                "summary": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text clustering remains valuable in real-world applications where manual\nlabeling is cost-prohibitive. It facilitates efficient organization and\nanalysis of information by grouping similar texts based on their\nrepresentations. However, implementing this approach necessitates fine-tuned\nembedders for downstream data and sophisticated similarity metrics. To address\nthis issue, this study presents a novel framework for text clustering that\neffectively leverages the in-context learning capacity of Large Language Models\n(LLMs). Instead of fine-tuning embedders, we propose to transform the text\nclustering into a classification task via LLM. First, we prompt LLM to generate\npotential labels for a given dataset. Second, after integrating similar labels\ngenerated by the LLM, we prompt the LLM to assign the most appropriate label to\neach sample in the dataset. Our framework has been experimentally proven to\nachieve comparable or superior performance to state-of-the-art clustering\nmethods that employ embeddings, without requiring complex fine-tuning or\nclustering algorithms. We make our code available to the public for utilization\nat https://github.com/ECNU-Text-Computing/Text-Clustering-via-LLM."
                },
                "authors": [
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Guoxiu He"
                    }
                ],
                "author_detail": {
                    "name": "Guoxiu He"
                },
                "author": "Guoxiu He",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01141v1",
                "updated": "2025-01-02T08:48:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    48,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T08:48:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    48,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language\n  Models and Reinforcement Learning Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language\n  Models and Reinforcement Learning Method"
                },
                "summary": "This paper investigates adaptive transmission strategies in embodied\nAI-enhanced vehicular networks by integrating large language models (LLMs) for\nsemantic information extraction and deep reinforcement learning (DRL) for\ndecision-making. The proposed framework aims to optimize both data transmission\nefficiency and decision accuracy by formulating an optimization problem that\nincorporates the Weber-Fechner law, serving as a metric for balancing bandwidth\nutilization and quality of experience (QoE). Specifically, we employ the large\nlanguage and vision assistant (LLAVA) model to extract critical semantic\ninformation from raw image data captured by embodied AI agents (i.e.,\nvehicles), reducing transmission data size by approximately more than 90\\%\nwhile retaining essential content for vehicular communication and\ndecision-making. In the dynamic vehicular environment, we employ a generalized\nadvantage estimation-based proximal policy optimization (GAE-PPO) method to\nstabilize decision-making under uncertainty. Simulation results show that\nattention maps from LLAVA highlight the model's focus on relevant image\nregions, enhancing semantic representation accuracy. Additionally, our proposed\ntransmission strategy improves QoE by up to 36\\% compared to DDPG and\naccelerates convergence by reducing required steps by up to 47\\% compared to\npure PPO. Further analysis indicates that adapting semantic symbol length\nprovides an effective trade-off between transmission quality and bandwidth,\nachieving up to a 61.4\\% improvement in QoE when scaling from 4 to 8 vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates adaptive transmission strategies in embodied\nAI-enhanced vehicular networks by integrating large language models (LLMs) for\nsemantic information extraction and deep reinforcement learning (DRL) for\ndecision-making. The proposed framework aims to optimize both data transmission\nefficiency and decision accuracy by formulating an optimization problem that\nincorporates the Weber-Fechner law, serving as a metric for balancing bandwidth\nutilization and quality of experience (QoE). Specifically, we employ the large\nlanguage and vision assistant (LLAVA) model to extract critical semantic\ninformation from raw image data captured by embodied AI agents (i.e.,\nvehicles), reducing transmission data size by approximately more than 90\\%\nwhile retaining essential content for vehicular communication and\ndecision-making. In the dynamic vehicular environment, we employ a generalized\nadvantage estimation-based proximal policy optimization (GAE-PPO) method to\nstabilize decision-making under uncertainty. Simulation results show that\nattention maps from LLAVA highlight the model's focus on relevant image\nregions, enhancing semantic representation accuracy. Additionally, our proposed\ntransmission strategy improves QoE by up to 36\\% compared to DDPG and\naccelerates convergence by reducing required steps by up to 47\\% compared to\npure PPO. Further analysis indicates that adapting semantic symbol length\nprovides an effective trade-off between transmission quality and bandwidth,\nachieving up to a 61.4\\% improvement in QoE when scaling from 4 to 8 vehicles."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Suttinee Sawadsitang"
                    },
                    {
                        "name": "Xuemin Shen"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09893v3",
                "updated": "2025-01-02T08:43:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    43,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-13T13:58:24Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    13,
                    58,
                    24,
                    5,
                    195,
                    0
                ],
                "title": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Multi-Agent Framework with Trajectory Learning for\n  Knowledge-Intensive Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to significant\nbreakthroughs in various natural language processing tasks. However, generating\nfactually consistent responses in knowledge-intensive scenarios remains a\nchallenge due to issues such as hallucination, difficulty in acquiring\nlong-tailed knowledge, and limited memory expansion. This paper introduces\nSMART, a novel multi-agent framework that leverages external knowledge to\nenhance the interpretability and factual consistency of LLM-generated\nresponses. SMART comprises four specialized agents, each performing a specific\nsub-trajectory action to navigate complex knowledge-intensive tasks. We propose\na multi-agent co-training paradigm, Long-Short Trajectory Learning, which\nensures synergistic collaboration among agents while maintaining fine-grained\nexecution by each agent. Extensive experiments on five knowledge-intensive\ntasks demonstrate SMART's superior performance compared to widely adopted\nknowledge internalization and knowledge enhancement methods. Our framework can\nextend beyond knowledge-intensive tasks to more complex scenarios. Our code is\navailable at https://github.com/yueshengbin/SMART."
                },
                "authors": [
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01124v1",
                "updated": "2025-01-02T07:45:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    45,
                    34,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T07:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    45,
                    34,
                    3,
                    2,
                    0
                ],
                "title": "Graph2text or Graph2token: A Perspective of Large Language Models for\n  Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph2text or Graph2token: A Perspective of Large Language Models for\n  Graph Learning"
                },
                "summary": "Graphs are data structures used to represent irregular networks and are\nprevalent in numerous real-world applications. Previous methods directly model\ngraph structures and achieve significant success. However, these methods\nencounter bottlenecks due to the inherent irregularity of graphs. An innovative\nsolution is converting graphs into textual representations, thereby harnessing\nthe powerful capabilities of Large Language Models (LLMs) to process and\ncomprehend graphs. In this paper, we present a comprehensive review of\nmethodologies for applying LLMs to graphs, termed LLM4graph. The core of\nLLM4graph lies in transforming graphs into texts for LLMs to understand and\nanalyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of\nthe transformation. Specifically, existing methods can be divided into two\nparadigms: Graph2text and Graph2token, which transform graphs into texts or\ntokens as the input of LLMs, respectively. We point out four challenges during\nthe transformation to systematically present existing methods in a\nproblem-oriented perspective. For practical concerns, we provide a guideline\nfor researchers on selecting appropriate models and LLMs for different graphs\nand hardware constraints. We also identify five future research directions for\nLLM4graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are data structures used to represent irregular networks and are\nprevalent in numerous real-world applications. Previous methods directly model\ngraph structures and achieve significant success. However, these methods\nencounter bottlenecks due to the inherent irregularity of graphs. An innovative\nsolution is converting graphs into textual representations, thereby harnessing\nthe powerful capabilities of Large Language Models (LLMs) to process and\ncomprehend graphs. In this paper, we present a comprehensive review of\nmethodologies for applying LLMs to graphs, termed LLM4graph. The core of\nLLM4graph lies in transforming graphs into texts for LLMs to understand and\nanalyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of\nthe transformation. Specifically, existing methods can be divided into two\nparadigms: Graph2text and Graph2token, which transform graphs into texts or\ntokens as the input of LLMs, respectively. We point out four challenges during\nthe transformation to systematically present existing methods in a\nproblem-oriented perspective. For practical concerns, we provide a guideline\nfor researchers on selecting appropriate models and LLMs for different graphs\nand hardware constraints. We also identify five future research directions for\nLLM4graph."
                },
                "authors": [
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Ruolin Li"
                    },
                    {
                        "name": "Guchun Liu"
                    },
                    {
                        "name": "Yanming Shen"
                    },
                    {
                        "name": "Shaoxiong Ji"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Fengling Han"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v2",
                "updated": "2025-01-02T07:29:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    7,
                    29,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01078v1",
                "updated": "2025-01-02T05:53:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    53,
                    14,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T05:53:14Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    53,
                    14,
                    3,
                    2,
                    0
                ],
                "title": "Communication-and-Computation Efficient Split Federated Learning:\n  Gradient Aggregation and Resource Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication-and-Computation Efficient Split Federated Learning:\n  Gradient Aggregation and Resource Management"
                },
                "summary": "With the prevalence of Large Learning Models (LLM), Split Federated Learning\n(SFL), which divides a learning model into server-side and client-side models,\nhas emerged as an appealing technology to deal with the heavy computational\nburden for network edge clients. However, existing SFL frameworks would\nfrequently upload smashed data and download gradients between the server and\neach client, leading to severe communication overheads. To address this issue,\nthis work proposes a novel communication-and-computation efficient SFL\nframework, which allows dynamic model splitting (server- and client-side model\ncutting point selection) and broadcasting of aggregated smashed data gradients.\nWe theoretically analyze the impact of the cutting point selection on the\nconvergence rate of the proposed framework, revealing that model splitting with\na smaller client-side model size leads to a better convergence performance and\nvise versa. Based on the above insights, we formulate an optimization problem\nto minimize the model convergence rate and latency under the consideration of\ndata privacy via a joint Cutting point selection, Communication and Computation\nresource allocation (CCC) strategy. To deal with the proposed mixed integer\nnonlinear programming optimization problem, we develop an algorithm by\nintegrating the Double Deep Q-learning Network (DDQN) with convex optimization\nmethods. Extensive experiments validate our theoretical analyses across various\ndatasets, and the numerical results demonstrate the effectiveness and\nsuperiority of the proposed communication-efficient SFL compared with existing\nschemes, including parallel split learning and traditional SFL mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the prevalence of Large Learning Models (LLM), Split Federated Learning\n(SFL), which divides a learning model into server-side and client-side models,\nhas emerged as an appealing technology to deal with the heavy computational\nburden for network edge clients. However, existing SFL frameworks would\nfrequently upload smashed data and download gradients between the server and\neach client, leading to severe communication overheads. To address this issue,\nthis work proposes a novel communication-and-computation efficient SFL\nframework, which allows dynamic model splitting (server- and client-side model\ncutting point selection) and broadcasting of aggregated smashed data gradients.\nWe theoretically analyze the impact of the cutting point selection on the\nconvergence rate of the proposed framework, revealing that model splitting with\na smaller client-side model size leads to a better convergence performance and\nvise versa. Based on the above insights, we formulate an optimization problem\nto minimize the model convergence rate and latency under the consideration of\ndata privacy via a joint Cutting point selection, Communication and Computation\nresource allocation (CCC) strategy. To deal with the proposed mixed integer\nnonlinear programming optimization problem, we develop an algorithm by\nintegrating the Double Deep Q-learning Network (DDQN) with convex optimization\nmethods. Extensive experiments validate our theoretical analyses across various\ndatasets, and the numerical results demonstrate the effectiveness and\nsuperiority of the proposed communication-efficient SFL compared with existing\nschemes, including parallel split learning and traditional SFL mechanisms."
                },
                "authors": [
                    {
                        "name": "Yipeng Liang"
                    },
                    {
                        "name": "Qimei Chen"
                    },
                    {
                        "name": "Guangxu Zhu"
                    },
                    {
                        "name": "Muhammad Kaleem Awan"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "13 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01062v1",
                "updated": "2025-01-02T05:13:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    13,
                    22,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T05:13:22Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    13,
                    22,
                    3,
                    2,
                    0
                ],
                "title": "Fides: Scalable Censorship-Resistant DAG Consensus via Trusted\n  Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fides: Scalable Censorship-Resistant DAG Consensus via Trusted\n  Components"
                },
                "summary": "Recently, consensus protocols based on Directed Acyclic Graph (DAG) have\ngained significant attention due to their potential to build robust blockchain\nsystems, particularly in asynchronous networks. In this paper, we propose\nFides, an asynchronous DAG-based BFT consensus protocol that leverages Trusted\nExecution Environments (TEEs) to tackle three major scalability and security\nchallenges faced by existing protocols: (i) the need for a larger quorum size\n(i.e., at least 3x larger) to tolerate Byzantine replicas, (ii) high\ncommunication costs and reliance on expensive cryptographic primitives (i.e.,\nglobal common coin) to reach agreement in asynchronous networks, and (iii) poor\ncensorship resilience undermining the liveness guarantee. Specifically, Fides\nadopts four trusted components-Reliable Broadcast, Vertex Validation, Common\nCoin, and Transaction Disclosure-within TEEs. Incorporating these components\nenables Fides to achieve linear message complexity, guaranteed censorship\nresilience, 2x larger quorum size, and lightweight common coin usage. Besides,\nabstracting these essential components rather than porting the entire protocol\ninto TEE can significantly reduce the Trusted Computing Base (TCB).\nExperimental evaluations of Fides in local and geo-distributed networks\ndemonstrate its superior performance compared to established state-of-the-art\nprotocols such as Tusk, RCC, HotStuff, and PBFT. The results indicate that\nFides achieves a throughput of 400k transactions per second in a\ngeo-distributed network and 810k transactions per second in a local network.\nOur analysis further explores the protocol's overhead, highlighting its\nsuitability and effectiveness for practical deployment in real-world blockchain\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, consensus protocols based on Directed Acyclic Graph (DAG) have\ngained significant attention due to their potential to build robust blockchain\nsystems, particularly in asynchronous networks. In this paper, we propose\nFides, an asynchronous DAG-based BFT consensus protocol that leverages Trusted\nExecution Environments (TEEs) to tackle three major scalability and security\nchallenges faced by existing protocols: (i) the need for a larger quorum size\n(i.e., at least 3x larger) to tolerate Byzantine replicas, (ii) high\ncommunication costs and reliance on expensive cryptographic primitives (i.e.,\nglobal common coin) to reach agreement in asynchronous networks, and (iii) poor\ncensorship resilience undermining the liveness guarantee. Specifically, Fides\nadopts four trusted components-Reliable Broadcast, Vertex Validation, Common\nCoin, and Transaction Disclosure-within TEEs. Incorporating these components\nenables Fides to achieve linear message complexity, guaranteed censorship\nresilience, 2x larger quorum size, and lightweight common coin usage. Besides,\nabstracting these essential components rather than porting the entire protocol\ninto TEE can significantly reduce the Trusted Computing Base (TCB).\nExperimental evaluations of Fides in local and geo-distributed networks\ndemonstrate its superior performance compared to established state-of-the-art\nprotocols such as Tusk, RCC, HotStuff, and PBFT. The results indicate that\nFides achieves a throughput of 400k transactions per second in a\ngeo-distributed network and 810k transactions per second in a local network.\nOur analysis further explores the protocol's overhead, highlighting its\nsuitability and effectiveness for practical deployment in real-world blockchain\nsystems."
                },
                "authors": [
                    {
                        "name": "Shaokang Xie"
                    },
                    {
                        "name": "Dakai Kang"
                    },
                    {
                        "name": "Hanzheng Lyu"
                    },
                    {
                        "name": "Jianyu Niu"
                    },
                    {
                        "name": "Mohammad Sadoghi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Sadoghi"
                },
                "author": "Mohammad Sadoghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01059v1",
                "updated": "2025-01-02T05:07:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    7,
                    6,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T05:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    5,
                    7,
                    6,
                    3,
                    2,
                    0
                ],
                "title": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models"
                },
                "summary": "Large language models (LLMs) often suffer from context faithfulness\nhallucinations, where outputs deviate from retrieved information due to\ninsufficient context utilization and high output uncertainty. Our uncertainty\nevaluation experiments reveal a strong correlation between high uncertainty and\nhallucinations. We hypothesize that attention mechanisms encode signals\nindicative of contextual utilization, validated through probing analysis. Based\non these insights, we propose Dynamic Attention-Guided Context Decoding\n(DAGCD), a lightweight framework that integrates attention distributions and\nuncertainty signals in a single-pass decoding process. Experiments across QA\ndatasets demonstrate DAGCD's effectiveness, achieving significant improvements\nin faithfulness and robustness while maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from context faithfulness\nhallucinations, where outputs deviate from retrieved information due to\ninsufficient context utilization and high output uncertainty. Our uncertainty\nevaluation experiments reveal a strong correlation between high uncertainty and\nhallucinations. We hypothesize that attention mechanisms encode signals\nindicative of contextual utilization, validated through probing analysis. Based\non these insights, we propose Dynamic Attention-Guided Context Decoding\n(DAGCD), a lightweight framework that integrates attention distributions and\nuncertainty signals in a single-pass decoding process. Experiments across QA\ndatasets demonstrate DAGCD's effectiveness, achieving significant improvements\nin faithfulness and robustness while maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Ning Cheng"
                    },
                    {
                        "name": "Zhitao Li"
                    },
                    {
                        "name": "Shaojun Wang"
                    },
                    {
                        "name": "Jing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xiao"
                },
                "author": "Jing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01054v1",
                "updated": "2025-01-02T04:33:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    33,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T04:33:31Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    33,
                    31,
                    3,
                    2,
                    0
                ],
                "title": "Dynamic Scaling of Unit Tests for Code Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Scaling of Unit Tests for Code Reward Modeling"
                },
                "summary": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus)."
                },
                "authors": [
                    {
                        "name": "Zeyao Ma"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Sijia Luo"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "Homepage: https://code-reward-model.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01985v2",
                "updated": "2025-01-02T04:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    15,
                    37,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-02T19:45:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    19,
                    45,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost-in-Distance: Impact of Contextual Proximity on LLM Performance in\n  Graph Tasks"
                },
                "summary": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, Large Language Models (LLMs) exhibit blind\nspots that impair their ability to retrieve and process relevant contextual\ndata effectively. We demonstrate that LLM performance in graph tasks with\ncomplexities beyond the \"needle-in-a-haystack\" scenario-where solving the\nproblem requires cross-referencing and reasoning across multiple subproblems\njointly-is influenced by the proximity of relevant information within the\ncontext, a phenomenon we term \"lost-in-distance\". We examine two fundamental\ngraph tasks: identifying common connections between two nodes and assessing\nsimilarity among three nodes, and show that the model's performance in these\ntasks significantly depends on the relative positioning of common edges. We\nevaluate three publicly available LLMs using various graph encoding techniques\nthat represent graph structures for LLM input. We propose a formulation for the\nlost-in-distance phenomenon and demonstrate that lost-in-distance and\nlost-in-the middle phenomenas occur independently. Results indicate that model\naccuracy can decline by up to 6x as the distance between node connections\nincreases, independent of graph encoding and model size."
                },
                "authors": [
                    {
                        "name": "Hamed Firooz"
                    },
                    {
                        "name": "Maziar Sanjabi"
                    },
                    {
                        "name": "Wenlong Jiang"
                    },
                    {
                        "name": "Xiaoling Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Zhai"
                },
                "author": "Xiaoling Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01046v1",
                "updated": "2025-01-02T04:11:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    11,
                    23,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T04:11:23Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    11,
                    23,
                    3,
                    2,
                    0
                ],
                "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FED: Fast and Efficient Dataset Deduplication Framework with GPU\n  Acceleration"
                },
                "summary": "Dataset deduplication plays a crucial role in enhancing data quality,\nultimately improving training performance and efficiency of LLMs. A commonly\nused method for data deduplication is the MinHash LSH algorithm. Recently,\nNVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains\nsuboptimal, leaving room for further improvement in processing efficiency. This\npaper proposes a GPU-accelerated deduplication framework \\sys that optimizes\nMinHash LSH for GPU clusters and leverages computationally efficient and\npartially reusable non-cryptographic hash functions. \\sys significantly\noutperforms the CPU-based deduplication tool included in SlimPajama by up to\n58.3 times and the GPU-based deduplication tool included in NVIDIA NeMo Curator\nby up to 8.6 times when processing 1 million documents with a node of four\nGPUs. Deduplication of 1.2 trillion tokens is completed in just 5.1 hours in a\nfour-node, 16-GPU environment. The related code is publicly available on GitHub\n(https://github.com/mcrl/FED).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset deduplication plays a crucial role in enhancing data quality,\nultimately improving training performance and efficiency of LLMs. A commonly\nused method for data deduplication is the MinHash LSH algorithm. Recently,\nNVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains\nsuboptimal, leaving room for further improvement in processing efficiency. This\npaper proposes a GPU-accelerated deduplication framework \\sys that optimizes\nMinHash LSH for GPU clusters and leverages computationally efficient and\npartially reusable non-cryptographic hash functions. \\sys significantly\noutperforms the CPU-based deduplication tool included in SlimPajama by up to\n58.3 times and the GPU-based deduplication tool included in NVIDIA NeMo Curator\nby up to 8.6 times when processing 1 million documents with a node of four\nGPUs. Deduplication of 1.2 trillion tokens is completed in just 5.1 hours in a\nfour-node, 16-GPU environment. The related code is publicly available on GitHub\n(https://github.com/mcrl/FED)."
                },
                "authors": [
                    {
                        "name": "Youngjun Son"
                    },
                    {
                        "name": "Chaewon Kim"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13334v3",
                "updated": "2025-01-02T04:06:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    4,
                    6,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-17T08:46:09Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    46,
                    9,
                    3,
                    291,
                    0
                ],
                "title": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiasJailbreak:Analyzing Ethical Biases and Jailbreak Vulnerabilities in\n  Large Language Models"
                },
                "summary": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate impressive proficiency in\nvarious tasks, they present potential safety risks, such as `jailbreaks', where\nmalicious inputs can coerce LLMs into generating harmful content bypassing\nsafety alignments. In this paper, we delve into the ethical biases in LLMs and\nexamine how those biases could be exploited for jailbreaks. Notably, these\nbiases result in a jailbreaking success rate in GPT-4o models that differs by\n20\\% between non-binary and cisgender keywords and by 16\\% between white and\nblack keywords, even when the other parts of the prompts are identical. We\nintroduce the concept of BiasJailbreak, highlighting the inherent risks posed\nby these safety-induced biases. BiasJailbreak generates biased keywords\nautomatically by asking the target LLM itself, and utilizes the keywords to\ngenerate harmful output. Additionally, we propose an efficient defense method\nBiasDefense, which prevents jailbreak attempts by injecting defense prompts\nprior to generation. BiasDefense stands as an appealing alternative to Guard\nModels, such as Llama-Guard, that require additional inference cost after text\ngeneration. Our findings emphasize that ethical biases in LLMs can actually\nlead to generating unsafe output, and suggest a method to make the LLMs more\nsecure and unbiased. To enable further research and improvements, we\nopen-source our code and artifacts of BiasJailbreak, providing the community\nwith tools to better understand and mitigate safety-induced biases in LLMs."
                },
                "authors": [
                    {
                        "name": "Isack Lee"
                    },
                    {
                        "name": "Haebin Seong"
                    }
                ],
                "author_detail": {
                    "name": "Haebin Seong"
                },
                "author": "Haebin Seong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20151v2",
                "updated": "2025-01-02T03:59:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    59,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-28T13:32:36Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    13,
                    32,
                    36,
                    5,
                    363,
                    0
                ],
                "title": "Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks"
                },
                "summary": "As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms."
                },
                "authors": [
                    {
                        "name": "Xinlei Ge"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Yukun Sun"
                    },
                    {
                        "name": "Yunji Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Zhao"
                },
                "author": "Yunji Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01340v2",
                "updated": "2025-01-02T03:29:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    29,
                    31,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-02T10:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    10,
                    7,
                    1,
                    0,
                    337,
                    0
                ],
                "title": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 2-step Framework for Automated Literary Translation Evaluation: Its\n  Promises and Pitfalls"
                },
                "summary": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose and evaluate the feasibility of a two-stage pipeline\nto evaluate literary machine translation, in a fine-grained manner, from\nEnglish to Korean. The results show that our framework provides fine-grained,\ninterpretable metrics suited for literary translation and obtains a higher\ncorrelation with human judgment than traditional machine translation metrics.\nNonetheless, it still fails to match inter-human agreement, especially in\nmetrics like Korean Honorifics. We also observe that LLMs tend to favor\ntranslations generated by other LLMs, and we highlight the necessity of\ndeveloping more sophisticated evaluation methods to ensure accurate and\nculturally sensitive machine translation of literary works."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Woori Jang"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Alice Oh"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01031v1",
                "updated": "2025-01-02T03:26:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    26,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:26:13Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    26,
                    13,
                    3,
                    2,
                    0
                ],
                "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning"
                },
                "summary": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nin-context learning to integrate cultural and demographic knowledge dynamically\nduring text generation. Leveraging the World Values Survey (WVS) dataset,\nValuesRAG first generates summaries of values for each individual.\nSubsequently, we curated several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided, highlighting ValuesRAG's potential to foster culturally aligned AI\nsystems and enhance the inclusivity of AI-driven applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nin-context learning to integrate cultural and demographic knowledge dynamically\nduring text generation. Leveraging the World Values Survey (WVS) dataset,\nValuesRAG first generates summaries of values for each individual.\nSubsequently, we curated several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided, highlighting ValuesRAG's potential to foster culturally aligned AI\nsystems and enhance the inclusivity of AI-driven applications."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Zonghao Yuan"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v1",
                "updated": "2025-01-02T03:17:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01027v1",
                "updated": "2025-01-02T03:17:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    10,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:17:10Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    10,
                    3,
                    2,
                    0
                ],
                "title": "RealTime Health Monitoring Using 5G Networks: A Deep Learning-Based\n  Architecture for Remote Patient Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealTime Health Monitoring Using 5G Networks: A Deep Learning-Based\n  Architecture for Remote Patient Care"
                },
                "summary": "Remote patient monitoring is crucial in modern healthcare, but current\nsystems struggle with real-time analysis and prediction of vital signs. This\npaper presents a novel architecture combining deep learning with 5G network\ncapabilities to enable real-time vital sign monitoring and prediction. The\nproposed system utilizes a hybrid CNN-LSTM model optimized for edge deployment,\npaired with 5G Ultra-Reliable Low-Latency Communication (URLLC) for efficient\ndata transmission. The architecture achieves end-to-end latency of 14.4ms while\nmaintaining 96.5% prediction accuracy across multiple vital signs. Our system\nshows significant improvements over existing solutions, reducing latency by 47%\nand increasing prediction accuracy by 4.2% compared to current state-of-the-art\nsystems. Performance evaluations conducted over three months with data from\n1000 patients validate the system's reliability and scalability in clinical\nsettings. The results demonstrate that integrating deep learning with 5G\ntechnology can effectively address the challenges of real-time patient\nmonitoring, leading to early detection of deteriorating conditions and improved\nclinical outcomes. This research establishes a framework for reliable,\nreal-time vital sign monitoring and prediction in digital healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote patient monitoring is crucial in modern healthcare, but current\nsystems struggle with real-time analysis and prediction of vital signs. This\npaper presents a novel architecture combining deep learning with 5G network\ncapabilities to enable real-time vital sign monitoring and prediction. The\nproposed system utilizes a hybrid CNN-LSTM model optimized for edge deployment,\npaired with 5G Ultra-Reliable Low-Latency Communication (URLLC) for efficient\ndata transmission. The architecture achieves end-to-end latency of 14.4ms while\nmaintaining 96.5% prediction accuracy across multiple vital signs. Our system\nshows significant improvements over existing solutions, reducing latency by 47%\nand increasing prediction accuracy by 4.2% compared to current state-of-the-art\nsystems. Performance evaluations conducted over three months with data from\n1000 patients validate the system's reliability and scalability in clinical\nsettings. The results demonstrate that integrating deep learning with 5G\ntechnology can effectively address the challenges of real-time patient\nmonitoring, leading to early detection of deteriorating conditions and improved\nclinical outcomes. This research establishes a framework for reliable,\nreal-time vital sign monitoring and prediction in digital healthcare."
                },
                "authors": [
                    {
                        "name": "Iqra Batool"
                    }
                ],
                "author_detail": {
                    "name": "Iqra Batool"
                },
                "author": "Iqra Batool",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v4",
                "updated": "2025-01-02T03:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    14,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16668v2",
                "updated": "2025-01-02T03:02:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    2,
                    13,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-22T03:53:46Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    3,
                    53,
                    46,
                    1,
                    296,
                    0
                ],
                "title": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satori: Towards Proactive AR Assistant with Belief-Desire-Intention User\n  Modeling"
                },
                "summary": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality assistance are increasingly popular for supporting users\nwith tasks like assembly and cooking. However, current practice typically\nprovide reactive responses initialized from user requests, lacking\nconsideration of rich contextual and user-specific information. To address this\nlimitation, we propose a novel AR assistance system, Satori, that models both\nuser states and environmental contexts to deliver proactive guidance. Our\nsystem combines the Belief-Desire-Intention (BDI) model with a state-of-the-art\nmulti-modal large language model (LLM) to infer contextually appropriate\nguidance. The design is informed by two formative studies involving twelve\nexperts. A sixteen within-subject study find that Satori achieves performance\ncomparable to an designer-created Wizard-of-Oz (WoZ) system without relying on\nmanual configurations or heuristics, thereby enhancing generalizability,\nreusability and opening up new possibilities for AR assistance."
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Gromit Yeuk-Yin Chan"
                    },
                    {
                        "name": "Dishita G Turakhia"
                    },
                    {
                        "name": "Sonia Castelo Quispe"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Leslie Welch"
                    },
                    {
                        "name": "Claudio Silva"
                    },
                    {
                        "name": "Jing Qian"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qian"
                },
                "author": "Jing Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01014v1",
                "updated": "2025-01-02T02:35:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    35,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:35:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    35,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based\n  on Large language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based\n  on Large language Model"
                },
                "summary": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail."
                },
                "authors": [
                    {
                        "name": "Chengze Zhang"
                    },
                    {
                        "name": "Changshan Li"
                    },
                    {
                        "name": "Shiyang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyang Gao"
                },
                "author": "Shiyang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00999v1",
                "updated": "2025-01-02T01:33:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T01:33:58Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    33,
                    58,
                    3,
                    2,
                    0
                ],
                "title": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment."
                },
                "authors": [
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Zhengyu Qi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhikai Jia"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "9 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v2",
                "updated": "2025-01-02T01:11:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    1,
                    11,
                    46,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00982v1",
                "updated": "2025-01-02T00:01:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    0,
                    1,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T00:01:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    0,
                    1,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for\n  interpretable mental health screening through psychometric practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for\n  interpretable mental health screening through psychometric practice"
                },
                "summary": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices."
                },
                "authors": [
                    {
                        "name": "Federico Ravenda"
                    },
                    {
                        "name": "Seyed Ali Bahrainian"
                    },
                    {
                        "name": "Andrea Raballo"
                    },
                    {
                        "name": "Antonietta Mira"
                    },
                    {
                        "name": "Noriko Kando"
                    }
                ],
                "author_detail": {
                    "name": "Noriko Kando"
                },
                "author": "Noriko Kando",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13184v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13184v5",
                "updated": "2025-01-01T23:34:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    34,
                    53,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-20T17:49:46Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    17,
                    49,
                    46,
                    1,
                    51,
                    0
                ],
                "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents"
                },
                "summary": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \"CosmoAgent,\" an innovative artificial intelligence\nsystem that utilizes Large Language Models (LLMs) to simulate complex\ninteractions between human and extraterrestrial civilizations. This paper\nintroduces a mathematical model for quantifying the levels of civilization\ndevelopment and further employs a state transition matrix approach to evaluate\ntheir trajectories. Through this methodology, our study quantitatively analyzes\nthe growth trajectories of civilizations, providing insights into future\ndecision-making at critical points of growth and saturation. Furthermore, this\npaper acknowledges the vast diversity of potential living conditions across the\nuniverse, which could foster unique cosmologies, ethical codes, and worldviews\namong different civilizations. Recognizing the Earth-centric bias inherent in\ncurrent LLM designs, we propose the novel concept of using LLM agents with\ndiverse ethical paradigms and simulating interactions between entities with\ndistinct moral principles. This innovative research not only introduces a novel\nmethod for comprehending potential inter-civilizational dynamics but also holds\npractical value in enabling entities with divergent value systems to\nstrategize, prevent conflicts, and engage in games under conditions of\nasymmetric information. The accompanying code is available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents."
                },
                "authors": [
                    {
                        "name": "Zhaoqian Xue"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Beichen Wang"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hua Tang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13184v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13184v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00977v1",
                "updated": "2025-01-01T23:08:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    8,
                    54,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T23:08:54Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    23,
                    8,
                    54,
                    2,
                    1,
                    0
                ],
                "title": "Host-guided data placement: whose job is it anyway?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Host-guided data placement: whose job is it anyway?"
                },
                "summary": "The increasing demand for SSDs coupled with scaling difficulties have left\nmanufacturers scrambling for newer SSD interfaces which promise better\nperformance and durability. While these interfaces reduce the rigidity of\ntraditional abstractions, they require application or system-level changes that\ncan impact the stability, security, and portability of systems. To make matters\nworse, such changes are rendered futile with introduction of next-generation\ninterfaces. Further, there is little guidance on data placement and hardware\nspecifics are often abstracted from the application layer. It is no surprise\ntherefore that such interfaces have seen limited adoption, leaving behind a\ngraveyard of experimental interfaces ranging from open-channel SSDs to zoned\nnamespaces.\n  In this paper, we show how shim layers can to shield systems from changing\nhardware interfaces while benefiting from them. We present Reshim, an\nall-userspace shim layer that performs affinity and lifetime based data\nplacement with no change to the operating system or the application. We\ndemonstrate Reshim's ease of adoption with host-device coordination for three\nwidely-used data-intensive systems: RocksDB, MongoDB, and CacheLib. With\nReshim, these systems see 2-6 times highe write throughput, up to 6 times lower\nlatency, and reduced write amplification compared to filesystems like F2FS.\nReshim performs on par with application-specific backends like ZenFS while\noffering more generality, lower latency, and richer data placement. With Reshim\nwe demonstrate the value of isolating the complexity of the placement logic,\nallowing easy deployment of dynamic placement rules across several applications\nand storage interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for SSDs coupled with scaling difficulties have left\nmanufacturers scrambling for newer SSD interfaces which promise better\nperformance and durability. While these interfaces reduce the rigidity of\ntraditional abstractions, they require application or system-level changes that\ncan impact the stability, security, and portability of systems. To make matters\nworse, such changes are rendered futile with introduction of next-generation\ninterfaces. Further, there is little guidance on data placement and hardware\nspecifics are often abstracted from the application layer. It is no surprise\ntherefore that such interfaces have seen limited adoption, leaving behind a\ngraveyard of experimental interfaces ranging from open-channel SSDs to zoned\nnamespaces.\n  In this paper, we show how shim layers can to shield systems from changing\nhardware interfaces while benefiting from them. We present Reshim, an\nall-userspace shim layer that performs affinity and lifetime based data\nplacement with no change to the operating system or the application. We\ndemonstrate Reshim's ease of adoption with host-device coordination for three\nwidely-used data-intensive systems: RocksDB, MongoDB, and CacheLib. With\nReshim, these systems see 2-6 times highe write throughput, up to 6 times lower\nlatency, and reduced write amplification compared to filesystems like F2FS.\nReshim performs on par with application-specific backends like ZenFS while\noffering more generality, lower latency, and richer data placement. With Reshim\nwe demonstrate the value of isolating the complexity of the placement logic,\nallowing easy deployment of dynamic placement rules across several applications\nand storage interfaces."
                },
                "authors": [
                    {
                        "name": "Devashish R. Purandare"
                    },
                    {
                        "name": "Peter Alvaro"
                    },
                    {
                        "name": "Avani Wildani"
                    },
                    {
                        "name": "Darrell D. E. Long"
                    },
                    {
                        "name": "Ethan L. Miller"
                    }
                ],
                "author_detail": {
                    "name": "Ethan L. Miller"
                },
                "author": "Ethan L. Miller",
                "arxiv_comment": "14 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00959v1",
                "updated": "2025-01-01T21:31:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:31:47Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    31,
                    47,
                    2,
                    1,
                    0
                ],
                "title": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGGA: A Dataset of Industrial Guidelines and Policy Statements for\n  Generative AIs"
                },
                "summary": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces IGGA, a dataset of 160 industry guidelines and policy\nstatements for the use of Generative AIs (GAIs) and Large Language Models\n(LLMs) in industry and workplace settings, collected from official company\nwebsites, and trustworthy news sources. The dataset contains 104,565 words and\nserves as a valuable resource for natural language processing tasks commonly\napplied in requirements engineering, such as model synthesis, abstraction\nidentification, and document structure assessment. Additionally, IGGA can be\nfurther annotated to function as a benchmark for various tasks, including\nambiguity detection, requirements categorization, and the identification of\nequivalent requirements. Our methodologically rigorous approach ensured a\nthorough examination, with a selection of reputable and influential companies\nthat represent a diverse range of global institutions across six continents.\nThe dataset captures perspectives from fourteen industry sectors, including\ntechnology, finance, and both public and private institutions, offering a broad\nspectrum of insights into the integration of GAIs and LLMs in industry."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v1",
                "updated": "2025-01-01T21:29:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00957v1",
                "updated": "2025-01-01T21:23:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T21:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors"
                },
                "summary": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00950v1",
                "updated": "2025-01-01T20:42:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    42,
                    42,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:42:42Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    42,
                    42,
                    2,
                    1,
                    0
                ],
                "title": "Intent-based Radio Scheduler for RAN Slicing: Learning to deal with\n  different network scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based Radio Scheduler for RAN Slicing: Learning to deal with\n  different network scenarios"
                },
                "summary": "The future mobile network has the complex mission of distributing available\nradio resources among various applications with different requirements. The\nradio access network slicing enables the creation of different logical networks\nby isolating and using dedicated resources for each group of applications. In\nthis scenario, the radio resource scheduling (RRS) is responsible for\ndistributing the radio resources available among the slices to fulfill their\nservice-level agreement (SLA) requirements, prioritizing critical slices while\nminimizing the number of intent violations. Moreover, ensuring that the RRS can\ndeal with a high diversity of network scenarios is essential. Several recent\npapers present advances in machine learning-based RRS. However, the scenarios\nand slice variety are restricted, which inhibits solid conclusions about the\ngeneralization capabilities of the models after deployment in real networks.\nThis paper proposes an intent-based RRS using multi-agent reinforcement\nlearning in a radio access network (RAN) slicing context. The proposed method\nprotects high-priority slices when the available radio resources cannot fulfill\nall the slices. It uses transfer learning to reduce the number of training\nsteps required. The proposed method and baselines are evaluated in different\nnetwork scenarios that comprehend combinations of different slice types,\nchannel trajectories, number of active slices and users' equipment (UEs), and\nUE characteristics. The proposed method outperformed the baselines in\nprotecting slices with higher priority, obtaining an improvement of 40% and,\nwhen considering all the slices, obtaining an improvement of 20% in relation to\nthe baselines. The results show that by using transfer learning, the required\nnumber of training steps could be reduced by a factor of eight without hurting\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The future mobile network has the complex mission of distributing available\nradio resources among various applications with different requirements. The\nradio access network slicing enables the creation of different logical networks\nby isolating and using dedicated resources for each group of applications. In\nthis scenario, the radio resource scheduling (RRS) is responsible for\ndistributing the radio resources available among the slices to fulfill their\nservice-level agreement (SLA) requirements, prioritizing critical slices while\nminimizing the number of intent violations. Moreover, ensuring that the RRS can\ndeal with a high diversity of network scenarios is essential. Several recent\npapers present advances in machine learning-based RRS. However, the scenarios\nand slice variety are restricted, which inhibits solid conclusions about the\ngeneralization capabilities of the models after deployment in real networks.\nThis paper proposes an intent-based RRS using multi-agent reinforcement\nlearning in a radio access network (RAN) slicing context. The proposed method\nprotects high-priority slices when the available radio resources cannot fulfill\nall the slices. It uses transfer learning to reduce the number of training\nsteps required. The proposed method and baselines are evaluated in different\nnetwork scenarios that comprehend combinations of different slice types,\nchannel trajectories, number of active slices and users' equipment (UEs), and\nUE characteristics. The proposed method outperformed the baselines in\nprotecting slices with higher priority, obtaining an improvement of 40% and,\nwhen considering all the slices, obtaining an improvement of 20% in relation to\nthe baselines. The results show that by using transfer learning, the required\nnumber of training steps could be reduced by a factor of eight without hurting\nperformance."
                },
                "authors": [
                    {
                        "name": "Cleverson Nahum"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Pedro Batista"
                    },
                    {
                        "name": "Cristiano Both"
                    },
                    {
                        "name": "Kleber Cardoso"
                    },
                    {
                        "name": "Aldebaro Klautau"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00940v1",
                "updated": "2025-01-01T19:44:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    19,
                    44,
                    30,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T19:44:30Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    19,
                    44,
                    30,
                    2,
                    1,
                    0
                ],
                "title": "SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI\n  and Structured Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Enhancing Adaptive Cyber Deception Strategies with Generative AI\n  and Structured Prompt Engineering"
                },
                "summary": "The rapid evolution of modern malware presents significant challenges to the\ndevelopment of effective defense mechanisms. Traditional cyber deception\ntechniques often rely on static or manually configured parameters, limiting\ntheir adaptability to dynamic and sophisticated threats. This study leverages\nGenerative AI (GenAI) models to automate the creation of adaptive cyber\ndeception ploys, focusing on structured prompt engineering (PE) to enhance\nrelevance, actionability, and deployability. We introduce a systematic\nframework (SPADE) to address inherent challenges large language models (LLMs)\npose to adaptive deceptions, including generalized outputs, ambiguity,\nunder-utilization of contextual information, and scalability constraints.\nEvaluations across diverse malware scenarios using metrics such as Recall,\nExact Match (EM), BLEU Score, and expert quality assessments identified\nChatGPT-4o as the top performer. Additionally, it achieved high engagement\n(93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini\ndemonstrated competitive performance, with Llama3.2 showing promise despite\nrequiring further optimization. These findings highlight the transformative\npotential of GenAI in automating scalable, adaptive deception strategies and\nunderscore the critical role of structured PE in advancing real-world\ncybersecurity applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of modern malware presents significant challenges to the\ndevelopment of effective defense mechanisms. Traditional cyber deception\ntechniques often rely on static or manually configured parameters, limiting\ntheir adaptability to dynamic and sophisticated threats. This study leverages\nGenerative AI (GenAI) models to automate the creation of adaptive cyber\ndeception ploys, focusing on structured prompt engineering (PE) to enhance\nrelevance, actionability, and deployability. We introduce a systematic\nframework (SPADE) to address inherent challenges large language models (LLMs)\npose to adaptive deceptions, including generalized outputs, ambiguity,\nunder-utilization of contextual information, and scalability constraints.\nEvaluations across diverse malware scenarios using metrics such as Recall,\nExact Match (EM), BLEU Score, and expert quality assessments identified\nChatGPT-4o as the top performer. Additionally, it achieved high engagement\n(93%) and accuracy (96%) with minimal refinements. Gemini and ChatGPT-4o Mini\ndemonstrated competitive performance, with Llama3.2 showing promise despite\nrequiring further optimization. These findings highlight the transformative\npotential of GenAI in automating scalable, adaptive deception strategies and\nunderscore the critical role of structured PE in advancing real-world\ncybersecurity applications."
                },
                "authors": [
                    {
                        "name": "Shihab Ahmed"
                    },
                    {
                        "name": "A B M Mohaimenur Rahman"
                    },
                    {
                        "name": "Md Morshed Alam"
                    },
                    {
                        "name": "Md Sajidul Islam Sajid"
                    }
                ],
                "author_detail": {
                    "name": "Md Sajidul Islam Sajid"
                },
                "author": "Md Sajidul Islam Sajid",
                "arxiv_journal_ref": "2025 IEEE 15th Annual Computing and Communication Workshop and\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00170v3",
                "updated": "2025-01-01T18:42:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    18,
                    42,
                    0,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-31T21:43:55Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    43,
                    55,
                    2,
                    213,
                    0
                ],
                "title": "CREW: Facilitating Human-AI Teaming Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CREW: Facilitating Human-AI Teaming Research"
                },
                "summary": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing deployment of artificial intelligence (AI) technologies,\nthe potential of humans working with AI agents has been growing at a great\nspeed. Human-AI teaming is an important paradigm for studying various aspects\nwhen humans and AI agents work together. The unique aspect of Human-AI teaming\nresearch is the need to jointly study humans and AI agents, demanding\nmultidisciplinary research efforts from machine learning to human-computer\ninteraction, robotics, cognitive science, neuroscience, psychology, social\nscience, and complex systems. However, existing platforms for Human-AI teaming\nresearch are limited, often supporting oversimplified scenarios and a single\ntask, or specifically focusing on either human-teaming research or multi-agent\nAI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming\nresearch in real-time decision-making scenarios and engage collaborations from\nmultiple scientific disciplines, with a strong emphasis on human involvement.\nIt includes pre-built tasks for cognitive studies and Human-AI teaming with\nexpandable potentials from our modular design. Following conventional cognitive\nneuroscience research, CREW also supports multimodal human physiological signal\nrecording for behavior analysis. Moreover, CREW benchmarks real-time\nhuman-guided reinforcement learning agents using state-of-the-art algorithms\nand well-tuned baselines. With CREW, we were able to conduct 50 human subject\nstudies within a week to verify the effectiveness of our benchmark."
                },
                "authors": [
                    {
                        "name": "Lingyu Zhang"
                    },
                    {
                        "name": "Zhengran Ji"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at: http://generalroboticslab.com/CREW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00911v1",
                "updated": "2025-01-01T17:58:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    58,
                    31,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T17:58:31Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    58,
                    31,
                    2,
                    1,
                    0
                ],
                "title": "Aligning LLMs with Domain Invariant Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs with Domain Invariant Reward Models"
                },
                "summary": "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https://github.com/portal-cornell/dial}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https://github.com/portal-cornell/dial}."
                },
                "authors": [
                    {
                        "name": "David Wu"
                    },
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00906v1",
                "updated": "2025-01-01T17:38:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T17:38:40Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "title": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things"
                },
                "summary": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures."
                },
                "authors": [
                    {
                        "name": "Talha Zeeshan"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Susanna Pirttikangas"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    }
                ],
                "author_detail": {
                    "name": "Sasu Tarkoma"
                },
                "author": "Sasu Tarkoma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00888v1",
                "updated": "2025-01-01T16:28:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    28,
                    21,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:28:21Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    28,
                    21,
                    2,
                    1,
                    0
                ],
                "title": "Unfolding the Headline: Iterative Self-Questioning for News Retrieval\n  and Timeline Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unfolding the Headline: Iterative Self-Questioning for News Retrieval\n  and Timeline Summarization"
                },
                "summary": "In the fast-changing realm of information, the capacity to construct coherent\ntimelines from extensive event-related content has become increasingly\nsignificant and challenging. The complexity arises in aggregating related\ndocuments to build a meaningful event graph around a central topic. This paper\nproposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline\nSummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective\non the integration of Large Language Models (LLMs) to tackle the task of\nTimeline Summarization (TLS). By iteratively reflecting on how events are\nlinked and posing new questions regarding a specific news topic to gather\ninformation online or from an offline knowledge base, LLMs produce and refresh\nchronological summaries based on documents retrieved in each round.\nFurthermore, we curate Open-TLS, a novel dataset of timelines on recent news\ntopics authored by professional journalists to evaluate open-domain TLS where\ninformation overload makes it impossible to find comprehensive relevant\ndocuments from the web. Our experiments indicate that CHRONOS is not only adept\nat open-domain timeline summarization, but it also rivals the performance of\nexisting state-of-the-art systems designed for closed-domain applications,\nwhere a related news corpus is provided for summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-changing realm of information, the capacity to construct coherent\ntimelines from extensive event-related content has become increasingly\nsignificant and challenging. The complexity arises in aggregating related\ndocuments to build a meaningful event graph around a central topic. This paper\nproposes CHRONOS - Causal Headline Retrieval for Open-domain News Timeline\nSummarizatiOn via Iterative Self-Questioning, which offers a fresh perspective\non the integration of Large Language Models (LLMs) to tackle the task of\nTimeline Summarization (TLS). By iteratively reflecting on how events are\nlinked and posing new questions regarding a specific news topic to gather\ninformation online or from an offline knowledge base, LLMs produce and refresh\nchronological summaries based on documents retrieved in each round.\nFurthermore, we curate Open-TLS, a novel dataset of timelines on recent news\ntopics authored by professional journalists to evaluate open-domain TLS where\ninformation overload makes it impossible to find comprehensive relevant\ndocuments from the web. Our experiments indicate that CHRONOS is not only adept\nat open-domain timeline summarization, but it also rivals the performance of\nexisting state-of-the-art systems designed for closed-domain applications,\nwhere a related news corpus is provided for summarization."
                },
                "authors": [
                    {
                        "name": "Weiqi Wu"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00885v1",
                "updated": "2025-01-01T16:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    19,
                    48,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    19,
                    48,
                    2,
                    1,
                    0
                ],
                "title": "Representation in large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation in large language models"
                },
                "summary": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors."
                },
                "authors": [
                    {
                        "name": "Cameron C. Yetman"
                    }
                ],
                "author_detail": {
                    "name": "Cameron C. Yetman"
                },
                "author": "Cameron C. Yetman",
                "arxiv_comment": "Draft of paper under review. 27 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00883v1",
                "updated": "2025-01-01T16:07:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    7,
                    34,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:07:34Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    7,
                    34,
                    2,
                    1,
                    0
                ],
                "title": "DMSA: A Decentralized Microservice Architecture for Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMSA: A Decentralized Microservice Architecture for Edge Networks"
                },
                "summary": "The dispersed node locations and complex topologies of edge networks,\ncombined with intricate dynamic microservice dependencies, render traditional\ncentralized microservice architectures (MSAs) unsuitable. In this paper, we\npropose a decentralized microservice architecture (DMSA), which delegates\nscheduling functions from the control plane to edge nodes. DMSA redesigns and\nimplements three core modules of microservice discovery, monitoring, and\nscheduling for edge networks to achieve precise awareness of instance\ndeployments, low monitoring overhead and measurement errors, and accurate\ndynamic scheduling, respectively. Particularly, DMSA has customized a\nmicroservice scheduling scheme that leverages multi-port listening and\nzero-copy forwarding to guarantee high data forwarding efficiency. Moreover, a\ndynamic weighted multi-level load balancing algorithm is proposed to adjust\nscheduling dynamically with consideration of reliability, priority, and\nresponse delay. Finally, we have implemented a physical verification platform\nfor DMSA. Extensive empirical results demonstrate that compared to\nstate-of-the-art and traditional scheduling schemes, DMSA effectively\ncounteracts link failures and network fluctuations, improving the service\nresponse delay and execution success rate by approximately $60\\% \\sim 75\\%$ and\n$10\\%\\sim15\\%$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dispersed node locations and complex topologies of edge networks,\ncombined with intricate dynamic microservice dependencies, render traditional\ncentralized microservice architectures (MSAs) unsuitable. In this paper, we\npropose a decentralized microservice architecture (DMSA), which delegates\nscheduling functions from the control plane to edge nodes. DMSA redesigns and\nimplements three core modules of microservice discovery, monitoring, and\nscheduling for edge networks to achieve precise awareness of instance\ndeployments, low monitoring overhead and measurement errors, and accurate\ndynamic scheduling, respectively. Particularly, DMSA has customized a\nmicroservice scheduling scheme that leverages multi-port listening and\nzero-copy forwarding to guarantee high data forwarding efficiency. Moreover, a\ndynamic weighted multi-level load balancing algorithm is proposed to adjust\nscheduling dynamically with consideration of reliability, priority, and\nresponse delay. Finally, we have implemented a physical verification platform\nfor DMSA. Extensive empirical results demonstrate that compared to\nstate-of-the-art and traditional scheduling schemes, DMSA effectively\ncounteracts link failures and network fluctuations, improving the service\nresponse delay and execution success rate by approximately $60\\% \\sim 75\\%$ and\n$10\\%\\sim15\\%$, respectively."
                },
                "authors": [
                    {
                        "name": "Yuang Chen"
                    },
                    {
                        "name": "Chengdi Lu"
                    },
                    {
                        "name": "Yongsheng Huang"
                    },
                    {
                        "name": "Chang Wu"
                    },
                    {
                        "name": "Fengqian Guo"
                    },
                    {
                        "name": "Hancheng Lu"
                    },
                    {
                        "name": "Chang Wen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chang Wen Chen"
                },
                "author": "Chang Wen Chen",
                "arxiv_comment": "12 pages, 17 figures, submitted to IEEE Transactions for potential\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00881v1",
                "updated": "2025-01-01T16:00:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    0,
                    18,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T16:00:18Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    16,
                    0,
                    18,
                    2,
                    1,
                    0
                ],
                "title": "Agentic Systems: A Guide to Transforming Industries with Vertical AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Systems: A Guide to Transforming Industries with Vertical AI\n  Agents"
                },
                "summary": "The evolution of agentic systems represents a significant milestone in\nartificial intelligence and modern software systems, driven by the demand for\nvertical intelligence tailored to diverse industries. These systems enhance\nbusiness outcomes through adaptability, learning, and interaction with dynamic\nenvironments. At the forefront of this revolution are Large Language Model\n(LLM) agents, which serve as the cognitive backbone of these intelligent\nsystems. In response to the need for consistency and scalability, this work\nattempts to define a level of standardization for Vertical AI agent design\npatterns by identifying core building blocks and proposing a \\textbf{Cognitive\nSkills } Module, which incorporates domain-specific, purpose-built inference\ncapabilities. Building on these foundational concepts, this paper offers a\ncomprehensive introduction to agentic systems, detailing their core components,\noperational patterns, and implementation strategies. It further explores\npractical use cases and examples across various industries, highlighting the\ntransformative potential of LLM agents in driving industry-specific\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of agentic systems represents a significant milestone in\nartificial intelligence and modern software systems, driven by the demand for\nvertical intelligence tailored to diverse industries. These systems enhance\nbusiness outcomes through adaptability, learning, and interaction with dynamic\nenvironments. At the forefront of this revolution are Large Language Model\n(LLM) agents, which serve as the cognitive backbone of these intelligent\nsystems. In response to the need for consistency and scalability, this work\nattempts to define a level of standardization for Vertical AI agent design\npatterns by identifying core building blocks and proposing a \\textbf{Cognitive\nSkills } Module, which incorporates domain-specific, purpose-built inference\ncapabilities. Building on these foundational concepts, this paper offers a\ncomprehensive introduction to agentic systems, detailing their core components,\noperational patterns, and implementation strategies. It further explores\npractical use cases and examples across various industries, highlighting the\ntransformative potential of LLM agents in driving industry-specific\napplications."
                },
                "authors": [
                    {
                        "name": "Fouad Bousetouane"
                    }
                ],
                "author_detail": {
                    "name": "Fouad Bousetouane"
                },
                "author": "Fouad Bousetouane",
                "arxiv_comment": "31 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00880v1",
                "updated": "2025-01-01T15:58:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    58,
                    51,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:58:51Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    58,
                    51,
                    2,
                    1,
                    0
                ],
                "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Autoregressive Visual Generation with Cluster-Oriented Token\n  Prediction"
                },
                "summary": "Employing LLMs for visual generation has recently become a research focus.\nHowever, the existing methods primarily transfer the LLM architecture to visual\ngeneration but rarely investigate the fundamental differences between language\nand vision. This oversight may lead to suboptimal utilization of visual\ngeneration capabilities within the LLM framework. In this paper, we explore the\ncharacteristics of visual embedding space under the LLM framework and discover\nthat the correlation between visual embeddings can help achieve more stable and\nrobust generation results. We present IAR, an Improved AutoRegressive Visual\nGeneration Method that enhances the training efficiency and generation quality\nof LLM-based visual generation models. Firstly, we propose a Codebook\nRearrangement strategy that uses balanced k-means clustering algorithm to\nrearrange the visual codebook into clusters, ensuring high similarity among\nvisual features within each cluster. Leveraging the rearranged codebook, we\npropose a Cluster-oriented Cross-entropy Loss that guides the model to\ncorrectly predict the cluster where the token is located. This approach ensures\nthat even if the model predicts the wrong token index, there is a high\nprobability the predicted token is located in the correct cluster, which\nsignificantly enhances the generation quality and robustness. Extensive\nexperiments demonstrate that our method consistently enhances the model\ntraining efficiency and performance from 100M to 1.4B, reducing the training\ntime by half while achieving the same FID. Additionally, our approach can be\napplied to various LLM-based visual generation models and adheres to the\nscaling law, providing a promising direction for future research in LLM-based\nvisual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing LLMs for visual generation has recently become a research focus.\nHowever, the existing methods primarily transfer the LLM architecture to visual\ngeneration but rarely investigate the fundamental differences between language\nand vision. This oversight may lead to suboptimal utilization of visual\ngeneration capabilities within the LLM framework. In this paper, we explore the\ncharacteristics of visual embedding space under the LLM framework and discover\nthat the correlation between visual embeddings can help achieve more stable and\nrobust generation results. We present IAR, an Improved AutoRegressive Visual\nGeneration Method that enhances the training efficiency and generation quality\nof LLM-based visual generation models. Firstly, we propose a Codebook\nRearrangement strategy that uses balanced k-means clustering algorithm to\nrearrange the visual codebook into clusters, ensuring high similarity among\nvisual features within each cluster. Leveraging the rearranged codebook, we\npropose a Cluster-oriented Cross-entropy Loss that guides the model to\ncorrectly predict the cluster where the token is located. This approach ensures\nthat even if the model predicts the wrong token index, there is a high\nprobability the predicted token is located in the correct cluster, which\nsignificantly enhances the generation quality and robustness. Extensive\nexperiments demonstrate that our method consistently enhances the model\ntraining efficiency and performance from 100M to 1.4B, reducing the training\ntime by half while achieving the same FID. Additionally, our approach can be\napplied to various LLM-based visual generation models and adheres to the\nscaling law, providing a promising direction for future research in LLM-based\nvisual generation."
                },
                "authors": [
                    {
                        "name": "Teng Hu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Jieyu Weng"
                    },
                    {
                        "name": "Yabiao Wang"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Zhucun Xue"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00879v1",
                "updated": "2025-01-01T15:57:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    57,
                    34,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:57:34Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    57,
                    34,
                    2,
                    1,
                    0
                ],
                "title": "TrustRAG: Enhancing Robustness and Trustworthiness in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustRAG: Enhancing Robustness and Trustworthiness in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user queries. However, these\nsystems remain vulnerable to corpus poisoning attacks that can significantly\ndegrade LLM performance through the injection of malicious content. To address\nthese challenges, we propose TrustRAG, a robust framework that systematically\nfilters compromised and irrelevant content before it reaches the language\nmodel. Our approach implements a two-stage defense mechanism: first, it employs\nK-means clustering to identify potential attack patterns in retrieved documents\nbased on their semantic embeddings, effectively isolating suspicious content.\nSecond, it leverages cosine similarity and ROUGE metrics to detect malicious\ndocuments while resolving discrepancies between the model's internal knowledge\nand external information through a self-assessment process. TrustRAG functions\nas a plug-and-play, training-free module that integrates seamlessly with any\nlanguage model, whether open or closed-source, maintaining high contextual\nrelevance while strengthening defenses against attacks. Through extensive\nexperimental validation, we demonstrate that TrustRAG delivers substantial\nimprovements in retrieval accuracy, efficiency, and attack resistance compared\nto existing approaches across multiple model architectures and datasets. We\nhave made TrustRAG available as open-source software at\n\\url{https://github.com/HuichiZhou/TrustRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user queries. However, these\nsystems remain vulnerable to corpus poisoning attacks that can significantly\ndegrade LLM performance through the injection of malicious content. To address\nthese challenges, we propose TrustRAG, a robust framework that systematically\nfilters compromised and irrelevant content before it reaches the language\nmodel. Our approach implements a two-stage defense mechanism: first, it employs\nK-means clustering to identify potential attack patterns in retrieved documents\nbased on their semantic embeddings, effectively isolating suspicious content.\nSecond, it leverages cosine similarity and ROUGE metrics to detect malicious\ndocuments while resolving discrepancies between the model's internal knowledge\nand external information through a self-assessment process. TrustRAG functions\nas a plug-and-play, training-free module that integrates seamlessly with any\nlanguage model, whether open or closed-source, maintaining high contextual\nrelevance while strengthening defenses against attacks. Through extensive\nexperimental validation, we demonstrate that TrustRAG delivers substantial\nimprovements in retrieval accuracy, efficiency, and attack resistance compared\nto existing approaches across multiple model architectures and datasets. We\nhave made TrustRAG available as open-source software at\n\\url{https://github.com/HuichiZhou/TrustRAG}."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Kin-Hei Lee"
                    },
                    {
                        "name": "Zhonghao Zhan"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Zhenhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhao Li"
                },
                "author": "Zhenhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04264v3",
                "updated": "2025-01-01T15:53:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    53,
                    58,
                    2,
                    1,
                    0
                ],
                "published": "2024-06-06T17:09:32Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    9,
                    32,
                    3,
                    158,
                    0
                ],
                "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLVU: Benchmarking Multi-task Long Video Understanding"
                },
                "summary": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs."
                },
                "authors": [
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Boya Wu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Yongping Xiong"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00874v1",
                "updated": "2025-01-01T15:43:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    43,
                    7,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:43:07Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    43,
                    7,
                    2,
                    1,
                    0
                ],
                "title": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUSIFER: Language Universal Space Integration for Enhanced Multilingual\n  Embeddings with Large Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) based embedding models\nhave established new state-of-the-art benchmarks for text embedding tasks,\nparticularly in dense vector-based retrieval. However, these models\npredominantly focus on English, leaving multilingual embedding capabilities\nlargely unexplored. To address this limitation, we present LUSIFER, a novel\nzero-shot approach that adapts LLM-based embedding models for multilingual\ntasks without requiring multilingual supervision. LUSIFER's architecture\ncombines a multilingual encoder, serving as a language-universal learner, with\nan LLM-based embedding model optimized for embedding-specific tasks. These\ncomponents are seamlessly integrated through a minimal set of trainable\nparameters that act as a connector, effectively transferring the multilingual\nencoder's language understanding capabilities to the specialized embedding\nmodel. Additionally, to comprehensively evaluate multilingual embedding\nperformance, we introduce a new benchmark encompassing 5 primary embedding\ntasks, 123 diverse datasets, and coverage across 14 languages. Extensive\nexperimental results demonstrate that LUSIFER significantly enhances the\nmultilingual performance across various embedding tasks, particularly for\nmedium and low-resource languages, without requiring explicit multilingual\ntraining data."
                },
                "authors": [
                    {
                        "name": "Hieu Man"
                    },
                    {
                        "name": "Nghia Trung Ngo"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13621v2",
                "updated": "2025-01-01T15:40:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    40,
                    57,
                    2,
                    1,
                    0
                ],
                "published": "2023-11-22T08:34:33Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    8,
                    34,
                    33,
                    2,
                    326,
                    0
                ],
                "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EA-KD: Entropy-based Adaptive Knowledge Distillation"
                },
                "summary": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a\nlarger \"teacher\" model by transferring knowledge from the teacher's output or\nfeatures. However, most KD methods treat all samples uniformly, overlooking the\nvarying learning value of each sample and thereby limiting effectiveness. In\nthis paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a\nsimple yet effective plug-and-play KD method that prioritizes learning from\nvaluable samples. EA-KD quantifies each sample's learning value by\nstrategically combining the entropy of the teacher and student output, then\ndynamically reweights the distillation loss to place greater emphasis on\nhigh-value samples. Extensive experiments across diverse KD frameworks and\ntasks$\\unicode{x2014}$including image classification, object detection, and\nlarge language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD\nconsistently enhances performance, achieving state-of-the-art results with\nnegligible computational cost. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Chi-Ping Su"
                    },
                    {
                        "name": "Ching-Hsun Tseng"
                    },
                    {
                        "name": "Bin Pu"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Zhuangzhuang Chen"
                    },
                    {
                        "name": "Shin-Jye Lee"
                    }
                ],
                "author_detail": {
                    "name": "Shin-Jye Lee"
                },
                "author": "Shin-Jye Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00868v1",
                "updated": "2025-01-01T15:20:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    20,
                    35,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:20:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    20,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Large Language Models Are Read/Write Policy-Makers for Simultaneous\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Read/Write Policy-Makers for Simultaneous\n  Generation"
                },
                "summary": "Simultaneous generation models write generation results while reading\nstreaming inputs, necessitating a policy-maker to determine the appropriate\noutput timing. Existing simultaneous generation methods generally adopt the\ntraditional encoder-decoder architecture and learn the generation and\npolicy-making capabilities through complex dynamic programming techniques.\nAlthough LLMs excel at text generation, they face challenges in taking on the\nrole of policy-makers through traditional training methods, limiting their\nexploration in simultaneous generation. To overcome these limitations, we\npropose a novel LLM-driven Simultaneous Generation (LSG) framework, which\nallows the off-the-shelf LLM to decide the generation timing and produce output\nconcurrently. Specifically, LSG selects the generation policy that minimizes\nlatency as the baseline policy. Referring to the baseline policy, LSG enables\nthe LLM to devise an improved generation policy that better balances latency\nand generation quality, and writes generation results accordingly. Experiments\non simultaneous translation and streaming automatic speech recognition tasks\nshow that our method can achieve state-of-the-art performance utilizing the\nopen-source LLMs and demonstrate practicality in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous generation models write generation results while reading\nstreaming inputs, necessitating a policy-maker to determine the appropriate\noutput timing. Existing simultaneous generation methods generally adopt the\ntraditional encoder-decoder architecture and learn the generation and\npolicy-making capabilities through complex dynamic programming techniques.\nAlthough LLMs excel at text generation, they face challenges in taking on the\nrole of policy-makers through traditional training methods, limiting their\nexploration in simultaneous generation. To overcome these limitations, we\npropose a novel LLM-driven Simultaneous Generation (LSG) framework, which\nallows the off-the-shelf LLM to decide the generation timing and produce output\nconcurrently. Specifically, LSG selects the generation policy that minimizes\nlatency as the baseline policy. Referring to the baseline policy, LSG enables\nthe LLM to devise an improved generation policy that better balances latency\nand generation quality, and writes generation results accordingly. Experiments\non simultaneous translation and streaming automatic speech recognition tasks\nshow that our method can achieve state-of-the-art performance utilizing the\nopen-source LLMs and demonstrate practicality in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "Accepted at AAAI 2025. 13 pages, 7 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00867v1",
                "updated": "2025-01-01T15:20:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    20,
                    31,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:20:31Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    20,
                    31,
                    2,
                    1,
                    0
                ],
                "title": "Interactionalism: Re-Designing Higher Learning for the Large Language\n  Agent Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactionalism: Re-Designing Higher Learning for the Large Language\n  Agent Era"
                },
                "summary": "We introduce Interactionalism as a new set of guiding principles and\nheuristics for the design and architecture of learning now available due to\nGenerative AI (GenAI) platforms. Specifically, we articulate interactional\nintelligence as a net new skill set that is increasingly important when core\ncognitive tasks are automatable and augmentable by GenAI functions. We break\ndown these skills into core sets of meta-cognitive and meta-emotional\ncomponents and show how working with Large Language Model (LLM)-based agents\ncan be proactively used to help develop learners. Interactionalism is not\nadvanced as a theory of learning; but as a blueprint for the practice of\nlearning - in coordination with GenAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Interactionalism as a new set of guiding principles and\nheuristics for the design and architecture of learning now available due to\nGenerative AI (GenAI) platforms. Specifically, we articulate interactional\nintelligence as a net new skill set that is increasingly important when core\ncognitive tasks are automatable and augmentable by GenAI functions. We break\ndown these skills into core sets of meta-cognitive and meta-emotional\ncomponents and show how working with Large Language Model (LLM)-based agents\ncan be proactively used to help develop learners. Interactionalism is not\nadvanced as a theory of learning; but as a blueprint for the practice of\nlearning - in coordination with GenAI."
                },
                "authors": [
                    {
                        "name": "Mihnea C. Moldoveanu"
                    },
                    {
                        "name": "George Siemens"
                    }
                ],
                "author_detail": {
                    "name": "George Siemens"
                },
                "author": "George Siemens",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00865v1",
                "updated": "2025-01-01T15:18:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    18,
                    23,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T15:18:23Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    18,
                    23,
                    2,
                    1,
                    0
                ],
                "title": "Negative to Positive Co-learning with Aggressive Modality Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negative to Positive Co-learning with Aggressive Modality Dropout"
                },
                "summary": "This paper aims to document an effective way to improve multimodal\nco-learning by using aggressive modality dropout. We find that by using\naggressive modality dropout we are able to reverse negative co-learning (NCL)\nto positive co-learning (PCL). Aggressive modality dropout can be used to\n\"prep\" a multimodal model for unimodal deployment, and dramatically increases\nmodel performance during negative co-learning, where during some experiments we\nsaw a 20% gain in accuracy. We also benchmark our modality dropout technique\nagainst PCL to show that our modality drop out technique improves co-learning\nduring PCL, although it does not have as much as an substantial effect as it\ndoes during NCL. Github: https://github.com/nmagal/modality_drop_for_colearning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper aims to document an effective way to improve multimodal\nco-learning by using aggressive modality dropout. We find that by using\naggressive modality dropout we are able to reverse negative co-learning (NCL)\nto positive co-learning (PCL). Aggressive modality dropout can be used to\n\"prep\" a multimodal model for unimodal deployment, and dramatically increases\nmodel performance during negative co-learning, where during some experiments we\nsaw a 20% gain in accuracy. We also benchmark our modality dropout technique\nagainst PCL to show that our modality drop out technique improves co-learning\nduring PCL, although it does not have as much as an substantial effect as it\ndoes during NCL. Github: https://github.com/nmagal/modality_drop_for_colearning"
                },
                "authors": [
                    {
                        "name": "Nicholas Magal"
                    },
                    {
                        "name": "Minh Tran"
                    },
                    {
                        "name": "Riku Arakawa"
                    },
                    {
                        "name": "Suzanne Nie"
                    }
                ],
                "author_detail": {
                    "name": "Suzanne Nie"
                },
                "author": "Suzanne Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12698v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12698v4",
                "updated": "2025-01-01T14:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    14,
                    51,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2024-12-17T09:16:28Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    16,
                    28,
                    1,
                    352,
                    0
                ],
                "title": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling"
                },
                "summary": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations."
                },
                "authors": [
                    {
                        "name": "Allen Lei"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Jianfei Yang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "arxiv_comment": "Accepted for ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12698v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12698v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v4",
                "updated": "2025-01-01T13:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    46,
                    37,
                    2,
                    1,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix, accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00830v1",
                "updated": "2025-01-01T13:20:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    20,
                    1,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T13:20:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    20,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM+AL: Bridging Large Language Models and Action Languages for Complex\n  Reasoning about Actions"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in various\nintelligent tasks but still struggle with complex action reasoning tasks that\nrequire systematic search. To address this limitation, we propose a method that\nbridges the natural language understanding capabilities of LLMs with the\nsymbolic reasoning strengths of action languages. Our approach, termed\n\"LLM+AL,\" leverages the LLM's strengths in semantic parsing and commonsense\nknowledge generation alongside the action language's proficiency in automated\nreasoning based on encoded knowledge. We compare LLM+AL against\nstate-of-the-art LLMs, including ChatGPT-4, Claude 3 Opus, Gemini Ultra 1.0,\nand o1-preview, using benchmarks for complex reasoning about actions. Our\nfindings indicate that, although all methods exhibit errors, LLM+AL, with\nrelatively minimal human corrections, consistently leads to correct answers,\nwhereas standalone LLMs fail to improve even with human feedback. LLM+AL also\ncontributes to automated generation of action languages."
                },
                "authors": [
                    {
                        "name": "Adam Ishay"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "42 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00829v1",
                "updated": "2025-01-01T13:19:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    19,
                    58,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T13:19:58Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    19,
                    58,
                    2,
                    1,
                    0
                ],
                "title": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component\n  Deep Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Empowered Adaptive Evolutionary Algorithm For Multi-Component\n  Deep Learning Systems"
                },
                "summary": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching\noptimal solutions in complex multi-component applications. Traditional MOEAs\nfor multi-component deep learning (MCDL) systems face challenges in enhancing\nthe search efficiency while maintaining the diversity. To combat these, this\npaper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search\nalgorithm to detect safety violations in MCDL systems. Inspired by the\ncontext-understanding ability of Large Language Models (LLMs), $\\mu$MOEA\npromotes the LLM to comprehend the optimization problem and generate an initial\npopulation tailed to evolutionary objectives. Subsequently, it employs adaptive\nselection and variation to iteratively produce offspring, balancing the\nevolutionary efficiency and diversity. During the evolutionary process, to\nnavigate away from the local optima, $\\mu$MOEA integrates the evolutionary\nexperience back into the LLM. This utilization harnesses the LLM's quantitative\nreasoning prowess to generate differential seeds, breaking away from current\noptimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL\nsystems, and compare its performance with state-of-the-art MOEA methods.\nExperimental results show that $\\mu$MOEA can significantly improve the\nefficiency and diversity of the evolutionary search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective evolutionary algorithms (MOEAs) are widely used for searching\noptimal solutions in complex multi-component applications. Traditional MOEAs\nfor multi-component deep learning (MCDL) systems face challenges in enhancing\nthe search efficiency while maintaining the diversity. To combat these, this\npaper proposes $\\mu$MOEA, the first LLM-empowered adaptive evolutionary search\nalgorithm to detect safety violations in MCDL systems. Inspired by the\ncontext-understanding ability of Large Language Models (LLMs), $\\mu$MOEA\npromotes the LLM to comprehend the optimization problem and generate an initial\npopulation tailed to evolutionary objectives. Subsequently, it employs adaptive\nselection and variation to iteratively produce offspring, balancing the\nevolutionary efficiency and diversity. During the evolutionary process, to\nnavigate away from the local optima, $\\mu$MOEA integrates the evolutionary\nexperience back into the LLM. This utilization harnesses the LLM's quantitative\nreasoning prowess to generate differential seeds, breaking away from current\noptimal solutions. We evaluate $\\mu$MOEA in finding safety violations of MCDL\nsystems, and compare its performance with state-of-the-art MOEA methods.\nExperimental results show that $\\mu$MOEA can significantly improve the\nefficiency and diversity of the evolutionary search."
                },
                "authors": [
                    {
                        "name": "Haoxiang Tian"
                    },
                    {
                        "name": "Xingshuo Han"
                    },
                    {
                        "name": "Guoquan Wu"
                    },
                    {
                        "name": "An Guo"
                    },
                    {
                        "name": "Yuan Zhou. Jie Zhang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Jun Wei"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "arxiv_comment": "9",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00826v1",
                "updated": "2025-01-01T13:08:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    8,
                    17,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T13:08:17Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    8,
                    17,
                    2,
                    1,
                    0
                ],
                "title": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management"
                },
                "summary": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrency investment is inherently difficult due to its shorter history\ncompared to traditional assets, the need to integrate vast amounts of data from\nvarious modalities, and the requirement for complex reasoning. While deep\nlearning approaches have been applied to address these challenges, their\nblack-box nature raises concerns about trust and explainability. Recently,\nlarge language models (LLMs) have shown promise in financial applications due\nto their ability to understand multi-modal data and generate explainable\ndecisions. However, single LLM faces limitations in complex, comprehensive\ntasks such as asset investment. These limitations are even more pronounced in\ncryptocurrency investment, where LLMs have less domain-specific knowledge in\ntheir training corpora.\n  To overcome these challenges, we propose an explainable, multi-modal,\nmulti-agent framework for cryptocurrency investment. Our framework uses\nspecialized agents that collaborate within and across teams to handle subtasks\nsuch as data analysis, literature integration, and investment decision-making\nfor the top 30 cryptocurrencies by market capitalization. The expert training\nmodule fine-tunes agents using multi-modal historical data and professional\ninvestment literature, while the multi-agent investment module employs\nreal-time data to make informed cryptocurrency investment decisions. Unique\nintrateam and interteam collaboration mechanisms enhance prediction accuracy by\nadjusting final predictions based on confidence levels within agent teams and\nfacilitating information sharing between teams. Empirical evaluation using data\nfrom November 2023 to September 2024 demonstrates that our framework\noutperforms single-agent models and market benchmarks in classification, asset\npricing, portfolio, and explainability performance."
                },
                "authors": [
                    {
                        "name": "Yichen Luo"
                    },
                    {
                        "name": "Yebo Feng"
                    },
                    {
                        "name": "Jiahua Xu"
                    },
                    {
                        "name": "Paolo Tasca"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09583v2",
                "updated": "2025-01-01T13:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    2,
                    5,
                    2,
                    1,
                    0
                ],
                "published": "2023-08-18T14:23:21Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    14,
                    23,
                    21,
                    4,
                    230,
                    0
                ],
                "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WizardMath: Empowering Mathematical Reasoning for Large Language Models\n  via Reinforced Evol-Instruct"
                },
                "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM"
                },
                "authors": [
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jianguang Lou"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Xiubo Geng"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Shifeng Chen"
                    },
                    {
                        "name": "Yansong Tang"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "LLM, Mathematical Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00824v1",
                "updated": "2025-01-01T13:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T13:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    0,
                    1,
                    2,
                    1,
                    0
                ],
                "title": "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Sifting Funnel: Privacy-preserving Collaborative Inference\n  Against Model Inversion Attacks"
                },
                "summary": "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity of neural networks and inference tasks, coupled with demands\nfor computational efficiency and real-time feedback, poses significant\nchallenges for resource-constrained edge devices. Collaborative inference\nmitigates this by assigning shallow feature extraction to edge devices and\noffloading features to the cloud for further inference, reducing computational\nload. However, transmitted features remain susceptible to model inversion\nattacks (MIAs), which can reconstruct original input data. Current defenses,\nsuch as perturbation and information bottleneck techniques, offer explainable\nprotection but face limitations, including the lack of standardized criteria\nfor assessing MIA difficulty, challenges in mutual information estimation, and\ntrade-offs among usability, privacy, and deployability.\n  To address these challenges, we introduce the first criterion to evaluate MIA\ndifficulty in collaborative inference, supported by theoretical analysis of\nexisting attacks and defenses, validated using experiments with the Mutual\nInformation Neural Estimator (MINE). Based on these findings, we propose\nSiftFunnel, a privacy-preserving framework for collaborative inference. The\nedge model is trained with linear and non-linear correlation constraints to\nreduce redundant information in transmitted features, enhancing privacy\nprotection. Label smoothing and a cloud-based upsampling module are added to\nbalance usability and privacy. To improve deployability, the edge model\nincorporates a funnel-shaped structure and attention mechanisms, preserving\nboth privacy and usability. Extensive experiments demonstrate that SiftFunnel\noutperforms state-of-the-art defenses against MIAs, achieving superior privacy\nprotection with less than 3% accuracy loss and striking an optimal balance\namong usability, privacy, and practicality."
                },
                "authors": [
                    {
                        "name": "Rongke Liu"
                    }
                ],
                "author_detail": {
                    "name": "Rongke Liu"
                },
                "author": "Rongke Liu",
                "arxiv_comment": "15 pages, 4 figures, 7 tables. The experiment is still being\n  supplemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00819v1",
                "updated": "2025-01-01T12:23:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    23,
                    36,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T12:23:36Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    23,
                    36,
                    2,
                    1,
                    0
                ],
                "title": "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics"
                },
                "summary": "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation."
                },
                "authors": [
                    {
                        "name": "Chih-Yuan Yang"
                    },
                    {
                        "name": "Keng-Hou Leong"
                    },
                    {
                        "name": "Kexin Cao"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Wai Kin"
                    },
                    {
                        "name": "Chan"
                    }
                ],
                "author_detail": {
                    "name": "Chan"
                },
                "arxiv_affiliation": "Victor",
                "author": "Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00818v1",
                "updated": "2025-01-01T12:19:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    19,
                    17,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T12:19:17Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    19,
                    17,
                    2,
                    1,
                    0
                ],
                "title": "SPARNet: Continual Test-Time Adaptation via Sample Partitioning Strategy\n  and Anti-Forgetting Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARNet: Continual Test-Time Adaptation via Sample Partitioning Strategy\n  and Anti-Forgetting Regularization"
                },
                "summary": "Test-time Adaptation (TTA) aims to improve model performance when the model\nencounters domain changes after deployment. The standard TTA mainly considers\nthe case where the target domain is static, while the continual TTA needs to\nundergo a sequence of domain changes. This encounters a significant challenge\nas the model needs to adapt for the long-term and is unaware of when the domain\nchanges occur. The quality of pseudo-labels is hard to guarantee. Noisy\npseudo-labels produced by simple self-training methods can cause error\naccumulation and catastrophic forgetting. In this work, we propose a new\nframework named SPARNet which consists of two parts, sample partitioning\nstrategy and anti-forgetting regularization. The sample partition strategy\ndivides samples into two groups, namely reliable samples and unreliable\nsamples. According to the characteristics of each group of samples, we choose\ndifferent strategies to deal with different groups of samples. This ensures\nthat reliable samples contribute more to the model. At the same time, the\nnegative impacts of unreliable samples are eliminated by the mean teacher's\nconsistency learning. Finally, we introduce a regularization term to alleviate\nthe catastrophic forgetting problem, which can limit important parameters from\nexcessive changes. This term enables long-term adaptation of parameters in the\nnetwork. The effectiveness of our method is demonstrated in continual TTA\nscenario by conducting a large number of experiments on CIFAR10-C, CIFAR100-C\nand ImageNet-C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Adaptation (TTA) aims to improve model performance when the model\nencounters domain changes after deployment. The standard TTA mainly considers\nthe case where the target domain is static, while the continual TTA needs to\nundergo a sequence of domain changes. This encounters a significant challenge\nas the model needs to adapt for the long-term and is unaware of when the domain\nchanges occur. The quality of pseudo-labels is hard to guarantee. Noisy\npseudo-labels produced by simple self-training methods can cause error\naccumulation and catastrophic forgetting. In this work, we propose a new\nframework named SPARNet which consists of two parts, sample partitioning\nstrategy and anti-forgetting regularization. The sample partition strategy\ndivides samples into two groups, namely reliable samples and unreliable\nsamples. According to the characteristics of each group of samples, we choose\ndifferent strategies to deal with different groups of samples. This ensures\nthat reliable samples contribute more to the model. At the same time, the\nnegative impacts of unreliable samples are eliminated by the mean teacher's\nconsistency learning. Finally, we introduce a regularization term to alleviate\nthe catastrophic forgetting problem, which can limit important parameters from\nexcessive changes. This term enables long-term adaptation of parameters in the\nnetwork. The effectiveness of our method is demonstrated in continual TTA\nscenario by conducting a large number of experiments on CIFAR10-C, CIFAR100-C\nand ImageNet-C."
                },
                "authors": [
                    {
                        "name": "Xinru Meng"
                    },
                    {
                        "name": "Han Sun"
                    },
                    {
                        "name": "Jiamei Liu"
                    },
                    {
                        "name": "Ningzhong Liu"
                    },
                    {
                        "name": "Huiyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Huiyu Zhou"
                },
                "author": "Huiyu Zhou",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00746v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00746v8",
                "updated": "2025-01-01T12:12:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    12,
                    56,
                    2,
                    1,
                    0
                ],
                "published": "2024-02-01T16:40:32Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    16,
                    40,
                    32,
                    3,
                    32,
                    0
                ],
                "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System"
                },
                "summary": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence (AI), especially large\nlanguage models (LLMs), have significantly advanced healthcare applications and\ndemonstrated potentials in intelligent medical treatment. However, there are\nconspicuous challenges such as vast data volumes and inconsistent symptom\ncharacterization standards, preventing full integration of healthcare AI\nsystems with individual patients' needs. To promote professional and\npersonalized healthcare, we propose an innovative framework, Heath-LLM, which\ncombines large-scale feature extraction and medical knowledge trade-off\nscoring. Compared to traditional health management applications, our system has\nthree main advantages: (1) It integrates health reports and medical knowledge\ninto a large model to ask relevant questions to large language model for\ndisease prediction; (2) It leverages a retrieval augmented generation (RAG)\nmechanism to enhance feature extraction; (3) It incorporates a semi-automated\nfeature updating framework that can merge and delete features to improve\naccuracy of disease prediction. We experiment on a large number of health\nreports to assess the effectiveness of Health-LLM system. The results indicate\nthat the proposed system surpasses the existing ones and has the potential to\nsignificantly advance disease prediction and personalized health management."
                },
                "authors": [
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Lizhou Fan"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Suiyuan Zhu"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00746v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00746v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00805v1",
                "updated": "2025-01-01T11:11:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    11,
                    11,
                    7,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T11:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    11,
                    11,
                    7,
                    2,
                    1,
                    0
                ],
                "title": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken\n  Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIDE: Integrating Speech Language Model with LLM for Spontaneous Spoken\n  Dialogue Generation"
                },
                "summary": "Recently, ``textless\" speech language models (SLMs) based on speech units\nhave made huge progress in generating naturalistic speech, including non-verbal\nvocalizations. However, the generated speech samples often lack semantic\ncoherence. In this paper, we propose SLM and LLM Integration for spontaneous\nspoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to\ngenerate the textual content of spoken dialogue. Next, we convert the textual\ndialogues into phoneme sequences and use a two-tower transformer-based duration\npredictor to predict the duration of each phoneme. Finally, an SLM conditioned\non the spoken phoneme sequences is used to vocalize the textual dialogue.\nExperimental results on the Fisher dataset demonstrate that our system can\ngenerate naturalistic spoken dialogue while maintaining high semantic\ncoherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, ``textless\" speech language models (SLMs) based on speech units\nhave made huge progress in generating naturalistic speech, including non-verbal\nvocalizations. However, the generated speech samples often lack semantic\ncoherence. In this paper, we propose SLM and LLM Integration for spontaneous\nspoken Dialogue gEneration (SLIDE). Specifically, we first utilize an LLM to\ngenerate the textual content of spoken dialogue. Next, we convert the textual\ndialogues into phoneme sequences and use a two-tower transformer-based duration\npredictor to predict the duration of each phoneme. Finally, an SLM conditioned\non the spoken phoneme sequences is used to vocalize the textual dialogue.\nExperimental results on the Fisher dataset demonstrate that our system can\ngenerate naturalistic spoken dialogue while maintaining high semantic\ncoherence."
                },
                "authors": [
                    {
                        "name": "Haitian Lu"
                    },
                    {
                        "name": "Gaofeng Cheng"
                    },
                    {
                        "name": "Liuping Luo"
                    },
                    {
                        "name": "Leying Zhang"
                    },
                    {
                        "name": "Yanmin Qian"
                    },
                    {
                        "name": "Pengyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhang"
                },
                "author": "Pengyuan Zhang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00795v1",
                "updated": "2025-01-01T10:16:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    16,
                    10,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T10:16:10Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    16,
                    10,
                    2,
                    1,
                    0
                ],
                "title": "Multimodal Large Models Are Effective Action Anticipators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Models Are Effective Action Anticipators"
                },
                "summary": "The task of long-term action anticipation demands solutions that can\neffectively model temporal dynamics over extended periods while deeply\nunderstanding the inherent semantics of actions. Traditional approaches, which\nprimarily rely on recurrent units or Transformer layers to capture long-term\ndependencies, often fall short in addressing these challenges. Large Language\nModels (LLMs), with their robust sequential modeling capabilities and extensive\ncommonsense knowledge, present new opportunities for long-term action\nanticipation. In this work, we introduce the ActionLLM framework, a novel\napproach that treats video sequences as successive tokens, leveraging LLMs to\nanticipate future actions. Our baseline model simplifies the LLM architecture\nby setting future tokens, incorporating an action tuning module, and reducing\nthe textual decoder layer to a linear layer, enabling straightforward action\nprediction without the need for complex instructions or redundant descriptions.\nTo further harness the commonsense reasoning of LLMs, we predict action\ncategories for observed frames and use sequential textual clues to guide\nsemantic understanding. In addition, we introduce a Cross-Modality Interaction\nBlock, designed to explore the specificity within each modality and capture\ninteractions between vision and textual modalities, thereby enhancing\nmultimodal tuning. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of the proposed ActionLLM framework, encouraging a promising\ndirection to explore LLMs in the context of action anticipation. Code is\navailable at https://github.com/2tianyao1/ActionLLM.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of long-term action anticipation demands solutions that can\neffectively model temporal dynamics over extended periods while deeply\nunderstanding the inherent semantics of actions. Traditional approaches, which\nprimarily rely on recurrent units or Transformer layers to capture long-term\ndependencies, often fall short in addressing these challenges. Large Language\nModels (LLMs), with their robust sequential modeling capabilities and extensive\ncommonsense knowledge, present new opportunities for long-term action\nanticipation. In this work, we introduce the ActionLLM framework, a novel\napproach that treats video sequences as successive tokens, leveraging LLMs to\nanticipate future actions. Our baseline model simplifies the LLM architecture\nby setting future tokens, incorporating an action tuning module, and reducing\nthe textual decoder layer to a linear layer, enabling straightforward action\nprediction without the need for complex instructions or redundant descriptions.\nTo further harness the commonsense reasoning of LLMs, we predict action\ncategories for observed frames and use sequential textual clues to guide\nsemantic understanding. In addition, we introduce a Cross-Modality Interaction\nBlock, designed to explore the specificity within each modality and capture\ninteractions between vision and textual modalities, thereby enhancing\nmultimodal tuning. Extensive experiments on benchmark datasets demonstrate the\nsuperiority of the proposed ActionLLM framework, encouraging a promising\ndirection to explore LLMs in the context of action anticipation. Code is\navailable at https://github.com/2tianyao1/ActionLLM.git."
                },
                "authors": [
                    {
                        "name": "Binglu Wang"
                    },
                    {
                        "name": "Yao Tian"
                    },
                    {
                        "name": "Shunzhou Wang"
                    },
                    {
                        "name": "Le Yang"
                    }
                ],
                "author_detail": {
                    "name": "Le Yang"
                },
                "author": "Le Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]