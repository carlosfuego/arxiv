[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v1",
                "updated": "2025-07-30T15:03:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per\nsecond rate. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Consequently, the proposed model can utilize KV-cache\nacross chunks and incorporate future context within each chunk. Furthermore, it\nbridges continuous and discrete feature modeling, demonstrating that continuous\nAR flow-matching can predict discrete tokens with finite scalar quantizers.\nThis efficient codec and fast chunk-autoregressive architecture also makes the\nproposed model particularly effective for generating extended content.\nExperiment for demos of our work} on podcast datasets demonstrate its\ncapability to efficiently generate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v1",
                "updated": "2025-07-29T04:21:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v1",
                "updated": "2025-07-26T10:34:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v1",
                "updated": "2025-07-25T23:55:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pál András Papp"
                    },
                    {
                        "name": "Toni Böhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14204v1",
                "updated": "2025-07-14T19:09:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:09:57Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    9,
                    57,
                    0,
                    195,
                    0
                ],
                "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of\n  Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache."
                },
                "authors": [
                    {
                        "name": "Dachuan Shi"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Xiangchi Yuan"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Haoran You"
                    },
                    {
                        "name": "Sixu Li"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristóbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "José María Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muñoz"
                    },
                    {
                        "name": "Manuel Gil Pérez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anaïs Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "José Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "José Emilio Labra Gayo"
                },
                "author": "José Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.22887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22887v1",
                "updated": "2025-07-30T17:59:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    46,
                    2,
                    211,
                    0
                ],
                "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to show Demos in Your Prompt: A Positional Bias of In-Context\n  Learning"
                },
                "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Kwesi Cobbina"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v2",
                "updated": "2025-07-30T17:59:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    37,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenghailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22879v2",
                "updated": "2025-07-31T16:54:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    54,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T17:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    55,
                    6,
                    2,
                    211,
                    0
                ],
                "title": "RecGPT Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecGPT Technical Report"
                },
                "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."
                },
                "authors": [
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Gaoyang Guo"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Mao Zhang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Chi Li"
                    },
                    {
                        "name": "Dimin Wang"
                    },
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Haozhuang Liu"
                    },
                    {
                        "name": "Jialin Zhu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Lang Tian"
                    },
                    {
                        "name": "Liang Rao"
                    },
                    {
                        "name": "Longbin Li"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Na He"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Qiqi Huang"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiaoxiao He"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yaxuan Wu"
                    },
                    {
                        "name": "Yeqiu Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Yinnan Song"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yuliang Yan"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zhibo Xiao"
                    },
                    {
                        "name": "Zhixin Ma"
                    },
                    {
                        "name": "Zile Zhou"
                    },
                    {
                        "name": "Ziqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziqi Zhang"
                },
                "author": "Ziqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22876v1",
                "updated": "2025-07-30T17:52:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    52,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:52:25Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    52,
                    25,
                    2,
                    211,
                    0
                ],
                "title": "Automatically discovering heuristics in a complex SAT solver with large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically discovering heuristics in a complex SAT solver with large\n  language models"
                },
                "summary": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development."
                },
                "authors": [
                    {
                        "name": "Yiwen Sun"
                    },
                    {
                        "name": "Furong Ye"
                    },
                    {
                        "name": "Zhihan Chen"
                    },
                    {
                        "name": "Ke Wei"
                    },
                    {
                        "name": "Shaowei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Cai"
                },
                "author": "Shaowei Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22869v1",
                "updated": "2025-07-30T17:44:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    44,
                    7,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:44:07Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    44,
                    7,
                    2,
                    211,
                    0
                ],
                "title": "Inference on Common Trends in a Cointegrated Nonlinear SVAR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on Common Trends in a Cointegrated Nonlinear SVAR"
                },
                "summary": "We consider the problem of performing inference on the number of common\nstochastic trends when data is generated by a cointegrated CKSVAR (a\ntwo-regime, piecewise-linear SVAR; Mavroeidis, 2021), using a modified version\nof the Breitung (2002) multivariate variance ratio test that is robust to the\npresence of nonlinear cointegration (of a known form). To derive the\nasymptotics of our test statistic, we prove a fundamental LLN-type result for a\nclass of stable but nonstationary autoregressive processes, using a novel dual\nlinear process approximation. We show that our modified test yields correct\ninferences regarding the number of common trends in such a system, whereas the\nunmodified test tends to infer a higher number of common trends than are\nactually present, when cointegrating relations are nonlinear.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of performing inference on the number of common\nstochastic trends when data is generated by a cointegrated CKSVAR (a\ntwo-regime, piecewise-linear SVAR; Mavroeidis, 2021), using a modified version\nof the Breitung (2002) multivariate variance ratio test that is robust to the\npresence of nonlinear cointegration (of a known form). To derive the\nasymptotics of our test statistic, we prove a fundamental LLN-type result for a\nclass of stable but nonstationary autoregressive processes, using a novel dual\nlinear process approximation. We show that our modified test yields correct\ninferences regarding the number of common trends in such a system, whereas the\nunmodified test tends to infer a higher number of common trends than are\nactually present, when cointegrating relations are nonlinear."
                },
                "authors": [
                    {
                        "name": "James A. Duffy"
                    },
                    {
                        "name": "Xiyu Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiyu Jiao"
                },
                "author": "Xiyu Jiao",
                "arxiv_comment": "ii + 38 pp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10 (Primary), 91B84, 62E20, 60G65 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22867v1",
                "updated": "2025-07-30T17:42:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    42,
                    13,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:42:13Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    42,
                    13,
                    2,
                    211,
                    0
                ],
                "title": "Hawkes Processes with Variable Length Memory: Existence, Inference and\n  Application to Neuronal Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hawkes Processes with Variable Length Memory: Existence, Inference and\n  Application to Neuronal Activity"
                },
                "summary": "Motivated by applications in neuroscience, where the memory of a neuron may\nreset upon firing, we introduce a new class of nonlinear Hawkes processes with\nvariable length memory. Multivariate Hawkes processes are past-dependant point\nprocesses originally introduced tomodel excitation effects, later extended to a\nnonlinear framework to account for the opposite effect, known as inhibition.\nOur model generalises classical Hawkes processes, with or without inhibition,\nfocusing on the situation where the probability of an event occurring within a\ngiven subprocess depends solely on the history since its last event. Our main\ncontributions are to prove existence of such processes, and to derive a\nworkable likelihood maximisation method, capable of identifying both classical\nand variable memory dynamics. We demonstrate the effectiveness of our approach\nboth on synthetic data, and on a neuronal activity dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by applications in neuroscience, where the memory of a neuron may\nreset upon firing, we introduce a new class of nonlinear Hawkes processes with\nvariable length memory. Multivariate Hawkes processes are past-dependant point\nprocesses originally introduced tomodel excitation effects, later extended to a\nnonlinear framework to account for the opposite effect, known as inhibition.\nOur model generalises classical Hawkes processes, with or without inhibition,\nfocusing on the situation where the probability of an event occurring within a\ngiven subprocess depends solely on the history since its last event. Our main\ncontributions are to prove existence of such processes, and to derive a\nworkable likelihood maximisation method, capable of identifying both classical\nand variable memory dynamics. We demonstrate the effectiveness of our approach\nboth on synthetic data, and on a neuronal activity dataset."
                },
                "authors": [
                    {
                        "name": "Sacha Quayle"
                    },
                    {
                        "name": "Anna Bonnet"
                    },
                    {
                        "name": "Maxime Sangnier"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Sangnier"
                },
                "author": "Maxime Sangnier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10608v2",
                "updated": "2025-07-30T17:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    40,
                    47,
                    2,
                    211,
                    0
                ],
                "published": "2025-03-13T17:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    52,
                    43,
                    3,
                    72,
                    0
                ],
                "title": "Hierarchical Bayesian inference for uncertainty quantification of\n  thermal grease rheology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Bayesian inference for uncertainty quantification of\n  thermal grease rheology"
                },
                "summary": "Rheologically complex soft solids such as thermal greases consist of filler\nparticles within a polymer matrix. These materials find applications in\nimproving the conformity of solid-solid contacts and enhancing heat transfer.\nComplex soft solids exhibit a transient non-Newtonian rheological response,\nincluding thixotropy and viscoelasticity. Previously, stress relaxation and\nbuildup in sheared commercial thermal greases were successfully captured using\na nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic\n(TEVP). However, the previous model calibration methods ignored parameter\nuncertainty, providing only single values of the rheological parameters, and\ndid not quantitatively address the chosen model's identifiability from the data\nor credibility of the calibration. We address these limitations via\nhierarchical Bayesian inference, accounting for uncertainties arising from\nepistemic and aleatoric sources. Importantly, the hierarchical approach allows\nus to assimilate experiments measuring the stress responses at various startup\nshear rates by allowing the models' parameters to vary across different shear\nrates. Then, a global distribution and the associated uncertainty are obtained\nby pooling. We also propagate uncertainties to the transient shear stress\nresponse predicted by the models. Overall, we demonstrate that the chosen NEVP\nand TEVP models are identifiable from rheometric startup data. However, for the\nTEVP model, the uncertainty of the parameters is lower (narrower distributions)\nwhen higher shear rates are used for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheologically complex soft solids such as thermal greases consist of filler\nparticles within a polymer matrix. These materials find applications in\nimproving the conformity of solid-solid contacts and enhancing heat transfer.\nComplex soft solids exhibit a transient non-Newtonian rheological response,\nincluding thixotropy and viscoelasticity. Previously, stress relaxation and\nbuildup in sheared commercial thermal greases were successfully captured using\na nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic\n(TEVP). However, the previous model calibration methods ignored parameter\nuncertainty, providing only single values of the rheological parameters, and\ndid not quantitatively address the chosen model's identifiability from the data\nor credibility of the calibration. We address these limitations via\nhierarchical Bayesian inference, accounting for uncertainties arising from\nepistemic and aleatoric sources. Importantly, the hierarchical approach allows\nus to assimilate experiments measuring the stress responses at various startup\nshear rates by allowing the models' parameters to vary across different shear\nrates. Then, a global distribution and the associated uncertainty are obtained\nby pooling. We also propagate uncertainties to the transient shear stress\nresponse predicted by the models. Overall, we demonstrate that the chosen NEVP\nand TEVP models are identifiable from rheometric startup data. However, for the\nTEVP model, the uncertainty of the parameters is lower (narrower distributions)\nwhen higher shear rates are used for inference."
                },
                "authors": [
                    {
                        "name": "Pranay P. Nagrani"
                    },
                    {
                        "name": "Akshay J. Thomas"
                    },
                    {
                        "name": "Amy M. Marconnet"
                    },
                    {
                        "name": "Ivan C. Christov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan C. Christov"
                },
                "author": "Ivan C. Christov",
                "arxiv_doi": "10.1122/8.0001008",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1122/8.0001008",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 15 figures; v2: minor updates, add further appendices,\n  accepted for publication in Journal of Rheology",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18082v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18082v3",
                "updated": "2025-07-30T17:39:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    39,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-24T04:17:06Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    4,
                    17,
                    6,
                    3,
                    205,
                    0
                ],
                "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment\n  Pancreatic Tumor in Endoscopic Ultrasound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment\n  Pancreatic Tumor in Endoscopic Ultrasound"
                },
                "summary": "Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS ."
                },
                "authors": [
                    {
                        "name": "Pascal Spiegler"
                    },
                    {
                        "name": "Taha Koleilat"
                    },
                    {
                        "name": "Arash Harirpoush"
                    },
                    {
                        "name": "Corey S. Miller"
                    },
                    {
                        "name": "Hassan Rivaz"
                    },
                    {
                        "name": "Marta Kersten-Oertel"
                    },
                    {
                        "name": "Yiming Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xiao"
                },
                "author": "Yiming Xiao",
                "arxiv_comment": "Accepted to ICCV 2025 Workshop CVAMD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18082v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18082v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22863v1",
                "updated": "2025-07-30T17:38:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    38,
                    49,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:38:49Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    38,
                    49,
                    2,
                    211,
                    0
                ],
                "title": "Formation of over-massive black holes in high-redshift disk galaxies via\n  globular cluster accretion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formation of over-massive black holes in high-redshift disk galaxies via\n  globular cluster accretion"
                },
                "summary": "Recent observations with the James Webb Space Telescope (JWST) have suggested\nthe existence of over-massive black holes (OMBHs) in high-redshift galaxies. In\nthis paper, we propose a new mechanism for the formation of OMBHs, based on the\naccretion of globular clusters (GCs) in compact disk galaxies. We derive the\nconditions under which OMBHs can form, focusing on key parameters such as halo\nmass, redshift, and halo spin parameter. Our results show that at redshift $z =\n10$, a halo with mass $10^{11}~M_{\\odot}$ and a spin parameter of $0.02$ can\nform a black hole of $1.4 \\times 10^{8}~M_{\\odot}$ through GC migration and\naccretion via tidal disruption events (TDEs). The resulting black\nhole-to-stellar mass ratio can reach $\\sim 0.1$, corresponding to the fraction\nof GC mass accreted onto the black hole. This mechanism thus provides a\nplausible explanation for the OMBHs observed by JWST. Furthermore, by combining\nour model with halo mass functions and the distribution of spin parameters, we\nconstruct black hole mass functions that successfully reproduce the number\ndensities of massive BH candidates at $z \\sim 5$ inferred from JWST\nobservations, and UHZ1 and GHZ9 at $z \\sim 10$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations with the James Webb Space Telescope (JWST) have suggested\nthe existence of over-massive black holes (OMBHs) in high-redshift galaxies. In\nthis paper, we propose a new mechanism for the formation of OMBHs, based on the\naccretion of globular clusters (GCs) in compact disk galaxies. We derive the\nconditions under which OMBHs can form, focusing on key parameters such as halo\nmass, redshift, and halo spin parameter. Our results show that at redshift $z =\n10$, a halo with mass $10^{11}~M_{\\odot}$ and a spin parameter of $0.02$ can\nform a black hole of $1.4 \\times 10^{8}~M_{\\odot}$ through GC migration and\naccretion via tidal disruption events (TDEs). The resulting black\nhole-to-stellar mass ratio can reach $\\sim 0.1$, corresponding to the fraction\nof GC mass accreted onto the black hole. This mechanism thus provides a\nplausible explanation for the OMBHs observed by JWST. Furthermore, by combining\nour model with halo mass functions and the distribution of spin parameters, we\nconstruct black hole mass functions that successfully reproduce the number\ndensities of massive BH candidates at $z \\sim 5$ inferred from JWST\nobservations, and UHZ1 and GHZ9 at $z \\sim 10$."
                },
                "authors": [
                    {
                        "name": "Hidenobu Yajima"
                    }
                ],
                "author_detail": {
                    "name": "Hidenobu Yajima"
                },
                "author": "Hidenobu Yajima",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04395v2",
                "updated": "2025-07-30T17:33:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    33,
                    22,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-06T07:35:15Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    7,
                    35,
                    15,
                    6,
                    96,
                    0
                ],
                "title": "Human-Level Competitive Pokémon via Scalable Offline Reinforcement\n  Learning with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Level Competitive Pokémon via Scalable Offline Reinforcement\n  Learning with Transformers"
                },
                "summary": "Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players\nlearn to exploit their opponent based on imperfect information in battles that\ncan last more than one hundred stochastic turns. AI research in CPS has been\nled by heuristic tree search and online self-play, but the game may also create\na platform to study adaptive policies trained offline on large datasets. We\ndevelop a pipeline to reconstruct the first-person perspective of an agent from\nlogs saved from the third-person perspective of a spectator, thereby unlocking\na dataset of real human battles spanning more than a decade that grows larger\nevery day. This dataset enables a black-box approach where we train large\nsequence models to adapt to their opponent based solely on their input\ntrajectory while selecting moves without explicit search of any kind. We study\na progression from imitation learning to offline RL and offline fine-tuning on\nself-play data in the hardcore competitive setting of Pok\\'emon's four oldest\n(and most partially observed) game generations. The resulting agents outperform\na recent LLM Agent approach and a strong heuristic search engine. While playing\nanonymously in online battles against humans, our best agents climb to rankings\ninside the top 10% of active players. All agent checkpoints, training details,\ndatasets, and baselines are available at https://metamon.tech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players\nlearn to exploit their opponent based on imperfect information in battles that\ncan last more than one hundred stochastic turns. AI research in CPS has been\nled by heuristic tree search and online self-play, but the game may also create\na platform to study adaptive policies trained offline on large datasets. We\ndevelop a pipeline to reconstruct the first-person perspective of an agent from\nlogs saved from the third-person perspective of a spectator, thereby unlocking\na dataset of real human battles spanning more than a decade that grows larger\nevery day. This dataset enables a black-box approach where we train large\nsequence models to adapt to their opponent based solely on their input\ntrajectory while selecting moves without explicit search of any kind. We study\na progression from imitation learning to offline RL and offline fine-tuning on\nself-play data in the hardcore competitive setting of Pok\\'emon's four oldest\n(and most partially observed) game generations. The resulting agents outperform\na recent LLM Agent approach and a strong heuristic search engine. While playing\nanonymously in online battles against humans, our best agents climb to rankings\ninside the top 10% of active players. All agent checkpoints, training details,\ndatasets, and baselines are available at https://metamon.tech."
                },
                "authors": [
                    {
                        "name": "Jake Grigsby"
                    },
                    {
                        "name": "Yuqi Xie"
                    },
                    {
                        "name": "Justin Sasek"
                    },
                    {
                        "name": "Steven Zheng"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_comment": "Reinforcement Learning Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22853v1",
                "updated": "2025-07-30T17:24:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    24,
                    5,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    24,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "Repair-R1: Better Test Before Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repair-R1: Better Test Before Repair"
                },
                "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Xiaochen Xie"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14820v2",
                "updated": "2025-07-30T17:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    18,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2024-09-23T08:52:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    52,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "Past Meets Present: Creating Historical Analogy with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past Meets Present: Creating Historical Analogy with Large Language\n  Models"
                },
                "summary": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method."
                },
                "authors": [
                    {
                        "name": "Nianqi Li"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to ACL 2025 (Outstanding Paper Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03295v5",
                "updated": "2025-07-30T17:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    11,
                    26,
                    2,
                    211,
                    0
                ],
                "published": "2024-11-05T17:42:43Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online"
                },
                "summary": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online."
                },
                "authors": [
                    {
                        "name": "Farhana Shahid"
                    },
                    {
                        "name": "Maximilian Dittgen"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "arxiv_doi": "10.1145/3757591",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757591",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Hum.-Comput. Interact. 9, 7, Article 410 (November 2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22847v1",
                "updated": "2025-07-30T17:03:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    3,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:03:59Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    3,
                    59,
                    2,
                    211,
                    0
                ],
                "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology"
                },
                "summary": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems."
                },
                "authors": [
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Pengda Wang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20992v2",
                "updated": "2025-07-30T17:02:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    2,
                    4,
                    2,
                    211,
                    0
                ],
                "published": "2025-03-26T21:11:17Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    21,
                    11,
                    17,
                    2,
                    85,
                    0
                ],
                "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style\n  Transfer"
                },
                "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer."
                },
                "authors": [
                    {
                        "name": "Michael Brown"
                    },
                    {
                        "name": "Sofia Martinez"
                    },
                    {
                        "name": "Priya Singh"
                    }
                ],
                "author_detail": {
                    "name": "Priya Singh"
                },
                "author": "Priya Singh",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19959v2",
                "updated": "2025-07-30T16:46:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    46,
                    12,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-26T13:21:18Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    21,
                    18,
                    0,
                    146,
                    0
                ],
                "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for\n  Large Language Models"
                },
                "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."
                },
                "authors": [
                    {
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "name": "Guoming Ling"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Accepted by ACL'25 main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22827v1",
                "updated": "2025-07-30T16:41:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    41,
                    21,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:41:21Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    41,
                    21,
                    2,
                    211,
                    0
                ],
                "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents"
                },
                "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder."
                },
                "authors": [
                    {
                        "name": "Yilei Jiang"
                    },
                    {
                        "name": "Yaozhi Zheng"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Qunzhong Wang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22825v1",
                "updated": "2025-07-30T16:40:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    40,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:40:46Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    40,
                    46,
                    2,
                    211,
                    0
                ],
                "title": "DepR: Depth Guided Single-view Scene Reconstruction with Instance-level\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DepR: Depth Guided Single-view Scene Reconstruction with Instance-level\n  Diffusion"
                },
                "summary": "We propose DepR, a depth-guided single-view scene reconstruction framework\nthat integrates instance-level diffusion within a compositional paradigm.\nInstead of reconstructing the entire scene holistically, DepR generates\nindividual objects and subsequently composes them into a coherent 3D layout.\nUnlike previous methods that use depth solely for object layout estimation\nduring inference and therefore fail to fully exploit its rich geometric\ninformation, DepR leverages depth throughout both training and inference.\nSpecifically, we introduce depth-guided conditioning to effectively encode\nshape priors into diffusion models. During inference, depth further guides DDIM\nsampling and layout optimization, enhancing alignment between the\nreconstruction and the input image. Despite being trained on limited synthetic\ndata, DepR achieves state-of-the-art performance and demonstrates strong\ngeneralization in single-view scene reconstruction, as shown through\nevaluations on both synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose DepR, a depth-guided single-view scene reconstruction framework\nthat integrates instance-level diffusion within a compositional paradigm.\nInstead of reconstructing the entire scene holistically, DepR generates\nindividual objects and subsequently composes them into a coherent 3D layout.\nUnlike previous methods that use depth solely for object layout estimation\nduring inference and therefore fail to fully exploit its rich geometric\ninformation, DepR leverages depth throughout both training and inference.\nSpecifically, we introduce depth-guided conditioning to effectively encode\nshape priors into diffusion models. During inference, depth further guides DDIM\nsampling and layout optimization, enhancing alignment between the\nreconstruction and the input image. Despite being trained on limited synthetic\ndata, DepR achieves state-of-the-art performance and demonstrates strong\ngeneralization in single-view scene reconstruction, as shown through\nevaluations on both synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Qingcheng Zhao"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Jianwen Xie"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Zhuowen Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuowen Tu"
                },
                "author": "Zhuowen Tu",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22817v1",
                "updated": "2025-07-30T16:32:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    32,
                    47,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:32:47Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    32,
                    47,
                    2,
                    211,
                    0
                ],
                "title": "Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards\n  Generalisable Neural Surrogate Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards\n  Generalisable Neural Surrogate Models"
                },
                "summary": "Abdominal aortic aneurysms (AAAs) are pathologic dilatations of the abdominal\naorta posing a high fatality risk upon rupture. Studying AAA progression and\nrupture risk often involves in-silico blood flow modelling with computational\nfluid dynamics (CFD) and extraction of hemodynamic factors like time-averaged\nwall shear stress (TAWSS) or oscillatory shear index (OSI). However, CFD\nsimulations are known to be computationally demanding. Hence, in recent years,\ngeometric deep learning methods, operating directly on 3D shapes, have been\nproposed as compelling surrogates, estimating hemodynamic parameters in just a\nfew seconds. In this work, we propose a geometric deep learning approach to\nestimating hemodynamics in AAA patients, and study its generalisability to\ncommon factors of real-world variation. We propose an E(3)-equivariant deep\nlearning model utilising novel robust geometrical descriptors and projective\ngeometric algebra. Our model is trained to estimate transient WSS using a\ndataset of CT scans of 100 AAA patients, from which lumen geometries are\nextracted and reference CFD simulations with varying boundary conditions are\nobtained. Results show that the model generalizes well within the distribution,\nas well as to the external test set. Moreover, the model can accurately\nestimate hemodynamics across geometry remodelling and changes in boundary\nconditions. Furthermore, we find that a trained model can be applied to\ndifferent artery tree topologies, where new and unseen branches are added\nduring inference. Finally, we find that the model is to a large extent agnostic\nto mesh resolution. These results show the accuracy and generalisation of the\nproposed model, and highlight its potential to contribute to hemodynamic\nparameter estimation in clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abdominal aortic aneurysms (AAAs) are pathologic dilatations of the abdominal\naorta posing a high fatality risk upon rupture. Studying AAA progression and\nrupture risk often involves in-silico blood flow modelling with computational\nfluid dynamics (CFD) and extraction of hemodynamic factors like time-averaged\nwall shear stress (TAWSS) or oscillatory shear index (OSI). However, CFD\nsimulations are known to be computationally demanding. Hence, in recent years,\ngeometric deep learning methods, operating directly on 3D shapes, have been\nproposed as compelling surrogates, estimating hemodynamic parameters in just a\nfew seconds. In this work, we propose a geometric deep learning approach to\nestimating hemodynamics in AAA patients, and study its generalisability to\ncommon factors of real-world variation. We propose an E(3)-equivariant deep\nlearning model utilising novel robust geometrical descriptors and projective\ngeometric algebra. Our model is trained to estimate transient WSS using a\ndataset of CT scans of 100 AAA patients, from which lumen geometries are\nextracted and reference CFD simulations with varying boundary conditions are\nobtained. Results show that the model generalizes well within the distribution,\nas well as to the external test set. Moreover, the model can accurately\nestimate hemodynamics across geometry remodelling and changes in boundary\nconditions. Furthermore, we find that a trained model can be applied to\ndifferent artery tree topologies, where new and unseen branches are added\nduring inference. Finally, we find that the model is to a large extent agnostic\nto mesh resolution. These results show the accuracy and generalisation of the\nproposed model, and highlight its potential to contribute to hemodynamic\nparameter estimation in clinical practice."
                },
                "authors": [
                    {
                        "name": "Patryk Rygiel"
                    },
                    {
                        "name": "Julian Suk"
                    },
                    {
                        "name": "Christoph Brune"
                    },
                    {
                        "name": "Kak Khee Yeung"
                    },
                    {
                        "name": "Jelmer M. Wolterink"
                    }
                ],
                "author_detail": {
                    "name": "Jelmer M. Wolterink"
                },
                "author": "Jelmer M. Wolterink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22816v1",
                "updated": "2025-07-30T16:32:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    32,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:32:33Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    32,
                    33,
                    2,
                    211,
                    0
                ],
                "title": "Kan Approximations of the Persistent Homology Transform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kan Approximations of the Persistent Homology Transform"
                },
                "summary": "The persistent homology transform (PHT) of a subset $M \\subset \\mathbb{R}^d$\nis a map $\\text{PHT}(M):\\mathbb{S}^{d-1} \\to \\mathbf{Dgm}$ from the unit sphere\nto the space of persistence diagrams. This map assigns to each direction $v\\in\n\\mathbb{S}^{d-1}$ the persistent homology of the filtration of $M$ in direction\n$v$. In practice, one can only sample the map $\\text{PHT}(M)$ at a finite set\nof directions $A \\subset \\mathbb{S}^{d-1}$. This suggests two natural\nquestions: (1) Can we interpolate the PHT from this finite sample of directions\nto the entire sphere? If so, (2) can we prove that the resulting interpolation\nis close to the true PHT? In this paper we show that if we can sample the PHT\nat the module level, where we have information about how homology from each\ndirection interacts, a ready-made interpolation theory due to Bubenik, de\nSilva, and Nanda using Kan extensions can answer both of these questions in the\naffirmative. A close inspection of those techniques shows that we can infer the\nPHT from a finite sample of heights from each direction as well. Our paper\npresents the first known results for approximating the PHT from finite\ndirectional and scalar data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The persistent homology transform (PHT) of a subset $M \\subset \\mathbb{R}^d$\nis a map $\\text{PHT}(M):\\mathbb{S}^{d-1} \\to \\mathbf{Dgm}$ from the unit sphere\nto the space of persistence diagrams. This map assigns to each direction $v\\in\n\\mathbb{S}^{d-1}$ the persistent homology of the filtration of $M$ in direction\n$v$. In practice, one can only sample the map $\\text{PHT}(M)$ at a finite set\nof directions $A \\subset \\mathbb{S}^{d-1}$. This suggests two natural\nquestions: (1) Can we interpolate the PHT from this finite sample of directions\nto the entire sphere? If so, (2) can we prove that the resulting interpolation\nis close to the true PHT? In this paper we show that if we can sample the PHT\nat the module level, where we have information about how homology from each\ndirection interacts, a ready-made interpolation theory due to Bubenik, de\nSilva, and Nanda using Kan extensions can answer both of these questions in the\naffirmative. A close inspection of those techniques shows that we can infer the\nPHT from a finite sample of heights from each direction as well. Our paper\npresents the first known results for approximating the PHT from finite\ndirectional and scalar data."
                },
                "authors": [
                    {
                        "name": "Shreya Arya"
                    },
                    {
                        "name": "Justin Curry"
                    }
                ],
                "author_detail": {
                    "name": "Justin Curry"
                },
                "author": "Justin Curry",
                "arxiv_comment": "22 pages, 4 figures. Dedicated to our beloved graduate and post-doc\n  mentor, Sayan Mukherjee (1971--2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22811v1",
                "updated": "2025-07-30T16:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    29,
                    47,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:29:47Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    29,
                    47,
                    2,
                    211,
                    0
                ],
                "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph"
                },
                "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM."
                },
                "authors": [
                    {
                        "name": "Debayan Banerjee"
                    },
                    {
                        "name": "Tilahun Abedissa Taffa"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18044v2",
                "updated": "2025-07-30T16:28:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    28,
                    54,
                    2,
                    211,
                    0
                ],
                "published": "2024-07-25T13:47:01Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    13,
                    47,
                    1,
                    3,
                    207,
                    0
                ],
                "title": "The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented\n  Generation for Healthcare QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented\n  Generation for Healthcare QA"
                },
                "summary": "Deploying Large Language Models (LLMs) for healthcare question answering\nrequires robust methods to ensure accuracy and reliability. This work\nintroduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for\nenhancing Retrieval-Augmented Generation (RAG) systems in healthcare\nquestion-answering by pre-aligning user queries with a database of curated,\nanswerable questions derived from healthcare content. A key component of QB-RAG\nis an LLM-based filtering mechanism that ensures that only relevant and\nanswerable questions are included in the database, enabling reliable reference\nquery generation at scale. We provide theoretical motivation for QB-RAG,\nconduct a comparative analysis of existing retrieval enhancement techniques,\nand introduce a generalizable, comprehensive evaluation framework that assesses\nboth the retrieval effectiveness and the quality of the generated response\nbased on faithfulness, relevance, and adherence to the guideline. Our empirical\nevaluation on a healthcare data set demonstrates the superior performance of\nQB-RAG compared to existing retrieval methods, highlighting its practical value\nin building trustworthy digital health applications for health\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) for healthcare question answering\nrequires robust methods to ensure accuracy and reliability. This work\nintroduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for\nenhancing Retrieval-Augmented Generation (RAG) systems in healthcare\nquestion-answering by pre-aligning user queries with a database of curated,\nanswerable questions derived from healthcare content. A key component of QB-RAG\nis an LLM-based filtering mechanism that ensures that only relevant and\nanswerable questions are included in the database, enabling reliable reference\nquery generation at scale. We provide theoretical motivation for QB-RAG,\nconduct a comparative analysis of existing retrieval enhancement techniques,\nand introduce a generalizable, comprehensive evaluation framework that assesses\nboth the retrieval effectiveness and the quality of the generated response\nbased on faithfulness, relevance, and adherence to the guideline. Our empirical\nevaluation on a healthcare data set demonstrates the superior performance of\nQB-RAG compared to existing retrieval methods, highlighting its practical value\nin building trustworthy digital health applications for health\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Jonathan Amar"
                    },
                    {
                        "name": "Jong Ha Lee"
                    },
                    {
                        "name": "Bhawesh Kumar"
                    },
                    {
                        "name": "Yugang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yugang Jia"
                },
                "author": "Yugang Jia",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22805v1",
                "updated": "2025-07-30T16:15:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    15,
                    22,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:15:22Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    15,
                    22,
                    2,
                    211,
                    0
                ],
                "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention"
                },
                "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA."
                },
                "authors": [
                    {
                        "name": "Yuqi Pang"
                    },
                    {
                        "name": "Bowen Yang"
                    },
                    {
                        "name": "Yun Cao"
                    },
                    {
                        "name": "Fan Rong"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Chen He"
                    }
                ],
                "author_detail": {
                    "name": "Chen He"
                },
                "author": "Chen He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08450v2",
                "updated": "2025-07-30T16:11:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    11,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-13T11:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Shinya Kouda"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02744v3",
                "updated": "2025-07-31T14:02:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    2,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2024-10-03T17:55:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Neutral Residues: Revisiting Adapters for Model Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral Residues: Revisiting Adapters for Model Extension"
                },
                "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."
                },
                "authors": [
                    {
                        "name": "Franck Signe Talla"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Hervé Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Jégou"
                },
                "author": "Hervé Jégou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22800v1",
                "updated": "2025-07-30T16:03:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    3,
                    21,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:03:21Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    3,
                    21,
                    2,
                    211,
                    0
                ],
                "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree\n  Search Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree\n  Search Approach"
                },
                "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Rui Ren"
                },
                "author": "Rui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07052v4",
                "updated": "2025-07-30T16:00:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    0,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2023-11-13T03:36:18Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    3,
                    36,
                    18,
                    0,
                    317,
                    0
                ],
                "title": "Towards the Law of Capacity Gap in Distilling Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Law of Capacity Gap in Distilling Language Models"
                },
                "summary": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Dawei Song"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Hu"
                },
                "author": "Yan Hu",
                "arxiv_comment": "32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22789v2",
                "updated": "2025-07-31T02:18:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    2,
                    18,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T15:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    55,
                    8,
                    2,
                    211,
                    0
                ],
                "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Core: A Simple, Scalable and Balanced RLHF Trainer"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels."
                },
                "authors": [
                    {
                        "name": "Junyu Wu"
                    },
                    {
                        "name": "Weiming Chang"
                    },
                    {
                        "name": "Xiaotao Liu"
                    },
                    {
                        "name": "Guanyou He"
                    },
                    {
                        "name": "Haoqiang Hong"
                    },
                    {
                        "name": "Boqi Liu"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Ting Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Yao"
                },
                "author": "Ting Yao",
                "arxiv_comment": "I haven't received company approval yet, and I uploaded it by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19073v2",
                "updated": "2025-07-30T15:54:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    54,
                    38,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-23T19:44:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    19,
                    44,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral\n  Reasoning of LLMs through Hate Speech Multi-hop Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral\n  Reasoning of LLMs through Hate Speech Multi-hop Explanations"
                },
                "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning."
                },
                "authors": [
                    {
                        "name": "Jackson Trager"
                    },
                    {
                        "name": "Diego Alves"
                    },
                    {
                        "name": "Matteo Guida"
                    },
                    {
                        "name": "Mikel K. Ngueajio"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "Flor Plaza-del-Arco"
                    },
                    {
                        "name": "Yalda Daryanai"
                    },
                    {
                        "name": "Farzan Karimi-Malekabadi"
                    },
                    {
                        "name": "Francielle Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Francielle Vargas"
                },
                "author": "Francielle Vargas",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22050v2",
                "updated": "2025-07-30T15:51:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    51,
                    29,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T17:55:23Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    17,
                    55,
                    23,
                    1,
                    210,
                    0
                ],
                "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router"
                },
                "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve."
                },
                "authors": [
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Xujiang Zhao"
                    },
                    {
                        "name": "Yanchi Liu"
                    },
                    {
                        "name": "Wenchao Yu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Cheng"
                },
                "author": "Wei Cheng",
                "arxiv_comment": "22 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22772v1",
                "updated": "2025-07-30T15:35:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    35,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:35:51Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    35,
                    51,
                    2,
                    211,
                    0
                ],
                "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Evaluation of Concept Drift in ML-Based Android Malware\n  Detection"
                },
                "summary": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation."
                },
                "authors": [
                    {
                        "name": "Ahmed Sabbah"
                    },
                    {
                        "name": "Radi Jarrar"
                    },
                    {
                        "name": "Samer Zein"
                    },
                    {
                        "name": "David Mohaisen"
                    }
                ],
                "author_detail": {
                    "name": "David Mohaisen"
                },
                "author": "David Mohaisen",
                "arxiv_comment": "18 pages, 12 tables, 14 figures, paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22758v1",
                "updated": "2025-07-30T15:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    19,
                    38,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:19:38Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    19,
                    38,
                    2,
                    211,
                    0
                ],
                "title": "MASCA: LLM based-Multi Agents System for Credit Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASCA: LLM based-Multi Agents System for Credit Assessment"
                },
                "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring."
                },
                "authors": [
                    {
                        "name": "Gautam Jajoo"
                    },
                    {
                        "name": "Pranjal A Chitale"
                    },
                    {
                        "name": "Saksham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Saksham Agarwal"
                },
                "author": "Saksham Agarwal",
                "arxiv_comment": "Accepted at ACL REALM Workshop. Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22753v1",
                "updated": "2025-07-30T15:12:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    12,
                    12,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:12:12Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    12,
                    12,
                    2,
                    211,
                    0
                ],
                "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opportunities and Challenges of LLMs in Education: An NLP Perspective"
                },
                "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future."
                },
                "authors": [
                    {
                        "name": "Sowmya Vajjala"
                    },
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22752v1",
                "updated": "2025-07-30T15:10:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    10,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    10,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset"
                },
                "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering."
                },
                "authors": [
                    {
                        "name": "Jindřich Libovický"
                    },
                    {
                        "name": "Jindřich Helcl"
                    },
                    {
                        "name": "Andrei Manea"
                    },
                    {
                        "name": "Gianluca Vico"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Vico"
                },
                "author": "Gianluca Vico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22748v1",
                "updated": "2025-07-30T15:05:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:05:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index"
                },
                "summary": "We introduce the Generative AI Susceptibility Index (GAISI), a task-based\nmeasure of UK job exposure to large language models (LLMs), such as ChatGPT.\nGAISI is derived from probabilistic task ratings by LLMs and linked to\nworker-reported task data from the Skills and Employment Surveys. It reflects\nthe share of job activities where an LLM or LLM-powered system can reduce task\ncompletion time by at least 25 per cent beyond existing productivity tools. The\nindex demonstrates high reliability, strong alignment with AI capabilities, and\nsuperior predictive power compared to existing exposure measures. By 2023-24,\nnearly all UK jobs exhibited some exposure, yet only a minority were heavily\naffected. Aggregate exposure has risen since 2017, primarily due to\noccupational shifts rather than changes in task profiles. The price premium for\nAI-exposed tasks declined relative to 2017, measuring approximately 11 per cent\nlower in 2023-24. Job postings in high-exposure roles also fell by 6.5 per cent\nfollowing the release of ChatGPT. GAISI offers a robust framework for assessing\ngenerative AI's impact on work, providing early evidence that displacement\neffects may already outweigh productivity gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Generative AI Susceptibility Index (GAISI), a task-based\nmeasure of UK job exposure to large language models (LLMs), such as ChatGPT.\nGAISI is derived from probabilistic task ratings by LLMs and linked to\nworker-reported task data from the Skills and Employment Surveys. It reflects\nthe share of job activities where an LLM or LLM-powered system can reduce task\ncompletion time by at least 25 per cent beyond existing productivity tools. The\nindex demonstrates high reliability, strong alignment with AI capabilities, and\nsuperior predictive power compared to existing exposure measures. By 2023-24,\nnearly all UK jobs exhibited some exposure, yet only a minority were heavily\naffected. Aggregate exposure has risen since 2017, primarily due to\noccupational shifts rather than changes in task profiles. The price premium for\nAI-exposed tasks declined relative to 2017, measuring approximately 11 per cent\nlower in 2023-24. Job postings in high-exposure roles also fell by 6.5 per cent\nfollowing the release of ChatGPT. GAISI offers a robust framework for assessing\ngenerative AI's impact on work, providing early evidence that displacement\neffects may already outweigh productivity gains."
                },
                "authors": [
                    {
                        "name": "Golo Henseke"
                    },
                    {
                        "name": "Rhys Davies"
                    },
                    {
                        "name": "Alan Felstead"
                    },
                    {
                        "name": "Duncan Gallie"
                    },
                    {
                        "name": "Francis Green"
                    },
                    {
                        "name": "Ying Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhou"
                },
                "author": "Ying Zhou",
                "arxiv_comment": "52 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22747v1",
                "updated": "2025-07-30T15:03:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    41,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:03:41Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    41,
                    2,
                    211,
                    0
                ],
                "title": "A quantum experiment with joint exogeneity violation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantum experiment with joint exogeneity violation"
                },
                "summary": "In randomized experiments, the assumption of potential outcomes is usually\naccompanied by the \\emph{joint exogeneity} assumption. Although joint\nexogeneity has faced criticism as a counterfactual assumption since its\nproposal, no evidence has yet demonstrated its violation in randomized\nexperiments. In this paper, we reveal such a violation in a quantum experiment,\nthereby falsifying this assumption, at least in regimes where classical physics\ncannot provide a complete description. We further discuss its implications for\npotential outcome modelling, from both practial and philosophical perspectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In randomized experiments, the assumption of potential outcomes is usually\naccompanied by the \\emph{joint exogeneity} assumption. Although joint\nexogeneity has faced criticism as a counterfactual assumption since its\nproposal, no evidence has yet demonstrated its violation in randomized\nexperiments. In this paper, we reveal such a violation in a quantum experiment,\nthereby falsifying this assumption, at least in regimes where classical physics\ncannot provide a complete description. We further discuss its implications for\npotential outcome modelling, from both practial and philosophical perspectives."
                },
                "authors": [
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Xingjian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingjian Zhang"
                },
                "author": "Xingjian Zhang",
                "arxiv_comment": "This is a preliminary draft in accordance to the presentations of YW\n  at the 2025 Pacific Causal Inference Conference and 2025 Chinese Causal\n  Inference Conference. This draft is being circulated to collect further\n  feedback. A formal version will be publicly available in due course",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13820v3",
                "updated": "2025-07-30T14:58:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    58,
                    42,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-19T15:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scoring Verifiers: Evaluating Synthetic Verification for Code and\n  Reasoning"
                },
                "summary": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy."
                },
                "authors": [
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22729v1",
                "updated": "2025-07-30T14:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs."
                },
                "authors": [
                    {
                        "name": "Benedikt Roth"
                    },
                    {
                        "name": "Stephan Rappensperger"
                    },
                    {
                        "name": "Tianming Qiu"
                    },
                    {
                        "name": "Hamza Imamović"
                    },
                    {
                        "name": "Julian Wörmann"
                    },
                    {
                        "name": "Hao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shen"
                },
                "author": "Hao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22720v1",
                "updated": "2025-07-30T14:39:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    39,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:39:51Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    39,
                    51,
                    2,
                    211,
                    0
                ],
                "title": "Investigating Hallucination in Conversations for Low Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Hallucination in Conversations for Low Resource Languages"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi."
                },
                "authors": [
                    {
                        "name": "Amit Das"
                    },
                    {
                        "name": "Md. Najib Hasan"
                    },
                    {
                        "name": "Souvika Sarkar"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Fatemeh Jamshidi"
                    },
                    {
                        "name": "Tathagata Bhattacharya"
                    },
                    {
                        "name": "Nilanjana Raychawdhury"
                    },
                    {
                        "name": "Dongji Feng"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13932v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13932v3",
                "updated": "2025-07-30T14:37:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    37,
                    56,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-14T19:31:21Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    19,
                    31,
                    21,
                    0,
                    104,
                    0
                ],
                "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through\n  Saliency-Aware Partial Retraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through\n  Saliency-Aware Partial Retraining"
                },
                "summary": "The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference.\nServing these models to each user requires substantial energy and water for\ncooling. Model compression techniques like quantization can shrink large\nlanguage models and make them more resource efficient at the cost of potential\nperformance degradation. Quantization methods compress model size through\nreplacing their high-precision parameters by quantized values of lower\nprecision. Among existing methods, the ApiQ method achieves superior accuracy\npreservation at minimal memory and time overhead. We investigate two ideas to\nextend performance in ultra-low-bit quantization beyond ApiQ's level. First, we\nlook into combining existing quantization-aware training techniques with ApiQ's\npartial training. We show that this does not outperform the baseline ApiQ\nmethod with limited training data and frozen weights. This leads to two key\ninsights: (1) The substantial representational capacity that is gained through\nfull retraining is unlikely to be feasible through partial training. (2) This\ngain may depend on using a large and diverse dataset in quantization-aware\ntraining. Second, through a novel approach informed by the two insights, we\npropose an ultra-low-bit quantization method that builds upon ApiQ and extends\nits performance without the need for full retraining. This publicly available\nmethod relies on a saliency-aware regularization term that prioritizes\npreserving the most impactful parameters during quantization. Our experiments\non LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's\naccuracy degradation by 10.85% and 7.54% respectively. A Python implementation\nof the proposed quantization method is publicly available on GitHub\nhttps://github.com/TokuyuSou/ULB-SAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models has raised environmental and\neconomic concerns about their intensity of resource usage during inference.\nServing these models to each user requires substantial energy and water for\ncooling. Model compression techniques like quantization can shrink large\nlanguage models and make them more resource efficient at the cost of potential\nperformance degradation. Quantization methods compress model size through\nreplacing their high-precision parameters by quantized values of lower\nprecision. Among existing methods, the ApiQ method achieves superior accuracy\npreservation at minimal memory and time overhead. We investigate two ideas to\nextend performance in ultra-low-bit quantization beyond ApiQ's level. First, we\nlook into combining existing quantization-aware training techniques with ApiQ's\npartial training. We show that this does not outperform the baseline ApiQ\nmethod with limited training data and frozen weights. This leads to two key\ninsights: (1) The substantial representational capacity that is gained through\nfull retraining is unlikely to be feasible through partial training. (2) This\ngain may depend on using a large and diverse dataset in quantization-aware\ntraining. Second, through a novel approach informed by the two insights, we\npropose an ultra-low-bit quantization method that builds upon ApiQ and extends\nits performance without the need for full retraining. This publicly available\nmethod relies on a saliency-aware regularization term that prioritizes\npreserving the most impactful parameters during quantization. Our experiments\non LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's\naccuracy degradation by 10.85% and 7.54% respectively. A Python implementation\nof the proposed quantization method is publicly available on GitHub\nhttps://github.com/TokuyuSou/ULB-SAPR."
                },
                "authors": [
                    {
                        "name": "Deyu Cao"
                    },
                    {
                        "name": "Samin Aref"
                    }
                ],
                "author_detail": {
                    "name": "Samin Aref"
                },
                "author": "Samin Aref",
                "arxiv_doi": "10.1007/978-3-032-00891-6_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-00891-6_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13932v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13932v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is a post-peer-review accepted manuscript from the proceedings\n  of the 22nd International Conference on Modeling Decisions for Artificial\n  Intelligence (MDAI'25). The publisher authenticated version and full citation\n  details are available on Springer's website (LNAI 15957).\n  https://doi.org/10.1007/978-3-032-00891-6_28",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07, 68T09, 68U15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09372v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09372v2",
                "updated": "2025-07-30T14:36:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    36,
                    6,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-12T18:37:18Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    18,
                    37,
                    18,
                    5,
                    193,
                    0
                ],
                "title": "Controllable joint noise reduction and hearing loss compensation using a\n  differentiable auditory model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable joint noise reduction and hearing loss compensation using a\n  differentiable auditory model"
                },
                "summary": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based hearing loss compensation (HLC) seeks to enhance speech\nintelligibility and quality for hearing impaired listeners using neural\nnetworks. One major challenge of HLC is the lack of a ground-truth target.\nRecent works have used neural networks to emulate non-differentiable auditory\nperipheral models in closed-loop frameworks, but this approach lacks\nflexibility. Alternatively, differentiable auditory models allow direct\noptimization, yet previous studies focused on individual listener profiles, or\njoint noise reduction (NR) and HLC without balancing each task. This work\nformulates NR and HLC as a multi-task learning problem, training a system to\nsimultaneously predict denoised and compensated signals from noisy speech and\naudiograms using a differentiable auditory model. Results show the system\nachieves similar objective metric performance to systems trained for each task\nseparately, while being able to adjust the balance between NR and HLC during\ninference."
                },
                "authors": [
                    {
                        "name": "Philippe Gonzalez"
                    },
                    {
                        "name": "Torsten Dau"
                    },
                    {
                        "name": "Tobias May"
                    }
                ],
                "author_detail": {
                    "name": "Tobias May"
                },
                "author": "Tobias May",
                "arxiv_comment": "Accepted to Clarity 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09372v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09372v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22716v1",
                "updated": "2025-07-30T14:29:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:29:44Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs"
                },
                "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Victor Gutierrez Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22711v1",
                "updated": "2025-07-30T14:22:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    22,
                    42,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:22:42Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    22,
                    42,
                    2,
                    211,
                    0
                ],
                "title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness"
                },
                "summary": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere."
                },
                "authors": [
                    {
                        "name": "Hong-Jun Yoon"
                    },
                    {
                        "name": "Mariam Kiran"
                    },
                    {
                        "name": "Danial Ebling"
                    },
                    {
                        "name": "Joe Breen"
                    }
                ],
                "author_detail": {
                    "name": "Joe Breen"
                },
                "author": "Joe Breen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22702v1",
                "updated": "2025-07-30T14:10:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    58,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:10:58Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    58,
                    2,
                    211,
                    0
                ],
                "title": "Ecoscape: Fault Tolerance Benchmark for Adaptive Remediation Strategies\n  in Real-Time Edge ML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecoscape: Fault Tolerance Benchmark for Adaptive Remediation Strategies\n  in Real-Time Edge ML"
                },
                "summary": "Edge computing offers significant advantages for realtime data processing\ntasks, such as object recognition, by reducing network latency and bandwidth\nusage. However, edge environments are susceptible to various types of fault. A\nremediator is an automated software component designed to adjust the\nconfiguration parameters of a software service dynamically. Its primary\nfunction is to maintain the services operational state within predefined\nService Level Objectives by applying corrective actions in response to\ndeviations from these objectives. Remediators can be implemented based on the\nKubernetes container orchestration tool by implementing remediation strategies\nsuch as rescheduling or adjusting application parameters. However, currently,\nthere is no method to compare these remediation strategies fairly. This paper\nintroduces Ecoscape, a comprehensive benchmark designed to evaluate the\nperformance of remediation strategies in fault-prone environments. Using Chaos\nEngineering techniques, Ecoscape simulates realistic fault scenarios and\nprovides a quantifiable score to assess the efficacy of different remediation\napproaches. In addition, it is configurable to support domain-specific Service\nLevel Objectives. We demonstrate the capabilities of Ecoscape in edge machine\nlearning inference, offering a clear framework to optimize fault tolerance in\nthese systems without needing a physical edge testbed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing offers significant advantages for realtime data processing\ntasks, such as object recognition, by reducing network latency and bandwidth\nusage. However, edge environments are susceptible to various types of fault. A\nremediator is an automated software component designed to adjust the\nconfiguration parameters of a software service dynamically. Its primary\nfunction is to maintain the services operational state within predefined\nService Level Objectives by applying corrective actions in response to\ndeviations from these objectives. Remediators can be implemented based on the\nKubernetes container orchestration tool by implementing remediation strategies\nsuch as rescheduling or adjusting application parameters. However, currently,\nthere is no method to compare these remediation strategies fairly. This paper\nintroduces Ecoscape, a comprehensive benchmark designed to evaluate the\nperformance of remediation strategies in fault-prone environments. Using Chaos\nEngineering techniques, Ecoscape simulates realistic fault scenarios and\nprovides a quantifiable score to assess the efficacy of different remediation\napproaches. In addition, it is configurable to support domain-specific Service\nLevel Objectives. We demonstrate the capabilities of Ecoscape in edge machine\nlearning inference, offering a clear framework to optimize fault tolerance in\nthese systems without needing a physical edge testbed."
                },
                "authors": [
                    {
                        "name": "Hendrik Reiter"
                    },
                    {
                        "name": "Ahmad Rzgar Hamid"
                    },
                    {
                        "name": "Florian Schlösser"
                    },
                    {
                        "name": "Mikkel Baun Kjærgaard"
                    },
                    {
                        "name": "Wilhelm Hasselbring"
                    }
                ],
                "author_detail": {
                    "name": "Wilhelm Hasselbring"
                },
                "author": "Wilhelm Hasselbring",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07501v2",
                "updated": "2025-07-30T14:06:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    6,
                    57,
                    2,
                    211,
                    0
                ],
                "published": "2024-10-10T00:33:25Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    0,
                    33,
                    25,
                    3,
                    284,
                    0
                ],
                "title": "Inferring biological processes with intrinsic noise from cross-sectional\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring biological processes with intrinsic noise from cross-sectional\n  data"
                },
                "summary": "Inferring dynamical models from data continues to be a significant challenge\nin computational biology, especially given the stochastic nature of many\nbiological processes. We explore a common scenario in omics, where\nstatistically independent cross-sectional samples are available at a few time\npoints, and the goal is to infer the underlying diffusion process that\ngenerated the data. Existing inference approaches often simplify or ignore\nnoise intrinsic to the system, compromising accuracy for the sake of\noptimization ease. We circumvent this compromise by inferring the phase-space\nprobability flow that shares the same time-dependent marginal distributions as\nthe underlying stochastic process. Our approach, probability flow inference\n(PFI), disentangles force from intrinsic stochasticity while retaining the\nalgorithmic ease of ODE inference. Analytically, we prove that for\nOrnstein-Uhlenbeck processes the regularized PFI formalism yields a unique\nsolution in the limit of well-sampled distributions. In practical applications,\nwe show that PFI enables accurate parameter and force estimation in\nhigh-dimensional stochastic reaction networks, and that it allows inference of\ncell differentiation dynamics with molecular noise, outperforming\nstate-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring dynamical models from data continues to be a significant challenge\nin computational biology, especially given the stochastic nature of many\nbiological processes. We explore a common scenario in omics, where\nstatistically independent cross-sectional samples are available at a few time\npoints, and the goal is to infer the underlying diffusion process that\ngenerated the data. Existing inference approaches often simplify or ignore\nnoise intrinsic to the system, compromising accuracy for the sake of\noptimization ease. We circumvent this compromise by inferring the phase-space\nprobability flow that shares the same time-dependent marginal distributions as\nthe underlying stochastic process. Our approach, probability flow inference\n(PFI), disentangles force from intrinsic stochasticity while retaining the\nalgorithmic ease of ODE inference. Analytically, we prove that for\nOrnstein-Uhlenbeck processes the regularized PFI formalism yields a unique\nsolution in the limit of well-sampled distributions. In practical applications,\nwe show that PFI enables accurate parameter and force estimation in\nhigh-dimensional stochastic reaction networks, and that it allows inference of\ncell differentiation dynamics with molecular noise, outperforming\nstate-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Suryanarayana Maddu"
                    },
                    {
                        "name": "Victor Chardès"
                    },
                    {
                        "name": "Michael. J. Shelley"
                    }
                ],
                "author_detail": {
                    "name": "Michael. J. Shelley"
                },
                "author": "Michael. J. Shelley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14136v2",
                "updated": "2025-07-30T13:53:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    53,
                    32,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-20T09:39:54Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    9,
                    39,
                    54,
                    1,
                    140,
                    0
                ],
                "title": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model\n  Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Mixtures of Experts: Essentially Free Test-Time Training via Model\n  Merging"
                },
                "summary": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of expert (MoE) models are a promising approach to increasing model\ncapacity without increasing inference cost, and are core components of many\nstate-of-the-art language models. However, current MoE models typically use\nonly few experts due to prohibitive training and inference cost. We propose\nTest-Time Model Merging (TTMM) which scales the MoE paradigm to an order of\nmagnitude more experts and uses model merging to avoid almost any test-time\noverhead. We show that TTMM is an approximation of test-time training (TTT),\nwhich fine-tunes an expert model for each prediction task, i.e., prompt. TTT\nhas recently been shown to significantly improve language models, but is\ncomputationally expensive. We find that performance of TTMM improves with more\nexperts and approaches the performance of TTT. Moreover, we find that with a 1B\nparameter base model, TTMM is more than 100x faster than TTT at test-time by\namortizing the cost of TTT at train-time. Thus, TTMM offers a promising\ncost-effective approach to scale test-time training."
                },
                "authors": [
                    {
                        "name": "Ryo Bertolissi"
                    },
                    {
                        "name": "Jonas Hübotter"
                    },
                    {
                        "name": "Ido Hakimi"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08637v2",
                "updated": "2025-07-30T13:30:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    30,
                    52,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-11T15:39:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    39,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Simple low-dimensional computations explain variability in neuronal\n  activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple low-dimensional computations explain variability in neuronal\n  activity"
                },
                "summary": "Our understanding of neural computation is founded on the assumption that\nneurons fire in response to a linear summation of inputs. Yet experiments\ndemonstrate that some neurons are capable of complex computations that require\ninteractions between inputs. Here we show, across multiple brain regions and\nspecies, that simple computations (without interactions between inputs) explain\nmost of the variability in neuronal activity. Neurons are quantitatively\ndescribed by models that capture the measured dependence on each input\nindividually, but assume nothing about combinations of inputs. These minimal\nmodels, which are equivalent to binary artificial neurons, predict complex\nhigher-order dependencies and recover known features of synaptic connectivity.\nThe inferred computations are low-dimensional, indicating a highly redundant\nneural code that is necessary for error correction. These results suggest that,\ndespite intricate biophysical details, most neurons perform simple computations\ntypically reserved for artificial models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our understanding of neural computation is founded on the assumption that\nneurons fire in response to a linear summation of inputs. Yet experiments\ndemonstrate that some neurons are capable of complex computations that require\ninteractions between inputs. Here we show, across multiple brain regions and\nspecies, that simple computations (without interactions between inputs) explain\nmost of the variability in neuronal activity. Neurons are quantitatively\ndescribed by models that capture the measured dependence on each input\nindividually, but assume nothing about combinations of inputs. These minimal\nmodels, which are equivalent to binary artificial neurons, predict complex\nhigher-order dependencies and recover known features of synaptic connectivity.\nThe inferred computations are low-dimensional, indicating a highly redundant\nneural code that is necessary for error correction. These results suggest that,\ndespite intricate biophysical details, most neurons perform simple computations\ntypically reserved for artificial models."
                },
                "authors": [
                    {
                        "name": "Christopher W. Lynn"
                    }
                ],
                "author_detail": {
                    "name": "Christopher W. Lynn"
                },
                "author": "Christopher W. Lynn",
                "arxiv_comment": "34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01053v2",
                "updated": "2025-07-30T13:27:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    27,
                    0,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-27T16:24:17Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    24,
                    17,
                    4,
                    178,
                    0
                ],
                "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis"
                },
                "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight."
                },
                "authors": [
                    {
                        "name": "Rafi Al Attrach"
                    },
                    {
                        "name": "Pedro Moreira"
                    },
                    {
                        "name": "Rajna Fani"
                    },
                    {
                        "name": "Renato Umeton"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    }
                ],
                "author_detail": {
                    "name": "Leo Anthony Celi"
                },
                "author": "Leo Anthony Celi",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.3; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12474v2",
                "updated": "2025-07-30T13:18:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    18,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-18T15:52:24Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    52,
                    24,
                    6,
                    138,
                    0
                ],
                "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization"
                },
                "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration."
                },
                "authors": [
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Gengyao Li"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xinnian Liang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "20 pages, 17 figures and 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22659v1",
                "updated": "2025-07-30T13:17:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    17,
                    16,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T13:17:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    17,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "A Systematic Literature Review on Detecting Software Vulnerabilities\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Detecting Software Vulnerabilities\n  with Large Language Models"
                },
                "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies."
                },
                "authors": [
                    {
                        "name": "Sabrina Kaniewski"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Markus Enzweiler"
                    },
                    {
                        "name": "Michael Menth"
                    },
                    {
                        "name": "Tobias Heer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Heer"
                },
                "author": "Tobias Heer",
                "arxiv_comment": "36 pages + 17 pages references, 6 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02612v2",
                "updated": "2025-07-30T13:15:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    15,
                    26,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-03T14:12:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation"
                },
                "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive (VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, naive\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize minor\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive (VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, naive\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize minor\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage."
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Eunseo Koh"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://jiwoogit.github.io/ARBooth/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16440v2",
                "updated": "2025-07-30T13:06:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    6,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2024-08-29T11:05:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain"
                },
                "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."
                },
                "authors": [
                    {
                        "name": "Miguel Rios"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rios"
                },
                "author": "Miguel Rios",
                "arxiv_comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models\n  for Machine Translation in the Medical Domain. In Proceedings of Machine\n  Translation Summit XX Volume 1, pages 162-172",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09228v3",
                "updated": "2025-07-30T12:52:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    52,
                    43,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-12T09:56:02Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    9,
                    56,
                    2,
                    5,
                    193,
                    0
                ],
                "title": "Alleviating the Hubble tension with Torsion Condensation (TorC)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating the Hubble tension with Torsion Condensation (TorC)"
                },
                "summary": "Constraints on the cosmological parameters of Torsion Condensation (TorC) are\ninvestigated using Planck 2018 Cosmic Microwave Background data. TorC is a case\nof Poincar\\'e gauge theory -- a formulation of gravity motivated by the gauge\nfield theories underlying fundamental forces in the standard model of particle\nphysics. Unlike general relativity, TorC incorporates intrinsic torsion degrees\nof freedom while maintaining second-order field equations. At specific\nparameter values, it reduces to the $\\Lambda$CDM model, providing a natural\nextension to standard cosmology. The base model of TorC introduces two\nparameters beyond those in $\\Lambda$CDM: the initial value of the torsion\nscalar field and its time derivative -- one can absorb the latter by allowing\nthe dark energy density to float. To constrain these parameters, `PolyChord`\nnested sampling algorithm is employed, interfaced via `Cobaya` with a modified\nversion of `CAMB`. Our results indicate that TorC allows for a larger inferred\nHubble constant, offering a potential resolution to the Hubble tension. Tension\nanalysis using the $R$-statistic shows that TorC alleviates the statistical\ntension between the Planck 2018 and SH0Es 2020 datasets, though this\nimprovement is not sufficient to decisively favour TorC over $\\Lambda$CDM in a\nBayesian model comparison. This study highlights TorC as a compelling theory of\ngravity, demonstrating its potential to address cosmological tensions and\nmotivating further investigations of extended theories of gravity within a\ncosmological context. As current and upcoming surveys -- including Euclid,\nRoman Space Telescope, Vera C. Rubin Observatory, LISA, and Simons Observatory\n-- deliver data on gravity across all scales, they will offer critical tests of\ngravity models like TorC, making the present a pivotal moment for exploring\nextended theories of gravity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the cosmological parameters of Torsion Condensation (TorC) are\ninvestigated using Planck 2018 Cosmic Microwave Background data. TorC is a case\nof Poincar\\'e gauge theory -- a formulation of gravity motivated by the gauge\nfield theories underlying fundamental forces in the standard model of particle\nphysics. Unlike general relativity, TorC incorporates intrinsic torsion degrees\nof freedom while maintaining second-order field equations. At specific\nparameter values, it reduces to the $\\Lambda$CDM model, providing a natural\nextension to standard cosmology. The base model of TorC introduces two\nparameters beyond those in $\\Lambda$CDM: the initial value of the torsion\nscalar field and its time derivative -- one can absorb the latter by allowing\nthe dark energy density to float. To constrain these parameters, `PolyChord`\nnested sampling algorithm is employed, interfaced via `Cobaya` with a modified\nversion of `CAMB`. Our results indicate that TorC allows for a larger inferred\nHubble constant, offering a potential resolution to the Hubble tension. Tension\nanalysis using the $R$-statistic shows that TorC alleviates the statistical\ntension between the Planck 2018 and SH0Es 2020 datasets, though this\nimprovement is not sufficient to decisively favour TorC over $\\Lambda$CDM in a\nBayesian model comparison. This study highlights TorC as a compelling theory of\ngravity, demonstrating its potential to address cosmological tensions and\nmotivating further investigations of extended theories of gravity within a\ncosmological context. As current and upcoming surveys -- including Euclid,\nRoman Space Telescope, Vera C. Rubin Observatory, LISA, and Simons Observatory\n-- deliver data on gravity across all scales, they will offer critical tests of\ngravity models like TorC, making the present a pivotal moment for exploring\nextended theories of gravity."
                },
                "authors": [
                    {
                        "name": "Sinah Legner"
                    },
                    {
                        "name": "Will Handley"
                    },
                    {
                        "name": "Will Barker"
                    }
                ],
                "author_detail": {
                    "name": "Will Barker"
                },
                "author": "Will Barker",
                "arxiv_comment": "21 pages (main text: 12 pages), 9 figures, 7 tables, comments\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22623v1",
                "updated": "2025-07-30T12:42:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:42:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "Multilingual Political Views of Large Language Models: Identification\n  and Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Political Views of Large Language Models: Identification\n  and Steering"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Roberto Zamparelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Zamparelli"
                },
                "author": "Roberto Zamparelli",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22619v1",
                "updated": "2025-07-30T12:39:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    39,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:39:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    39,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "Enhancing Manufacturing Knowledge Access with LLMs and Context-aware\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Manufacturing Knowledge Access with LLMs and Context-aware\n  Prompting"
                },
                "summary": "Knowledge graphs (KGs) have transformed data management within the\nmanufacturing industry, offering effective means for integrating disparate data\nsources through shared and structured conceptual schemas. However, harnessing\nthe power of KGs can be daunting for non-experts, as it often requires\nformulating complex SPARQL queries to retrieve specific information. With the\nadvent of Large Language Models (LLMs), there is a growing potential to\nautomatically translate natural language queries into the SPARQL format, thus\nbridging the gap between user-friendly interfaces and the sophisticated\narchitecture of KGs. The challenge remains in adequately informing LLMs about\nthe relevant context and structure of domain-specific KGs, e.g., in\nmanufacturing, to improve the accuracy of generated queries. In this paper, we\nevaluate multiple strategies that use LLMs as mediators to facilitate\ninformation retrieval from KGs. We focus on the manufacturing domain,\nparticularly on the Bosch Line Information System KG and the I40 Core\nInformation Model. In our evaluation, we compare various approaches for feeding\nrelevant context from the KG to the LLM and analyze their proficiency in\ntransforming real-world questions into SPARQL queries. Our findings show that\nLLMs can significantly improve their performance on generating correct and\ncomplete queries when provided only the adequate context of the KG schema. Such\ncontext-aware prompting techniques help LLMs to focus on the relevant parts of\nthe ontology and reduce the risk of hallucination. We anticipate that the\nproposed techniques help LLMs to democratize access to complex data\nrepositories and empower informed decision-making in manufacturing settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) have transformed data management within the\nmanufacturing industry, offering effective means for integrating disparate data\nsources through shared and structured conceptual schemas. However, harnessing\nthe power of KGs can be daunting for non-experts, as it often requires\nformulating complex SPARQL queries to retrieve specific information. With the\nadvent of Large Language Models (LLMs), there is a growing potential to\nautomatically translate natural language queries into the SPARQL format, thus\nbridging the gap between user-friendly interfaces and the sophisticated\narchitecture of KGs. The challenge remains in adequately informing LLMs about\nthe relevant context and structure of domain-specific KGs, e.g., in\nmanufacturing, to improve the accuracy of generated queries. In this paper, we\nevaluate multiple strategies that use LLMs as mediators to facilitate\ninformation retrieval from KGs. We focus on the manufacturing domain,\nparticularly on the Bosch Line Information System KG and the I40 Core\nInformation Model. In our evaluation, we compare various approaches for feeding\nrelevant context from the KG to the LLM and analyze their proficiency in\ntransforming real-world questions into SPARQL queries. Our findings show that\nLLMs can significantly improve their performance on generating correct and\ncomplete queries when provided only the adequate context of the KG schema. Such\ncontext-aware prompting techniques help LLMs to focus on the relevant parts of\nthe ontology and reduce the risk of hallucination. We anticipate that the\nproposed techniques help LLMs to democratize access to complex data\nrepositories and empower informed decision-making in manufacturing settings."
                },
                "authors": [
                    {
                        "name": "Sebastian Monka"
                    },
                    {
                        "name": "Irlan Grangel-González"
                    },
                    {
                        "name": "Stefan Schmid"
                    },
                    {
                        "name": "Lavdim Halilaj"
                    },
                    {
                        "name": "Marc Rickart"
                    },
                    {
                        "name": "Oliver Rudolph"
                    },
                    {
                        "name": "Rui Dias"
                    }
                ],
                "author_detail": {
                    "name": "Rui Dias"
                },
                "author": "Rui Dias",
                "arxiv_comment": "European Conference on Artificial Intelligence (ECAI) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14879v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14879v3",
                "updated": "2025-07-30T12:30:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    30,
                    41,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-07T10:52:25Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    10,
                    52,
                    25,
                    4,
                    38,
                    0
                ],
                "title": "Limited attention and models of choice: A behavioral equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited attention and models of choice: A behavioral equivalence"
                },
                "summary": "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), singling out the properties of the unobserved attention filters\nthat explain the observed choices.For each specification, information about the\nDM's consideration sets and preference is inferred from violations of the\ncontraction consistency axiom, and it is compared with the welfare indications\nobtained from equivalent models. Remarkably, limited attention always supports\nthe elicitation of DM's taste arising from alternative methods. Finally, we\nexamine the intersections between subclasses, and we verify that each of them\nis independent of the others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), singling out the properties of the unobserved attention filters\nthat explain the observed choices.For each specification, information about the\nDM's consideration sets and preference is inferred from violations of the\ncontraction consistency axiom, and it is compared with the welfare indications\nobtained from equivalent models. Remarkably, limited attention always supports\nthe elicitation of DM's taste arising from alternative methods. Finally, we\nexamine the intersections between subclasses, and we verify that each of them\nis independent of the others."
                },
                "authors": [
                    {
                        "name": "Davide Carpentiere"
                    },
                    {
                        "name": "Angelo Petralia"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Petralia"
                },
                "author": "Angelo Petralia",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2302.00978",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14879v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14879v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22608v1",
                "updated": "2025-07-30T12:23:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:23:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation"
                },
                "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Yusser Al Ghussin"
                    },
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22606v1",
                "updated": "2025-07-30T12:22:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    22,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:22:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    22,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on\n  Finite State Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on\n  Finite State Machines"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to solve a wide\nrange of practical tasks within multi-agent systems. However, existing\nhuman-designed multi-agent frameworks are typically limited to a small set of\npre-defined scenarios, while current automated design methods suffer from\nseveral limitations, such as the lack of tool integration, dependence on\nexternal training data, and rigid communication structures. In this paper, we\npropose MetaAgent, a finite state machine based framework that can\nautomatically generate a multi-agent system. Given a task description,\nMetaAgent will design a multi-agent system and polish it through an\noptimization algorithm. When the multi-agent system is deployed, the finite\nstate machine will control the agent's actions and the state transitions. To\nevaluate our framework, we conduct experiments on both text-based tasks and\npractical tasks. The results indicate that the generated multi-agent system\nsurpasses other auto-designed methods and can achieve a comparable performance\nwith the human-designed multi-agent system, which is optimized for those\nspecific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to solve a wide\nrange of practical tasks within multi-agent systems. However, existing\nhuman-designed multi-agent frameworks are typically limited to a small set of\npre-defined scenarios, while current automated design methods suffer from\nseveral limitations, such as the lack of tool integration, dependence on\nexternal training data, and rigid communication structures. In this paper, we\npropose MetaAgent, a finite state machine based framework that can\nautomatically generate a multi-agent system. Given a task description,\nMetaAgent will design a multi-agent system and polish it through an\noptimization algorithm. When the multi-agent system is deployed, the finite\nstate machine will control the agent's actions and the state transitions. To\nevaluate our framework, we conduct experiments on both text-based tasks and\npractical tasks. The results indicate that the generated multi-agent system\nsurpasses other auto-designed methods and can achieve a comparable performance\nwith the human-designed multi-agent system, which is optimized for those\nspecific tasks."
                },
                "authors": [
                    {
                        "name": "Yaolun Zhang"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22025v2",
                "updated": "2025-07-30T12:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    17,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T17:22:07Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    17,
                    22,
                    7,
                    1,
                    210,
                    0
                ],
                "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding"
                },
                "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro."
                },
                "authors": [
                    {
                        "name": "Shuquan Lian"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Jia Ma"
                    },
                    {
                        "name": "Zihan Song"
                    },
                    {
                        "name": "Bingqi Chen"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Hui Li"
                    }
                ],
                "author_detail": {
                    "name": "Hui Li"
                },
                "author": "Hui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22603v1",
                "updated": "2025-07-30T12:16:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    16,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:16:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    16,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALSAM: A Platform for Benchmarking Arabic Large Language Models"
                },
                "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Rawan Al-Matham"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Raghad Al-Rasheed"
                    },
                    {
                        "name": "Waad Alshammari"
                    },
                    {
                        "name": "Muneera Alhoshan"
                    },
                    {
                        "name": "Amal Almazrua"
                    },
                    {
                        "name": "Asma Al Wazrah"
                    },
                    {
                        "name": "Mais Alheraki"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Norah Alzahrani"
                    },
                    {
                        "name": "Eman alBilali"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Abdelrahman El-Sheikh"
                    },
                    {
                        "name": "Muhammad Elmallah"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Hamdy Mubarak"
                    },
                    {
                        "name": "Mohamed Anwar"
                    },
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Ahmed Abdelali"
                    },
                    {
                        "name": "Nora Altwairesh"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Abdulmohsen Al Thubaity"
                    },
                    {
                        "name": "Shady Shehata"
                    },
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Injy Hamed"
                    },
                    {
                        "name": "Go Inoue"
                    },
                    {
                        "name": "Khalid Elmadani"
                    },
                    {
                        "name": "Ossama Obeid"
                    },
                    {
                        "name": "Fatima Haouari"
                    },
                    {
                        "name": "Tamer Elsayed"
                    },
                    {
                        "name": "Emad Alghamdi"
                    },
                    {
                        "name": "Khalid Almubarak"
                    },
                    {
                        "name": "Saied Alshahrani"
                    },
                    {
                        "name": "Ola Aljarrah"
                    },
                    {
                        "name": "Safa Alajlan"
                    },
                    {
                        "name": "Areej Alshaqarawi"
                    },
                    {
                        "name": "Maryam Alshihri"
                    },
                    {
                        "name": "Sultana Alghurabi"
                    },
                    {
                        "name": "Atikah Alzeghayer"
                    },
                    {
                        "name": "Afrah Altamimi"
                    },
                    {
                        "name": "Abdullah Alfaifi"
                    },
                    {
                        "name": "Abdulrahman AlOsaimy"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman AlOsaimy"
                },
                "author": "Abdulrahman AlOsaimy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15586v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15586v4",
                "updated": "2025-07-30T11:51:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    51,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-21T13:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."
                },
                "authors": [
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Shouzheng Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "16 pages, 7 Figures, 10 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15586v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15586v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07214v3",
                "updated": "2025-07-30T11:37:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    37,
                    10,
                    2,
                    211,
                    0
                ],
                "published": "2024-02-20T18:57:34Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    57,
                    34,
                    1,
                    51,
                    0
                ],
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements."
                },
                "authors": [
                    {
                        "name": "Akash Ghosh"
                    },
                    {
                        "name": "Arkadeep Acharya"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "One of the first survey on Visual Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22581v2",
                "updated": "2025-07-31T03:32:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    32,
                    19,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T11:23:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    23,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Unveiling the Influence of Amplifying Language-Specific Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Influence of Amplifying Language-Specific Neurons"
                },
                "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer."
                },
                "authors": [
                    {
                        "name": "Inaya Rahmanisa"
                    },
                    {
                        "name": "Lyzander Marciano Andrylie"
                    },
                    {
                        "name": "Mahardika Krisna Ihsani"
                    },
                    {
                        "name": "Alfan Farizki Wicaksono"
                    },
                    {
                        "name": "Haryo Akbarianto Wibowo"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "arxiv_comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22580v1",
                "updated": "2025-07-30T11:21:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    21,
                    9,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T11:21:09Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    21,
                    9,
                    2,
                    211,
                    0
                ],
                "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated\n  Patch Correctness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated\n  Patch Correctness Assessment"
                },
                "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability."
                },
                "authors": [
                    {
                        "name": "Marcos Fuster-Pena"
                    },
                    {
                        "name": "David de-Fitero-Dominguez"
                    },
                    {
                        "name": "Antonio Garcia-Cabot"
                    },
                    {
                        "name": "Eva Garcia-Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Eva Garcia-Lopez"
                },
                "author": "Eva Garcia-Lopez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22568v1",
                "updated": "2025-07-30T10:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    50,
                    41,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:50:41Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    50,
                    41,
                    2,
                    211,
                    0
                ],
                "title": "Subtyping Breast Lesions via Generative Augmentation based Long-tailed\n  Recognition in Ultrasound",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subtyping Breast Lesions via Generative Augmentation based Long-tailed\n  Recognition in Ultrasound"
                },
                "summary": "Accurate identification of breast lesion subtypes can facilitate personalized\ntreatment and interventions. Ultrasound (US), as a safe and accessible imaging\nmodality, is extensively employed in breast abnormality screening and\ndiagnosis. However, the incidence of different subtypes exhibits a skewed\nlong-tailed distribution, posing significant challenges for automated\nrecognition. Generative augmentation provides a promising solution to rectify\ndata distribution. Inspired by this, we propose a dual-phase framework for\nlong-tailed classification that mitigates distributional bias through\nhigh-fidelity data synthesis while avoiding overuse that corrupts holistic\nperformance. The framework incorporates a reinforcement learning-driven\nadaptive sampler, dynamically calibrating synthetic-real data ratios by\ntraining a strategic multi-agent to compensate for scarcities of real data\nwhile ensuring stable discriminative capability. Furthermore, our\nclass-controllable synthetic network integrates a sketch-grounded perception\nbranch that harnesses anatomical priors to maintain distinctive class features\nwhile enabling annotation-free inference. Extensive experiments on an in-house\nlong-tailed and a public imbalanced breast US datasets demonstrate that our\nmethod achieves promising performance compared to state-of-the-art approaches.\nMore synthetic images can be found at\nhttps://github.com/Stinalalala/Breast-LT-GenAug.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of breast lesion subtypes can facilitate personalized\ntreatment and interventions. Ultrasound (US), as a safe and accessible imaging\nmodality, is extensively employed in breast abnormality screening and\ndiagnosis. However, the incidence of different subtypes exhibits a skewed\nlong-tailed distribution, posing significant challenges for automated\nrecognition. Generative augmentation provides a promising solution to rectify\ndata distribution. Inspired by this, we propose a dual-phase framework for\nlong-tailed classification that mitigates distributional bias through\nhigh-fidelity data synthesis while avoiding overuse that corrupts holistic\nperformance. The framework incorporates a reinforcement learning-driven\nadaptive sampler, dynamically calibrating synthetic-real data ratios by\ntraining a strategic multi-agent to compensate for scarcities of real data\nwhile ensuring stable discriminative capability. Furthermore, our\nclass-controllable synthetic network integrates a sketch-grounded perception\nbranch that harnesses anatomical priors to maintain distinctive class features\nwhile enabling annotation-free inference. Extensive experiments on an in-house\nlong-tailed and a public imbalanced breast US datasets demonstrate that our\nmethod achieves promising performance compared to state-of-the-art approaches.\nMore synthetic images can be found at\nhttps://github.com/Stinalalala/Breast-LT-GenAug."
                },
                "authors": [
                    {
                        "name": "Shijing Chen"
                    },
                    {
                        "name": "Xinrui Zhou"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Yuhao Huang"
                    },
                    {
                        "name": "Ao Chang"
                    },
                    {
                        "name": "Dong Ni"
                    },
                    {
                        "name": "Ruobing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruobing Huang"
                },
                "author": "Ruobing Huang",
                "arxiv_comment": "MICCAI2025 Early Accept. 11 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22565v1",
                "updated": "2025-07-30T10:46:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    46,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:46:53Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    46,
                    53,
                    2,
                    211,
                    0
                ],
                "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning"
                },
                "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks."
                },
                "authors": [
                    {
                        "name": "Afshin Khadangi"
                    },
                    {
                        "name": "Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Ramin Bahmani"
                    },
                    {
                        "name": "Gilbert Fridgen"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Fridgen"
                },
                "author": "Gilbert Fridgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22564v1",
                "updated": "2025-07-30T10:40:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:40:53Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    53,
                    2,
                    211,
                    0
                ],
                "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems."
                },
                "authors": [
                    {
                        "name": "Xikang Yang"
                    },
                    {
                        "name": "Biyu Zhou"
                    },
                    {
                        "name": "Xuehai Tang"
                    },
                    {
                        "name": "Jizhong Han"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16936v2",
                "updated": "2025-07-30T10:40:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    50,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-22T09:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    9,
                    14,
                    35,
                    6,
                    357,
                    0
                ],
                "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-guided Prompting for Knowledge-based Visual Question Answering"
                },
                "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively."
                },
                "authors": [
                    {
                        "name": "Zhongjian Hu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Fengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fengyuan Liu"
                },
                "author": "Fengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22558v1",
                "updated": "2025-07-30T10:32:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    32,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:32:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    32,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "aLLoyM: A large language model for alloy phase diagram prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aLLoyM: A large language model for alloy phase diagram prediction"
                },
                "summary": "Large Language Models (LLMs) are general-purpose tools with wide-ranging\napplications, including in materials science. In this work, we introduce\naLLoyM, a fine-tuned LLM specifically trained on alloy compositions,\ntemperatures, and their corresponding phase information. To develop aLLoyM, we\ncurated question-and-answer (Q&A) pairs for binary and ternary phase diagrams\nusing the open-source Computational Phase Diagram Database (CPDDB) and\nassessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned\nMistral, an open-source pre-trained LLM, for two distinct Q&A formats:\nmultiple-choice and short-answer. Benchmark evaluations demonstrate that\nfine-tuning substantially enhances performance on multiple-choice phase diagram\nquestions. Moreover, the short-answer model of aLLoyM exhibits the ability to\ngenerate novel phase diagrams from its components alone, underscoring its\npotential to accelerate the discovery of previously unexplored materials\nsystems. To promote further research and adoption, we have publicly released\nthe short-answer fine-tuned version of aLLoyM, along with the complete\nbenchmarking Q&A dataset, on Hugging Face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are general-purpose tools with wide-ranging\napplications, including in materials science. In this work, we introduce\naLLoyM, a fine-tuned LLM specifically trained on alloy compositions,\ntemperatures, and their corresponding phase information. To develop aLLoyM, we\ncurated question-and-answer (Q&A) pairs for binary and ternary phase diagrams\nusing the open-source Computational Phase Diagram Database (CPDDB) and\nassessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned\nMistral, an open-source pre-trained LLM, for two distinct Q&A formats:\nmultiple-choice and short-answer. Benchmark evaluations demonstrate that\nfine-tuning substantially enhances performance on multiple-choice phase diagram\nquestions. Moreover, the short-answer model of aLLoyM exhibits the ability to\ngenerate novel phase diagrams from its components alone, underscoring its\npotential to accelerate the discovery of previously unexplored materials\nsystems. To promote further research and adoption, we have publicly released\nthe short-answer fine-tuned version of aLLoyM, along with the complete\nbenchmarking Q&A dataset, on Hugging Face."
                },
                "authors": [
                    {
                        "name": "Yuna Oikawa"
                    },
                    {
                        "name": "Guillaume Deffrennes"
                    },
                    {
                        "name": "Taichi Abe"
                    },
                    {
                        "name": "Ryo Tamura"
                    },
                    {
                        "name": "Koji Tsuda"
                    }
                ],
                "author_detail": {
                    "name": "Koji Tsuda"
                },
                "author": "Koji Tsuda",
                "arxiv_comment": "24 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22545v1",
                "updated": "2025-07-30T10:17:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    17,
                    7,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:17:07Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    17,
                    7,
                    2,
                    211,
                    0
                ],
                "title": "ControlMed: Adding Reasoning Control to Medical Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlMed: Adding Reasoning Control to Medical Language Model"
                },
                "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis."
                },
                "authors": [
                    {
                        "name": "Sung-Min Lee"
                    },
                    {
                        "name": "Siyoon Lee"
                    },
                    {
                        "name": "Juyeon Kim"
                    },
                    {
                        "name": "Kyungmin Roh"
                    }
                ],
                "author_detail": {
                    "name": "Kyungmin Roh"
                },
                "author": "Kyungmin Roh",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22542v1",
                "updated": "2025-07-30T10:14:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    14,
                    31,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:14:31Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    14,
                    31,
                    2,
                    211,
                    0
                ],
                "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large\n  Language Models in Customer Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large\n  Language Models in Customer Support"
                },
                "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA."
                },
                "authors": [
                    {
                        "name": "Long S. T. Nguyen"
                    },
                    {
                        "name": "Truong P. Hua"
                    },
                    {
                        "name": "Thanh M. Nguyen"
                    },
                    {
                        "name": "Toan Q. Pham"
                    },
                    {
                        "name": "Nam K. Ngo"
                    },
                    {
                        "name": "An X. Nguyen"
                    },
                    {
                        "name": "Nghi D. M. Pham"
                    },
                    {
                        "name": "Nghia H. Nguyen"
                    },
                    {
                        "name": "Tho T. Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho T. Quan"
                },
                "author": "Tho T. Quan",
                "arxiv_comment": "Under review at ICCCI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22533v1",
                "updated": "2025-07-30T10:02:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    2,
                    16,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:02:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    2,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for\n  Decision Support over Longitudinal Cancer Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliCARE: Grounding Large Language Models in Clinical Guidelines for\n  Decision Support over Longitudinal Cancer Electronic Health Records"
                },
                "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists."
                },
                "authors": [
                    {
                        "name": "Dongchen Li"
                    },
                    {
                        "name": "Jitao Liang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Longbing Cao"
                    },
                    {
                        "name": "Kun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yu"
                },
                "arxiv_affiliation": "College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China",
                "author": "Kun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22512v1",
                "updated": "2025-07-30T09:34:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    9,
                    34,
                    43,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T09:34:43Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    9,
                    34,
                    43,
                    2,
                    211,
                    0
                ],
                "title": "AlphaDent: A dataset for automated tooth pathology detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaDent: A dataset for automated tooth pathology detection"
                },
                "summary": "In this article, we present a new unique dataset for dental research -\nAlphaDent. This dataset is based on the DSLR camera photographs of the teeth of\n295 patients and contains over 1200 images. The dataset is labeled for solving\nthe instance segmentation problem and is divided into 9 classes. The article\nprovides a detailed description of the dataset and the labeling format. The\narticle also provides the details of the experiment on neural network training\nfor the Instance Segmentation problem using this dataset. The results obtained\nshow high quality of predictions. The dataset is published under an open\nlicense; and the training/inference code and model weights are also available\nunder open licenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we present a new unique dataset for dental research -\nAlphaDent. This dataset is based on the DSLR camera photographs of the teeth of\n295 patients and contains over 1200 images. The dataset is labeled for solving\nthe instance segmentation problem and is divided into 9 classes. The article\nprovides a detailed description of the dataset and the labeling format. The\narticle also provides the details of the experiment on neural network training\nfor the Instance Segmentation problem using this dataset. The results obtained\nshow high quality of predictions. The dataset is published under an open\nlicense; and the training/inference code and model weights are also available\nunder open licenses."
                },
                "authors": [
                    {
                        "name": "Evgeniy I. Sosnin"
                    },
                    {
                        "name": "Yuriy L. Vasilev"
                    },
                    {
                        "name": "Roman A. Solovyev"
                    },
                    {
                        "name": "Aleksandr L. Stempkovskiy"
                    },
                    {
                        "name": "Dmitry V. Telpukhov"
                    },
                    {
                        "name": "Artem A. Vasilev"
                    },
                    {
                        "name": "Aleksandr A. Amerikanov"
                    },
                    {
                        "name": "Aleksandr Y. Romanov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Y. Romanov"
                },
                "author": "Aleksandr Y. Romanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15697v2",
                "updated": "2025-07-30T09:31:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    9,
                    31,
                    22,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-21T15:07:51Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    15,
                    7,
                    51,
                    0,
                    202,
                    0
                ],
                "title": "Examining the Gap in the Chirp Mass Distribution of Binary Black Holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Gap in the Chirp Mass Distribution of Binary Black Holes"
                },
                "summary": "The mass distribution of binary black holes inferred from gravitational wave\nmeasurements is expected to shed light on their formation scenarios. An\nemerging structure in the mass distribution indicates the presence of multiple\npeaks around chirp masses of $8M_\\odot$, $14M_\\odot$, and $27M_\\odot$. In\nparticular, there is a lack of observations between chirp masses of 10 and 12\n$M_\\odot$. In this letter, we report that observations significantly favour the\nmodel supporting suppression of the rate in a narrow chirp mass range compared\nto the model that doesn't include suppression at a confidence greater than\n99.5\\%. Using another test, which measures the deviation between the inferred\nchirp mass distributions from the two models, we conservatively estimate a 95\\%\nconfidence in the presence of a feature. A lack of confidence has been reported\nin the presence of a gap around a comparable location in the component mass\ndistribution. The differing conclusions are due to a unique correlation between\nthe primary~(heavier of the two masses) and the secondary~(lighter of the two\nmasses) masses of binary black holes. This correlation results in increased\nclustering of measured chirp masses around specific values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mass distribution of binary black holes inferred from gravitational wave\nmeasurements is expected to shed light on their formation scenarios. An\nemerging structure in the mass distribution indicates the presence of multiple\npeaks around chirp masses of $8M_\\odot$, $14M_\\odot$, and $27M_\\odot$. In\nparticular, there is a lack of observations between chirp masses of 10 and 12\n$M_\\odot$. In this letter, we report that observations significantly favour the\nmodel supporting suppression of the rate in a narrow chirp mass range compared\nto the model that doesn't include suppression at a confidence greater than\n99.5\\%. Using another test, which measures the deviation between the inferred\nchirp mass distributions from the two models, we conservatively estimate a 95\\%\nconfidence in the presence of a feature. A lack of confidence has been reported\nin the presence of a gap around a comparable location in the component mass\ndistribution. The differing conclusions are due to a unique correlation between\nthe primary~(heavier of the two masses) and the secondary~(lighter of the two\nmasses) masses of binary black holes. This correlation results in increased\nclustering of measured chirp masses around specific values."
                },
                "authors": [
                    {
                        "name": "Vaibhav Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Vaibhav Tiwari"
                },
                "author": "Vaibhav Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17332v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17332v4",
                "updated": "2025-07-30T08:43:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    43,
                    58,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-23T09:00:13Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    0,
                    13,
                    2,
                    204,
                    0
                ],
                "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single\n  Image"
                },
                "summary": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/."
                },
                "authors": [
                    {
                        "name": "Hyeongjin Nam"
                    },
                    {
                        "name": "Donghwan Kim"
                    },
                    {
                        "name": "Gyeongsik Moon"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyoung Mu Lee"
                },
                "author": "Kyoung Mu Lee",
                "arxiv_comment": "Published at ICCV 2025, 22 pages including the supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17332v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17332v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22485v1",
                "updated": "2025-07-30T08:43:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    43,
                    48,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:43:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    43,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "Physics-constrained generative machine learning-based high-resolution\n  downscaling of Greenland's surface mass balance and surface temperature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-constrained generative machine learning-based high-resolution\n  downscaling of Greenland's surface mass balance and surface temperature"
                },
                "summary": "Accurate, high-resolution projections of the Greenland ice sheet's surface\nmass balance (SMB) and surface temperature are essential for understanding\nfuture sea-level rise, yet current approaches are either computationally\ndemanding or limited to coarse spatial scales. Here, we introduce a novel\nphysics-constrained generative modeling framework based on a consistency model\n(CM) to downscale low-resolution SMB and surface temperature fields by a factor\nof up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM\nis trained on monthly outputs of the regional climate model MARv3.12 and\nconditioned on ice-sheet topography and insolation. By enforcing a hard\nconservation constraint during inference, we ensure approximate preservation of\nSMB and temperature sums on the coarse spatial scale as well as robust\ngeneralization to extreme climate states without retraining. On the test set,\nour constrained CM achieves a continued ranked probability score of 6.31 mmWE\nfor the SMB and 0.1 K for the surface temperature, outperforming\ninterpolation-based downscaling. Together with spatial power-spectral analysis,\nwe demonstrate that the CM faithfully reproduces variability across spatial\nscales. We further apply bias-corrected outputs of the NorESM2 Earth System\nModel as inputs to our CM, to demonstrate the potential of our model to\ndirectly downscale ESM fields. Our approach delivers realistic, high-resolution\nclimate forcing for ice-sheet simulations with fast inference and can be\nreadily integrated into Earth-system and ice-sheet model workflows to improve\nprojections of the future contribution to sea-level rise from Greenland and\npotentially other ice sheets and glaciers too.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate, high-resolution projections of the Greenland ice sheet's surface\nmass balance (SMB) and surface temperature are essential for understanding\nfuture sea-level rise, yet current approaches are either computationally\ndemanding or limited to coarse spatial scales. Here, we introduce a novel\nphysics-constrained generative modeling framework based on a consistency model\n(CM) to downscale low-resolution SMB and surface temperature fields by a factor\nof up to 32 (from 160 km to 5 km grid spacing) in a few sampling steps. The CM\nis trained on monthly outputs of the regional climate model MARv3.12 and\nconditioned on ice-sheet topography and insolation. By enforcing a hard\nconservation constraint during inference, we ensure approximate preservation of\nSMB and temperature sums on the coarse spatial scale as well as robust\ngeneralization to extreme climate states without retraining. On the test set,\nour constrained CM achieves a continued ranked probability score of 6.31 mmWE\nfor the SMB and 0.1 K for the surface temperature, outperforming\ninterpolation-based downscaling. Together with spatial power-spectral analysis,\nwe demonstrate that the CM faithfully reproduces variability across spatial\nscales. We further apply bias-corrected outputs of the NorESM2 Earth System\nModel as inputs to our CM, to demonstrate the potential of our model to\ndirectly downscale ESM fields. Our approach delivers realistic, high-resolution\nclimate forcing for ice-sheet simulations with fast inference and can be\nreadily integrated into Earth-system and ice-sheet model workflows to improve\nprojections of the future contribution to sea-level rise from Greenland and\npotentially other ice sheets and glaciers too."
                },
                "authors": [
                    {
                        "name": "Nils Bochow"
                    },
                    {
                        "name": "Philipp Hess"
                    },
                    {
                        "name": "Alexander Robinson"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Robinson"
                },
                "author": "Alexander Robinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.09255v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.09255v4",
                "updated": "2025-07-30T08:38:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    38,
                    49,
                    2,
                    211,
                    0
                ],
                "published": "2023-02-18T08:01:47Z",
                "published_parsed": [
                    2023,
                    2,
                    18,
                    8,
                    1,
                    47,
                    5,
                    49,
                    0
                ],
                "title": "Clustered Covariate Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustered Covariate Regression"
                },
                "summary": "High covariate dimensionality is increasingly occurrent in model estimation,\nand existing techniques to address this issue typically require sparsity or\ndiscrete heterogeneity of the \\emph{unobservable} parameter vector. However,\nneither restriction may be supported by economic theory in some empirical\ncontexts, leading to severe bias and misleading inference. The clustering-based\ngrouped parameter estimator (GPE) introduced in this paper drops both\nrestrictions and maintains the natural one that the parameter support be\nbounded. GPE exhibits robust large sample properties under standard conditions\nand accommodates both sparse and non-sparse parameters whose support can be\nbounded away from zero. Extensive Monte Carlo simulations demonstrate the\nexcellent performance of GPE in terms of bias reduction and size control\ncompared to competing estimators. An empirical application of GPE to estimating\nprice and income elasticities of demand for gasoline highlights its practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High covariate dimensionality is increasingly occurrent in model estimation,\nand existing techniques to address this issue typically require sparsity or\ndiscrete heterogeneity of the \\emph{unobservable} parameter vector. However,\nneither restriction may be supported by economic theory in some empirical\ncontexts, leading to severe bias and misleading inference. The clustering-based\ngrouped parameter estimator (GPE) introduced in this paper drops both\nrestrictions and maintains the natural one that the parameter support be\nbounded. GPE exhibits robust large sample properties under standard conditions\nand accommodates both sparse and non-sparse parameters whose support can be\nbounded away from zero. Extensive Monte Carlo simulations demonstrate the\nexcellent performance of GPE in terms of bias reduction and size control\ncompared to competing estimators. An empirical application of GPE to estimating\nprice and income elasticities of demand for gasoline highlights its practical\nutility."
                },
                "authors": [
                    {
                        "name": "Abdul-Nasah Soale"
                    },
                    {
                        "name": "Emmanuel Selorm Tsyawo"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Selorm Tsyawo"
                },
                "author": "Emmanuel Selorm Tsyawo",
                "arxiv_doi": "10.2139/ssrn.3394012",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2139/ssrn.3394012",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2302.09255v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.09255v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "First draft: June 2019. Revised manuscript following referee comments",
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22478v1",
                "updated": "2025-07-30T08:29:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    29,
                    7,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:29:07Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    29,
                    7,
                    2,
                    211,
                    0
                ],
                "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "16 pages, 2 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22472v1",
                "updated": "2025-07-30T08:21:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    21,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:21:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    21,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "Inference in a generalized Bradley-Terry model for paired comparisons\n  with covariates and a growing number of subjects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in a generalized Bradley-Terry model for paired comparisons\n  with covariates and a growing number of subjects"
                },
                "summary": "Motivated by the home-field advantage in sports, we propose a generalized\nBradley--Terry model that incorporates covariate information for paired\ncomparisons. It has an $n$-dimensional merit parameter $\\bs{\\beta}$ and a\nfixed-dimensional regression coefficient $\\bs{\\gamma}$ for covariates. When the\nnumber of subjects $n$ approaches infinity and the number of comparisons\nbetween any two subjects is fixed, we show the uniform consistency of the\nmaximum likelihood estimator (MLE) $(\\widehat{\\bs{\\beta}},\n\\widehat{\\bs{\\gamma}})$ of $(\\bs{\\beta}, \\bs{\\gamma})$ Furthermore, we derive\nthe asymptotic normal distribution of the MLE by characterizing its asymptotic\nrepresentation. The asymptotic distribution of $\\widehat{\\bs{\\gamma}}$ is\nbiased, while that of $\\widehat{\\bs{\\beta}}$ is not. This phenomenon can be\nattributed to the different convergence rates of $\\widehat{\\bs{\\gamma}}$ and\n$\\widehat{\\bs{\\beta}}$. To the best of our knowledge, this is the first study\nto explore the asymptotic theory in paired comparison models with covariates in\na high-dimensional setting. The consistency result is further extended to an\nErd\\H{o}s--R\\'{e}nyi comparison graph with a diverging number of covariates.\nNumerical studies and a real data analysis demonstrate our theoretical\nfindings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the home-field advantage in sports, we propose a generalized\nBradley--Terry model that incorporates covariate information for paired\ncomparisons. It has an $n$-dimensional merit parameter $\\bs{\\beta}$ and a\nfixed-dimensional regression coefficient $\\bs{\\gamma}$ for covariates. When the\nnumber of subjects $n$ approaches infinity and the number of comparisons\nbetween any two subjects is fixed, we show the uniform consistency of the\nmaximum likelihood estimator (MLE) $(\\widehat{\\bs{\\beta}},\n\\widehat{\\bs{\\gamma}})$ of $(\\bs{\\beta}, \\bs{\\gamma})$ Furthermore, we derive\nthe asymptotic normal distribution of the MLE by characterizing its asymptotic\nrepresentation. The asymptotic distribution of $\\widehat{\\bs{\\gamma}}$ is\nbiased, while that of $\\widehat{\\bs{\\beta}}$ is not. This phenomenon can be\nattributed to the different convergence rates of $\\widehat{\\bs{\\gamma}}$ and\n$\\widehat{\\bs{\\beta}}$. To the best of our knowledge, this is the first study\nto explore the asymptotic theory in paired comparison models with covariates in\na high-dimensional setting. The consistency result is further extended to an\nErd\\H{o}s--R\\'{e}nyi comparison graph with a diverging number of covariates.\nNumerical studies and a real data analysis demonstrate our theoretical\nfindings."
                },
                "authors": [
                    {
                        "name": "Ting Yan"
                    }
                ],
                "author_detail": {
                    "name": "Ting Yan"
                },
                "author": "Ting Yan",
                "arxiv_comment": "58 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22467v1",
                "updated": "2025-07-30T08:14:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    14,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:14:40Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    14,
                    40,
                    2,
                    211,
                    0
                ],
                "title": "Towards Simulating Social Influence Dynamics with LLM-based Multi-agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Simulating Social Influence Dynamics with LLM-based Multi-agents"
                },
                "summary": "Recent advancements in Large Language Models offer promising capabilities to\nsimulate complex human social interactions. We investigate whether LLM-based\nmulti-agent simulations can reproduce core human social dynamics observed in\nonline forums. We evaluate conformity dynamics, group polarization, and\nfragmentation across different model scales and reasoning capabilities using a\nstructured simulation framework. Our findings indicate that smaller models\nexhibit higher conformity rates, whereas models optimized for reasoning are\nmore resistant to social influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models offer promising capabilities to\nsimulate complex human social interactions. We investigate whether LLM-based\nmulti-agent simulations can reproduce core human social dynamics observed in\nonline forums. We evaluate conformity dynamics, group polarization, and\nfragmentation across different model scales and reasoning capabilities using a\nstructured simulation framework. Our findings indicate that smaller models\nexhibit higher conformity rates, whereas models optimized for reasoning are\nmore resistant to social influence."
                },
                "authors": [
                    {
                        "name": "Hsien-Tsung Lin"
                    },
                    {
                        "name": "Pei-Cing Huang"
                    },
                    {
                        "name": "Chan-Tung Ku"
                    },
                    {
                        "name": "Chan Hsu"
                    },
                    {
                        "name": "Pei-Xuan Shieh"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05008v2",
                "updated": "2025-07-30T08:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    13,
                    44,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-07T12:35:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Voices of Freelance Professional Writers on AI: Limitations,\n  Expectations, and Fears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voices of Freelance Professional Writers on AI: Limitations,\n  Expectations, and Fears"
                },
                "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02596v2",
                "updated": "2025-07-30T08:10:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    10,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-03T13:20:00Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    13,
                    20,
                    0,
                    3,
                    184,
                    0
                ],
                "title": "Relativistic Limits of Decoding: Critical Divergence of Kullback-Leibler\n  Information and Free Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relativistic Limits of Decoding: Critical Divergence of Kullback-Leibler\n  Information and Free Energy"
                },
                "summary": "We present a statistical mechanical framework based on the Kullback-Leibler\ndivergence (KLD) to analyze the relativistic limits of decoding time-encoded\ninformation from a moving source. By modeling the symbol durations as\nentropy-maximizing sequences and treating the decoding process as\ncontext-sensitive inference, we identify KLD between the sender and receiver\ndistributions as a key indicator of contextual mismatch. We show that, under\nLorentz transformations, this divergence grows with the sender's velocity and\nexhibits critical divergence as the velocity approaches the speed of light.\nFurthermore, we derive an analytic expression for the Fisher information and\ndemonstrate that decoding sensitivity diverges similarly, indicating\ninstability near the relativistic limit. By introducing an\ninformation-theoretic free energy based on the decoding cost, we determine a\ncritical velocity beyond which decoding becomes thermodynamically impossible.\nThese results reveal a phase-transition-like behavior in relativistic\ninformation transfer and provide a unified interpretation of KLD, Fisher\ninformation, and free energy as measures of decodability. The formalism\ndeveloped here offers new insights into high-speed communication, relativistic\nsignal processing, and information geometry in non-inertial frames.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a statistical mechanical framework based on the Kullback-Leibler\ndivergence (KLD) to analyze the relativistic limits of decoding time-encoded\ninformation from a moving source. By modeling the symbol durations as\nentropy-maximizing sequences and treating the decoding process as\ncontext-sensitive inference, we identify KLD between the sender and receiver\ndistributions as a key indicator of contextual mismatch. We show that, under\nLorentz transformations, this divergence grows with the sender's velocity and\nexhibits critical divergence as the velocity approaches the speed of light.\nFurthermore, we derive an analytic expression for the Fisher information and\ndemonstrate that decoding sensitivity diverges similarly, indicating\ninstability near the relativistic limit. By introducing an\ninformation-theoretic free energy based on the decoding cost, we determine a\ncritical velocity beyond which decoding becomes thermodynamically impossible.\nThese results reveal a phase-transition-like behavior in relativistic\ninformation transfer and provide a unified interpretation of KLD, Fisher\ninformation, and free energy as measures of decodability. The formalism\ndeveloped here offers new insights into high-speed communication, relativistic\nsignal processing, and information geometry in non-inertial frames."
                },
                "authors": [
                    {
                        "name": "Tatsuaki Tsuruyama"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuaki Tsuruyama"
                },
                "author": "Tatsuaki Tsuruyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22462v1",
                "updated": "2025-07-30T08:08:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:08:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "IFEvalCode: Controlled Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEvalCode: Controlled Code Generation"
                },
                "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09213v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09213v3",
                "updated": "2025-07-30T08:05:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    5,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-01-16T00:19:19Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    19,
                    19,
                    3,
                    16,
                    0
                ],
                "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Hongzhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Yingwen Wang"
                    },
                    {
                        "name": "Wen He"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Yuejie Zhang"
                    },
                    {
                        "name": "Rui Feng"
                    },
                    {
                        "name": "Xiaobo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Zhang"
                },
                "author": "Xiaobo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09213v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09213v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22457v1",
                "updated": "2025-07-30T08:04:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    4,
                    19,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:04:19Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    4,
                    19,
                    2,
                    211,
                    0
                ],
                "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments\n  about Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments\n  about Large Language Models"
                },
                "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill."
                },
                "authors": [
                    {
                        "name": "Tian Yun"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Ellie Pavlick"
                    }
                ],
                "author_detail": {
                    "name": "Ellie Pavlick"
                },
                "author": "Ellie Pavlick",
                "arxiv_comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22454v1",
                "updated": "2025-07-30T08:02:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    2,
                    42,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:02:42Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    2,
                    42,
                    2,
                    211,
                    0
                ],
                "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and\n  Realistic LiDAR Point Cloud Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and\n  Realistic LiDAR Point Cloud Generation"
                },
                "summary": "LiDAR scene generation is critical for mitigating real-world LiDAR data\ncollection costs and enhancing the robustness of downstream perception tasks in\nautonomous driving. However, existing methods commonly struggle to capture\ngeometric realism and global topological consistency. Recent LiDAR Diffusion\nModels (LiDMs) predominantly embed LiDAR points into the latent space for\nimproved generation efficiency, which limits their interpretable ability to\nmodel detailed geometric structures and preserve global topological\nconsistency. To address these challenges, we propose TopoLiDM, a novel\nframework that integrates graph neural networks (GNNs) with diffusion models\nunder topological regularization for high-fidelity LiDAR generation. Our\napproach first trains a topological-preserving VAE to extract latent graph\nrepresentations by graph construction and multiple graph convolutional layers.\nThen we freeze the VAE and generate novel latent topological graphs through the\nlatent diffusion models. We also introduce 0-dimensional persistent homology\n(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world\nglobal topological structures. Extensive experiments on the KITTI-360 dataset\ndemonstrate TopoLiDM's superiority over state-of-the-art methods, achieving\nimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower\nMinimum Matching Distance (MMD). Notably, our model also enables fast\ngeneration speed with an average inference time of 1.68 samples/s, showcasing\nits scalability for real-world applications. We will release the related codes\nat https://github.com/IRMVLab/TopoLiDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR scene generation is critical for mitigating real-world LiDAR data\ncollection costs and enhancing the robustness of downstream perception tasks in\nautonomous driving. However, existing methods commonly struggle to capture\ngeometric realism and global topological consistency. Recent LiDAR Diffusion\nModels (LiDMs) predominantly embed LiDAR points into the latent space for\nimproved generation efficiency, which limits their interpretable ability to\nmodel detailed geometric structures and preserve global topological\nconsistency. To address these challenges, we propose TopoLiDM, a novel\nframework that integrates graph neural networks (GNNs) with diffusion models\nunder topological regularization for high-fidelity LiDAR generation. Our\napproach first trains a topological-preserving VAE to extract latent graph\nrepresentations by graph construction and multiple graph convolutional layers.\nThen we freeze the VAE and generate novel latent topological graphs through the\nlatent diffusion models. We also introduce 0-dimensional persistent homology\n(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world\nglobal topological structures. Extensive experiments on the KITTI-360 dataset\ndemonstrate TopoLiDM's superiority over state-of-the-art methods, achieving\nimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower\nMinimum Matching Distance (MMD). Notably, our model also enables fast\ngeneration speed with an average inference time of 1.68 samples/s, showcasing\nits scalability for real-world applications. We will release the related codes\nat https://github.com/IRMVLab/TopoLiDM."
                },
                "authors": [
                    {
                        "name": "Jiuming Liu"
                    },
                    {
                        "name": "Zheng Huang"
                    },
                    {
                        "name": "Mengmeng Liu"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Francesco Nex"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "arxiv_comment": "Accepted by IROS 2025. Code:https://github.com/IRMVLab/TopoLiDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19703v2",
                "updated": "2025-07-30T07:58:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    58,
                    56,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-25T22:48:37Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    22,
                    48,
                    37,
                    4,
                    206,
                    0
                ],
                "title": "The wall confronting large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wall confronting large language models"
                },
                "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Peter V. Coveney"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21830v2",
                "updated": "2025-07-30T07:57:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    57,
                    31,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T14:08:09Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    14,
                    8,
                    9,
                    1,
                    210,
                    0
                ],
                "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework"
                },
                "summary": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Ruijie jian"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Luqi Gong"
                    },
                    {
                        "name": "Yishan Jiang"
                    },
                    {
                        "name": "Chunjie Luo an Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Chunjie Luo an Jianfeng Zhan"
                },
                "author": "Chunjie Luo an Jianfeng Zhan",
                "arxiv_doi": "10.1145/3746027.3755458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22448v1",
                "updated": "2025-07-30T07:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:55:33Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance"
                },
                "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research."
                },
                "authors": [
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Maksim Velikanov"
                    },
                    {
                        "name": "Ilyas Chahed"
                    },
                    {
                        "name": "Younes Belkada"
                    },
                    {
                        "name": "Dhia Eddine Rhayem"
                    },
                    {
                        "name": "Guillaume Kunsch"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Hamza Yous"
                    },
                    {
                        "name": "Brahim Farhat"
                    },
                    {
                        "name": "Ibrahim Khadraoui"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ruxandra Cojocaru"
                    },
                    {
                        "name": "Yasser Djilali"
                    },
                    {
                        "name": "Shi Hu"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Puneesh Khanna"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Ngoc Dung Huynh"
                    },
                    {
                        "name": "Phuc Le Khac"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Billel Mokeddem"
                    },
                    {
                        "name": "Mohamed Chami"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Mikhail Lubinets"
                    },
                    {
                        "name": "Kacper Piskorski"
                    },
                    {
                        "name": "Slim Frikha"
                    }
                ],
                "author_detail": {
                    "name": "Slim Frikha"
                },
                "author": "Slim Frikha",
                "arxiv_comment": "Technical report of Falcon-H1 model series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22447v1",
                "updated": "2025-07-30T07:46:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    46,
                    49,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:46:49Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    46,
                    49,
                    2,
                    211,
                    0
                ],
                "title": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for\n  Malicious JavaScript Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for\n  Malicious JavaScript Detection"
                },
                "summary": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code."
                },
                "authors": [
                    {
                        "name": "Zhihong Liang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhenhuang Hu"
                    },
                    {
                        "name": "Liangliang Song"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Jingjing Guo"
                    },
                    {
                        "name": "Yanbin Wang"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20422v2",
                "updated": "2025-07-30T07:45:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    45,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-25T13:36:23Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    36,
                    23,
                    2,
                    176,
                    0
                ],
                "title": "Delayed Feedback in High-$z$ Starbursts Revealed by Lyman-$α$\n  Profiles and Metal Line Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delayed Feedback in High-$z$ Starbursts Revealed by Lyman-$α$\n  Profiles and Metal Line Diagnostics"
                },
                "summary": "Lyman-$\\alpha$ emission, which owing to its resonant nature strongly couples\nthe emergent line profile to gas kinematics, is a key observable for probing\noutflows from star-forming galaxies in the early universe. Inferences of\noutflow properties from Lyman-$\\alpha$, however, often lack contextual\ncomparisons with more direct outflow diagnostics from down-the-barrel metal\nabsorption lines and driving-source properties from metal emission lines. Here,\nwe make such checks by taking advantage of the lensing magnification provided\nby galaxy clusters for 338 Lyman-$\\alpha$ sources observed with the Multi-Unit\nSpectroscopic Explorer (MUSE). Using metal emission lines to measure systemic\nredshifts, we confirm that the Lyman-$\\alpha$ profiles are consistent with\noutflowing gas: single peaks redshifted relative to, or double peaks\nstraddling, the systemic redshift. In cases where metal absorption lines are\ndetected, blueshifted velocities indicate outflows, while line ratios point to\nabsorption by a clumpy medium. We find systematic differences in both metal\nabsorption and emission lines associated with single- versus double-peaked\nLyman-$\\alpha$ profiles, such that the latter are preferentially associated\nwith weaker and narrower metal absorption profiles, but stronger emission lines\nindicating younger stellar ages ($\\lesssim4\\,$Myr for double-peaked\nLyman-$\\alpha$ vs $\\gtrsim10\\,$Myr for single-peaked Lyman-$\\alpha$).\nDouble-peaked Lyman-$\\alpha$ profiles may therefore reflect weaker feedback in\nextremely young starbursts due to the delayed onset of core-collapse\nsupernovae. Fitting model Lyman-$\\alpha$ profiles based on simple expanding\nshell geometry to those observed, we find that such models successfully\nreproduce the data, yet systematically overestimate systemic redshifts and\nyield unphysical parameters -- calling for caution when inferring outflow\nproperties from such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyman-$\\alpha$ emission, which owing to its resonant nature strongly couples\nthe emergent line profile to gas kinematics, is a key observable for probing\noutflows from star-forming galaxies in the early universe. Inferences of\noutflow properties from Lyman-$\\alpha$, however, often lack contextual\ncomparisons with more direct outflow diagnostics from down-the-barrel metal\nabsorption lines and driving-source properties from metal emission lines. Here,\nwe make such checks by taking advantage of the lensing magnification provided\nby galaxy clusters for 338 Lyman-$\\alpha$ sources observed with the Multi-Unit\nSpectroscopic Explorer (MUSE). Using metal emission lines to measure systemic\nredshifts, we confirm that the Lyman-$\\alpha$ profiles are consistent with\noutflowing gas: single peaks redshifted relative to, or double peaks\nstraddling, the systemic redshift. In cases where metal absorption lines are\ndetected, blueshifted velocities indicate outflows, while line ratios point to\nabsorption by a clumpy medium. We find systematic differences in both metal\nabsorption and emission lines associated with single- versus double-peaked\nLyman-$\\alpha$ profiles, such that the latter are preferentially associated\nwith weaker and narrower metal absorption profiles, but stronger emission lines\nindicating younger stellar ages ($\\lesssim4\\,$Myr for double-peaked\nLyman-$\\alpha$ vs $\\gtrsim10\\,$Myr for single-peaked Lyman-$\\alpha$).\nDouble-peaked Lyman-$\\alpha$ profiles may therefore reflect weaker feedback in\nextremely young starbursts due to the delayed onset of core-collapse\nsupernovae. Fitting model Lyman-$\\alpha$ profiles based on simple expanding\nshell geometry to those observed, we find that such models successfully\nreproduce the data, yet systematically overestimate systemic redshifts and\nyield unphysical parameters -- calling for caution when inferring outflow\nproperties from such models."
                },
                "authors": [
                    {
                        "name": "James Nianias"
                    },
                    {
                        "name": "Jeremy Lim"
                    },
                    {
                        "name": "Yik Lok Wong"
                    },
                    {
                        "name": "Gordon Wong"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wong"
                },
                "author": "Gordon Wong",
                "arxiv_comment": "27 pages, 19 figures, 4 tables. Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15790v3",
                "updated": "2025-07-30T07:32:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    32,
                    19,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-18T18:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    18,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis"
                },
                "summary": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results."
                },
                "authors": [
                    {
                        "name": "Chenyang Peng"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yitao Zhao"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_doi": "10.1145/3755881.3755934",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3755881.3755934",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.15790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure. To appear in Proceedings of the 16th Asia-Pacific\n  Symposium on Internetware (Internetware 2025), ACM ICPS. DOI:\n  https://doi.org/10.1145/3755881.3755934",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21990v2",
                "updated": "2025-07-30T07:23:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    23,
                    58,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T16:40:49Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    16,
                    40,
                    49,
                    1,
                    210,
                    0
                ],
                "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge"
                },
                "summary": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Ziping Wan"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Xuanze Lin"
                    },
                    {
                        "name": "Shiyang Yu"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zichen Zhu"
                    },
                    {
                        "name": "Danyang Zhang"
                    },
                    {
                        "name": "Huayang Wang"
                    },
                    {
                        "name": "Zhongyang Dai"
                    },
                    {
                        "name": "Liyang Wen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22424v1",
                "updated": "2025-07-30T07:04:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    4,
                    9,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:04:09Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    4,
                    9,
                    2,
                    211,
                    0
                ],
                "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with\n  Relaxed Acceptance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with\n  Relaxed Acceptance"
                },
                "summary": "Vision-Language-Action (VLA) models have made substantial progress by\nleveraging the robust capabilities of Visual Language Models (VLMs). However,\nVLMs' significant parameter size and autoregressive (AR) decoding nature impose\nconsiderable computational demands on VLA models. While Speculative Decoding\n(SD) has shown efficacy in accelerating Large Language Models (LLMs) by\nincorporating efficient drafting and parallel verification, allowing multiple\ntokens to be generated in one forward pass, its application to VLA models\nremains unexplored. This work introduces Spec-VLA, an SD framework designed to\naccelerate VLA models. Due to the difficulty of the action prediction task and\nthe greedy decoding mechanism of the VLA models, the direct application of the\nadvanced SD framework to the VLA prediction task yields a minor speed\nimprovement. To boost the generation speed, we propose an effective mechanism\nto relax acceptance utilizing the relative distances represented by the action\ntokens of the VLA model. Empirical results across diverse test scenarios affirm\nthe effectiveness of the Spec-VLA framework, and further analysis substantiates\nthe impact of our proposed strategies, which enhance the acceptance length by\n44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without\ncompromising the success rate. The success of the Spec-VLA framework highlights\nthe potential for broader application of speculative execution in VLA\nprediction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have made substantial progress by\nleveraging the robust capabilities of Visual Language Models (VLMs). However,\nVLMs' significant parameter size and autoregressive (AR) decoding nature impose\nconsiderable computational demands on VLA models. While Speculative Decoding\n(SD) has shown efficacy in accelerating Large Language Models (LLMs) by\nincorporating efficient drafting and parallel verification, allowing multiple\ntokens to be generated in one forward pass, its application to VLA models\nremains unexplored. This work introduces Spec-VLA, an SD framework designed to\naccelerate VLA models. Due to the difficulty of the action prediction task and\nthe greedy decoding mechanism of the VLA models, the direct application of the\nadvanced SD framework to the VLA prediction task yields a minor speed\nimprovement. To boost the generation speed, we propose an effective mechanism\nto relax acceptance utilizing the relative distances represented by the action\ntokens of the VLA model. Empirical results across diverse test scenarios affirm\nthe effectiveness of the Spec-VLA framework, and further analysis substantiates\nthe impact of our proposed strategies, which enhance the acceptance length by\n44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without\ncompromising the success rate. The success of the Spec-VLA framework highlights\nthe potential for broader application of speculative execution in VLA\nprediction scenarios."
                },
                "authors": [
                    {
                        "name": "Songsheng Wang"
                    },
                    {
                        "name": "Rucheng Yu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Derek F. Wong"
                },
                "author": "Derek F. Wong",
                "arxiv_comment": "12 pages, 5 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22421v1",
                "updated": "2025-07-30T06:49:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    49,
                    11,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:49:11Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    49,
                    11,
                    2,
                    211,
                    0
                ],
                "title": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A\n  Unified Framework for Action Recognition and Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A\n  Unified Framework for Action Recognition and Object Tracking"
                },
                "summary": "Real-time video analysis remains a challenging problem in computer vision,\nrequiring efficient processing of both spatial and temporal information while\nmaintaining computational efficiency. Existing approaches often struggle to\nbalance accuracy and speed, particularly in resource-constrained environments.\nIn this work, we present a unified framework that leverages advanced\nspatial-temporal modeling techniques for simultaneous action recognition and\nobject tracking. Our approach builds upon recent advances in parallel sequence\nmodeling and introduces a novel hierarchical attention mechanism that\nadaptively focuses on relevant spatial regions across temporal sequences. We\ndemonstrate that our method achieves state-of-the-art performance on standard\nbenchmarks while maintaining real-time inference speeds. Extensive experiments\non UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action\nrecognition accuracy and 2.8% in tracking precision compared to existing\nmethods, with 40% faster inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time video analysis remains a challenging problem in computer vision,\nrequiring efficient processing of both spatial and temporal information while\nmaintaining computational efficiency. Existing approaches often struggle to\nbalance accuracy and speed, particularly in resource-constrained environments.\nIn this work, we present a unified framework that leverages advanced\nspatial-temporal modeling techniques for simultaneous action recognition and\nobject tracking. Our approach builds upon recent advances in parallel sequence\nmodeling and introduces a novel hierarchical attention mechanism that\nadaptively focuses on relevant spatial regions across temporal sequences. We\ndemonstrate that our method achieves state-of-the-art performance on standard\nbenchmarks while maintaining real-time inference speeds. Extensive experiments\non UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action\nrecognition accuracy and 2.8% in tracking precision compared to existing\nmethods, with 40% faster inference time."
                },
                "authors": [
                    {
                        "name": "Shahla John"
                    }
                ],
                "author_detail": {
                    "name": "Shahla John"
                },
                "author": "Shahla John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22414v1",
                "updated": "2025-07-30T06:34:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    34,
                    2,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:34:02Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    34,
                    2,
                    2,
                    211,
                    0
                ],
                "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents"
                },
                "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output."
                },
                "authors": [
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22411v1",
                "updated": "2025-07-30T06:29:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    50,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:29:50Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    50,
                    2,
                    211,
                    0
                ],
                "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large\n  Language Models"
                },
                "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain"
                },
                "authors": [
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22403v1",
                "updated": "2025-07-30T06:12:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    12,
                    3,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:12:03Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    12,
                    3,
                    2,
                    211,
                    0
                ],
                "title": "Bayesian spatiotemporal modeling of passenger trip assignment in metro\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian spatiotemporal modeling of passenger trip assignment in metro\n  networks"
                },
                "summary": "Assigning passenger trips to specific network paths using automatic fare\ncollection (AFC) data is a fundamental application in urban transit analysis.\nThe task is a difficult inverse problem: the only available information\nconsists of each passenger's total travel time and their origin and\ndestination, while individual passenger path choices and dynamic network costs\nare unobservable, and behavior varies significantly across space and time. We\npropose a novel Bayesian hierarchical model to resolve this problem by jointly\nestimating dynamic network costs and passenger path choices while quantifying\ntheir uncertainty. Our model decomposes trip travel time into four components\n-- access, in-vehicle, transfer, and egress -- each modeled as a time-varying\nrandom walk. To capture heterogeneous passenger behavior, we introduce a\nmultinomial logit model with spatiotemporally varying coefficients. We manage\nthe high dimensionality of these coefficients using kernelized tensor\nfactorization with Gaussian process priors to effectively model complex\nspatiotemporal correlations. We develop a tailored and efficient Markov chain\nMonte Carlo (MCMC) algorithm for model inference. A simulation study\ndemonstrates the method's effectiveness in recovering the underlying model\nparameters. On a large-scale dataset from the Hong Kong Mass Transit Railway,\nour framework demonstrates superior estimation accuracy over established\nbenchmarks. The results reveal significant spatiotemporal variations in\npassenger preferences and provide robust uncertainty quantification, offering\ntransit operators a powerful tool for enhancing service planning and\noperational management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assigning passenger trips to specific network paths using automatic fare\ncollection (AFC) data is a fundamental application in urban transit analysis.\nThe task is a difficult inverse problem: the only available information\nconsists of each passenger's total travel time and their origin and\ndestination, while individual passenger path choices and dynamic network costs\nare unobservable, and behavior varies significantly across space and time. We\npropose a novel Bayesian hierarchical model to resolve this problem by jointly\nestimating dynamic network costs and passenger path choices while quantifying\ntheir uncertainty. Our model decomposes trip travel time into four components\n-- access, in-vehicle, transfer, and egress -- each modeled as a time-varying\nrandom walk. To capture heterogeneous passenger behavior, we introduce a\nmultinomial logit model with spatiotemporally varying coefficients. We manage\nthe high dimensionality of these coefficients using kernelized tensor\nfactorization with Gaussian process priors to effectively model complex\nspatiotemporal correlations. We develop a tailored and efficient Markov chain\nMonte Carlo (MCMC) algorithm for model inference. A simulation study\ndemonstrates the method's effectiveness in recovering the underlying model\nparameters. On a large-scale dataset from the Hong Kong Mass Transit Railway,\nour framework demonstrates superior estimation accuracy over established\nbenchmarks. The results reveal significant spatiotemporal variations in\npassenger preferences and provide robust uncertainty quantification, offering\ntransit operators a powerful tool for enhancing service planning and\noperational management."
                },
                "authors": [
                    {
                        "name": "Xiaoxu Chen"
                    },
                    {
                        "name": "Alexandra M. Schmidt"
                    },
                    {
                        "name": "Zhenliang Ma"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12767v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12767v6",
                "updated": "2025-07-30T06:04:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    4,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    31,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs"
                },
                "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/."
                },
                "authors": [
                    {
                        "name": "Sumin Jo"
                    },
                    {
                        "name": "Junseong Choi"
                    },
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12767v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12767v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.22887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22887v1",
                "updated": "2025-07-30T17:59:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    46,
                    2,
                    211,
                    0
                ],
                "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to show Demos in Your Prompt: A Positional Bias of In-Context\n  Learning"
                },
                "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Kwesi Cobbina"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21046v2",
                "updated": "2025-07-30T17:59:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    59,
                    37,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T17:59:05Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    17,
                    59,
                    5,
                    0,
                    209,
                    0
                ],
                "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks."
                },
                "authors": [
                    {
                        "name": "Huan-ang Gao"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Xinzhe Juan"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Xuan Qi"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Shaokun Zhang"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinyu Xiang"
                    },
                    {
                        "name": "Yixiong Fang"
                    },
                    {
                        "name": "Qiwen Zhao"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhenghailong Wang"
                    },
                    {
                        "name": "Minda Hu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "51 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22879v2",
                "updated": "2025-07-31T16:54:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    54,
                    43,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T17:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    55,
                    6,
                    2,
                    211,
                    0
                ],
                "title": "RecGPT Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecGPT Technical Report"
                },
                "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem."
                },
                "authors": [
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Gaoyang Guo"
                    },
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Mao Zhang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Wenjun Yang"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Chi Li"
                    },
                    {
                        "name": "Dimin Wang"
                    },
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Haibin Chen"
                    },
                    {
                        "name": "Haozhuang Liu"
                    },
                    {
                        "name": "Jialin Zhu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Jiawei Wu"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Lang Tian"
                    },
                    {
                        "name": "Liang Rao"
                    },
                    {
                        "name": "Longbin Li"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Na He"
                    },
                    {
                        "name": "Peiyang Wang"
                    },
                    {
                        "name": "Qiqi Huang"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Xiaoxiao He"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yaxuan Wu"
                    },
                    {
                        "name": "Yeqiu Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Yinnan Song"
                    },
                    {
                        "name": "Yuchen Li"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yujin Yuan"
                    },
                    {
                        "name": "Yuliang Yan"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zhibo Xiao"
                    },
                    {
                        "name": "Zhixin Ma"
                    },
                    {
                        "name": "Zile Zhou"
                    },
                    {
                        "name": "Ziqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ziqi Zhang"
                },
                "author": "Ziqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22876v1",
                "updated": "2025-07-30T17:52:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    52,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:52:25Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    52,
                    25,
                    2,
                    211,
                    0
                ],
                "title": "Automatically discovering heuristics in a complex SAT solver with large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically discovering heuristics in a complex SAT solver with large\n  language models"
                },
                "summary": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development."
                },
                "authors": [
                    {
                        "name": "Yiwen Sun"
                    },
                    {
                        "name": "Furong Ye"
                    },
                    {
                        "name": "Zhihan Chen"
                    },
                    {
                        "name": "Ke Wei"
                    },
                    {
                        "name": "Shaowei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Cai"
                },
                "author": "Shaowei Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04395v2",
                "updated": "2025-07-30T17:33:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    33,
                    22,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-06T07:35:15Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    7,
                    35,
                    15,
                    6,
                    96,
                    0
                ],
                "title": "Human-Level Competitive Pokémon via Scalable Offline Reinforcement\n  Learning with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Level Competitive Pokémon via Scalable Offline Reinforcement\n  Learning with Transformers"
                },
                "summary": "Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players\nlearn to exploit their opponent based on imperfect information in battles that\ncan last more than one hundred stochastic turns. AI research in CPS has been\nled by heuristic tree search and online self-play, but the game may also create\na platform to study adaptive policies trained offline on large datasets. We\ndevelop a pipeline to reconstruct the first-person perspective of an agent from\nlogs saved from the third-person perspective of a spectator, thereby unlocking\na dataset of real human battles spanning more than a decade that grows larger\nevery day. This dataset enables a black-box approach where we train large\nsequence models to adapt to their opponent based solely on their input\ntrajectory while selecting moves without explicit search of any kind. We study\na progression from imitation learning to offline RL and offline fine-tuning on\nself-play data in the hardcore competitive setting of Pok\\'emon's four oldest\n(and most partially observed) game generations. The resulting agents outperform\na recent LLM Agent approach and a strong heuristic search engine. While playing\nanonymously in online battles against humans, our best agents climb to rankings\ninside the top 10% of active players. All agent checkpoints, training details,\ndatasets, and baselines are available at https://metamon.tech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Pok\\'emon Singles (CPS) is a popular strategy game where players\nlearn to exploit their opponent based on imperfect information in battles that\ncan last more than one hundred stochastic turns. AI research in CPS has been\nled by heuristic tree search and online self-play, but the game may also create\na platform to study adaptive policies trained offline on large datasets. We\ndevelop a pipeline to reconstruct the first-person perspective of an agent from\nlogs saved from the third-person perspective of a spectator, thereby unlocking\na dataset of real human battles spanning more than a decade that grows larger\nevery day. This dataset enables a black-box approach where we train large\nsequence models to adapt to their opponent based solely on their input\ntrajectory while selecting moves without explicit search of any kind. We study\na progression from imitation learning to offline RL and offline fine-tuning on\nself-play data in the hardcore competitive setting of Pok\\'emon's four oldest\n(and most partially observed) game generations. The resulting agents outperform\na recent LLM Agent approach and a strong heuristic search engine. While playing\nanonymously in online battles against humans, our best agents climb to rankings\ninside the top 10% of active players. All agent checkpoints, training details,\ndatasets, and baselines are available at https://metamon.tech."
                },
                "authors": [
                    {
                        "name": "Jake Grigsby"
                    },
                    {
                        "name": "Yuqi Xie"
                    },
                    {
                        "name": "Justin Sasek"
                    },
                    {
                        "name": "Steven Zheng"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_comment": "Reinforcement Learning Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22853v1",
                "updated": "2025-07-30T17:24:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    24,
                    5,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:24:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    24,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "Repair-R1: Better Test Before Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repair-R1: Better Test Before Repair"
                },
                "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
                },
                "authors": [
                    {
                        "name": "Haichuan Hu"
                    },
                    {
                        "name": "Xiaochen Xie"
                    },
                    {
                        "name": "Quanjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Quanjun Zhang"
                },
                "author": "Quanjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12920v3",
                "updated": "2025-07-30T17:23:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    23,
                    56,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-18T15:01:02Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    15,
                    1,
                    2,
                    1,
                    49,
                    0
                ],
                "title": "Lightweight Online Adaption for Time Series Foundation Model Forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Online Adaption for Time Series Foundation Model Forecasts"
                },
                "summary": "Foundation models (FMs) have emerged as a promising approach for time series\nforecasting. While effective, FMs typically remain fixed during deployment due\nto the high computational costs of learning them online. Consequently, deployed\nFMs fail to adapt their forecasts to current data characteristics, despite the\navailability of online feedback from newly arriving data. This raises the\nquestion of whether FM performance can be enhanced by the efficient usage of\nthis feedback. We propose ELF to answer this question. ELF is a lightweight\nmechanism for the online adaption of FM forecasts in response to online\nfeedback. ELF consists of two parts: a) the ELF-Forecaster which is used to\nlearn the current data distribution; and b) the ELF-Weighter which is used to\ncombine the forecasts of the FM and the ELF-Forecaster. We evaluate the\nperformance of ELF in conjunction with several recent FMs across a suite of\nstandard time series datasets. In all of our experiments we find that using ELF\nimproves performance. This work demonstrates how efficient usage of online\nfeedback can be used to improve FM forecasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) have emerged as a promising approach for time series\nforecasting. While effective, FMs typically remain fixed during deployment due\nto the high computational costs of learning them online. Consequently, deployed\nFMs fail to adapt their forecasts to current data characteristics, despite the\navailability of online feedback from newly arriving data. This raises the\nquestion of whether FM performance can be enhanced by the efficient usage of\nthis feedback. We propose ELF to answer this question. ELF is a lightweight\nmechanism for the online adaption of FM forecasts in response to online\nfeedback. ELF consists of two parts: a) the ELF-Forecaster which is used to\nlearn the current data distribution; and b) the ELF-Weighter which is used to\ncombine the forecasts of the FM and the ELF-Forecaster. We evaluate the\nperformance of ELF in conjunction with several recent FMs across a suite of\nstandard time series datasets. In all of our experiments we find that using ELF\nimproves performance. This work demonstrates how efficient usage of online\nfeedback can be used to improve FM forecasts."
                },
                "authors": [
                    {
                        "name": "Thomas L. Lee"
                    },
                    {
                        "name": "William Toner"
                    },
                    {
                        "name": "Rajkarn Singh"
                    },
                    {
                        "name": "Artjom Joosen"
                    },
                    {
                        "name": "Martin Asenov"
                    }
                ],
                "author_detail": {
                    "name": "Martin Asenov"
                },
                "author": "Martin Asenov",
                "arxiv_comment": "9 pages, Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14820v2",
                "updated": "2025-07-30T17:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    18,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2024-09-23T08:52:09Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    52,
                    9,
                    0,
                    267,
                    0
                ],
                "title": "Past Meets Present: Creating Historical Analogy with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Past Meets Present: Creating Historical Analogy with Large Language\n  Models"
                },
                "summary": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method."
                },
                "authors": [
                    {
                        "name": "Nianqi Li"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "Accepted to ACL 2025 (Outstanding Paper Award)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06680v2",
                "updated": "2025-07-30T17:16:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    16,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2025-01-12T01:31:07Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    1,
                    31,
                    7,
                    6,
                    12,
                    0
                ],
                "title": "Application of Vision-Language Model to Pedestrians Behavior and Scene\n  Understanding in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application of Vision-Language Model to Pedestrians Behavior and Scene\n  Understanding in Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) have become a promising approach to enhancing\nperception and decision-making in autonomous driving. The gap remains in\napplying VLMs to understand complex scenarios interacting with pedestrians and\nefficient vehicle deployment. In this paper, we propose a knowledge\ndistillation method that transfers knowledge from large-scale vision-language\nfoundation models to efficient vision networks, and we apply it to pedestrian\nbehavior prediction and scene understanding tasks, achieving promising results\nin generating more diverse and comprehensive semantic attributes. We also\nutilize multiple pre-trained models and ensemble techniques to boost the\nmodel's performance. We further examined the effectiveness of the model after\nknowledge distillation; the results show significant metric improvements in\nopen-vocabulary perception and trajectory prediction tasks, which can\npotentially enhance the end-to-end performance of autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have become a promising approach to enhancing\nperception and decision-making in autonomous driving. The gap remains in\napplying VLMs to understand complex scenarios interacting with pedestrians and\nefficient vehicle deployment. In this paper, we propose a knowledge\ndistillation method that transfers knowledge from large-scale vision-language\nfoundation models to efficient vision networks, and we apply it to pedestrian\nbehavior prediction and scene understanding tasks, achieving promising results\nin generating more diverse and comprehensive semantic attributes. We also\nutilize multiple pre-trained models and ensemble techniques to boost the\nmodel's performance. We further examined the effectiveness of the model after\nknowledge distillation; the results show significant metric improvements in\nopen-vocabulary perception and trajectory prediction tasks, which can\npotentially enhance the end-to-end performance of autonomous driving."
                },
                "authors": [
                    {
                        "name": "Haoxiang Gao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Jinghan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jinghan Cao"
                },
                "author": "Jinghan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03295v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03295v5",
                "updated": "2025-07-30T17:11:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    11,
                    26,
                    2,
                    211,
                    0
                ],
                "published": "2024-11-05T17:42:43Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    42,
                    43,
                    1,
                    310,
                    0
                ],
                "title": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Human-AI Collaboration for Co-Writing Constructive Comments\n  Online"
                },
                "summary": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines if large language models (LLMs) can help people write\nconstructive comments on divisive social issues due to the difficulty of\nexpressing constructive disagreement online. Through controlled experiments\nwith 600 participants from India and the US, who reviewed and wrote\nconstructive comments on threads related to Islamophobia and homophobia, we\nobserved potential misalignment between how LLMs and humans perceive\nconstructiveness in online comments. While the LLM was more likely to\nprioritize politeness and balance among contrasting viewpoints when evaluating\nconstructiveness, participants emphasized logic and facts more than the LLM\ndid. Despite these differences, participants rated both LLM-generated and\nhuman-AI co-written comments as significantly more constructive than those\nwritten independently by humans. Our analysis also revealed that LLM-generated\ncomments integrated significantly more linguistic features of constructiveness\ncompared to human-written comments. When participants used LLMs to refine their\ncomments, the resulting comments were more constructive, more positive, less\ntoxic, and retained the original intent. However, occasionally LLMs distorted\npeople's original views -- especially when their stances were not outright\npolarizing. Based on these findings, we discuss ethical and design\nconsiderations in using LLMs to facilitate constructive discourse online."
                },
                "authors": [
                    {
                        "name": "Farhana Shahid"
                    },
                    {
                        "name": "Maximilian Dittgen"
                    },
                    {
                        "name": "Mor Naaman"
                    },
                    {
                        "name": "Aditya Vashistha"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vashistha"
                },
                "author": "Aditya Vashistha",
                "arxiv_doi": "10.1145/3757591",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3757591",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03295v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03295v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Hum.-Comput. Interact. 9, 7, Article 410 (November 2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22847v1",
                "updated": "2025-07-30T17:03:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    3,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T17:03:59Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    3,
                    59,
                    2,
                    211,
                    0
                ],
                "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology"
                },
                "summary": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems."
                },
                "authors": [
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Pengda Wang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Ziang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Ziang Xiao"
                },
                "author": "Ziang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09509v2",
                "updated": "2025-07-30T16:58:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    58,
                    48,
                    2,
                    211,
                    0
                ],
                "published": "2025-03-12T16:18:45Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    18,
                    45,
                    2,
                    71,
                    0
                ],
                "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba"
                },
                "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba)\nto various vision tasks and demonstrate significant potential. As a promising\ncompression technique, vector quantization (VQ) decomposes network weights into\ncodebooks and assignments, significantly reducing memory usage and\ncomputational latency, thereby enabling the deployment of ViMs on edge devices.\nAlthough existing VQ methods have achieved extremely low-bit quantization\n(e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and\nTransformer-based networks, directly applying these methods to ViMs results in\nunsatisfactory accuracy. We identify several key challenges: 1) The weights of\nMamba-based blocks in ViMs contain numerous outliers, significantly amplifying\nquantization errors. 2) When applied to ViMs, the latest VQ methods suffer from\nexcessive memory consumption, lengthy calibration procedures, and suboptimal\nperformance in the search for optimal codewords. In this paper, we propose\nViM-VQ, an efficient post-training vector quantization method tailored for\nViMs. ViM-VQ consists of two innovative components: 1) a fast convex\ncombination optimization algorithm that efficiently updates both the convex\ncombinations and the convex hulls to search for optimal codewords, and 2) an\nincremental vector quantization strategy that incrementally confirms optimal\ncodewords to mitigate truncation errors. Experimental results demonstrate that\nViM-VQ achieves state-of-the-art performance in low-bit quantization across\nvarious visual tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba)\nto various vision tasks and demonstrate significant potential. As a promising\ncompression technique, vector quantization (VQ) decomposes network weights into\ncodebooks and assignments, significantly reducing memory usage and\ncomputational latency, thereby enabling the deployment of ViMs on edge devices.\nAlthough existing VQ methods have achieved extremely low-bit quantization\n(e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and\nTransformer-based networks, directly applying these methods to ViMs results in\nunsatisfactory accuracy. We identify several key challenges: 1) The weights of\nMamba-based blocks in ViMs contain numerous outliers, significantly amplifying\nquantization errors. 2) When applied to ViMs, the latest VQ methods suffer from\nexcessive memory consumption, lengthy calibration procedures, and suboptimal\nperformance in the search for optimal codewords. In this paper, we propose\nViM-VQ, an efficient post-training vector quantization method tailored for\nViMs. ViM-VQ consists of two innovative components: 1) a fast convex\ncombination optimization algorithm that efficiently updates both the convex\ncombinations and the convex hulls to search for optimal codewords, and 2) an\nincremental vector quantization strategy that incrementally confirms optimal\ncodewords to mitigate truncation errors. Experimental results demonstrate that\nViM-VQ achieves state-of-the-art performance in low-bit quantization across\nvarious visual tasks."
                },
                "authors": [
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19959v2",
                "updated": "2025-07-30T16:46:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    46,
                    12,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-26T13:21:18Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    21,
                    18,
                    0,
                    146,
                    0
                ],
                "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for\n  Large Language Models"
                },
                "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."
                },
                "authors": [
                    {
                        "name": "Zhongzhan Huang"
                    },
                    {
                        "name": "Guoming Ling"
                    },
                    {
                        "name": "Shanshan Zhong"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Accepted by ACL'25 main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22827v1",
                "updated": "2025-07-30T16:41:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    41,
                    21,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:41:21Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    41,
                    21,
                    2,
                    211,
                    0
                ],
                "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents"
                },
                "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder."
                },
                "authors": [
                    {
                        "name": "Yilei Jiang"
                    },
                    {
                        "name": "Yaozhi Zheng"
                    },
                    {
                        "name": "Yuxuan Wan"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Qunzhong Wang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22811v1",
                "updated": "2025-07-30T16:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    29,
                    47,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:29:47Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    29,
                    47,
                    2,
                    211,
                    0
                ],
                "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph"
                },
                "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM."
                },
                "authors": [
                    {
                        "name": "Debayan Banerjee"
                    },
                    {
                        "name": "Tilahun Abedissa Taffa"
                    },
                    {
                        "name": "Ricardo Usbeck"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Usbeck"
                },
                "author": "Ricardo Usbeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18044v2",
                "updated": "2025-07-30T16:28:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    28,
                    54,
                    2,
                    211,
                    0
                ],
                "published": "2024-07-25T13:47:01Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    13,
                    47,
                    1,
                    3,
                    207,
                    0
                ],
                "title": "The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented\n  Generation for Healthcare QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented\n  Generation for Healthcare QA"
                },
                "summary": "Deploying Large Language Models (LLMs) for healthcare question answering\nrequires robust methods to ensure accuracy and reliability. This work\nintroduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for\nenhancing Retrieval-Augmented Generation (RAG) systems in healthcare\nquestion-answering by pre-aligning user queries with a database of curated,\nanswerable questions derived from healthcare content. A key component of QB-RAG\nis an LLM-based filtering mechanism that ensures that only relevant and\nanswerable questions are included in the database, enabling reliable reference\nquery generation at scale. We provide theoretical motivation for QB-RAG,\nconduct a comparative analysis of existing retrieval enhancement techniques,\nand introduce a generalizable, comprehensive evaluation framework that assesses\nboth the retrieval effectiveness and the quality of the generated response\nbased on faithfulness, relevance, and adherence to the guideline. Our empirical\nevaluation on a healthcare data set demonstrates the superior performance of\nQB-RAG compared to existing retrieval methods, highlighting its practical value\nin building trustworthy digital health applications for health\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) for healthcare question answering\nrequires robust methods to ensure accuracy and reliability. This work\nintroduces Query-Based Retrieval Augmented Generation (QB-RAG), a framework for\nenhancing Retrieval-Augmented Generation (RAG) systems in healthcare\nquestion-answering by pre-aligning user queries with a database of curated,\nanswerable questions derived from healthcare content. A key component of QB-RAG\nis an LLM-based filtering mechanism that ensures that only relevant and\nanswerable questions are included in the database, enabling reliable reference\nquery generation at scale. We provide theoretical motivation for QB-RAG,\nconduct a comparative analysis of existing retrieval enhancement techniques,\nand introduce a generalizable, comprehensive evaluation framework that assesses\nboth the retrieval effectiveness and the quality of the generated response\nbased on faithfulness, relevance, and adherence to the guideline. Our empirical\nevaluation on a healthcare data set demonstrates the superior performance of\nQB-RAG compared to existing retrieval methods, highlighting its practical value\nin building trustworthy digital health applications for health\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Jonathan Amar"
                    },
                    {
                        "name": "Jong Ha Lee"
                    },
                    {
                        "name": "Bhawesh Kumar"
                    },
                    {
                        "name": "Yugang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Yugang Jia"
                },
                "author": "Yugang Jia",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22805v1",
                "updated": "2025-07-30T16:15:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    15,
                    22,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:15:22Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    15,
                    22,
                    2,
                    211,
                    0
                ],
                "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention"
                },
                "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA."
                },
                "authors": [
                    {
                        "name": "Yuqi Pang"
                    },
                    {
                        "name": "Bowen Yang"
                    },
                    {
                        "name": "Yun Cao"
                    },
                    {
                        "name": "Fan Rong"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Chen He"
                    }
                ],
                "author_detail": {
                    "name": "Chen He"
                },
                "author": "Chen He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08450v2",
                "updated": "2025-07-30T16:11:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    11,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-13T11:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    11,
                    25,
                    15,
                    1,
                    133,
                    0
                ],
                "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Shinya Kouda"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02744v3",
                "updated": "2025-07-31T14:02:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    2,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2024-10-03T17:55:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Neutral Residues: Revisiting Adapters for Model Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutral Residues: Revisiting Adapters for Model Extension"
                },
                "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English."
                },
                "authors": [
                    {
                        "name": "Franck Signe Talla"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Hervé Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Jégou"
                },
                "author": "Hervé Jégou",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22800v1",
                "updated": "2025-07-30T16:03:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    3,
                    21,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:03:21Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    3,
                    21,
                    2,
                    211,
                    0
                ],
                "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree\n  Search Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree\n  Search Approach"
                },
                "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy."
                },
                "authors": [
                    {
                        "name": "Rui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Rui Ren"
                },
                "author": "Rui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07052v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07052v4",
                "updated": "2025-07-30T16:00:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    0,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2023-11-13T03:36:18Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    3,
                    36,
                    18,
                    0,
                    317,
                    0
                ],
                "title": "Towards the Law of Capacity Gap in Distilling Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Law of Capacity Gap in Distilling Language Models"
                },
                "summary": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Dawei Song"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Hu"
                },
                "author": "Yan Hu",
                "arxiv_comment": "32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07052v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07052v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22789v2",
                "updated": "2025-07-31T02:18:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    2,
                    18,
                    13,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T15:55:08Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    55,
                    8,
                    2,
                    211,
                    0
                ],
                "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Core: A Simple, Scalable and Balanced RLHF Trainer"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels."
                },
                "authors": [
                    {
                        "name": "Junyu Wu"
                    },
                    {
                        "name": "Weiming Chang"
                    },
                    {
                        "name": "Xiaotao Liu"
                    },
                    {
                        "name": "Guanyou He"
                    },
                    {
                        "name": "Haoqiang Hong"
                    },
                    {
                        "name": "Boqi Liu"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Feng Lin"
                    },
                    {
                        "name": "Ting Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Yao"
                },
                "author": "Ting Yao",
                "arxiv_comment": "I haven't received company approval yet, and I uploaded it by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19073v2",
                "updated": "2025-07-30T15:54:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    54,
                    38,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-23T19:44:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    19,
                    44,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral\n  Reasoning of LLMs through Hate Speech Multi-hop Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral\n  Reasoning of LLMs through Hate Speech Multi-hop Explanations"
                },
                "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning."
                },
                "authors": [
                    {
                        "name": "Jackson Trager"
                    },
                    {
                        "name": "Diego Alves"
                    },
                    {
                        "name": "Matteo Guida"
                    },
                    {
                        "name": "Mikel K. Ngueajio"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    },
                    {
                        "name": "Flor Plaza-del-Arco"
                    },
                    {
                        "name": "Yalda Daryanai"
                    },
                    {
                        "name": "Farzan Karimi-Malekabadi"
                    },
                    {
                        "name": "Francielle Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Francielle Vargas"
                },
                "author": "Francielle Vargas",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22050v2",
                "updated": "2025-07-30T15:51:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    51,
                    29,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T17:55:23Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    17,
                    55,
                    23,
                    1,
                    210,
                    0
                ],
                "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router"
                },
                "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches. Our codes are available at\nhttps://github.com/MinghoKwok/DeepSieve."
                },
                "authors": [
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Xujiang Zhao"
                    },
                    {
                        "name": "Yanchi Liu"
                    },
                    {
                        "name": "Wenchao Yu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Wei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Cheng"
                },
                "author": "Wei Cheng",
                "arxiv_comment": "22 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22776v1",
                "updated": "2025-07-30T15:37:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    37,
                    58,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:37:58Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    37,
                    58,
                    2,
                    211,
                    0
                ],
                "title": "Label-free estimation of clinically relevant performance metrics under\n  distribution shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label-free estimation of clinically relevant performance metrics under\n  distribution shifts"
                },
                "summary": "Performance monitoring is essential for safe clinical deployment of image\nclassification models. However, because ground-truth labels are typically\nunavailable in the target dataset, direct assessment of real-world model\nperformance is infeasible. State-of-the-art performance estimation methods\naddress this by leveraging confidence scores to estimate the target accuracy.\nDespite being a promising direction, the established methods mainly estimate\nthe model's accuracy and are rarely evaluated in a clinical domain, where\nstrong class imbalances and dataset shifts are common. Our contributions are\ntwofold: First, we introduce generalisations of existing performance prediction\nmethods that directly estimate the full confusion matrix. Then, we benchmark\ntheir performance on chest x-ray data in real-world distribution shifts as well\nas simulated covariate and prevalence shifts. The proposed confusion matrix\nestimation methods reliably predicted clinically relevant counting metrics on\nmedical images under distribution shifts. However, our simulated shift\nscenarios exposed important failure modes of current performance estimation\ntechniques, calling for a better understanding of real-world deployment\ncontexts when implementing these performance monitoring techniques for\npostmarket surveillance of medical AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance monitoring is essential for safe clinical deployment of image\nclassification models. However, because ground-truth labels are typically\nunavailable in the target dataset, direct assessment of real-world model\nperformance is infeasible. State-of-the-art performance estimation methods\naddress this by leveraging confidence scores to estimate the target accuracy.\nDespite being a promising direction, the established methods mainly estimate\nthe model's accuracy and are rarely evaluated in a clinical domain, where\nstrong class imbalances and dataset shifts are common. Our contributions are\ntwofold: First, we introduce generalisations of existing performance prediction\nmethods that directly estimate the full confusion matrix. Then, we benchmark\ntheir performance on chest x-ray data in real-world distribution shifts as well\nas simulated covariate and prevalence shifts. The proposed confusion matrix\nestimation methods reliably predicted clinically relevant counting metrics on\nmedical images under distribution shifts. However, our simulated shift\nscenarios exposed important failure modes of current performance estimation\ntechniques, calling for a better understanding of real-world deployment\ncontexts when implementing these performance monitoring techniques for\npostmarket surveillance of medical AI models."
                },
                "authors": [
                    {
                        "name": "Tim Flühmann"
                    },
                    {
                        "name": "Alceu Bissoto"
                    },
                    {
                        "name": "Trung-Dung Hoang"
                    },
                    {
                        "name": "Lisa M. Koch"
                    }
                ],
                "author_detail": {
                    "name": "Lisa M. Koch"
                },
                "author": "Lisa M. Koch",
                "arxiv_comment": "Accepted oral at UNSURE 2025 @ MICCAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22772v1",
                "updated": "2025-07-30T15:35:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    35,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:35:51Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    35,
                    51,
                    2,
                    211,
                    0
                ],
                "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Evaluation of Concept Drift in ML-Based Android Malware\n  Detection"
                },
                "summary": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation."
                },
                "authors": [
                    {
                        "name": "Ahmed Sabbah"
                    },
                    {
                        "name": "Radi Jarrar"
                    },
                    {
                        "name": "Samer Zein"
                    },
                    {
                        "name": "David Mohaisen"
                    }
                ],
                "author_detail": {
                    "name": "David Mohaisen"
                },
                "author": "David Mohaisen",
                "arxiv_comment": "18 pages, 12 tables, 14 figures, paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22758v1",
                "updated": "2025-07-30T15:19:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    19,
                    38,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:19:38Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    19,
                    38,
                    2,
                    211,
                    0
                ],
                "title": "MASCA: LLM based-Multi Agents System for Credit Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASCA: LLM based-Multi Agents System for Credit Assessment"
                },
                "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring."
                },
                "authors": [
                    {
                        "name": "Gautam Jajoo"
                    },
                    {
                        "name": "Pranjal A Chitale"
                    },
                    {
                        "name": "Saksham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Saksham Agarwal"
                },
                "author": "Saksham Agarwal",
                "arxiv_comment": "Accepted at ACL REALM Workshop. Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22753v1",
                "updated": "2025-07-30T15:12:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    12,
                    12,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:12:12Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    12,
                    12,
                    2,
                    211,
                    0
                ],
                "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opportunities and Challenges of LLMs in Education: An NLP Perspective"
                },
                "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future."
                },
                "authors": [
                    {
                        "name": "Sowmya Vajjala"
                    },
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Stefano Bannò"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22752v1",
                "updated": "2025-07-30T15:10:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    10,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:10:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    10,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset"
                },
                "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering."
                },
                "authors": [
                    {
                        "name": "Jindřich Libovický"
                    },
                    {
                        "name": "Jindřich Helcl"
                    },
                    {
                        "name": "Andrei Manea"
                    },
                    {
                        "name": "Gianluca Vico"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Vico"
                },
                "author": "Gianluca Vico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22748v1",
                "updated": "2025-07-30T15:05:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T15:05:05Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    5,
                    5,
                    2,
                    211,
                    0
                ],
                "title": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index"
                },
                "summary": "We introduce the Generative AI Susceptibility Index (GAISI), a task-based\nmeasure of UK job exposure to large language models (LLMs), such as ChatGPT.\nGAISI is derived from probabilistic task ratings by LLMs and linked to\nworker-reported task data from the Skills and Employment Surveys. It reflects\nthe share of job activities where an LLM or LLM-powered system can reduce task\ncompletion time by at least 25 per cent beyond existing productivity tools. The\nindex demonstrates high reliability, strong alignment with AI capabilities, and\nsuperior predictive power compared to existing exposure measures. By 2023-24,\nnearly all UK jobs exhibited some exposure, yet only a minority were heavily\naffected. Aggregate exposure has risen since 2017, primarily due to\noccupational shifts rather than changes in task profiles. The price premium for\nAI-exposed tasks declined relative to 2017, measuring approximately 11 per cent\nlower in 2023-24. Job postings in high-exposure roles also fell by 6.5 per cent\nfollowing the release of ChatGPT. GAISI offers a robust framework for assessing\ngenerative AI's impact on work, providing early evidence that displacement\neffects may already outweigh productivity gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Generative AI Susceptibility Index (GAISI), a task-based\nmeasure of UK job exposure to large language models (LLMs), such as ChatGPT.\nGAISI is derived from probabilistic task ratings by LLMs and linked to\nworker-reported task data from the Skills and Employment Surveys. It reflects\nthe share of job activities where an LLM or LLM-powered system can reduce task\ncompletion time by at least 25 per cent beyond existing productivity tools. The\nindex demonstrates high reliability, strong alignment with AI capabilities, and\nsuperior predictive power compared to existing exposure measures. By 2023-24,\nnearly all UK jobs exhibited some exposure, yet only a minority were heavily\naffected. Aggregate exposure has risen since 2017, primarily due to\noccupational shifts rather than changes in task profiles. The price premium for\nAI-exposed tasks declined relative to 2017, measuring approximately 11 per cent\nlower in 2023-24. Job postings in high-exposure roles also fell by 6.5 per cent\nfollowing the release of ChatGPT. GAISI offers a robust framework for assessing\ngenerative AI's impact on work, providing early evidence that displacement\neffects may already outweigh productivity gains."
                },
                "authors": [
                    {
                        "name": "Golo Henseke"
                    },
                    {
                        "name": "Rhys Davies"
                    },
                    {
                        "name": "Alan Felstead"
                    },
                    {
                        "name": "Duncan Gallie"
                    },
                    {
                        "name": "Francis Green"
                    },
                    {
                        "name": "Ying Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhou"
                },
                "author": "Ying Zhou",
                "arxiv_comment": "52 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13820v3",
                "updated": "2025-07-30T14:58:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    58,
                    42,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-19T15:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "Scoring Verifiers: Evaluating Synthetic Verification for Code and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scoring Verifiers: Evaluating Synthetic Verification for Code and\n  Reasoning"
                },
                "summary": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic verification techniques such as generating test cases and reward\nmodelling are common ways to enhance the coding capabilities of large language\nmodels (LLM) beyond predefined tests. Additionally, code verification has\nrecently found great success as a critical component in improving reasoning\ncapability of LLMs via reinforcement learning. In this paper, we propose an\napproach which can transform existing coding benchmarks into scoring and\nranking datasets to evaluate the effectiveness of synthetic verifiers. We also\npropose multiple metrics to measure different aspects of the synthetic\nverifiers with the proposed benchmarks. By employing the proposed approach, we\nrelease four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed\nsynthetic verification methods with standard, reasoning-based, and reward-based\nLLMs. Our experiments show that reasoning can significantly improve test case\ngeneration and that scaling the number of test cases enhances the verification\naccuracy."
                },
                "authors": [
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22729v1",
                "updated": "2025-07-30T14:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    49,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning"
                },
                "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs."
                },
                "authors": [
                    {
                        "name": "Benedikt Roth"
                    },
                    {
                        "name": "Stephan Rappensperger"
                    },
                    {
                        "name": "Tianming Qiu"
                    },
                    {
                        "name": "Hamza Imamović"
                    },
                    {
                        "name": "Julian Wörmann"
                    },
                    {
                        "name": "Hao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shen"
                },
                "author": "Hao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22720v1",
                "updated": "2025-07-30T14:39:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    39,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:39:51Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    39,
                    51,
                    2,
                    211,
                    0
                ],
                "title": "Investigating Hallucination in Conversations for Low Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Hallucination in Conversations for Low Resource Languages"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi."
                },
                "authors": [
                    {
                        "name": "Amit Das"
                    },
                    {
                        "name": "Md. Najib Hasan"
                    },
                    {
                        "name": "Souvika Sarkar"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Fatemeh Jamshidi"
                    },
                    {
                        "name": "Tathagata Bhattacharya"
                    },
                    {
                        "name": "Nilanjana Raychawdhury"
                    },
                    {
                        "name": "Dongji Feng"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22716v1",
                "updated": "2025-07-30T14:29:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:29:44Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    29,
                    44,
                    2,
                    211,
                    0
                ],
                "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs"
                },
                "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Victor Gutierrez Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22711v1",
                "updated": "2025-07-30T14:22:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    22,
                    42,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T14:22:42Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    22,
                    42,
                    2,
                    211,
                    0
                ],
                "title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness"
                },
                "summary": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere."
                },
                "authors": [
                    {
                        "name": "Hong-Jun Yoon"
                    },
                    {
                        "name": "Mariam Kiran"
                    },
                    {
                        "name": "Danial Ebling"
                    },
                    {
                        "name": "Joe Breen"
                    }
                ],
                "author_detail": {
                    "name": "Joe Breen"
                },
                "author": "Joe Breen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01053v2",
                "updated": "2025-07-30T13:27:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    27,
                    0,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-27T16:24:17Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    16,
                    24,
                    17,
                    4,
                    178,
                    0
                ],
                "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding,\n  and Analysis"
                },
                "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight."
                },
                "authors": [
                    {
                        "name": "Rafi Al Attrach"
                    },
                    {
                        "name": "Pedro Moreira"
                    },
                    {
                        "name": "Rajna Fani"
                    },
                    {
                        "name": "Renato Umeton"
                    },
                    {
                        "name": "Leo Anthony Celi"
                    }
                ],
                "author_detail": {
                    "name": "Leo Anthony Celi"
                },
                "author": "Leo Anthony Celi",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.3; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12474v2",
                "updated": "2025-07-30T13:18:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    18,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-18T15:52:24Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    52,
                    24,
                    6,
                    138,
                    0
                ],
                "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Are They Talking About? A Benchmark of Knowledge-Grounded\n  Discussion Summarization"
                },
                "summary": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration."
                },
                "authors": [
                    {
                        "name": "Weixiao Zhou"
                    },
                    {
                        "name": "Junnan Zhu"
                    },
                    {
                        "name": "Gengyao Li"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Xinnian Liang"
                    },
                    {
                        "name": "Feifei Zhai"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "arxiv_comment": "20 pages, 17 figures and 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22659v1",
                "updated": "2025-07-30T13:17:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    17,
                    16,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T13:17:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    17,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "A Systematic Literature Review on Detecting Software Vulnerabilities\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Detecting Software Vulnerabilities\n  with Large Language Models"
                },
                "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies."
                },
                "authors": [
                    {
                        "name": "Sabrina Kaniewski"
                    },
                    {
                        "name": "Fabian Schmidt"
                    },
                    {
                        "name": "Markus Enzweiler"
                    },
                    {
                        "name": "Michael Menth"
                    },
                    {
                        "name": "Tobias Heer"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Heer"
                },
                "author": "Tobias Heer",
                "arxiv_comment": "36 pages + 17 pages references, 6 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22656v1",
                "updated": "2025-07-30T13:15:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    15,
                    37,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T13:15:37Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    15,
                    37,
                    2,
                    211,
                    0
                ],
                "title": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Scale Spatial Attention Network for Near-field MIMO Channel\n  Estimation"
                },
                "summary": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of extremely large-scale array (ELAA) brings higher spectral\nefficiency and spatial degree of freedom, but triggers issues on near-field\nchannel estimation.\n  Existing near-field channel estimation schemes primarily exploit sparsity in\nthe transform domain.\n  However, these schemes are sensitive to the transform matrix selection and\nthe stopping criteria.\n  Inspired by the success of deep learning (DL) in far-field channel\nestimation, this paper proposes a novel spatial-attention-based method for\nreconstructing extremely large-scale MIMO (XL-MIMO) channel.\n  Initially, the spatial antenna correlations of near-field channels are\nanalyzed as an expectation over the angle-distance space, which demonstrate\ncorrelation range of an antenna element varies with its position.\n  Due to the strong correlation between adjacent antenna elements, interactions\nof inter-subchannel are applied to describe inherent correlation of near-field\nchannels instead of inter-element.\n  Subsequently, a multi-scale spatial attention network (MsSAN) with the\ninter-subchannel correlation learning capabilities is proposed tailed to\nnear-field MIMO channel estimation.\n  We employ the multi-scale architecture to refine the subchannel size in\nMsSAN.\n  Specially, we inventively introduce the sum of dot products as spatial\nattention (SA) instead of cross-covariance to weight subchannel features at\ndifferent scales in the SA module.\n  Simulation results are presented to validate the proposed MsSAN achieves\nremarkable the inter-subchannel correlation learning capabilities and\noutperforms others in terms of near-field channel reconstruction."
                },
                "authors": [
                    {
                        "name": "Zhiming Zhu"
                    },
                    {
                        "name": "Shu Xu"
                    },
                    {
                        "name": "Jiexin Zhang"
                    },
                    {
                        "name": "Chunguo Li"
                    },
                    {
                        "name": "Yongming Huang"
                    },
                    {
                        "name": "Luxi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Luxi Yang"
                },
                "author": "Luxi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02612v2",
                "updated": "2025-07-30T13:15:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    15,
                    26,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-03T14:12:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    12,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Visual Autoregressive Models for Subject-Driven Generation"
                },
                "summary": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive (VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, naive\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize minor\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image generative models have enabled numerous\npractical applications, including subject-driven generation, which fine-tunes\npretrained models to capture subject semantics from only a few examples. While\ndiffusion-based models produce high-quality images, their extensive denoising\nsteps result in significant computational overhead, limiting real-world\napplicability. Visual autoregressive (VAR) models, which predict next-scale\ntokens rather than spatially adjacent ones, offer significantly faster\ninference suitable for practical deployment. In this paper, we propose the\nfirst VAR-based approach for subject-driven generation. However, naive\nfine-tuning VAR leads to computational overhead, language drift, and reduced\ndiversity. To address these challenges, we introduce selective layer tuning to\nreduce complexity and prior distillation to mitigate language drift.\nAdditionally, we found that the early stages have a greater influence on the\ngeneration of subject than the latter stages, which merely synthesize minor\ndetails. Based on this finding, we propose scale-wise weighted tuning, which\nprioritizes coarser resolutions for promoting the model to focus on the\nsubject-relevant information instead of local details. Extensive experiments\nvalidate that our method significantly outperforms diffusion-based baselines\nacross various metrics and demonstrates its practical usage."
                },
                "authors": [
                    {
                        "name": "Jiwoo Chung"
                    },
                    {
                        "name": "Sangeek Hyun"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Eunseo Koh"
                    },
                    {
                        "name": "MinKyu Lee"
                    },
                    {
                        "name": "Jae-Pil Heo"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Pil Heo"
                },
                "author": "Jae-Pil Heo",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://jiwoogit.github.io/ARBooth/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04858v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04858v3",
                "updated": "2025-07-30T13:13:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    13,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-07T09:14:47Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    14,
                    47,
                    0,
                    97,
                    0
                ],
                "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG"
                },
                "summary": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial patch attacks pose a major threat to vision systems by embedding\nlocalized perturbations that mislead deep models. Traditional defense methods\noften require retraining or fine-tuning, making them impractical for real-world\ndeployment. We propose a training-free Visual Retrieval-Augmented Generation\n(VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial\npatch detection. By retrieving visually similar patches and images that\nresemble stored attacks in a continuously expanding database, VRAG performs\ngenerative reasoning to identify diverse attack types, all without additional\ntraining or fine-tuning. We extensively evaluate open-source large-scale VLMs,\nincluding Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside\nGemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO\nmodel achieves up to 95 percent classification accuracy, setting a new\nstate-of-the-art for open-source adversarial patch detection. Gemini-2.0\nattains the highest overall accuracy, 98 percent, but remains closed-source.\nExperimental results demonstrate VRAG's effectiveness in identifying a variety\nof adversarial patches with minimal human annotation, paving the way for\nrobust, practical defenses against evolving adversarial patch attacks."
                },
                "authors": [
                    {
                        "name": "Roie Kazoom"
                    },
                    {
                        "name": "Raz Lapid"
                    },
                    {
                        "name": "Moshe Sipper"
                    },
                    {
                        "name": "Ofer Hadar"
                    }
                ],
                "author_detail": {
                    "name": "Ofer Hadar"
                },
                "author": "Ofer Hadar",
                "arxiv_comment": "Accepted at VecDB @ ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04858v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04858v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22653v2",
                "updated": "2025-07-31T03:22:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    22,
                    12,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T13:12:54Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    12,
                    54,
                    2,
                    211,
                    0
                ],
                "title": "UniLegs: Universal Multi-Legged Robot Control through\n  Morphology-Agnostic Policy Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniLegs: Universal Multi-Legged Robot Control through\n  Morphology-Agnostic Policy Distillation"
                },
                "summary": "Developing controllers that generalize across diverse robot morphologies\nremains a significant challenge in legged locomotion. Traditional approaches\neither create specialized controllers for each morphology or compromise\nperformance for generality. This paper introduces a two-stage teacher-student\nframework that bridges this gap through policy distillation. First, we train\nspecialized teacher policies optimized for individual morphologies, capturing\nthe unique optimal control strategies for each robot design. Then, we distill\nthis specialized expertise into a single Transformer-based student policy\ncapable of controlling robots with varying leg configurations. Our experiments\nacross five distinct legged morphologies demonstrate that our approach\npreserves morphology-specific optimal behaviors, with the Transformer\narchitecture achieving 94.47% of teacher performance on training morphologies\nand 72.64% on unseen robot designs. Comparative analysis reveals that\nTransformer-based architectures consistently outperform MLP baselines by\nleveraging attention mechanisms to effectively model joint relationships across\ndifferent kinematic structures. We validate our approach through successful\ndeployment on a physical quadruped robot, demonstrating the practical viability\nof our morphology-agnostic control framework. This work presents a scalable\nsolution for developing universal legged robot controllers that maintain\nnear-optimal performance while generalizing across diverse morphologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing controllers that generalize across diverse robot morphologies\nremains a significant challenge in legged locomotion. Traditional approaches\neither create specialized controllers for each morphology or compromise\nperformance for generality. This paper introduces a two-stage teacher-student\nframework that bridges this gap through policy distillation. First, we train\nspecialized teacher policies optimized for individual morphologies, capturing\nthe unique optimal control strategies for each robot design. Then, we distill\nthis specialized expertise into a single Transformer-based student policy\ncapable of controlling robots with varying leg configurations. Our experiments\nacross five distinct legged morphologies demonstrate that our approach\npreserves morphology-specific optimal behaviors, with the Transformer\narchitecture achieving 94.47% of teacher performance on training morphologies\nand 72.64% on unseen robot designs. Comparative analysis reveals that\nTransformer-based architectures consistently outperform MLP baselines by\nleveraging attention mechanisms to effectively model joint relationships across\ndifferent kinematic structures. We validate our approach through successful\ndeployment on a physical quadruped robot, demonstrating the practical viability\nof our morphology-agnostic control framework. This work presents a scalable\nsolution for developing universal legged robot controllers that maintain\nnear-optimal performance while generalizing across diverse morphologies."
                },
                "authors": [
                    {
                        "name": "Weijie Xi"
                    },
                    {
                        "name": "Zhanxiang Cao"
                    },
                    {
                        "name": "Chenlin Ming"
                    },
                    {
                        "name": "Jianying Zheng"
                    },
                    {
                        "name": "Guyue Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guyue Zhou"
                },
                "author": "Guyue Zhou",
                "arxiv_comment": "6 pages, 3 figures, IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16440v2",
                "updated": "2025-07-30T13:06:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    13,
                    6,
                    51,
                    2,
                    211,
                    0
                ],
                "published": "2024-08-29T11:05:54Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    11,
                    5,
                    54,
                    3,
                    242,
                    0
                ],
                "title": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models for Machine Translation in the\n  Medical Domain"
                },
                "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics."
                },
                "authors": [
                    {
                        "name": "Miguel Rios"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Rios"
                },
                "author": "Miguel Rios",
                "arxiv_comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models\n  for Machine Translation in the Medical Domain. In Proceedings of Machine\n  Translation Summit XX Volume 1, pages 162-172",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22640v1",
                "updated": "2025-07-30T12:58:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    58,
                    2,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:58:02Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    58,
                    2,
                    2,
                    211,
                    0
                ],
                "title": "Safe Deployment of Offline Reinforcement Learning via Input Convex\n  Action Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Deployment of Offline Reinforcement Learning via Input Convex\n  Action Correction"
                },
                "summary": "Offline reinforcement learning (offline RL) offers a promising framework for\ndeveloping control strategies in chemical process systems using historical\ndata, without the risks or costs of online experimentation. This work\ninvestigates the application of offline RL to the safe and efficient control of\nan exothermic polymerisation continuous stirred-tank reactor. We introduce a\nGymnasium-compatible simulation environment that captures the reactor's\nnonlinear dynamics, including reaction kinetics, energy balances, and\noperational constraints. The environment supports three industrially relevant\nscenarios: startup, grade change down, and grade change up. It also includes\nreproducible offline datasets generated from proportional-integral controllers\nwith randomised tunings, providing a benchmark for evaluating offline RL\nalgorithms in realistic process control tasks.\n  We assess behaviour cloning and implicit Q-learning as baseline algorithms,\nhighlighting the challenges offline agents face, including steady-state offsets\nand degraded performance near setpoints. To address these issues, we propose a\nnovel deployment-time safety layer that performs gradient-based action\ncorrection using input convex neural networks (PICNNs) as learned cost models.\nThe PICNN enables real-time, differentiable correction of policy actions by\ndescending a convex, state-conditioned cost surface, without requiring\nretraining or environment interaction.\n  Experimental results show that offline RL, particularly when combined with\nconvex action correction, can outperform traditional control approaches and\nmaintain stability across all scenarios. These findings demonstrate the\nfeasibility of integrating offline RL with interpretable and safety-aware\ncorrections for high-stakes chemical process control, and lay the groundwork\nfor more reliable data-driven automation in industrial systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (offline RL) offers a promising framework for\ndeveloping control strategies in chemical process systems using historical\ndata, without the risks or costs of online experimentation. This work\ninvestigates the application of offline RL to the safe and efficient control of\nan exothermic polymerisation continuous stirred-tank reactor. We introduce a\nGymnasium-compatible simulation environment that captures the reactor's\nnonlinear dynamics, including reaction kinetics, energy balances, and\noperational constraints. The environment supports three industrially relevant\nscenarios: startup, grade change down, and grade change up. It also includes\nreproducible offline datasets generated from proportional-integral controllers\nwith randomised tunings, providing a benchmark for evaluating offline RL\nalgorithms in realistic process control tasks.\n  We assess behaviour cloning and implicit Q-learning as baseline algorithms,\nhighlighting the challenges offline agents face, including steady-state offsets\nand degraded performance near setpoints. To address these issues, we propose a\nnovel deployment-time safety layer that performs gradient-based action\ncorrection using input convex neural networks (PICNNs) as learned cost models.\nThe PICNN enables real-time, differentiable correction of policy actions by\ndescending a convex, state-conditioned cost surface, without requiring\nretraining or environment interaction.\n  Experimental results show that offline RL, particularly when combined with\nconvex action correction, can outperform traditional control approaches and\nmaintain stability across all scenarios. These findings demonstrate the\nfeasibility of integrating offline RL with interpretable and safety-aware\ncorrections for high-stakes chemical process control, and lay the groundwork\nfor more reliable data-driven automation in industrial systems."
                },
                "authors": [
                    {
                        "name": "Alex Durkin"
                    },
                    {
                        "name": "Jasper Stolte"
                    },
                    {
                        "name": "Matthew Jones"
                    },
                    {
                        "name": "Raghuraman Pitchumani"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Christian Michler"
                    },
                    {
                        "name": "Mehmet Mercangöz"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Mercangöz"
                },
                "author": "Mehmet Mercangöz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22623v1",
                "updated": "2025-07-30T12:42:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:42:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "Multilingual Political Views of Large Language Models: Identification\n  and Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Political Views of Large Language Models: Identification\n  and Steering"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Roberto Zamparelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Zamparelli"
                },
                "author": "Roberto Zamparelli",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22619v1",
                "updated": "2025-07-30T12:39:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    39,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:39:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    39,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "Enhancing Manufacturing Knowledge Access with LLMs and Context-aware\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Manufacturing Knowledge Access with LLMs and Context-aware\n  Prompting"
                },
                "summary": "Knowledge graphs (KGs) have transformed data management within the\nmanufacturing industry, offering effective means for integrating disparate data\nsources through shared and structured conceptual schemas. However, harnessing\nthe power of KGs can be daunting for non-experts, as it often requires\nformulating complex SPARQL queries to retrieve specific information. With the\nadvent of Large Language Models (LLMs), there is a growing potential to\nautomatically translate natural language queries into the SPARQL format, thus\nbridging the gap between user-friendly interfaces and the sophisticated\narchitecture of KGs. The challenge remains in adequately informing LLMs about\nthe relevant context and structure of domain-specific KGs, e.g., in\nmanufacturing, to improve the accuracy of generated queries. In this paper, we\nevaluate multiple strategies that use LLMs as mediators to facilitate\ninformation retrieval from KGs. We focus on the manufacturing domain,\nparticularly on the Bosch Line Information System KG and the I40 Core\nInformation Model. In our evaluation, we compare various approaches for feeding\nrelevant context from the KG to the LLM and analyze their proficiency in\ntransforming real-world questions into SPARQL queries. Our findings show that\nLLMs can significantly improve their performance on generating correct and\ncomplete queries when provided only the adequate context of the KG schema. Such\ncontext-aware prompting techniques help LLMs to focus on the relevant parts of\nthe ontology and reduce the risk of hallucination. We anticipate that the\nproposed techniques help LLMs to democratize access to complex data\nrepositories and empower informed decision-making in manufacturing settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs) have transformed data management within the\nmanufacturing industry, offering effective means for integrating disparate data\nsources through shared and structured conceptual schemas. However, harnessing\nthe power of KGs can be daunting for non-experts, as it often requires\nformulating complex SPARQL queries to retrieve specific information. With the\nadvent of Large Language Models (LLMs), there is a growing potential to\nautomatically translate natural language queries into the SPARQL format, thus\nbridging the gap between user-friendly interfaces and the sophisticated\narchitecture of KGs. The challenge remains in adequately informing LLMs about\nthe relevant context and structure of domain-specific KGs, e.g., in\nmanufacturing, to improve the accuracy of generated queries. In this paper, we\nevaluate multiple strategies that use LLMs as mediators to facilitate\ninformation retrieval from KGs. We focus on the manufacturing domain,\nparticularly on the Bosch Line Information System KG and the I40 Core\nInformation Model. In our evaluation, we compare various approaches for feeding\nrelevant context from the KG to the LLM and analyze their proficiency in\ntransforming real-world questions into SPARQL queries. Our findings show that\nLLMs can significantly improve their performance on generating correct and\ncomplete queries when provided only the adequate context of the KG schema. Such\ncontext-aware prompting techniques help LLMs to focus on the relevant parts of\nthe ontology and reduce the risk of hallucination. We anticipate that the\nproposed techniques help LLMs to democratize access to complex data\nrepositories and empower informed decision-making in manufacturing settings."
                },
                "authors": [
                    {
                        "name": "Sebastian Monka"
                    },
                    {
                        "name": "Irlan Grangel-González"
                    },
                    {
                        "name": "Stefan Schmid"
                    },
                    {
                        "name": "Lavdim Halilaj"
                    },
                    {
                        "name": "Marc Rickart"
                    },
                    {
                        "name": "Oliver Rudolph"
                    },
                    {
                        "name": "Rui Dias"
                    }
                ],
                "author_detail": {
                    "name": "Rui Dias"
                },
                "author": "Rui Dias",
                "arxiv_comment": "European Conference on Artificial Intelligence (ECAI) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22608v1",
                "updated": "2025-07-30T12:23:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:23:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    23,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Arithmetics: Towards Systematic Language Neuron Identification\n  and Manipulation"
                },
                "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Yusser Al Ghussin"
                    },
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22606v1",
                "updated": "2025-07-30T12:22:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    22,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:22:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    22,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on\n  Finite State Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaAgent: Automatically Constructing Multi-Agent Systems Based on\n  Finite State Machines"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to solve a wide\nrange of practical tasks within multi-agent systems. However, existing\nhuman-designed multi-agent frameworks are typically limited to a small set of\npre-defined scenarios, while current automated design methods suffer from\nseveral limitations, such as the lack of tool integration, dependence on\nexternal training data, and rigid communication structures. In this paper, we\npropose MetaAgent, a finite state machine based framework that can\nautomatically generate a multi-agent system. Given a task description,\nMetaAgent will design a multi-agent system and polish it through an\noptimization algorithm. When the multi-agent system is deployed, the finite\nstate machine will control the agent's actions and the state transitions. To\nevaluate our framework, we conduct experiments on both text-based tasks and\npractical tasks. The results indicate that the generated multi-agent system\nsurpasses other auto-designed methods and can achieve a comparable performance\nwith the human-designed multi-agent system, which is optimized for those\nspecific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to solve a wide\nrange of practical tasks within multi-agent systems. However, existing\nhuman-designed multi-agent frameworks are typically limited to a small set of\npre-defined scenarios, while current automated design methods suffer from\nseveral limitations, such as the lack of tool integration, dependence on\nexternal training data, and rigid communication structures. In this paper, we\npropose MetaAgent, a finite state machine based framework that can\nautomatically generate a multi-agent system. Given a task description,\nMetaAgent will design a multi-agent system and polish it through an\noptimization algorithm. When the multi-agent system is deployed, the finite\nstate machine will control the agent's actions and the state transitions. To\nevaluate our framework, we conduct experiments on both text-based tasks and\npractical tasks. The results indicate that the generated multi-agent system\nsurpasses other auto-designed methods and can achieve a comparable performance\nwith the human-designed multi-agent system, which is optimized for those\nspecific tasks."
                },
                "authors": [
                    {
                        "name": "Yaolun Zhang"
                    },
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22603v1",
                "updated": "2025-07-30T12:16:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    16,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:16:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    16,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALSAM: A Platform for Benchmarking Arabic Large Language Models"
                },
                "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities."
                },
                "authors": [
                    {
                        "name": "Rawan Al-Matham"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Raghad Al-Rasheed"
                    },
                    {
                        "name": "Waad Alshammari"
                    },
                    {
                        "name": "Muneera Alhoshan"
                    },
                    {
                        "name": "Amal Almazrua"
                    },
                    {
                        "name": "Asma Al Wazrah"
                    },
                    {
                        "name": "Mais Alheraki"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Norah Alzahrani"
                    },
                    {
                        "name": "Eman alBilali"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Abdelrahman El-Sheikh"
                    },
                    {
                        "name": "Muhammad Elmallah"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Hamdy Mubarak"
                    },
                    {
                        "name": "Mohamed Anwar"
                    },
                    {
                        "name": "Zaid Alyafeai"
                    },
                    {
                        "name": "Ahmed Abdelali"
                    },
                    {
                        "name": "Nora Altwairesh"
                    },
                    {
                        "name": "Maram Hasanain"
                    },
                    {
                        "name": "Abdulmohsen Al Thubaity"
                    },
                    {
                        "name": "Shady Shehata"
                    },
                    {
                        "name": "Bashar Alhafni"
                    },
                    {
                        "name": "Injy Hamed"
                    },
                    {
                        "name": "Go Inoue"
                    },
                    {
                        "name": "Khalid Elmadani"
                    },
                    {
                        "name": "Ossama Obeid"
                    },
                    {
                        "name": "Fatima Haouari"
                    },
                    {
                        "name": "Tamer Elsayed"
                    },
                    {
                        "name": "Emad Alghamdi"
                    },
                    {
                        "name": "Khalid Almubarak"
                    },
                    {
                        "name": "Saied Alshahrani"
                    },
                    {
                        "name": "Ola Aljarrah"
                    },
                    {
                        "name": "Safa Alajlan"
                    },
                    {
                        "name": "Areej Alshaqarawi"
                    },
                    {
                        "name": "Maryam Alshihri"
                    },
                    {
                        "name": "Sultana Alghurabi"
                    },
                    {
                        "name": "Atikah Alzeghayer"
                    },
                    {
                        "name": "Afrah Altamimi"
                    },
                    {
                        "name": "Abdullah Alfaifi"
                    },
                    {
                        "name": "Abdulrahman AlOsaimy"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman AlOsaimy"
                },
                "author": "Abdulrahman AlOsaimy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10078v2",
                "updated": "2025-07-30T11:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    57,
                    54,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-14T09:03:44Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    3,
                    44,
                    0,
                    195,
                    0
                ],
                "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$\n  Optimal Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Method for Deep Diagonal State Space Model Based on $H^2$\n  Optimal Reduction"
                },
                "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models."
                },
                "authors": [
                    {
                        "name": "Hiroki Sakamoto"
                    },
                    {
                        "name": "Kazuhiro Sato"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Sato"
                },
                "author": "Kazuhiro Sato",
                "arxiv_comment": "Accepted to IEEE Control Systems Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15586v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15586v4",
                "updated": "2025-07-30T11:51:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    51,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-21T13:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    13,
                    3,
                    55,
                    0,
                    202,
                    0
                ],
                "title": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Extract Rational Evidence via Reinforcement Learning for\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose EviOmni, which learns to extract\nrational evidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of EviOmni, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems."
                },
                "authors": [
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Shouzheng Huang"
                    },
                    {
                        "name": "Yan Zhong"
                    },
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "16 pages, 7 Figures, 10 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15586v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15586v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07214v3",
                "updated": "2025-07-30T11:37:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    37,
                    10,
                    2,
                    211,
                    0
                ],
                "published": "2024-02-20T18:57:34Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    18,
                    57,
                    34,
                    1,
                    51,
                    0
                ],
                "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Frontier of Vision-Language Models: A Survey of Current\n  Methodologies and Future Directions"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly reshaped the\ntrajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable\nlimitation, as they are primarily adept at processing textual information. To\naddress this constraint, researchers have endeavored to integrate visual\ncapabilities with LLMs, resulting in the emergence of Vision-Language Models\n(VLMs). These advanced models are instrumental in tackling more intricate tasks\nsuch as image captioning and visual question answering. In our comprehensive\nsurvey paper, we delve into the key advancements within the realm of VLMs. Our\nclassification organizes VLMs into three distinct categories: models dedicated\nto vision-language understanding, models that process multimodal inputs to\ngenerate unimodal (textual) outputs and models that both accept and produce\nmultimodal inputs and outputs.This classification is based on their respective\ncapabilities and functionalities in processing and generating various\nmodalities of data.We meticulously dissect each model, offering an extensive\nanalysis of its foundational architecture, training data sources, as well as\nits strengths and limitations wherever possible, providing readers with a\ncomprehensive understanding of its essential components. We also analyzed the\nperformance of VLMs in various benchmark datasets. By doing so, we aim to offer\na nuanced understanding of the diverse landscape of VLMs. Additionally, we\nunderscore potential avenues for future research in this dynamic domain,\nanticipating further breakthroughs and advancements."
                },
                "authors": [
                    {
                        "name": "Akash Ghosh"
                    },
                    {
                        "name": "Arkadeep Acharya"
                    },
                    {
                        "name": "Sriparna Saha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "One of the first survey on Visual Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22581v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22581v2",
                "updated": "2025-07-31T03:32:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    32,
                    19,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T11:23:30Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    23,
                    30,
                    2,
                    211,
                    0
                ],
                "title": "Unveiling the Influence of Amplifying Language-Specific Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Influence of Amplifying Language-Specific Neurons"
                },
                "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer."
                },
                "authors": [
                    {
                        "name": "Inaya Rahmanisa"
                    },
                    {
                        "name": "Lyzander Marciano Andrylie"
                    },
                    {
                        "name": "Mahardika Krisna Ihsani"
                    },
                    {
                        "name": "Alfan Farizki Wicaksono"
                    },
                    {
                        "name": "Haryo Akbarianto Wibowo"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "arxiv_comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22581v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22581v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22580v1",
                "updated": "2025-07-30T11:21:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    21,
                    9,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T11:21:09Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    11,
                    21,
                    9,
                    2,
                    211,
                    0
                ],
                "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated\n  Patch Correctness Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated\n  Patch Correctness Assessment"
                },
                "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability."
                },
                "authors": [
                    {
                        "name": "Marcos Fuster-Pena"
                    },
                    {
                        "name": "David de-Fitero-Dominguez"
                    },
                    {
                        "name": "Antonio Garcia-Cabot"
                    },
                    {
                        "name": "Eva Garcia-Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Eva Garcia-Lopez"
                },
                "author": "Eva Garcia-Lopez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22565v1",
                "updated": "2025-07-30T10:46:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    46,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:46:53Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    46,
                    53,
                    2,
                    211,
                    0
                ],
                "title": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement\n  Learning"
                },
                "summary": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tension between data privacy and model utility has become the defining\nbottleneck for the practical deployment of large language models (LLMs) trained\non sensitive corpora including healthcare. Differentially private stochastic\ngradient descent (DP-SGD) guarantees formal privacy, yet it does so at a\npronounced cost: gradients are forcibly clipped and perturbed with noise,\ndegrading sample efficiency and final accuracy. Numerous variants have been\nproposed to soften this trade-off, but they all share a handicap: their control\nknobs are hard-coded, global, and oblivious to the evolving optimization\nlandscape. Consequently, practitioners are forced either to over-spend privacy\nbudget in pursuit of utility, or to accept mediocre models in order to stay\nwithin privacy constraints. We present RLDP, the first framework to cast DP\noptimization itself as a closed-loop control problem amenable to modern deep\nreinforcement learning (RL). RLDP continuously senses rich statistics of the\nlearning dynamics and acts by selecting fine-grained per parameter\ngradient-clipping thresholds as well as the magnitude of injected Gaussian\nnoise. A soft actor-critic (SAC) hyper-policy is trained online during language\nmodel fine-tuning; it learns, from scratch, how to allocate the privacy budget\nwhere it matters and when it matters. Across more than 1,600 ablation\nexperiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers\nperplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream\nutility gain. RLDP reaches each baseline's final utility after only 13-43% of\nthe gradient-update budget (mean speed-up 71%), all while honoring the same\n($\\epsilon$, $\\delta$)-DP contract and exhibiting equal or lower susceptibility\nto membership-inference and canary-extraction attacks."
                },
                "authors": [
                    {
                        "name": "Afshin Khadangi"
                    },
                    {
                        "name": "Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Ramin Bahmani"
                    },
                    {
                        "name": "Gilbert Fridgen"
                    }
                ],
                "author_detail": {
                    "name": "Gilbert Fridgen"
                },
                "author": "Gilbert Fridgen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22564v1",
                "updated": "2025-07-30T10:40:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    53,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:40:53Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    53,
                    2,
                    211,
                    0
                ],
                "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems."
                },
                "authors": [
                    {
                        "name": "Xikang Yang"
                    },
                    {
                        "name": "Biyu Zhou"
                    },
                    {
                        "name": "Xuehai Tang"
                    },
                    {
                        "name": "Jizhong Han"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16936v2",
                "updated": "2025-07-30T10:40:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    40,
                    50,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-22T09:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    9,
                    14,
                    35,
                    6,
                    357,
                    0
                ],
                "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-guided Prompting for Knowledge-based Visual Question Answering"
                },
                "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively."
                },
                "authors": [
                    {
                        "name": "Zhongjian Hu"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Fengyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fengyuan Liu"
                },
                "author": "Fengyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22558v1",
                "updated": "2025-07-30T10:32:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    32,
                    39,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:32:39Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    32,
                    39,
                    2,
                    211,
                    0
                ],
                "title": "aLLoyM: A large language model for alloy phase diagram prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "aLLoyM: A large language model for alloy phase diagram prediction"
                },
                "summary": "Large Language Models (LLMs) are general-purpose tools with wide-ranging\napplications, including in materials science. In this work, we introduce\naLLoyM, a fine-tuned LLM specifically trained on alloy compositions,\ntemperatures, and their corresponding phase information. To develop aLLoyM, we\ncurated question-and-answer (Q&A) pairs for binary and ternary phase diagrams\nusing the open-source Computational Phase Diagram Database (CPDDB) and\nassessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned\nMistral, an open-source pre-trained LLM, for two distinct Q&A formats:\nmultiple-choice and short-answer. Benchmark evaluations demonstrate that\nfine-tuning substantially enhances performance on multiple-choice phase diagram\nquestions. Moreover, the short-answer model of aLLoyM exhibits the ability to\ngenerate novel phase diagrams from its components alone, underscoring its\npotential to accelerate the discovery of previously unexplored materials\nsystems. To promote further research and adoption, we have publicly released\nthe short-answer fine-tuned version of aLLoyM, along with the complete\nbenchmarking Q&A dataset, on Hugging Face.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are general-purpose tools with wide-ranging\napplications, including in materials science. In this work, we introduce\naLLoyM, a fine-tuned LLM specifically trained on alloy compositions,\ntemperatures, and their corresponding phase information. To develop aLLoyM, we\ncurated question-and-answer (Q&A) pairs for binary and ternary phase diagrams\nusing the open-source Computational Phase Diagram Database (CPDDB) and\nassessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned\nMistral, an open-source pre-trained LLM, for two distinct Q&A formats:\nmultiple-choice and short-answer. Benchmark evaluations demonstrate that\nfine-tuning substantially enhances performance on multiple-choice phase diagram\nquestions. Moreover, the short-answer model of aLLoyM exhibits the ability to\ngenerate novel phase diagrams from its components alone, underscoring its\npotential to accelerate the discovery of previously unexplored materials\nsystems. To promote further research and adoption, we have publicly released\nthe short-answer fine-tuned version of aLLoyM, along with the complete\nbenchmarking Q&A dataset, on Hugging Face."
                },
                "authors": [
                    {
                        "name": "Yuna Oikawa"
                    },
                    {
                        "name": "Guillaume Deffrennes"
                    },
                    {
                        "name": "Taichi Abe"
                    },
                    {
                        "name": "Ryo Tamura"
                    },
                    {
                        "name": "Koji Tsuda"
                    }
                ],
                "author_detail": {
                    "name": "Koji Tsuda"
                },
                "author": "Koji Tsuda",
                "arxiv_comment": "24 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22545v1",
                "updated": "2025-07-30T10:17:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    17,
                    7,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:17:07Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    17,
                    7,
                    2,
                    211,
                    0
                ],
                "title": "ControlMed: Adding Reasoning Control to Medical Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlMed: Adding Reasoning Control to Medical Language Model"
                },
                "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis."
                },
                "authors": [
                    {
                        "name": "Sung-Min Lee"
                    },
                    {
                        "name": "Siyoon Lee"
                    },
                    {
                        "name": "Juyeon Kim"
                    },
                    {
                        "name": "Kyungmin Roh"
                    }
                ],
                "author_detail": {
                    "name": "Kyungmin Roh"
                },
                "author": "Kyungmin Roh",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22542v1",
                "updated": "2025-07-30T10:14:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    14,
                    31,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:14:31Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    14,
                    31,
                    2,
                    211,
                    0
                ],
                "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large\n  Language Models in Customer Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large\n  Language Models in Customer Support"
                },
                "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA."
                },
                "authors": [
                    {
                        "name": "Long S. T. Nguyen"
                    },
                    {
                        "name": "Truong P. Hua"
                    },
                    {
                        "name": "Thanh M. Nguyen"
                    },
                    {
                        "name": "Toan Q. Pham"
                    },
                    {
                        "name": "Nam K. Ngo"
                    },
                    {
                        "name": "An X. Nguyen"
                    },
                    {
                        "name": "Nghi D. M. Pham"
                    },
                    {
                        "name": "Nghia H. Nguyen"
                    },
                    {
                        "name": "Tho T. Quan"
                    }
                ],
                "author_detail": {
                    "name": "Tho T. Quan"
                },
                "author": "Tho T. Quan",
                "arxiv_comment": "Under review at ICCCI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22538v1",
                "updated": "2025-07-30T10:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    6,
                    35,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:06:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    6,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "Inside madupite: Technical Design and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside madupite: Technical Design and Performance"
                },
                "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes."
                },
                "authors": [
                    {
                        "name": "Matilde Gargiani"
                    },
                    {
                        "name": "Robin Sieber"
                    },
                    {
                        "name": "Philip Pawlowsky"
                    },
                    {
                        "name": "John Lygeros"
                    }
                ],
                "author_detail": {
                    "name": "John Lygeros"
                },
                "author": "John Lygeros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22533v1",
                "updated": "2025-07-30T10:02:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    2,
                    16,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T10:02:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    10,
                    2,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for\n  Decision Support over Longitudinal Cancer Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliCARE: Grounding Large Language Models in Clinical Guidelines for\n  Decision Support over Longitudinal Cancer Electronic Health Records"
                },
                "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists."
                },
                "authors": [
                    {
                        "name": "Dongchen Li"
                    },
                    {
                        "name": "Jitao Liang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Longbing Cao"
                    },
                    {
                        "name": "Kun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yu"
                },
                "arxiv_affiliation": "College of Medicine and Biological Information Engineering, Northeastern University, Shenyang, China",
                "author": "Kun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15562v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15562v2",
                "updated": "2025-07-30T09:53:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    9,
                    53,
                    31,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-18T15:36:37Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    36,
                    37,
                    2,
                    169,
                    0
                ],
                "title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and\n  Efficient Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and\n  Efficient Attention"
                },
                "summary": "Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment."
                },
                "authors": [
                    {
                        "name": "Syed Haider Ali"
                    },
                    {
                        "name": "Asrar Ahmad"
                    },
                    {
                        "name": "Muhammad Ali"
                    },
                    {
                        "name": "Asifullah Khan"
                    },
                    {
                        "name": "Nadeem Shaukat"
                    }
                ],
                "author_detail": {
                    "name": "Nadeem Shaukat"
                },
                "author": "Nadeem Shaukat",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15562v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15562v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6; I.2.6; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21572v2",
                "updated": "2025-07-30T08:58:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    58,
                    30,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T08:01:37Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    8,
                    1,
                    37,
                    1,
                    210,
                    0
                ],
                "title": "No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for\n  Real-time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Redundancy, No Stall: Lightweight Streaming 3D Gaussian Splatting for\n  Real-time Rendering"
                },
                "summary": "3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and\nis getting increasing adoption in domains like autonomous driving and embodied\nintelligence. However, 3DGS still faces major efficiency challenges when faced\nwith high frame rate requirements and resource-constrained edge deployment. To\nenable efficient 3DGS, in this paper, we propose LS-Gaussian, an\nalgorithm/hardware co-design framework for lightweight streaming 3D rendering.\nLS-Gaussian is motivated by the core observation that 3DGS suffers from\nsubstantial computation redundancy and stalls. On one hand, in practical\nscenarios, high-frame-rate 3DGS is often applied in settings where a camera\nobserves and renders the same scene continuously but from slightly different\nviewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian\nproposes a viewpoint transformation algorithm that leverages inter-frame\ncontinuity for efficient sparse rendering. On the other hand, as different\ntiles within an image are rendered in parallel but have imbalanced workloads,\nfrequent hardware stalls also slow down the rendering process. LS-Gaussian\npredicts the workload for each tile based on viewpoint transformation to enable\nmore balanced parallel computation and co-designs a customized 3DGS accelerator\nto support the workload-aware mapping in real-time. Experimental results\ndemonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline\non average and up to 17.3x speedup with the customized accelerator, while\nincurring only minimal visual quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) enables high-quality rendering of 3D scenes and\nis getting increasing adoption in domains like autonomous driving and embodied\nintelligence. However, 3DGS still faces major efficiency challenges when faced\nwith high frame rate requirements and resource-constrained edge deployment. To\nenable efficient 3DGS, in this paper, we propose LS-Gaussian, an\nalgorithm/hardware co-design framework for lightweight streaming 3D rendering.\nLS-Gaussian is motivated by the core observation that 3DGS suffers from\nsubstantial computation redundancy and stalls. On one hand, in practical\nscenarios, high-frame-rate 3DGS is often applied in settings where a camera\nobserves and renders the same scene continuously but from slightly different\nviewpoints. Therefore, instead of rendering each frame separately, LS-Gaussian\nproposes a viewpoint transformation algorithm that leverages inter-frame\ncontinuity for efficient sparse rendering. On the other hand, as different\ntiles within an image are rendered in parallel but have imbalanced workloads,\nfrequent hardware stalls also slow down the rendering process. LS-Gaussian\npredicts the workload for each tile based on viewpoint transformation to enable\nmore balanced parallel computation and co-designs a customized 3DGS accelerator\nto support the workload-aware mapping in real-time. Experimental results\ndemonstrate that LS-Gaussian achieves 5.41x speedup over the edge GPU baseline\non average and up to 17.3x speedup with the customized accelerator, while\nincurring only minimal visual quality degradation."
                },
                "authors": [
                    {
                        "name": "Linye Wei"
                    },
                    {
                        "name": "Jiajun Tang"
                    },
                    {
                        "name": "Fan Fei"
                    },
                    {
                        "name": "Boxin Shi"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by International Conference on Computer-Aided Design (ICCAD)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22478v1",
                "updated": "2025-07-30T08:29:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    29,
                    7,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:29:07Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    29,
                    7,
                    2,
                    211,
                    0
                ],
                "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL"
                },
                "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql."
                },
                "authors": [
                    {
                        "name": "Lei Sheng"
                    },
                    {
                        "name": "Shuai-Shuai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shuai-Shuai Xu"
                },
                "author": "Shuai-Shuai Xu",
                "arxiv_comment": "16 pages, 2 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22467v1",
                "updated": "2025-07-30T08:14:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    14,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:14:40Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    14,
                    40,
                    2,
                    211,
                    0
                ],
                "title": "Towards Simulating Social Influence Dynamics with LLM-based Multi-agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Simulating Social Influence Dynamics with LLM-based Multi-agents"
                },
                "summary": "Recent advancements in Large Language Models offer promising capabilities to\nsimulate complex human social interactions. We investigate whether LLM-based\nmulti-agent simulations can reproduce core human social dynamics observed in\nonline forums. We evaluate conformity dynamics, group polarization, and\nfragmentation across different model scales and reasoning capabilities using a\nstructured simulation framework. Our findings indicate that smaller models\nexhibit higher conformity rates, whereas models optimized for reasoning are\nmore resistant to social influence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models offer promising capabilities to\nsimulate complex human social interactions. We investigate whether LLM-based\nmulti-agent simulations can reproduce core human social dynamics observed in\nonline forums. We evaluate conformity dynamics, group polarization, and\nfragmentation across different model scales and reasoning capabilities using a\nstructured simulation framework. Our findings indicate that smaller models\nexhibit higher conformity rates, whereas models optimized for reasoning are\nmore resistant to social influence."
                },
                "authors": [
                    {
                        "name": "Hsien-Tsung Lin"
                    },
                    {
                        "name": "Pei-Cing Huang"
                    },
                    {
                        "name": "Chan-Tung Ku"
                    },
                    {
                        "name": "Chan Hsu"
                    },
                    {
                        "name": "Pei-Xuan Shieh"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05008v2",
                "updated": "2025-07-30T08:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    13,
                    44,
                    2,
                    211,
                    0
                ],
                "published": "2025-04-07T12:35:17Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    12,
                    35,
                    17,
                    0,
                    97,
                    0
                ],
                "title": "Voices of Freelance Professional Writers on AI: Limitations,\n  Expectations, and Fears",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voices of Freelance Professional Writers on AI: Limitations,\n  Expectations, and Fears"
                },
                "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ivanova"
                    },
                    {
                        "name": "Natalia Fedorova"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22464v1",
                "updated": "2025-07-30T08:11:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    11,
                    6,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:11:06Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    11,
                    6,
                    2,
                    211,
                    0
                ],
                "title": "Towards Interpretable Renal Health Decline Forecasting via Multi-LMM\n  Collaborative Reasoning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interpretable Renal Health Decline Forecasting via Multi-LMM\n  Collaborative Reasoning Framework"
                },
                "summary": "Accurate and interpretable prediction of estimated glomerular filtration rate\n(eGFR) is essential for managing chronic kidney disease (CKD) and supporting\nclinical decisions. Recent advances in Large Multimodal Models (LMMs) have\nshown strong potential in clinical prediction tasks due to their ability to\nprocess visual and textual information. However, challenges related to\ndeployment cost, data privacy, and model reliability hinder their adoption. In\nthis study, we propose a collaborative framework that enhances the performance\nof open-source LMMs for eGFR forecasting while generating clinically meaningful\nexplanations. The framework incorporates visual knowledge transfer, abductive\nreasoning, and a short-term memory mechanism to enhance prediction accuracy and\ninterpretability. Experimental results show that the proposed framework\nachieves predictive performance and interpretability comparable to proprietary\nmodels. It also provides plausible clinical reasoning processes behind each\nprediction. Our method sheds new light on building AI systems for healthcare\nthat combine predictive accuracy with clinically grounded interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and interpretable prediction of estimated glomerular filtration rate\n(eGFR) is essential for managing chronic kidney disease (CKD) and supporting\nclinical decisions. Recent advances in Large Multimodal Models (LMMs) have\nshown strong potential in clinical prediction tasks due to their ability to\nprocess visual and textual information. However, challenges related to\ndeployment cost, data privacy, and model reliability hinder their adoption. In\nthis study, we propose a collaborative framework that enhances the performance\nof open-source LMMs for eGFR forecasting while generating clinically meaningful\nexplanations. The framework incorporates visual knowledge transfer, abductive\nreasoning, and a short-term memory mechanism to enhance prediction accuracy and\ninterpretability. Experimental results show that the proposed framework\nachieves predictive performance and interpretability comparable to proprietary\nmodels. It also provides plausible clinical reasoning processes behind each\nprediction. Our method sheds new light on building AI systems for healthcare\nthat combine predictive accuracy with clinically grounded interpretability."
                },
                "authors": [
                    {
                        "name": "Peng-Yi Wu"
                    },
                    {
                        "name": "Pei-Cing Huang"
                    },
                    {
                        "name": "Ting-Yu Chen"
                    },
                    {
                        "name": "Chantung Ku"
                    },
                    {
                        "name": "Ming-Yen Lin"
                    },
                    {
                        "name": "Yihuang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yihuang Kang"
                },
                "author": "Yihuang Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22462v1",
                "updated": "2025-07-30T08:08:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:08:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    8,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "IFEvalCode: Controlled Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEvalCode: Controlled Code Generation"
                },
                "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions."
                },
                "authors": [
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Shukai Liu"
                    },
                    {
                        "name": "Linzheng Chai"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Zhoujun Li"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09213v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09213v3",
                "updated": "2025-07-30T08:05:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    5,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-01-16T00:19:19Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    0,
                    19,
                    19,
                    3,
                    16,
                    0
                ],
                "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from\n  Supervised Fine-Tuning to Test-Time Training"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Hongzhou Yu"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Yingwen Wang"
                    },
                    {
                        "name": "Wen He"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Yuejie Zhang"
                    },
                    {
                        "name": "Rui Feng"
                    },
                    {
                        "name": "Xiaobo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Zhang"
                },
                "author": "Xiaobo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09213v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09213v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22457v1",
                "updated": "2025-07-30T08:04:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    4,
                    19,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T08:04:19Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    4,
                    19,
                    2,
                    211,
                    0
                ],
                "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments\n  about Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments\n  about Large Language Models"
                },
                "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill."
                },
                "authors": [
                    {
                        "name": "Tian Yun"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Ellie Pavlick"
                    }
                ],
                "author_detail": {
                    "name": "Ellie Pavlick"
                },
                "author": "Ellie Pavlick",
                "arxiv_comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19703v2",
                "updated": "2025-07-30T07:58:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    58,
                    56,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-25T22:48:37Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    22,
                    48,
                    37,
                    4,
                    206,
                    0
                ],
                "title": "The wall confronting large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wall confronting large language models"
                },
                "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated."
                },
                "authors": [
                    {
                        "name": "Peter V. Coveney"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21830v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21830v2",
                "updated": "2025-07-30T07:57:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    57,
                    31,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T14:08:09Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    14,
                    8,
                    9,
                    1,
                    210,
                    0
                ],
                "title": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series\n  Forecasting Framework"
                },
                "summary": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Time Series Forecasting plays a key role in many applications.\nRecent works have explored using Large Language Models for MTSF to take\nadvantage of their reasoning abilities. However, many methods treat LLMs as\nend-to-end forecasters, which often leads to a loss of numerical precision and\nforces LLMs to handle patterns beyond their intended design. Alternatively,\nmethods that attempt to align textual and time series modalities within latent\nspace frequently encounter alignment difficulty. In this paper, we propose to\ntreat LLMs not as standalone forecasters, but as semantic guidance modules\nwithin a dual-stream framework. We propose DualSG, a dual-stream framework that\nprovides explicit semantic guidance, where LLMs act as Semantic Guides to\nrefine rather than replace traditional predictions. As part of DualSG, we\nintroduce Time Series Caption, an explicit prompt format that summarizes trend\npatterns in natural language and provides interpretable context for LLMs,\nrather than relying on implicit alignment between text and time series in the\nlatent space. We also design a caption-guided fusion module that explicitly\nmodels inter-variable relationships while reducing noise and computation.\nExperiments on real-world datasets from diverse domains show that DualSG\nconsistently outperforms 15 state-of-the-art baselines, demonstrating the value\nof explicitly combining numerical forecasting with semantic guidance."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Ruijie jian"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Luqi Gong"
                    },
                    {
                        "name": "Yishan Jiang"
                    },
                    {
                        "name": "Chunjie Luo an Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Chunjie Luo an Jianfeng Zhan"
                },
                "author": "Chunjie Luo an Jianfeng Zhan",
                "arxiv_doi": "10.1145/3746027.3755458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21830v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22448v1",
                "updated": "2025-07-30T07:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:55:33Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency\n  and Performance"
                },
                "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research."
                },
                "authors": [
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Maksim Velikanov"
                    },
                    {
                        "name": "Ilyas Chahed"
                    },
                    {
                        "name": "Younes Belkada"
                    },
                    {
                        "name": "Dhia Eddine Rhayem"
                    },
                    {
                        "name": "Guillaume Kunsch"
                    },
                    {
                        "name": "Hakim Hacid"
                    },
                    {
                        "name": "Hamza Yous"
                    },
                    {
                        "name": "Brahim Farhat"
                    },
                    {
                        "name": "Ibrahim Khadraoui"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Giulia Campesan"
                    },
                    {
                        "name": "Ruxandra Cojocaru"
                    },
                    {
                        "name": "Yasser Djilali"
                    },
                    {
                        "name": "Shi Hu"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Puneesh Khanna"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Ngoc Dung Huynh"
                    },
                    {
                        "name": "Phuc Le Khac"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Billel Mokeddem"
                    },
                    {
                        "name": "Mohamed Chami"
                    },
                    {
                        "name": "Abdalgader Abubaker"
                    },
                    {
                        "name": "Mikhail Lubinets"
                    },
                    {
                        "name": "Kacper Piskorski"
                    },
                    {
                        "name": "Slim Frikha"
                    }
                ],
                "author_detail": {
                    "name": "Slim Frikha"
                },
                "author": "Slim Frikha",
                "arxiv_comment": "Technical report of Falcon-H1 model series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22447v1",
                "updated": "2025-07-30T07:46:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    46,
                    49,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:46:49Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    46,
                    49,
                    2,
                    211,
                    0
                ],
                "title": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for\n  Malicious JavaScript Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Obfuscation: Cluster-Aware Graph with LLM-Aided Recovery for\n  Malicious JavaScript Detection"
                },
                "summary": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid expansion of web-based applications and cloud services,\nmalicious JavaScript code continues to pose significant threats to user\nprivacy, system integrity, and enterprise security. But, detecting such threats\nremains challenging due to sophisticated code obfuscation techniques and\nJavaScript's inherent language characteristics, particularly its nested closure\nstructures and syntactic flexibility. In this work, we propose DeCoda, a hybrid\ndefense framework that combines large language model (LLM)-based deobfuscation\nwith code graph learning: (1) We first construct a sophisticated\nprompt-learning pipeline with multi-stage refinement, where the LLM\nprogressively reconstructs the original code structure from obfuscated inputs\nand then generates normalized Abstract Syntax Tree (AST) representations; (2)\nIn JavaScript ASTs, dynamic typing scatters semantically similar nodes while\ndeeply nested functions fracture scope capturing, introducing structural noise\nand semantic ambiguity. To address these challenges, we then propose to learn\nhierarchical code graph representations via a Cluster-wise Graph that\nsynergistically integrates graph transformer network, node clustering, and\nnode-to-cluster attention to simultaneously capture both local node-level\nsemantics and global cluster-induced structural relationships from AST graph.\nExperimental results demonstrate that our method achieves F1-scores of 94.64%\nand 97.71% on two benchmark datasets, demonstrating absolute improvements of\n10.74% and 13.85% over state-of-the-art baselines. In false-positive control\nevaluation at fixed FPR levels (0.0001, 0.001, 0.01), our approach delivers\n4.82, 5.91, and 2.53 higher TPR respectively compared to the best-performing\nbaseline. These results highlight the effectiveness of LLM-based deobfuscation\nand underscore the importance of modeling cluster-level relationships in\ndetecting malicious code."
                },
                "authors": [
                    {
                        "name": "Zhihong Liang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhenhuang Hu"
                    },
                    {
                        "name": "Liangliang Song"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Jingjing Guo"
                    },
                    {
                        "name": "Yanbin Wang"
                    },
                    {
                        "name": "Ye Tian"
                    }
                ],
                "author_detail": {
                    "name": "Ye Tian"
                },
                "author": "Ye Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19098v2",
                "updated": "2025-07-30T07:43:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    43,
                    50,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-25T11:23:06Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    11,
                    23,
                    6,
                    6,
                    145,
                    0
                ],
                "title": "SPADE: Towards Scalable Path Planning Architecture on Actionable\n  Multi-Domain 3D Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Towards Scalable Path Planning Architecture on Actionable\n  Multi-Domain 3D Scene Graphs"
                },
                "summary": "In this work, we introduce SPADE, a path planning framework designed for\nautonomous navigation in dynamic environments using 3D scene graphs. SPADE\ncombines hierarchical path planning with local geometric awareness to enable\ncollision-free movement in dynamic scenes. The framework bifurcates the\nplanning problem into two: (a) solving the sparse abstract global layer plan\nand (b) iterative path refinement across denser lower local layers in step with\nlocal geometric scene navigation. To ensure efficient extraction of a feasible\nroute in a dense multi-task domain scene graphs, the framework enforces\ninformed sampling of traversable edges prior to path-planning. This removes\nextraneous information not relevant to path-planning and reduces the overall\nplanning complexity over a graph. Existing approaches address the problem of\npath planning over scene graphs by decoupling hierarchical and geometric path\nevaluation processes. Specifically, this results in an inefficient replanning\nover the entire scene graph when encountering path obstructions blocking the\noriginal route. In contrast, SPADE prioritizes local layer planning coupled\nwith local geometric scene navigation, enabling navigation through dynamic\nscenes while maintaining efficiency in computing a traversable route. We\nvalidate SPADE through extensive simulation experiments and real-world\ndeployment on a quadrupedal robot, demonstrating its efficacy in handling\ncomplex and dynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce SPADE, a path planning framework designed for\nautonomous navigation in dynamic environments using 3D scene graphs. SPADE\ncombines hierarchical path planning with local geometric awareness to enable\ncollision-free movement in dynamic scenes. The framework bifurcates the\nplanning problem into two: (a) solving the sparse abstract global layer plan\nand (b) iterative path refinement across denser lower local layers in step with\nlocal geometric scene navigation. To ensure efficient extraction of a feasible\nroute in a dense multi-task domain scene graphs, the framework enforces\ninformed sampling of traversable edges prior to path-planning. This removes\nextraneous information not relevant to path-planning and reduces the overall\nplanning complexity over a graph. Existing approaches address the problem of\npath planning over scene graphs by decoupling hierarchical and geometric path\nevaluation processes. Specifically, this results in an inefficient replanning\nover the entire scene graph when encountering path obstructions blocking the\noriginal route. In contrast, SPADE prioritizes local layer planning coupled\nwith local geometric scene navigation, enabling navigation through dynamic\nscenes while maintaining efficiency in computing a traversable route. We\nvalidate SPADE through extensive simulation experiments and real-world\ndeployment on a quadrupedal robot, demonstrating its efficacy in handling\ncomplex and dynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Akash Patel"
                    },
                    {
                        "name": "Mario Alberto Valdes Saucedo"
                    },
                    {
                        "name": "Sumeet Satpute"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Accepted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19582v3",
                "updated": "2025-07-30T07:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    39,
                    23,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T10:57:17Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    57,
                    17,
                    4,
                    362,
                    0
                ],
                "title": "An Actionable Hierarchical Scene Representation Enhancing Autonomous\n  Inspection Missions in Unknown Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Actionable Hierarchical Scene Representation Enhancing Autonomous\n  Inspection Missions in Unknown Environments"
                },
                "summary": "In this article, we present the Layered Semantic Graphs (LSG), a novel\nactionable hierarchical scene graph, fully integrated with a multi-modal\nmission planner, the FLIE: A First-Look based Inspection and Exploration\nplanner. The novelty of this work stems from aiming to address the task of\nmaintaining an intuitive and multi-resolution scene representation, while\nsimultaneously offering a tractable foundation for planning and scene\nunderstanding during an ongoing inspection mission of apriori unknown\ntargets-of-interest in an unknown environment. The proposed LSG scheme is\ncomposed of locally nested hierarchical graphs, at multiple layers of\nabstraction, with the abstract concepts grounded on the functionality of the\nintegrated FLIE planner. Furthermore, LSG encapsulates real-time semantic\nsegmentation models that offer extraction and localization of desired semantic\nelements within the hierarchical representation. This extends the capability of\nthe inspection planner, which can then leverage LSG to make an informed\ndecision to inspect a particular semantic of interest. We also emphasize the\nhierarchical and semantic path-planning capabilities of LSG, which could extend\ninspection missions by improving situational awareness for human operators in\nan unknown environment. The validity of the proposed scheme is proven through\nextensive evaluations of the proposed architecture in simulations, as well as\nexperimental field deployments on a Boston Dynamics Spot quadruped robot in\nurban outdoor environment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we present the Layered Semantic Graphs (LSG), a novel\nactionable hierarchical scene graph, fully integrated with a multi-modal\nmission planner, the FLIE: A First-Look based Inspection and Exploration\nplanner. The novelty of this work stems from aiming to address the task of\nmaintaining an intuitive and multi-resolution scene representation, while\nsimultaneously offering a tractable foundation for planning and scene\nunderstanding during an ongoing inspection mission of apriori unknown\ntargets-of-interest in an unknown environment. The proposed LSG scheme is\ncomposed of locally nested hierarchical graphs, at multiple layers of\nabstraction, with the abstract concepts grounded on the functionality of the\nintegrated FLIE planner. Furthermore, LSG encapsulates real-time semantic\nsegmentation models that offer extraction and localization of desired semantic\nelements within the hierarchical representation. This extends the capability of\nthe inspection planner, which can then leverage LSG to make an informed\ndecision to inspect a particular semantic of interest. We also emphasize the\nhierarchical and semantic path-planning capabilities of LSG, which could extend\ninspection missions by improving situational awareness for human operators in\nan unknown environment. The validity of the proposed scheme is proven through\nextensive evaluations of the proposed architecture in simulations, as well as\nexperimental field deployments on a Boston Dynamics Spot quadruped robot in\nurban outdoor environment settings."
                },
                "authors": [
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Mario Alberto Valdes Saucedo"
                    },
                    {
                        "name": "Sumeet Gajanan Satpute"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Accepted to IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15790v3",
                "updated": "2025-07-30T07:32:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    32,
                    19,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-18T18:18:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    18,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETrace:Event-Driven Vulnerability Detection in Smart Contracts via\n  LLM-Based Trace Analysis"
                },
                "summary": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance application of blockchain technology in various fields,\nensuring the security and stability of smart contracts has emerged as a\ncritical challenge. Current security analysis methodologies in vulnerability\ndetection can be categorized into static analysis and dynamic analysis\nmethods.However, these existing traditional vulnerability detection methods\npredominantly rely on analyzing original contract code, not all smart contracts\nprovide accessible code.We present ETrace, a novel event-driven vulnerability\ndetection framework for smart contracts, which uniquely identifies potential\nvulnerabilities through LLM-powered trace analysis without requiring source\ncode access. By extracting fine-grained event sequences from transaction logs,\nthe framework leverages Large Language Models (LLMs) as adaptive semantic\ninterpreters to reconstruct event analysis through chain-of-thought reasoning.\nETrace implements pattern-matching to establish causal links between\ntransaction behavior patterns and known attack behaviors. Furthermore, we\nvalidate the effectiveness of ETrace through preliminary experimental results."
                },
                "authors": [
                    {
                        "name": "Chenyang Peng"
                    },
                    {
                        "name": "Haijun Wang"
                    },
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Yitao Zhao"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_doi": "10.1145/3755881.3755934",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3755881.3755934",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.15790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure. To appear in Proceedings of the 16th Asia-Pacific\n  Symposium on Internetware (Internetware 2025), ACM ICPS. DOI:\n  https://doi.org/10.1145/3755881.3755934",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21990v2",
                "updated": "2025-07-30T07:23:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    23,
                    58,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-29T16:40:49Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    16,
                    40,
                    49,
                    1,
                    210,
                    0
                ],
                "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge"
                },
                "summary": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves cutting-edge performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios."
                },
                "authors": [
                    {
                        "name": "Zihan Zhao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Ziping Wan"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Xuanze Lin"
                    },
                    {
                        "name": "Shiyang Yu"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Zichen Zhu"
                    },
                    {
                        "name": "Danyang Zhang"
                    },
                    {
                        "name": "Huayang Wang"
                    },
                    {
                        "name": "Zhongyang Dai"
                    },
                    {
                        "name": "Liyang Wen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "13 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22429v1",
                "updated": "2025-07-30T07:16:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    16,
                    59,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:16:59Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    16,
                    59,
                    2,
                    211,
                    0
                ],
                "title": "Comparing Normalizing Flows with Kernel Density Estimation in Estimating\n  Risk of Automated Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Normalizing Flows with Kernel Density Estimation in Estimating\n  Risk of Automated Driving Systems"
                },
                "summary": "The development of safety validation methods is essential for the safe\ndeployment and operation of Automated Driving Systems (ADSs). One of the goals\nof safety validation is to prospectively evaluate the risk of an ADS dealing\nwith real-world traffic. Scenario-based assessment is a widely-used approach,\nwhere test cases are derived from real-world driving data. To allow for a\nquantitative analysis of the system performance, the exposure of the scenarios\nmust be accurately estimated. The exposure of scenarios at parameter level is\nexpressed using a Probability Density Function (PDF). However, assumptions\nabout the PDF, such as parameter independence, can introduce errors, while\navoiding assumptions often leads to oversimplified models with limited\nparameters to mitigate the curse of dimensionality.\n  This paper considers the use of Normalizing Flows (NF) for estimating the PDF\nof the parameters. NF are a class of generative models that transform a simple\nbase distribution into a complex one using a sequence of invertible and\ndifferentiable mappings, enabling flexible, high-dimensional density estimation\nwithout restrictive assumptions on the PDF's shape. We demonstrate the\neffectiveness of NF in quantifying risk and risk uncertainty of an ADS,\ncomparing its performance with Kernel Density Estimation (KDE), a traditional\nmethod for non-parametric PDF estimation. While NF require more computational\nresources compared to KDE, NF is less sensitive to the curse of dimensionality.\nAs a result, NF can improve risk uncertainty estimation, offering a more\nprecise assessment of an ADS's safety.\n  This work illustrates the potential of NF in scenario-based safety. Future\nwork involves experimenting more with using NF for scenario generation and\noptimizing the NF architecture, transformation types, and training\nhyperparameters to further enhance their applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of safety validation methods is essential for the safe\ndeployment and operation of Automated Driving Systems (ADSs). One of the goals\nof safety validation is to prospectively evaluate the risk of an ADS dealing\nwith real-world traffic. Scenario-based assessment is a widely-used approach,\nwhere test cases are derived from real-world driving data. To allow for a\nquantitative analysis of the system performance, the exposure of the scenarios\nmust be accurately estimated. The exposure of scenarios at parameter level is\nexpressed using a Probability Density Function (PDF). However, assumptions\nabout the PDF, such as parameter independence, can introduce errors, while\navoiding assumptions often leads to oversimplified models with limited\nparameters to mitigate the curse of dimensionality.\n  This paper considers the use of Normalizing Flows (NF) for estimating the PDF\nof the parameters. NF are a class of generative models that transform a simple\nbase distribution into a complex one using a sequence of invertible and\ndifferentiable mappings, enabling flexible, high-dimensional density estimation\nwithout restrictive assumptions on the PDF's shape. We demonstrate the\neffectiveness of NF in quantifying risk and risk uncertainty of an ADS,\ncomparing its performance with Kernel Density Estimation (KDE), a traditional\nmethod for non-parametric PDF estimation. While NF require more computational\nresources compared to KDE, NF is less sensitive to the curse of dimensionality.\nAs a result, NF can improve risk uncertainty estimation, offering a more\nprecise assessment of an ADS's safety.\n  This work illustrates the potential of NF in scenario-based safety. Future\nwork involves experimenting more with using NF for scenario generation and\noptimizing the NF architecture, transformation types, and training\nhyperparameters to further enhance their applicability."
                },
                "authors": [
                    {
                        "name": "Erwin de Gelder"
                    },
                    {
                        "name": "Maren Buermann"
                    },
                    {
                        "name": "Olaf Op den Camp"
                    }
                ],
                "author_detail": {
                    "name": "Olaf Op den Camp"
                },
                "author": "Olaf Op den Camp",
                "arxiv_comment": "Accepted for publication in proceedings of the 2025 IEEE\n  International Automated Vehicle Validation Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22424v1",
                "updated": "2025-07-30T07:04:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    4,
                    9,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T07:04:09Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    7,
                    4,
                    9,
                    2,
                    211,
                    0
                ],
                "title": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with\n  Relaxed Acceptance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with\n  Relaxed Acceptance"
                },
                "summary": "Vision-Language-Action (VLA) models have made substantial progress by\nleveraging the robust capabilities of Visual Language Models (VLMs). However,\nVLMs' significant parameter size and autoregressive (AR) decoding nature impose\nconsiderable computational demands on VLA models. While Speculative Decoding\n(SD) has shown efficacy in accelerating Large Language Models (LLMs) by\nincorporating efficient drafting and parallel verification, allowing multiple\ntokens to be generated in one forward pass, its application to VLA models\nremains unexplored. This work introduces Spec-VLA, an SD framework designed to\naccelerate VLA models. Due to the difficulty of the action prediction task and\nthe greedy decoding mechanism of the VLA models, the direct application of the\nadvanced SD framework to the VLA prediction task yields a minor speed\nimprovement. To boost the generation speed, we propose an effective mechanism\nto relax acceptance utilizing the relative distances represented by the action\ntokens of the VLA model. Empirical results across diverse test scenarios affirm\nthe effectiveness of the Spec-VLA framework, and further analysis substantiates\nthe impact of our proposed strategies, which enhance the acceptance length by\n44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without\ncompromising the success rate. The success of the Spec-VLA framework highlights\nthe potential for broader application of speculative execution in VLA\nprediction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have made substantial progress by\nleveraging the robust capabilities of Visual Language Models (VLMs). However,\nVLMs' significant parameter size and autoregressive (AR) decoding nature impose\nconsiderable computational demands on VLA models. While Speculative Decoding\n(SD) has shown efficacy in accelerating Large Language Models (LLMs) by\nincorporating efficient drafting and parallel verification, allowing multiple\ntokens to be generated in one forward pass, its application to VLA models\nremains unexplored. This work introduces Spec-VLA, an SD framework designed to\naccelerate VLA models. Due to the difficulty of the action prediction task and\nthe greedy decoding mechanism of the VLA models, the direct application of the\nadvanced SD framework to the VLA prediction task yields a minor speed\nimprovement. To boost the generation speed, we propose an effective mechanism\nto relax acceptance utilizing the relative distances represented by the action\ntokens of the VLA model. Empirical results across diverse test scenarios affirm\nthe effectiveness of the Spec-VLA framework, and further analysis substantiates\nthe impact of our proposed strategies, which enhance the acceptance length by\n44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without\ncompromising the success rate. The success of the Spec-VLA framework highlights\nthe potential for broader application of speculative execution in VLA\nprediction scenarios."
                },
                "authors": [
                    {
                        "name": "Songsheng Wang"
                    },
                    {
                        "name": "Rucheng Yu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Derek F. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Derek F. Wong"
                },
                "author": "Derek F. Wong",
                "arxiv_comment": "12 pages, 5 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22414v1",
                "updated": "2025-07-30T06:34:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    34,
                    2,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:34:02Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    34,
                    2,
                    2,
                    211,
                    0
                ],
                "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents"
                },
                "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output."
                },
                "authors": [
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22411v1",
                "updated": "2025-07-30T06:29:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    50,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T06:29:50Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    50,
                    2,
                    211,
                    0
                ],
                "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large\n  Language Models"
                },
                "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain"
                },
                "authors": [
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12767v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12767v6",
                "updated": "2025-07-30T06:04:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    4,
                    25,
                    2,
                    211,
                    0
                ],
                "published": "2025-02-18T11:31:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    11,
                    31,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs"
                },
                "summary": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks still suffer two practical drawbacks: they must be re-tuned whenever\nthe KG or reasoning task changes, and they depend on a single, high-capacity\nLLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce\nR2-KG, a plug-and-play, dual-agent framework that separates reasoning into two\nroles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor\n(a high-capacity LLM) that makes final judgments. This design is cost-efficient\nfor LLM inference while still maintaining strong reasoning accuracy.\nAdditionally, R2-KG employs an Abstention mechanism, generating answers only\nwhen sufficient evidence is collected from KG, which significantly enhances\nreliability. Experiments across five diverse benchmarks show that R2-KG\nconsistently outperforms baselines in both accuracy and reliability, regardless\nof the inherent capability of LLMs used as the Operator. Further experiments\nreveal that the single-agent version of R2-KG, equipped with a strict\nself-consistency strategy, achieves significantly higher-than-baseline\nreliability with reduced inference cost but increased abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning, reducing reliance on high-capacity LLMs while\nensuring trustworthy inference. The code is available at\nhttps://github.com/ekrxjwh2009/R2-KG/."
                },
                "authors": [
                    {
                        "name": "Sumin Jo"
                    },
                    {
                        "name": "Junseong Choi"
                    },
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12767v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12767v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15779v2",
                "updated": "2025-07-30T05:37:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    37,
                    5,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-21T16:35:38Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    16,
                    35,
                    38,
                    0,
                    202,
                    0
                ],
                "title": "Reservoir Computing as a Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir Computing as a Language Model"
                },
                "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance."
                },
                "authors": [
                    {
                        "name": "Felix Köster"
                    },
                    {
                        "name": "Atsushi Uchida"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Uchida"
                },
                "author": "Atsushi Uchida",
                "arxiv_comment": "8 pages, 5 figures, 1 table Code available at:\n  https://github.com/fekoester/Shakespeare_Res",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22387v1",
                "updated": "2025-07-30T05:17:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    17,
                    35,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T05:17:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    17,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs"
                },
                "summary": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research."
                },
                "authors": [
                    {
                        "name": "Homaira Huda Shomee"
                    },
                    {
                        "name": "Suman Kalyan Maity"
                    },
                    {
                        "name": "Sourav Medya"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Medya"
                },
                "author": "Sourav Medya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21391v2",
                "updated": "2025-07-30T04:49:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    49,
                    38,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T23:52:53Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    23,
                    52,
                    53,
                    0,
                    209,
                    0
                ],
                "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation"
                },
                "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations. In addition, LLaVA-Reward supports\ndifferent types of preference data for efficient fine-tuning, including paired\npreference data and unpaired data. We train LLaVA-Reward on four evaluation\nperspectives: text-image alignment, fidelity/artifact, safety, and overall\nranking. Empirical results demonstrate that LLaVA-Reward outperforms\nconventional and MLLM-based methods in generating human-aligned scores for\nautomatic evaluations and inference-time scaling in text-to-image generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations. In addition, LLaVA-Reward supports\ndifferent types of preference data for efficient fine-tuning, including paired\npreference data and unpaired data. We train LLaVA-Reward on four evaluation\nperspectives: text-image alignment, fidelity/artifact, safety, and overall\nranking. Empirical results demonstrate that LLaVA-Reward outperforms\nconventional and MLLM-based methods in generating human-aligned scores for\nautomatic evaluations and inference-time scaling in text-to-image generations."
                },
                "authors": [
                    {
                        "name": "Shijie Zhou"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "name": "Branislav Kveton"
                    },
                    {
                        "name": "Yufan Zhou"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Changyou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Changyou Chen"
                },
                "author": "Changyou Chen",
                "arxiv_comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22380v1",
                "updated": "2025-07-30T04:46:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    46,
                    48,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T04:46:48Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    46,
                    48,
                    2,
                    211,
                    0
                ],
                "title": "Improving Generalization Ability of Robotic Imitation Learning by\n  Resolving Causal Confusion in Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Generalization Ability of Robotic Imitation Learning by\n  Resolving Causal Confusion in Observations"
                },
                "summary": "Recent developments in imitation learning have considerably advanced robotic\nmanipulation. However, current techniques in imitation learning can suffer from\npoor generalization, limiting performance even under relatively minor domain\nshifts. In this work, we aim to enhance the generalization capabilities of\ncomplex imitation learning algorithms to handle unpredictable changes from the\ntraining environments to deployment environments. To avoid confusion caused by\nobservations that are not relevant to the target task, we propose to explicitly\nlearn the causal relationship between observation components and expert\nactions, employing a framework similar to [6], where a causal structural\nfunction is learned by intervention on the imitation learning policy.\nDisentangling the feature representation from image input as in [6] is hard to\nsatisfy in complex imitation learning process in robotic manipulation, we\ntheoretically clarify that this requirement is not necessary in causal\nrelationship learning. Therefore, we propose a simple causal structure learning\nframework that can be easily embedded in recent imitation learning\narchitectures, such as the Action Chunking Transformer [31]. We demonstrate our\napproach using a simulation of the ALOHA [31] bimanual robot arms in Mujoco,\nand show that the method can considerably mitigate the generalization problem\nof existing complex imitation learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in imitation learning have considerably advanced robotic\nmanipulation. However, current techniques in imitation learning can suffer from\npoor generalization, limiting performance even under relatively minor domain\nshifts. In this work, we aim to enhance the generalization capabilities of\ncomplex imitation learning algorithms to handle unpredictable changes from the\ntraining environments to deployment environments. To avoid confusion caused by\nobservations that are not relevant to the target task, we propose to explicitly\nlearn the causal relationship between observation components and expert\nactions, employing a framework similar to [6], where a causal structural\nfunction is learned by intervention on the imitation learning policy.\nDisentangling the feature representation from image input as in [6] is hard to\nsatisfy in complex imitation learning process in robotic manipulation, we\ntheoretically clarify that this requirement is not necessary in causal\nrelationship learning. Therefore, we propose a simple causal structure learning\nframework that can be easily embedded in recent imitation learning\narchitectures, such as the Action Chunking Transformer [31]. We demonstrate our\napproach using a simulation of the ALOHA [31] bimanual robot arms in Mujoco,\nand show that the method can considerably mitigate the generalization problem\nof existing complex imitation learning algorithms."
                },
                "authors": [
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Yuzhe Zhang"
                    },
                    {
                        "name": "Giovanni D'urso"
                    },
                    {
                        "name": "Nicholas Lawrance"
                    },
                    {
                        "name": "Brendan Tidd"
                    }
                ],
                "author_detail": {
                    "name": "Brendan Tidd"
                },
                "author": "Brendan Tidd",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09950v2",
                "updated": "2025-07-30T04:37:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    37,
                    6,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-14T05:59:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    5,
                    59,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion\n  Product Attributes? A Zero-Shot Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion\n  Product Attributes? A Zero-Shot Analysis"
                },
                "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction."
                },
                "authors": [
                    {
                        "name": "Shubham Shukla"
                    },
                    {
                        "name": "Kunal Sonalkar"
                    }
                ],
                "author_detail": {
                    "name": "Kunal Sonalkar"
                },
                "author": "Kunal Sonalkar",
                "arxiv_comment": "Version 2: Added a missing citation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22371v1",
                "updated": "2025-07-30T04:28:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    28,
                    0,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T04:28:00Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    28,
                    0,
                    2,
                    211,
                    0
                ],
                "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts\n  for Smart Contract Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts\n  for Smart Contract Vulnerability Detection"
                },
                "summary": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Shiqi Cheng"
                    },
                    {
                        "name": "Zhirong Huang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Chenjie Shen"
                    },
                    {
                        "name": "Junyi Lu"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Fengjun Zhang"
                    },
                    {
                        "name": "Jiajia Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiajia Ma"
                },
                "author": "Jiajia Ma",
                "arxiv_comment": "Accepted to ICSME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08771v2",
                "updated": "2025-07-30T04:14:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    14,
                    15,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-11T17:28:56Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    28,
                    56,
                    4,
                    192,
                    0
                ],
                "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity"
                },
                "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN)."
                },
                "authors": [
                    {
                        "name": "Chenyang Song"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Yuxuan Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "21 pages, 7 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22367v1",
                "updated": "2025-07-30T04:12:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    12,
                    14,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T04:12:14Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    12,
                    14,
                    2,
                    211,
                    0
                ],
                "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided\n  LLM Representations and Multimodal Apparent Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided\n  LLM Representations and Multimodal Apparent Behaviors"
                },
                "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yichao He"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Tianhao Luo"
                    },
                    {
                        "name": "Zhenzhen Hu"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "8 pages, 3 figures, ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20300v2",
                "updated": "2025-07-30T04:02:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    4,
                    2,
                    44,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-27T14:34:29Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    14,
                    34,
                    29,
                    6,
                    208,
                    0
                ],
                "title": "Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance\n  and Experience in Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance\n  and Experience in Minecraft"
                },
                "summary": "With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) on the rise, in-game interactions are\nshifting from rigid commands to natural conversations. However, the impacts of\nLLMs on player performance and game experience remain underexplored. This work\nexplores LLM's role as a co-builder during gameplay, examining its impact on\ntask performance, usability, and player experience. Using Minecraft as a\nsandbox, we present an LLM-assisted interface that engages players through\nnatural language, aiming to facilitate creativity and simplify complex gaming\ncommands. We conducted a mixed-methods study with 30 participants, comparing\nLLM-assisted and command-based interfaces across simple and complex game tasks.\nQuantitative and qualitative analyses reveal that the LLM-assisted interface\nsignificantly improves player performance, engagement, and overall game\nexperience. Additionally, task complexity has a notable effect on player\nperformance and experience across both interfaces. Our findings highlight the\npotential of LLM-assisted interfaces to revolutionize virtual experiences,\nemphasizing the importance of balancing intuitiveness with predictability,\ntransparency, and user agency in AI-driven, multimodal gaming environments."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Massimo Poesio"
                    },
                    {
                        "name": "Julian Frommel"
                    },
                    {
                        "name": "Koen Hinriks"
                    },
                    {
                        "name": "Jiahuan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Jiahuan Pei"
                },
                "author": "Jiahuan Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06157v2",
                "updated": "2025-07-30T03:58:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    58,
                    6,
                    2,
                    211,
                    0
                ],
                "published": "2025-06-06T15:21:24Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    15,
                    21,
                    24,
                    4,
                    157,
                    0
                ],
                "title": "Masked Language Models are Good Heterogeneous Graph Generalizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Language Models are Good Heterogeneous Graph Generalizers"
                },
                "summary": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. With the rapid advancement of large\nlanguage models (LLMs), a recent study explored the integration of HGNNs with\nLLMs for generalizable heterogeneous graph learning. However, this approach\ntypically encodes structural information as HG tokens using HGNNs, and\ndisparities in embedding spaces between HGNNs and LLMs have been shown to bias\nthe LLM's comprehension of HGs. Moreover, since these HG tokens are often\nderived from node-level tasks, the model's ability to generalize across tasks\nremains limited. To this end, we propose a simple yet effective Masked Language\nModeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual\nsequences instead of HG tokens to extract structural and semantic information\ninherent in HGs, and designs customized textual templates to unify different\ngraph tasks into a coherent cloze-style 'mask' token prediction paradigm.\nSpecifically,MLM4HG first converts HGs from various domains to texts based on\nmetapaths, and subsequently combines them with the unified task texts to form a\nHG-based corpus. Moreover, the corpus is fed into a pretrained LM for\nfine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to\ngeneralize to unseen target HGs. Extensive cross-domain and multi-task\nexperiments on four real-world datasets demonstrate the superior generalization\nperformance of MLM4HG over state-of-the-art methods in both few-shot and\nzero-shot scenarios. Our code is available at\nhttps://github.com/BUPT-GAMMA/MLM4HG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. With the rapid advancement of large\nlanguage models (LLMs), a recent study explored the integration of HGNNs with\nLLMs for generalizable heterogeneous graph learning. However, this approach\ntypically encodes structural information as HG tokens using HGNNs, and\ndisparities in embedding spaces between HGNNs and LLMs have been shown to bias\nthe LLM's comprehension of HGs. Moreover, since these HG tokens are often\nderived from node-level tasks, the model's ability to generalize across tasks\nremains limited. To this end, we propose a simple yet effective Masked Language\nModeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual\nsequences instead of HG tokens to extract structural and semantic information\ninherent in HGs, and designs customized textual templates to unify different\ngraph tasks into a coherent cloze-style 'mask' token prediction paradigm.\nSpecifically,MLM4HG first converts HGs from various domains to texts based on\nmetapaths, and subsequently combines them with the unified task texts to form a\nHG-based corpus. Moreover, the corpus is fed into a pretrained LM for\nfine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to\ngeneralize to unseen target HGs. Extensive cross-domain and multi-task\nexperiments on four real-world datasets demonstrate the superior generalization\nperformance of MLM4HG over state-of-the-art methods in both few-shot and\nzero-shot scenarios. Our code is available at\nhttps://github.com/BUPT-GAMMA/MLM4HG."
                },
                "authors": [
                    {
                        "name": "Jinyu Yang"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Shanyuan Cui"
                    },
                    {
                        "name": "Zeyuan Guo"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22359v2",
                "updated": "2025-07-31T03:28:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    3,
                    28,
                    30,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T03:50:46Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    50,
                    46,
                    2,
                    211,
                    0
                ],
                "title": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of\n  Large Language Models"
                },
                "summary": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) demonstrate remarkable capabilities\nacross various tasks, evaluating their capabilities remains a challenging task.\nExisting evaluation methods suffer from issues such as data contamination,\nblack-box operation, and subjective preference. These issues make it difficult\nto evaluate the LLMs' true capabilities comprehensively. To tackle these\nchallenges, we propose a novel benchmark-free evaluation paradigm,\nLLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,\nand evaluate mutually. This method integrates four key evaluation criteria:\ndynamic, transparent, objective, and professional, which existing evaluation\nmethods cannot satisfy simultaneously. Experiments on eight mainstream LLMs\nacross mathematics and programming verify the advantages of our method in\ndistinguishing LLM performance. Furthermore, our study reveals several novel\nfindings that are difficult for traditional methods to detect, including but\nnot limited to: (1) Gemini demonstrates the highest original and professional\nquestion-design capabilities among others; (2) Some LLMs exhibit\n''memorization-based answering'' by misrecognizing questions as familiar ones\nwith a similar structure; (3) LLM evaluation results demonstrate high\nconsistency (robustness)."
                },
                "authors": [
                    {
                        "name": "Qianhong Guo"
                    },
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Xiaofang Cai"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22352v1",
                "updated": "2025-07-30T03:33:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    33,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T03:33:46Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    33,
                    46,
                    2,
                    211,
                    0
                ],
                "title": "Mitigating Response Delays in Free-Form Conversations with LLM-powered\n  Intelligent Virtual Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Response Delays in Free-Form Conversations with LLM-powered\n  Intelligent Virtual Agents"
                },
                "summary": "We investigated the challenges of mitigating response delays in free-form\nconversations with virtual agents powered by Large Language Models (LLMs)\nwithin Virtual Reality (VR). For this, we used conversational fillers, such as\ngestures and verbal cues, to bridge delays between user input and system\nresponses and evaluate their effectiveness across various latency levels and\ninteraction scenarios. We found that latency above 4 seconds degrades quality\nof experience, while natural conversational fillers improve perceived response\ntime, especially in high-delay conditions. Our findings provide insights for\npractitioners and researchers to optimize user engagement whenever\nconversational systems' responses are delayed by network limitations or slow\nhardware. We also contribute an open-source pipeline that streamlines deploying\nconversational agents in virtual environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigated the challenges of mitigating response delays in free-form\nconversations with virtual agents powered by Large Language Models (LLMs)\nwithin Virtual Reality (VR). For this, we used conversational fillers, such as\ngestures and verbal cues, to bridge delays between user input and system\nresponses and evaluate their effectiveness across various latency levels and\ninteraction scenarios. We found that latency above 4 seconds degrades quality\nof experience, while natural conversational fillers improve perceived response\ntime, especially in high-delay conditions. Our findings provide insights for\npractitioners and researchers to optimize user engagement whenever\nconversational systems' responses are delayed by network limitations or slow\nhardware. We also contribute an open-source pipeline that streamlines deploying\nconversational agents in virtual environments."
                },
                "authors": [
                    {
                        "name": "Mykola Maslych"
                    },
                    {
                        "name": "Mohammadreza Katebi"
                    },
                    {
                        "name": "Christopher Lee"
                    },
                    {
                        "name": "Yahya Hmaiti"
                    },
                    {
                        "name": "Amirpouya Ghasemaghaei"
                    },
                    {
                        "name": "Christian Pumarada"
                    },
                    {
                        "name": "Janneese Palmer"
                    },
                    {
                        "name": "Esteban Segarra Martinez"
                    },
                    {
                        "name": "Marco Emporio"
                    },
                    {
                        "name": "Warren Snipes"
                    },
                    {
                        "name": "Ryan P. McMahan"
                    },
                    {
                        "name": "Joseph J. LaViola Jr"
                    }
                ],
                "author_detail": {
                    "name": "Joseph J. LaViola Jr"
                },
                "author": "Joseph J. LaViola Jr",
                "arxiv_doi": "10.1145/3719160.3736636",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719160.3736636",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.22352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 8 figures. Published at the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25), July 8-10, 2025, Waterloo, Canada.\n  Open-source code available at https://github.com/ISUE/iva-cui",
                "arxiv_journal_ref": "Proceedings of the 7th ACM Conference on Conversational User\n  Interfaces (CUI '25), 2025, Article 49, 1-15",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2; H.5.2; I.2.7; I.3.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22349v1",
                "updated": "2025-07-30T03:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    21,
                    29,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T03:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    21,
                    29,
                    2,
                    211,
                    0
                ],
                "title": "MSQ: Memory-Efficient Bit Sparsification Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSQ: Memory-Efficient Bit Sparsification Quantization"
                },
                "summary": "As deep neural networks (DNNs) see increased deployment on mobile and edge\ndevices, optimizing model efficiency has become crucial. Mixed-precision\nquantization is widely favored, as it offers a superior balance between\nefficiency and accuracy compared to uniform quantization. However, finding the\noptimal precision for each layer is challenging. Recent studies utilizing\nbit-level sparsity have shown promise, yet they often introduce substantial\ntraining complexity and high GPU memory requirements. In this paper, we propose\nMemory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that\naddresses these limitations. MSQ applies a round-clamp quantizer to enable\ndifferentiable computation of the least significant bits (LSBs) from model\nweights. It further employs regularization to induce sparsity in these LSBs,\nenabling effective precision reduction without explicit bit-level parameter\nsplitting. Additionally, MSQ incorporates Hessian information, allowing the\nsimultaneous pruning of multiple LSBs to further enhance training efficiency.\nExperimental results show that MSQ achieves up to 8.00x reduction in trainable\nparameters and up to 86% reduction in training time compared to previous\nbit-level quantization, while maintaining competitive accuracy and compression\nrates. This makes it a practical solution for training efficient DNNs on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep neural networks (DNNs) see increased deployment on mobile and edge\ndevices, optimizing model efficiency has become crucial. Mixed-precision\nquantization is widely favored, as it offers a superior balance between\nefficiency and accuracy compared to uniform quantization. However, finding the\noptimal precision for each layer is challenging. Recent studies utilizing\nbit-level sparsity have shown promise, yet they often introduce substantial\ntraining complexity and high GPU memory requirements. In this paper, we propose\nMemory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that\naddresses these limitations. MSQ applies a round-clamp quantizer to enable\ndifferentiable computation of the least significant bits (LSBs) from model\nweights. It further employs regularization to induce sparsity in these LSBs,\nenabling effective precision reduction without explicit bit-level parameter\nsplitting. Additionally, MSQ incorporates Hessian information, allowing the\nsimultaneous pruning of multiple LSBs to further enhance training efficiency.\nExperimental results show that MSQ achieves up to 8.00x reduction in trainable\nparameters and up to 86% reduction in training time compared to previous\nbit-level quantization, while maintaining competitive accuracy and compression\nrates. This makes it a practical solution for training efficient DNNs on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Seokho Han"
                    },
                    {
                        "name": "Seoyeon Yoon"
                    },
                    {
                        "name": "Jinhee Kim"
                    },
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Kang Eun Jeon"
                    },
                    {
                        "name": "Huanrui Yang"
                    },
                    {
                        "name": "Jong Hwan Ko"
                    }
                ],
                "author_detail": {
                    "name": "Jong Hwan Ko"
                },
                "author": "Jong Hwan Ko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21354v2",
                "updated": "2025-07-30T03:20:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    3,
                    20,
                    16,
                    2,
                    211,
                    0
                ],
                "published": "2025-05-27T15:47:10Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    47,
                    10,
                    1,
                    147,
                    0
                ],
                "title": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Bengali Math Word Problem Solving\n  with Chain of Thought Reasoning"
                },
                "summary": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Bengali Math Word Problems (MWPs) remains a major challenge in\nnatural language processing (NLP) due to the language's low-resource status and\nthe multi-step reasoning required. Existing models struggle with complex\nBengali MWPs, largely because no human-annotated Bengali dataset has previously\naddressed this task. This gap has limited progress in Bengali mathematical\nreasoning. To address this, we created SOMADHAN, a dataset of 8792 complex\nBengali MWPs with manually written, step-by-step solutions. We designed this\ndataset to support reasoning-focused evaluation and model development in a\nlinguistically underrepresented context. Using SOMADHAN, we evaluated a range\nof large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series\nmodels, Deepseek, and Qwen - through both zero-shot and few-shot prompting with\nand without Chain of Thought (CoT) reasoning. CoT prompting consistently\nimproved performance over standard prompting, especially in tasks requiring\nmulti-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with\nfew-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune\nmodels efficiently, enabling them to adapt to Bengali MWPs with minimal\ncomputational cost. Our work fills a critical gap in Bengali NLP by providing a\nhigh-quality reasoning dataset and a scalable framework for solving complex\nMWPs. We aim to advance equitable research in low-resource languages and\nenhance reasoning capabilities in educational and language technologies."
                },
                "authors": [
                    {
                        "name": "Bidyarthi Paul"
                    },
                    {
                        "name": "Jalisha Jashim Era"
                    },
                    {
                        "name": "Mirazur Rahman Zim"
                    },
                    {
                        "name": "Tahmid Sattar Aothoi"
                    },
                    {
                        "name": "Faisal Muhammad Shah"
                    }
                ],
                "author_detail": {
                    "name": "Faisal Muhammad Shah"
                },
                "author": "Faisal Muhammad Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]